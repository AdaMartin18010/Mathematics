# çŸ©é˜µåˆ†è§£ (Matrix Decompositions)

> **The Computational Foundation of Machine Learning**
>
> æœºå™¨å­¦ä¹ çš„è®¡ç®—åŸºç¡€

---

## ç›®å½•

- [çŸ©é˜µåˆ†è§£ (Matrix Decompositions)](#çŸ©é˜µåˆ†è§£-matrix-decompositions)
  - [ç›®å½•](#ç›®å½•)
  - [ðŸ“‹ æ ¸å¿ƒæ€æƒ³](#-æ ¸å¿ƒæ€æƒ³)
  - [ðŸŽ¯ ç‰¹å¾å€¼åˆ†è§£ (Eigendecomposition)](#-ç‰¹å¾å€¼åˆ†è§£-eigendecomposition)
    - [1. ç‰¹å¾å€¼ä¸Žç‰¹å¾å‘é‡](#1-ç‰¹å¾å€¼ä¸Žç‰¹å¾å‘é‡)
    - [2. è°±å®šç†](#2-è°±å®šç†)
    - [3. å¯¹è§’åŒ–](#3-å¯¹è§’åŒ–)
  - [ðŸ“Š å¥‡å¼‚å€¼åˆ†è§£ (SVD)](#-å¥‡å¼‚å€¼åˆ†è§£-svd)
    - [1. SVDå®šä¹‰](#1-svdå®šä¹‰)
    - [2. å‡ ä½•è§£é‡Š](#2-å‡ ä½•è§£é‡Š)
    - [3. æˆªæ–­SVDä¸Žä½Žç§©è¿‘ä¼¼](#3-æˆªæ–­svdä¸Žä½Žç§©è¿‘ä¼¼)
    - [4. SVDçš„æ€§è´¨](#4-svdçš„æ€§è´¨)
  - [ðŸ”¬ QRåˆ†è§£](#-qråˆ†è§£)
    - [1. QRåˆ†è§£å®šä¹‰](#1-qråˆ†è§£å®šä¹‰)
    - [2. Gram-Schmidtæ­£äº¤åŒ–](#2-gram-schmidtæ­£äº¤åŒ–)
    - [3. Householderå˜æ¢](#3-householderå˜æ¢)
  - [ðŸ’¡ Choleskyåˆ†è§£](#-choleskyåˆ†è§£)
    - [1. Choleskyåˆ†è§£å®šä¹‰](#1-choleskyåˆ†è§£å®šä¹‰)
    - [2. ç®—æ³•](#2-ç®—æ³•)
  - [ðŸŽ¨ LUåˆ†è§£](#-luåˆ†è§£)
    - [1. LUåˆ†è§£å®šä¹‰](#1-luåˆ†è§£å®šä¹‰)
    - [2. é«˜æ–¯æ¶ˆå…ƒæ³•](#2-é«˜æ–¯æ¶ˆå…ƒæ³•)
  - [ðŸ”§ åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„åº”ç”¨](#-åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„åº”ç”¨)
    - [1. ä¸»æˆåˆ†åˆ†æž (PCA)](#1-ä¸»æˆåˆ†åˆ†æž-pca)
    - [2. å¥‡å¼‚å€¼åˆ†è§£ä¸Žé™ç»´](#2-å¥‡å¼‚å€¼åˆ†è§£ä¸Žé™ç»´)
    - [3. çŸ©é˜µæ±‚é€†ä¸Žçº¿æ€§ç³»ç»Ÿ](#3-çŸ©é˜µæ±‚é€†ä¸Žçº¿æ€§ç³»ç»Ÿ)
    - [4. æƒé‡åˆå§‹åŒ–](#4-æƒé‡åˆå§‹åŒ–)
  - [ðŸ’» Pythonå®žçŽ°](#-pythonå®žçŽ°)
  - [ðŸ“š ç»ƒä¹ é¢˜](#-ç»ƒä¹ é¢˜)
    - [ç»ƒä¹ 1ï¼šç‰¹å¾å€¼è®¡ç®—](#ç»ƒä¹ 1ç‰¹å¾å€¼è®¡ç®—)
    - [ç»ƒä¹ 2ï¼šSVDåº”ç”¨](#ç»ƒä¹ 2svdåº”ç”¨)
    - [ç»ƒä¹ 3ï¼šPCAå®žçŽ°](#ç»ƒä¹ 3pcaå®žçŽ°)
    - [ç»ƒä¹ 4ï¼šä½Žç§©è¿‘ä¼¼](#ç»ƒä¹ 4ä½Žç§©è¿‘ä¼¼)
  - [ðŸŽ“ ç›¸å…³è¯¾ç¨‹](#-ç›¸å…³è¯¾ç¨‹)
  - [ðŸ“– å‚è€ƒæ–‡çŒ®](#-å‚è€ƒæ–‡çŒ®)

---

## ðŸ“‹ æ ¸å¿ƒæ€æƒ³

**çŸ©é˜µåˆ†è§£**æ˜¯å°†çŸ©é˜µè¡¨ç¤ºä¸ºæ›´ç®€å•çŸ©é˜µçš„ä¹˜ç§¯ï¼Œæ˜¯çº¿æ€§ä»£æ•°è®¡ç®—çš„æ ¸å¿ƒå·¥å…·ã€‚

**ä¸ºä»€ä¹ˆçŸ©é˜µåˆ†è§£é‡è¦**:

```text
è®¡ç®—ä¼˜åŠ¿:
â”œâ”€ ç®€åŒ–è®¡ç®— (å¦‚æ±‚é€†ã€æ±‚è§£çº¿æ€§ç³»ç»Ÿ)
â”œâ”€ æ•°å€¼ç¨³å®šæ€§
â””â”€ æ­ç¤ºçŸ©é˜µç»“æž„

æœºå™¨å­¦ä¹ åº”ç”¨:
â”œâ”€ PCA (ä¸»æˆåˆ†åˆ†æž) â†’ SVD
â”œâ”€ æŽ¨èç³»ç»Ÿ â†’ çŸ©é˜µåˆ†è§£
â”œâ”€ é™ç»´ â†’ SVD/ç‰¹å¾å€¼åˆ†è§£
â””â”€ ä¼˜åŒ– â†’ Choleskyåˆ†è§£
```

**ä¸»è¦åˆ†è§£**:

```text
ç‰¹å¾å€¼åˆ†è§£ (Eigendecomposition):
    A = QÎ›Qâ»Â¹  (æ–¹é˜µ, å¯å¯¹è§’åŒ–)

å¥‡å¼‚å€¼åˆ†è§£ (SVD):
    A = UÎ£Váµ€  (ä»»æ„çŸ©é˜µ)

QRåˆ†è§£:
    A = QR  (Qæ­£äº¤, Rä¸Šä¸‰è§’)

Choleskyåˆ†è§£:
    A = LLáµ€  (æ­£å®šçŸ©é˜µ)

LUåˆ†è§£:
    A = LU  (Lä¸‹ä¸‰è§’, Uä¸Šä¸‰è§’)
```

---

## ðŸŽ¯ ç‰¹å¾å€¼åˆ†è§£ (Eigendecomposition)

### 1. ç‰¹å¾å€¼ä¸Žç‰¹å¾å‘é‡

**å®šä¹‰ 1.1 (ç‰¹å¾å€¼ä¸Žç‰¹å¾å‘é‡)**:

è®¾ $A \in \mathbb{R}^{n \times n}$ï¼Œå¦‚æžœå­˜åœ¨æ ‡é‡ $\lambda$ å’Œéžé›¶å‘é‡ $v$ ä½¿å¾—ï¼š

$$
Av = \lambda v
$$

åˆ™ $\lambda$ ç§°ä¸º $A$ çš„**ç‰¹å¾å€¼**ï¼Œ$v$ ç§°ä¸ºå¯¹åº”çš„**ç‰¹å¾å‘é‡**ã€‚

**å‡ ä½•æ„ä¹‰**ï¼š$A$ ä½œç”¨åœ¨ $v$ ä¸Šåªæ”¹å˜å…¶é•¿åº¦ï¼Œä¸æ”¹å˜æ–¹å‘ã€‚

**ç‰¹å¾å¤šé¡¹å¼**:

$$
\det(A - \lambda I) = 0
$$

**ç¤ºä¾‹**:

$$
A = \begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix}
$$

ç‰¹å¾å¤šé¡¹å¼ï¼š$\det(A - \lambda I) = (2 - \lambda)^2 - 1 = 0$

ç‰¹å¾å€¼ï¼š$\lambda_1 = 3, \lambda_2 = 1$

ç‰¹å¾å‘é‡ï¼š$v_1 = \begin{bmatrix} 1 \\ 1 \end{bmatrix}, v_2 = \begin{bmatrix} 1 \\ -1 \end{bmatrix}$

---

### 2. è°±å®šç†

**å®šç† 2.1 (è°±å®šç†)**:

è®¾ $A \in \mathbb{R}^{n \times n}$ æ˜¯**å¯¹ç§°çŸ©é˜µ**ï¼Œåˆ™ï¼š

1. $A$ çš„æ‰€æœ‰ç‰¹å¾å€¼éƒ½æ˜¯**å®žæ•°**
2. ä¸åŒç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡**æ­£äº¤**
3. $A$ å¯ä»¥**æ­£äº¤å¯¹è§’åŒ–**ï¼š

$$
A = Q\Lambda Q^T
$$

å…¶ä¸­ $Q$ æ˜¯æ­£äº¤çŸ©é˜µï¼ˆ$Q^T Q = I$ï¼‰ï¼Œ$\Lambda = \text{diag}(\lambda_1, \ldots, \lambda_n)$ã€‚

**æ„ä¹‰**ï¼šå¯¹ç§°çŸ©é˜µæœ‰å®Œæ•´çš„æ­£äº¤ç‰¹å¾å‘é‡åŸºã€‚

---

### 3. å¯¹è§’åŒ–

**å®šä¹‰ 3.1 (å¯å¯¹è§’åŒ–)**:

çŸ©é˜µ $A$ å¯å¯¹è§’åŒ–ï¼Œå¦‚æžœå­˜åœ¨å¯é€†çŸ©é˜µ $P$ å’Œå¯¹è§’çŸ©é˜µ $D$ ä½¿å¾—ï¼š

$$
A = PDP^{-1}
$$

**æ¡ä»¶**ï¼š$A$ æœ‰ $n$ ä¸ªçº¿æ€§æ— å…³çš„ç‰¹å¾å‘é‡ã€‚

**åº”ç”¨**ï¼šè®¡ç®—çŸ©é˜µå¹‚

$$
A^k = PD^kP^{-1}
$$

å…¶ä¸­ $D^k = \text{diag}(\lambda_1^k, \ldots, \lambda_n^k)$ã€‚

---

## ðŸ“Š å¥‡å¼‚å€¼åˆ†è§£ (SVD)

### 1. SVDå®šä¹‰

**å®šç† 1.1 (å¥‡å¼‚å€¼åˆ†è§£)**:

å¯¹äºŽä»»æ„çŸ©é˜µ $A \in \mathbb{R}^{m \times n}$ï¼Œå­˜åœ¨åˆ†è§£ï¼š

$$
A = U\Sigma V^T
$$

å…¶ä¸­ï¼š

- $U \in \mathbb{R}^{m \times m}$ æ˜¯æ­£äº¤çŸ©é˜µï¼ˆå·¦å¥‡å¼‚å‘é‡ï¼‰
- $\Sigma \in \mathbb{R}^{m \times n}$ æ˜¯å¯¹è§’çŸ©é˜µï¼Œå¯¹è§’å…ƒç´  $\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_r > 0$ ç§°ä¸º**å¥‡å¼‚å€¼**
- $V \in \mathbb{R}^{n \times n}$ æ˜¯æ­£äº¤çŸ©é˜µï¼ˆå³å¥‡å¼‚å‘é‡ï¼‰

**ä¸Žç‰¹å¾å€¼çš„å…³ç³»**:

- $A^T A$ çš„ç‰¹å¾å€¼æ˜¯ $\sigma_i^2$
- $A A^T$ çš„ç‰¹å¾å€¼ä¹Ÿæ˜¯ $\sigma_i^2$
- $V$ çš„åˆ—æ˜¯ $A^T A$ çš„ç‰¹å¾å‘é‡
- $U$ çš„åˆ—æ˜¯ $A A^T$ çš„ç‰¹å¾å‘é‡

---

### 2. å‡ ä½•è§£é‡Š

**SVDçš„å‡ ä½•æ„ä¹‰**:

ä»»æ„çº¿æ€§å˜æ¢ $A$ å¯ä»¥åˆ†è§£ä¸ºï¼š

```text
A = U Î£ Váµ€
    â†“
1. Váµ€: æ—‹è½¬ (æ­£äº¤å˜æ¢)
2. Î£:  ç¼©æ”¾ (æ²¿åæ ‡è½´)
3. U:  æ—‹è½¬ (æ­£äº¤å˜æ¢)
```

**ç¤ºä¾‹**:

$$
A = \begin{bmatrix} 3 & 0 \\ 0 & 1 \end{bmatrix}
$$

å·²ç»æ˜¯å¯¹è§’çŸ©é˜µï¼ŒSVDä¸ºï¼š

$$
U = I, \quad \Sigma = A, \quad V = I
$$

---

### 3. æˆªæ–­SVDä¸Žä½Žç§©è¿‘ä¼¼

**å®šç† 3.1 (Eckart-Youngå®šç†)**:

è®¾ $A = U\Sigma V^T$ æ˜¯SVDï¼Œå®šä¹‰ç§©ä¸º $k$ çš„æˆªæ–­SVDï¼š

$$
A_k = U_k \Sigma_k V_k^T = \sum_{i=1}^k \sigma_i u_i v_i^T
$$

åˆ™ $A_k$ æ˜¯æ‰€æœ‰ç§©ä¸è¶…è¿‡ $k$ çš„çŸ©é˜µä¸­ï¼Œä¸Ž $A$ çš„FrobeniusèŒƒæ•°è·ç¦»æœ€å°çš„çŸ©é˜µï¼š

$$
A_k = \arg\min_{\text{rank}(B) \leq k} \|A - B\|_F
$$

**è¯¯å·®**:

$$
\|A - A_k\|_F = \sqrt{\sum_{i=k+1}^r \sigma_i^2}
$$

**åº”ç”¨**ï¼šæ•°æ®åŽ‹ç¼©ã€é™ç»´ã€åŽ»å™ªã€‚

---

### 4. SVDçš„æ€§è´¨

**æ€§è´¨ 4.1**:

1. **ç§©**: $\text{rank}(A) = r$ (éžé›¶å¥‡å¼‚å€¼çš„ä¸ªæ•°)
2. **èŒƒæ•°**: $\|A\|_2 = \sigma_1$ (æœ€å¤§å¥‡å¼‚å€¼)
3. **FrobeniusèŒƒæ•°**: $\|A\|_F = \sqrt{\sum_{i=1}^r \sigma_i^2}$
4. **æ¡ä»¶æ•°**: $\kappa(A) = \frac{\sigma_1}{\sigma_r}$
5. **ä¼ªé€†**: $A^+ = V\Sigma^+ U^T$ï¼Œå…¶ä¸­ $\Sigma^+$ æ˜¯ $\Sigma$ çš„ä¼ªé€†

---

## ðŸ”¬ QRåˆ†è§£

### 1. QRåˆ†è§£å®šä¹‰

**å®šç† 1.1 (QRåˆ†è§£)**:

å¯¹äºŽä»»æ„çŸ©é˜µ $A \in \mathbb{R}^{m \times n}$ ($m \geq n$)ï¼Œå­˜åœ¨åˆ†è§£ï¼š

$$
A = QR
$$

å…¶ä¸­ï¼š

- $Q \in \mathbb{R}^{m \times n}$ æ˜¯æ­£äº¤çŸ©é˜µï¼ˆ$Q^T Q = I$ï¼‰
- $R \in \mathbb{R}^{n \times n}$ æ˜¯ä¸Šä¸‰è§’çŸ©é˜µ

**åº”ç”¨**ï¼š

- æ±‚è§£æœ€å°äºŒä¹˜é—®é¢˜
- è®¡ç®—ç‰¹å¾å€¼ï¼ˆQRç®—æ³•ï¼‰
- æ­£äº¤åŒ–

---

### 2. Gram-Schmidtæ­£äº¤åŒ–

**ç®—æ³• 2.1 (Gram-Schmidtæ­£äº¤åŒ–)**:

ç»™å®šçº¿æ€§æ— å…³å‘é‡ $a_1, \ldots, a_n$ï¼Œæž„é€ æ­£äº¤å‘é‡ $q_1, \ldots, q_n$ï¼š

$$
\begin{align}
u_1 &= a_1 \\
u_i &= a_i - \sum_{j=1}^{i-1} \frac{\langle a_i, q_j \rangle}{\langle q_j, q_j \rangle} q_j \\
q_i &= \frac{u_i}{\|u_i\|}
\end{align}
$$

**é—®é¢˜**ï¼šæ•°å€¼ä¸ç¨³å®šï¼ˆä¿®æ­£Gram-Schmidtæ›´ç¨³å®šï¼‰ã€‚

---

### 3. Householderå˜æ¢

**å®šä¹‰ 3.1 (Householderå˜æ¢)**:

$$
H = I - 2vv^T
$$

å…¶ä¸­ $v$ æ˜¯å•ä½å‘é‡ï¼ˆ$\|v\| = 1$ï¼‰ã€‚

**æ€§è´¨**:

- $H$ æ˜¯å¯¹ç§°æ­£äº¤çŸ©é˜µï¼ˆ$H = H^T$ï¼Œ$H^2 = I$ï¼‰
- $H$ æ˜¯å…³äºŽè¶…å¹³é¢ $\{x : v^T x = 0\}$ çš„åå°„

**åº”ç”¨**ï¼šQRåˆ†è§£ï¼ˆHouseholder QRï¼‰

---

## ðŸ’¡ Choleskyåˆ†è§£

### 1. Choleskyåˆ†è§£å®šä¹‰

**å®šç† 1.1 (Choleskyåˆ†è§£)**:

è®¾ $A \in \mathbb{R}^{n \times n}$ æ˜¯**å¯¹ç§°æ­£å®šçŸ©é˜µ**ï¼Œåˆ™å­˜åœ¨å”¯ä¸€çš„ä¸‹ä¸‰è§’çŸ©é˜µ $L$ï¼ˆå¯¹è§’å…ƒç´ ä¸ºæ­£ï¼‰ä½¿å¾—ï¼š

$$
A = LL^T
$$

**ä¼˜åŠ¿**:

- è®¡ç®—æ•ˆçŽ‡é«˜ï¼ˆçº¦ä¸ºLUåˆ†è§£çš„ä¸€åŠï¼‰
- æ•°å€¼ç¨³å®š
- ä¿è¯æ­£å®šæ€§

**åº”ç”¨**ï¼š

- æ±‚è§£çº¿æ€§ç³»ç»Ÿ $Ax = b$
- é«˜æ–¯è¿‡ç¨‹
- ä¼˜åŒ–ç®—æ³•

---

### 2. ç®—æ³•

**ç®—æ³• 2.1 (Choleskyåˆ†è§£ç®—æ³•)**:

$$
L_{ij} = \begin{cases}
\sqrt{A_{ii} - \sum_{k=1}^{i-1} L_{ik}^2} & \text{if } i = j \\
\frac{1}{L_{jj}} \left( A_{ij} - \sum_{k=1}^{j-1} L_{ik} L_{jk} \right) & \text{if } i > j \\
0 & \text{if } i < j
\end{cases}
$$

**å¤æ‚åº¦**: $O(n^3/3)$

---

## ðŸŽ¨ LUåˆ†è§£

### 1. LUåˆ†è§£å®šä¹‰

**å®šç† 1.1 (LUåˆ†è§£)**:

è®¾ $A \in \mathbb{R}^{n \times n}$ï¼Œå¦‚æžœ $A$ çš„æ‰€æœ‰é¡ºåºä¸»å­å¼éžé›¶ï¼Œåˆ™å­˜åœ¨åˆ†è§£ï¼š

$$
A = LU
$$

å…¶ä¸­ï¼š

- $L$ æ˜¯ä¸‹ä¸‰è§’çŸ©é˜µï¼ˆå¯¹è§’å…ƒç´ ä¸º1ï¼‰
- $U$ æ˜¯ä¸Šä¸‰è§’çŸ©é˜µ

**å¸¦ä¸»å…ƒçš„LUåˆ†è§£**:

$$
PA = LU
$$

å…¶ä¸­ $P$ æ˜¯ç½®æ¢çŸ©é˜µã€‚

---

### 2. é«˜æ–¯æ¶ˆå…ƒæ³•

**ç®—æ³• 2.1 (é«˜æ–¯æ¶ˆå…ƒæ³•)**:

é€šè¿‡è¡Œå˜æ¢å°† $A$ åŒ–ä¸ºä¸Šä¸‰è§’çŸ©é˜µ $U$ï¼ŒåŒæ—¶è®°å½•å˜æ¢å¾—åˆ° $L$ã€‚

**åº”ç”¨**ï¼š

- æ±‚è§£çº¿æ€§ç³»ç»Ÿ
- è®¡ç®—è¡Œåˆ—å¼
- æ±‚é€†çŸ©é˜µ

---

## ðŸ”§ åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„åº”ç”¨

### 1. ä¸»æˆåˆ†åˆ†æž (PCA)

**é—®é¢˜**ï¼šæ‰¾åˆ°æ•°æ®çš„ä¸»è¦æ–¹å‘ã€‚

**æ–¹æ³•**ï¼š

1. ä¸­å¿ƒåŒ–æ•°æ®ï¼š$X_c = X - \bar{X}$
2. è®¡ç®—åæ–¹å·®çŸ©é˜µï¼š$C = \frac{1}{n} X_c^T X_c$
3. ç‰¹å¾å€¼åˆ†è§£ï¼š$C = Q\Lambda Q^T$
4. ä¸»æˆåˆ†ï¼š$Q$ çš„å‰ $k$ åˆ—

**ç­‰ä»·æ–¹æ³•ï¼ˆSVDï¼‰**:

1. SVD: $X_c = U\Sigma V^T$
2. ä¸»æˆåˆ†ï¼š$V$ çš„å‰ $k$ åˆ—
3. é™ç»´ï¼š$Z = X_c V_k$

---

### 2. å¥‡å¼‚å€¼åˆ†è§£ä¸Žé™ç»´

**åº”ç”¨**ï¼š

- **å›¾åƒåŽ‹ç¼©**ï¼šæˆªæ–­SVD
- **æŽ¨èç³»ç»Ÿ**ï¼šçŸ©é˜µåˆ†è§£
- **åŽ»å™ª**ï¼šä¿ç•™å¤§å¥‡å¼‚å€¼

**ç¤ºä¾‹**ï¼ˆå›¾åƒåŽ‹ç¼©ï¼‰:

```python
A_k = U[:, :k] @ Sigma[:k, :k] @ V[:k, :]
```

åŽ‹ç¼©çŽ‡ï¼š$\frac{k(m + n)}{mn}$

---

### 3. çŸ©é˜µæ±‚é€†ä¸Žçº¿æ€§ç³»ç»Ÿ

**æ±‚è§£ $Ax = b$**:

- **Choleskyåˆ†è§£**ï¼ˆ$A$ æ­£å®šï¼‰ï¼š
  1. $A = LL^T$
  2. æ±‚è§£ $Ly = b$ (å‰å‘æ›¿æ¢)
  3. æ±‚è§£ $L^T x = y$ (åŽå‘æ›¿æ¢)

- **LUåˆ†è§£**ï¼š
  1. $A = LU$
  2. æ±‚è§£ $Ly = b$
  3. æ±‚è§£ $Ux = y$

**ä¼˜åŠ¿**ï¼šé¿å…ç›´æŽ¥æ±‚é€†ï¼ˆæ•°å€¼ä¸ç¨³å®šï¼‰ã€‚

---

### 4. æƒé‡åˆå§‹åŒ–

**Xavieråˆå§‹åŒ–**ï¼ˆåŸºäºŽç‰¹å¾å€¼åˆ†æžï¼‰:

$$
W \sim \mathcal{N}\left(0, \frac{2}{n_{\text{in}} + n_{\text{out}}}\right)
$$

**Heåˆå§‹åŒ–**ï¼ˆReLUç½‘ç»œï¼‰:

$$
W \sim \mathcal{N}\left(0, \frac{2}{n_{\text{in}}}\right)
$$

**ç†è®ºåŸºç¡€**ï¼šä¿æŒæ¿€æ´»å€¼å’Œæ¢¯åº¦çš„æ–¹å·®ã€‚

---

## ðŸ’» Pythonå®žçŽ°

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.linalg import svd, qr, cholesky, lu

# 1. ç‰¹å¾å€¼åˆ†è§£
def eigendecomposition_demo():
    """ç‰¹å¾å€¼åˆ†è§£ç¤ºä¾‹"""
    # å¯¹ç§°çŸ©é˜µ
    A = np.array([[2, 1], [1, 2]])
    
    # ç‰¹å¾å€¼åˆ†è§£
    eigenvalues, eigenvectors = np.linalg.eig(A)
    
    print("çŸ©é˜µ A:")
    print(A)
    print("\nç‰¹å¾å€¼:")
    print(eigenvalues)
    print("\nç‰¹å¾å‘é‡:")
    print(eigenvectors)
    
    # éªŒè¯: A = QÎ›Q^T
    Lambda = np.diag(eigenvalues)
    Q = eigenvectors
    A_reconstructed = Q @ Lambda @ Q.T
    
    print("\né‡æž„è¯¯å·®:")
    print(np.linalg.norm(A - A_reconstructed))


# 2. SVDåˆ†è§£
def svd_demo():
    """SVDåˆ†è§£ç¤ºä¾‹"""
    # åˆ›å»ºçŸ©é˜µ
    A = np.array([[3, 2, 2],
                  [2, 3, -2]])
    
    # SVDåˆ†è§£
    U, S, Vt = svd(A)
    
    print("çŸ©é˜µ A:")
    print(A)
    print(f"\nAçš„å½¢çŠ¶: {A.shape}")
    print(f"Uçš„å½¢çŠ¶: {U.shape}")
    print(f"Sçš„å½¢çŠ¶: {S.shape}")
    print(f"Vtçš„å½¢çŠ¶: {Vt.shape}")
    
    print("\nå¥‡å¼‚å€¼:")
    print(S)
    
    # é‡æž„
    Sigma = np.zeros((A.shape[0], A.shape[1]))
    Sigma[:len(S), :len(S)] = np.diag(S)
    A_reconstructed = U @ Sigma @ Vt
    
    print("\né‡æž„è¯¯å·®:")
    print(np.linalg.norm(A - A_reconstructed))


# 3. PCAå®žçŽ°
def pca_demo():
    """PCAé™ç»´ç¤ºä¾‹"""
    np.random.seed(42)
    
    # ç”Ÿæˆ2Dæ•°æ®
    mean = [0, 0]
    cov = [[3, 1.5], [1.5, 1]]
    X = np.random.multivariate_normal(mean, cov, 200)
    
    # PCA (ä½¿ç”¨SVD)
    X_centered = X - X.mean(axis=0)
    U, S, Vt = svd(X_centered, full_matrices=False)
    
    # ä¸»æˆåˆ†
    principal_components = Vt.T
    
    # æŠ•å½±åˆ°ç¬¬ä¸€ä¸»æˆåˆ†
    Z = X_centered @ principal_components[:, 0:1]
    X_reconstructed = Z @ principal_components[:, 0:1].T + X.mean(axis=0)
    
    # å¯è§†åŒ–
    plt.figure(figsize=(12, 5))
    
    # åŽŸå§‹æ•°æ®
    plt.subplot(1, 2, 1)
    plt.scatter(X[:, 0], X[:, 1], alpha=0.5)
    plt.arrow(0, 0, principal_components[0, 0]*S[0], principal_components[1, 0]*S[0],
              head_width=0.3, head_length=0.3, fc='r', ec='r', label='PC1')
    plt.arrow(0, 0, principal_components[0, 1]*S[1], principal_components[1, 1]*S[1],
              head_width=0.3, head_length=0.3, fc='g', ec='g', label='PC2')
    plt.title('Original Data with Principal Components')
    plt.xlabel('x1')
    plt.ylabel('x2')
    plt.legend()
    plt.axis('equal')
    plt.grid(True)
    
    # é‡æž„æ•°æ®
    plt.subplot(1, 2, 2)
    plt.scatter(X[:, 0], X[:, 1], alpha=0.3, label='Original')
    plt.scatter(X_reconstructed[:, 0], X_reconstructed[:, 1], alpha=0.5, label='Reconstructed (1D)')
    plt.title('PCA Reconstruction')
    plt.xlabel('x1')
    plt.ylabel('x2')
    plt.legend()
    plt.axis('equal')
    plt.grid(True)
    
    plt.tight_layout()
    # plt.show()
    
    # è§£é‡Šæ–¹å·®æ¯”ä¾‹
    explained_variance_ratio = S**2 / np.sum(S**2)
    print("è§£é‡Šæ–¹å·®æ¯”ä¾‹:")
    print(explained_variance_ratio)


# 4. ä½Žç§©è¿‘ä¼¼
def low_rank_approximation_demo():
    """ä½Žç§©è¿‘ä¼¼ç¤ºä¾‹ï¼ˆå›¾åƒåŽ‹ç¼©ï¼‰"""
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„"å›¾åƒ"
    np.random.seed(42)
    img = np.random.randn(50, 50)
    
    # SVD
    U, S, Vt = svd(img, full_matrices=False)
    
    # ä¸åŒç§©çš„è¿‘ä¼¼
    ranks = [1, 5, 10, 20, 50]
    
    plt.figure(figsize=(15, 3))
    
    for i, k in enumerate(ranks):
        # æˆªæ–­SVD
        img_k = U[:, :k] @ np.diag(S[:k]) @ Vt[:k, :]
        
        # è®¡ç®—è¯¯å·®
        error = np.linalg.norm(img - img_k, 'fro') / np.linalg.norm(img, 'fro')
        
        plt.subplot(1, len(ranks), i+1)
        plt.imshow(img_k, cmap='gray')
        plt.title(f'Rank {k}\nError: {error:.3f}')
        plt.axis('off')
    
    plt.tight_layout()
    # plt.show()


# 5. Choleskyåˆ†è§£
def cholesky_demo():
    """Choleskyåˆ†è§£ç¤ºä¾‹"""
    # åˆ›å»ºæ­£å®šçŸ©é˜µ
    A = np.array([[4, 2, 1],
                  [2, 3, 1],
                  [1, 1, 2]])
    
    print("çŸ©é˜µ A (æ­£å®š):")
    print(A)
    
    # Choleskyåˆ†è§£
    L = cholesky(A, lower=True)
    
    print("\nCholeskyåˆ†è§£ L:")
    print(L)
    
    # éªŒè¯
    A_reconstructed = L @ L.T
    print("\né‡æž„è¯¯å·®:")
    print(np.linalg.norm(A - A_reconstructed))
    
    # æ±‚è§£çº¿æ€§ç³»ç»Ÿ Ax = b
    b = np.array([1, 2, 3])
    
    # ä½¿ç”¨Choleskyåˆ†è§£æ±‚è§£
    y = np.linalg.solve(L, b)  # Ly = b
    x = np.linalg.solve(L.T, y)  # L^T x = y
    
    print("\næ±‚è§£ Ax = b:")
    print(f"x = {x}")
    print(f"éªŒè¯ Ax = {A @ x}")


# 6. QRåˆ†è§£
def qr_demo():
    """QRåˆ†è§£ç¤ºä¾‹"""
    A = np.array([[1, 2, 3],
                  [4, 5, 6],
                  [7, 8, 9],
                  [10, 11, 12]], dtype=float)
    
    print("çŸ©é˜µ A:")
    print(A)
    
    # QRåˆ†è§£
    Q, R = qr(A)
    
    print("\nQ (æ­£äº¤çŸ©é˜µ):")
    print(Q)
    print("\nR (ä¸Šä¸‰è§’çŸ©é˜µ):")
    print(R)
    
    # éªŒè¯æ­£äº¤æ€§
    print("\nQ^T Q:")
    print(Q.T @ Q)
    
    # é‡æž„
    A_reconstructed = Q @ R
    print("\né‡æž„è¯¯å·®:")
    print(np.linalg.norm(A - A_reconstructed))


if __name__ == "__main__":
    print("=== çŸ©é˜µåˆ†è§£ç¤ºä¾‹ ===\n")
    
    print("1. ç‰¹å¾å€¼åˆ†è§£")
    eigendecomposition_demo()
    
    print("\n" + "="*50 + "\n")
    print("2. SVDåˆ†è§£")
    svd_demo()
    
    print("\n" + "="*50 + "\n")
    print("3. PCAé™ç»´")
    pca_demo()
    
    print("\n" + "="*50 + "\n")
    print("4. ä½Žç§©è¿‘ä¼¼")
    low_rank_approximation_demo()
    
    print("\n" + "="*50 + "\n")
    print("5. Choleskyåˆ†è§£")
    cholesky_demo()
    
    print("\n" + "="*50 + "\n")
    print("6. QRåˆ†è§£")
    qr_demo()
```

---

## ðŸ“š ç»ƒä¹ é¢˜

### ç»ƒä¹ 1ï¼šç‰¹å¾å€¼è®¡ç®—

è®¡ç®—ä»¥ä¸‹çŸ©é˜µçš„ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡ï¼š

$$
A = \begin{bmatrix} 1 & 2 \\ 2 & 1 \end{bmatrix}
$$

### ç»ƒä¹ 2ï¼šSVDåº”ç”¨

å¯¹çŸ©é˜µ $A = \begin{bmatrix} 1 & 2 \\ 3 & 4 \\ 5 & 6 \end{bmatrix}$ è¿›è¡ŒSVDåˆ†è§£ï¼Œå¹¶è®¡ç®—ç§©-1è¿‘ä¼¼ã€‚

### ç»ƒä¹ 3ï¼šPCAå®žçŽ°

ç»™å®šæ•°æ®çŸ©é˜µ $X \in \mathbb{R}^{100 \times 5}$ï¼Œä½¿ç”¨PCAå°†å…¶é™è‡³2ç»´ã€‚

### ç»ƒä¹ 4ï¼šä½Žç§©è¿‘ä¼¼

è¯æ˜ŽEckart-Youngå®šç†ï¼šæˆªæ–­SVDç»™å‡ºæœ€ä¼˜ä½Žç§©è¿‘ä¼¼ã€‚

---

## ðŸŽ“ ç›¸å…³è¯¾ç¨‹

| å¤§å­¦ | è¯¾ç¨‹ |
|------|------|
| **MIT** | 18.06 - Linear Algebra (Gilbert Strang) |
| **MIT** | 18.065 - Matrix Methods in Data Analysis |
| **Stanford** | CS205L - Continuous Mathematical Methods |
| **UC Berkeley** | Math 110 - Linear Algebra |
| **CMU** | 21-241 - Matrices and Linear Transformations |

---

## ðŸ“– å‚è€ƒæ–‡çŒ®

1. **Strang, G. (2016)**. *Introduction to Linear Algebra*. Wellesley-Cambridge Press.

2. **Trefethen & Bau (1997)**. *Numerical Linear Algebra*. SIAM.

3. **Golub & Van Loan (2013)**. *Matrix Computations*. Johns Hopkins University Press.

4. **Horn & Johnson (2012)**. *Matrix Analysis*. Cambridge University Press.

5. **Goodfellow et al. (2016)**. *Deep Learning*. MIT Press. (Chapter 2: Linear Algebra)

---

*æœ€åŽæ›´æ–°ï¼š2025å¹´10æœˆ*-
