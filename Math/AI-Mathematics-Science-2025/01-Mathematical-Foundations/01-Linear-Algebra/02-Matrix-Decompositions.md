# çŸ©é˜µåˆ†è§£ (Matrix Decompositions)

> **The Computational Foundation of Machine Learning**
>
> æœºå™¨å­¦ä¹ çš„è®¡ç®—åŸºç¡€

---

## ç›®å½•

- [çŸ©é˜µåˆ†è§£ (Matrix Decompositions)](#çŸ©é˜µåˆ†è§£-matrix-decompositions)
  - [ç›®å½•](#ç›®å½•)
  - [ğŸ“‹ æ ¸å¿ƒæ€æƒ³](#-æ ¸å¿ƒæ€æƒ³)
  - [ğŸ¯ ç‰¹å¾å€¼åˆ†è§£ (Eigendecomposition)](#-ç‰¹å¾å€¼åˆ†è§£-eigendecomposition)
    - [1. ç‰¹å¾å€¼ä¸ç‰¹å¾å‘é‡](#1-ç‰¹å¾å€¼ä¸ç‰¹å¾å‘é‡)
    - [2. è°±å®šç†](#2-è°±å®šç†)
    - [3. å¯¹è§’åŒ–](#3-å¯¹è§’åŒ–)
  - [ğŸ“Š å¥‡å¼‚å€¼åˆ†è§£ (SVD)](#-å¥‡å¼‚å€¼åˆ†è§£-svd)
    - [1. SVDå®šä¹‰](#1-svdå®šä¹‰)
    - [2. å‡ ä½•è§£é‡Š](#2-å‡ ä½•è§£é‡Š)
    - [3. æˆªæ–­SVDä¸ä½ç§©è¿‘ä¼¼](#3-æˆªæ–­svdä¸ä½ç§©è¿‘ä¼¼)
    - [4. SVDçš„æ€§è´¨](#4-svdçš„æ€§è´¨)
  - [ğŸ”¬ QRåˆ†è§£](#-qråˆ†è§£)
    - [1. QRåˆ†è§£å®šä¹‰](#1-qråˆ†è§£å®šä¹‰)
    - [2. Gram-Schmidtæ­£äº¤åŒ–](#2-gram-schmidtæ­£äº¤åŒ–)
      - [ç»å…¸Gram-Schmidtçš„æ•°å€¼ä¸ç¨³å®šæ€§](#ç»å…¸gram-schmidtçš„æ•°å€¼ä¸ç¨³å®šæ€§)
      - [ä¿®æ­£Gram-Schmidtç®—æ³•](#ä¿®æ­£gram-schmidtç®—æ³•)
      - [æ•°å€¼å®éªŒå¯¹æ¯”](#æ•°å€¼å®éªŒå¯¹æ¯”)
      - [æ¡ä»¶æ•°åˆ†æ](#æ¡ä»¶æ•°åˆ†æ)
      - [å®è·µå»ºè®®](#å®è·µå»ºè®®)
      - [AIåº”ç”¨ä¸­çš„é‡è¦æ€§](#aiåº”ç”¨ä¸­çš„é‡è¦æ€§)
      - [æ€»ç»“](#æ€»ç»“)
    - [3. Householderå˜æ¢](#3-householderå˜æ¢)
  - [ğŸ’¡ Choleskyåˆ†è§£](#-choleskyåˆ†è§£)
    - [1. Choleskyåˆ†è§£å®šä¹‰](#1-choleskyåˆ†è§£å®šä¹‰)
    - [2. ç®—æ³•](#2-ç®—æ³•)
  - [ğŸ¨ LUåˆ†è§£](#-luåˆ†è§£)
    - [1. LUåˆ†è§£å®šä¹‰](#1-luåˆ†è§£å®šä¹‰)
    - [2. é«˜æ–¯æ¶ˆå…ƒæ³•](#2-é«˜æ–¯æ¶ˆå…ƒæ³•)
  - [ğŸ”§ åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„åº”ç”¨](#-åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„åº”ç”¨)
    - [1. ä¸»æˆåˆ†åˆ†æ (PCA)](#1-ä¸»æˆåˆ†åˆ†æ-pca)
    - [2. å¥‡å¼‚å€¼åˆ†è§£ä¸é™ç»´](#2-å¥‡å¼‚å€¼åˆ†è§£ä¸é™ç»´)
    - [3. çŸ©é˜µæ±‚é€†ä¸çº¿æ€§ç³»ç»Ÿ](#3-çŸ©é˜µæ±‚é€†ä¸çº¿æ€§ç³»ç»Ÿ)
    - [4. æƒé‡åˆå§‹åŒ–](#4-æƒé‡åˆå§‹åŒ–)
  - [ğŸ’» Pythonå®ç°](#-pythonå®ç°)
  - [ğŸ“š ç»ƒä¹ é¢˜](#-ç»ƒä¹ é¢˜)
    - [ç»ƒä¹ 1ï¼šç‰¹å¾å€¼è®¡ç®—](#ç»ƒä¹ 1ç‰¹å¾å€¼è®¡ç®—)
    - [ç»ƒä¹ 2ï¼šSVDåº”ç”¨](#ç»ƒä¹ 2svdåº”ç”¨)
    - [ç»ƒä¹ 3ï¼šPCAå®ç°](#ç»ƒä¹ 3pcaå®ç°)
    - [ç»ƒä¹ 4ï¼šä½ç§©è¿‘ä¼¼](#ç»ƒä¹ 4ä½ç§©è¿‘ä¼¼)
  - [ğŸ“ ç›¸å…³è¯¾ç¨‹](#-ç›¸å…³è¯¾ç¨‹)
  - [ğŸ“– å‚è€ƒæ–‡çŒ®](#-å‚è€ƒæ–‡çŒ®)

---

## ğŸ“‹ æ ¸å¿ƒæ€æƒ³

**çŸ©é˜µåˆ†è§£**æ˜¯å°†çŸ©é˜µè¡¨ç¤ºä¸ºæ›´ç®€å•çŸ©é˜µçš„ä¹˜ç§¯ï¼Œæ˜¯çº¿æ€§ä»£æ•°è®¡ç®—çš„æ ¸å¿ƒå·¥å…·ã€‚

**ä¸ºä»€ä¹ˆçŸ©é˜µåˆ†è§£é‡è¦**:

```text
è®¡ç®—ä¼˜åŠ¿:
â”œâ”€ ç®€åŒ–è®¡ç®— (å¦‚æ±‚é€†ã€æ±‚è§£çº¿æ€§ç³»ç»Ÿ)
â”œâ”€ æ•°å€¼ç¨³å®šæ€§
â””â”€ æ­ç¤ºçŸ©é˜µç»“æ„

æœºå™¨å­¦ä¹ åº”ç”¨:
â”œâ”€ PCA (ä¸»æˆåˆ†åˆ†æ) â†’ SVD
â”œâ”€ æ¨èç³»ç»Ÿ â†’ çŸ©é˜µåˆ†è§£
â”œâ”€ é™ç»´ â†’ SVD/ç‰¹å¾å€¼åˆ†è§£
â””â”€ ä¼˜åŒ– â†’ Choleskyåˆ†è§£
```

**ä¸»è¦åˆ†è§£**:

```text
ç‰¹å¾å€¼åˆ†è§£ (Eigendecomposition):
    A = QÎ›Qâ»Â¹  (æ–¹é˜µ, å¯å¯¹è§’åŒ–)

å¥‡å¼‚å€¼åˆ†è§£ (SVD):
    A = UÎ£Váµ€  (ä»»æ„çŸ©é˜µ)

QRåˆ†è§£:
    A = QR  (Qæ­£äº¤, Rä¸Šä¸‰è§’)

Choleskyåˆ†è§£:
    A = LLáµ€  (æ­£å®šçŸ©é˜µ)

LUåˆ†è§£:
    A = LU  (Lä¸‹ä¸‰è§’, Uä¸Šä¸‰è§’)
```

---

## ğŸ¯ ç‰¹å¾å€¼åˆ†è§£ (Eigendecomposition)

### 1. ç‰¹å¾å€¼ä¸ç‰¹å¾å‘é‡

**å®šä¹‰ 1.1 (ç‰¹å¾å€¼ä¸ç‰¹å¾å‘é‡)**:

è®¾ $A \in \mathbb{R}^{n \times n}$ï¼Œå¦‚æœå­˜åœ¨æ ‡é‡ $\lambda$ å’Œéé›¶å‘é‡ $v$ ä½¿å¾—ï¼š

$$
Av = \lambda v
$$

åˆ™ $\lambda$ ç§°ä¸º $A$ çš„**ç‰¹å¾å€¼**ï¼Œ$v$ ç§°ä¸ºå¯¹åº”çš„**ç‰¹å¾å‘é‡**ã€‚

**å‡ ä½•æ„ä¹‰**ï¼š$A$ ä½œç”¨åœ¨ $v$ ä¸Šåªæ”¹å˜å…¶é•¿åº¦ï¼Œä¸æ”¹å˜æ–¹å‘ã€‚

**ç‰¹å¾å¤šé¡¹å¼**:

$$
\det(A - \lambda I) = 0
$$

**ç¤ºä¾‹**:

$$
A = \begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix}
$$

ç‰¹å¾å¤šé¡¹å¼ï¼š$\det(A - \lambda I) = (2 - \lambda)^2 - 1 = 0$

ç‰¹å¾å€¼ï¼š$\lambda_1 = 3, \lambda_2 = 1$

ç‰¹å¾å‘é‡ï¼š$v_1 = \begin{bmatrix} 1 \\ 1 \end{bmatrix}, v_2 = \begin{bmatrix} 1 \\ -1 \end{bmatrix}$

---

### 2. è°±å®šç†

**å®šç† 2.1 (è°±å®šç†)**:

è®¾ $A \in \mathbb{R}^{n \times n}$ æ˜¯**å¯¹ç§°çŸ©é˜µ**ï¼Œåˆ™ï¼š

1. $A$ çš„æ‰€æœ‰ç‰¹å¾å€¼éƒ½æ˜¯**å®æ•°**
2. ä¸åŒç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡**æ­£äº¤**
3. $A$ å¯ä»¥**æ­£äº¤å¯¹è§’åŒ–**ï¼š

$$
A = Q\Lambda Q^T
$$

å…¶ä¸­ $Q$ æ˜¯æ­£äº¤çŸ©é˜µï¼ˆ$Q^T Q = I$ï¼‰ï¼Œ$\Lambda = \text{diag}(\lambda_1, \ldots, \lambda_n)$ã€‚

**æ„ä¹‰**ï¼šå¯¹ç§°çŸ©é˜µæœ‰å®Œæ•´çš„æ­£äº¤ç‰¹å¾å‘é‡åŸºã€‚

---

**å®šç† 2.1 çš„å®Œæ•´è¯æ˜**:

æˆ‘ä»¬å°†åˆ†ä¸‰éƒ¨åˆ†è¯æ˜è°±å®šç†çš„ä¸‰ä¸ªç»“è®ºã€‚

**è¯æ˜ (1): ç‰¹å¾å€¼éƒ½æ˜¯å®æ•°**:

è®¾ $\lambda$ æ˜¯ $A$ çš„ç‰¹å¾å€¼ï¼Œ$v$ æ˜¯å¯¹åº”çš„ç‰¹å¾å‘é‡ï¼ˆå…è®¸ $v$ æ˜¯å¤å‘é‡ï¼‰ã€‚åˆ™ï¼š

$$
Av = \lambda v
$$

ä¸¤è¾¹å–å…±è½­è½¬ç½®å¹¶å·¦ä¹˜ $v^*$ï¼ˆ$v^*$ è¡¨ç¤º $v$ çš„å…±è½­è½¬ç½®ï¼‰ï¼š

$$
v^* A^* v^* = \bar{\lambda} v^* v^*
$$

ç”±äº $A$ æ˜¯å®å¯¹ç§°çŸ©é˜µï¼Œæœ‰ $A^* = A^T = A$ï¼Œå› æ­¤ï¼š

$$
v^* A v = \bar{\lambda} v^* v
$$

å¦ä¸€æ–¹é¢ï¼Œç”± $Av = \lambda v$ï¼Œä¸¤è¾¹å·¦ä¹˜ $v^*$ï¼š

$$
v^* A v = \lambda v^* v
$$

æ¯”è¾ƒä¸¤å¼ï¼Œå¾—ï¼š

$$
\lambda v^* v = \bar{\lambda} v^* v
$$

ç”±äº $v \neq 0$ï¼Œæœ‰ $v^* v = \|v\|^2 > 0$ï¼Œå› æ­¤ï¼š

$$
\lambda = \bar{\lambda}
$$

è¿™è¯´æ˜ $\lambda$ æ˜¯å®æ•°ã€‚ $\square$

**è¯æ˜ (2): ä¸åŒç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡æ­£äº¤**:

è®¾ $\lambda_1 \neq \lambda_2$ æ˜¯ $A$ çš„ä¸¤ä¸ªä¸åŒç‰¹å¾å€¼ï¼Œ$v_1, v_2$ æ˜¯å¯¹åº”çš„ç‰¹å¾å‘é‡ã€‚åˆ™ï¼š

$$
Av_1 = \lambda_1 v_1, \quad Av_2 = \lambda_2 v_2
$$

è®¡ç®—å†…ç§¯ $v_1^T A v_2$ï¼š

$$
v_1^T A v_2 = v_1^T (\lambda_2 v_2) = \lambda_2 (v_1^T v_2)
$$

å¦ä¸€æ–¹é¢ï¼Œç”±äº $A$ å¯¹ç§°ï¼ˆ$A^T = A$ï¼‰ï¼š

$$
v_1^T A v_2 = (A^T v_1)^T v_2 = (A v_1)^T v_2 = (\lambda_1 v_1)^T v_2 = \lambda_1 (v_1^T v_2)
$$

å› æ­¤ï¼š

$$
\lambda_2 (v_1^T v_2) = \lambda_1 (v_1^T v_2)
$$

$$
(\lambda_2 - \lambda_1)(v_1^T v_2) = 0
$$

ç”±äº $\lambda_1 \neq \lambda_2$ï¼Œå¿…æœ‰ï¼š

$$
v_1^T v_2 = 0
$$

å³ $v_1$ å’Œ $v_2$ æ­£äº¤ã€‚ $\square$

**è¯æ˜ (3): å¯ä»¥æ­£äº¤å¯¹è§’åŒ–**:

æˆ‘ä»¬ç”¨æ•°å­¦å½’çº³æ³•è¯æ˜ã€‚

**åŸºç¡€æ­¥éª¤** ($n=1$): æ˜¾ç„¶æˆç«‹ã€‚

**å½’çº³æ­¥éª¤**: å‡è®¾å¯¹æ‰€æœ‰ $(n-1) \times (n-1)$ å¯¹ç§°çŸ©é˜µå®šç†æˆç«‹ï¼Œç°åœ¨è¯æ˜å¯¹ $n \times n$ å¯¹ç§°çŸ©é˜µ $A$ ä¹Ÿæˆç«‹ã€‚

1. ç”±è¯æ˜(1)ï¼Œ$A$ è‡³å°‘æœ‰ä¸€ä¸ªå®ç‰¹å¾å€¼ $\lambda_1$ï¼Œè®¾å¯¹åº”çš„å•ä½ç‰¹å¾å‘é‡ä¸º $q_1$ï¼ˆ$\|q_1\| = 1$ï¼‰ã€‚

2. å°† $q_1$ æ‰©å……ä¸º $\mathbb{R}^n$ çš„æ ‡å‡†æ­£äº¤åŸº $\{q_1, q_2, \ldots, q_n\}$ã€‚

3. æ„é€ æ­£äº¤çŸ©é˜µ $Q_1 = [q_1 \mid q_2 \mid \cdots \mid q_n]$ï¼Œåˆ™ï¼š

    $$
    Q_1^T A Q_1 = \begin{bmatrix} \lambda_1 & w^T \\ w & B \end{bmatrix}
    $$

    å…¶ä¸­ $w \in \mathbb{R}^{n-1}$ï¼Œ$B \in \mathbb{R}^{(n-1) \times (n-1)}$ã€‚

4. ç”±äº $Q_1^T A Q_1$ ä»æ˜¯å¯¹ç§°çŸ©é˜µï¼Œå¿…æœ‰ $w = 0$ã€‚è¯æ˜å¦‚ä¸‹ï¼š

   çŸ©é˜µ $Q_1^T A Q_1$ çš„ $(1,2)$ å…ƒç´ ç­‰äº $(2,1)$ å…ƒç´ ï¼š

   $$
   (Q_1^T A Q_1)_{12} = q_1^T A q_2 = (A q_1)^T q_2 = (\lambda_1 q_1)^T q_2 = \lambda_1 (q_1^T q_2) = 0
   $$

   å› æ­¤ $w = 0$ã€‚

5. ç°åœ¨ï¼š

    $$
    Q_1^T A Q_1 = \begin{bmatrix} \lambda_1 & 0 \\ 0 & B \end{bmatrix}
    $$

    å…¶ä¸­ $B$ æ˜¯ $(n-1) \times (n-1)$ å¯¹ç§°çŸ©é˜µã€‚

6. ç”±å½’çº³å‡è®¾ï¼Œå­˜åœ¨ $(n-1) \times (n-1)$ æ­£äº¤çŸ©é˜µ $Q_2$ ä½¿å¾—ï¼š

    $$
    Q_2^T B Q_2 = \Lambda' = \text{diag}(\lambda_2, \ldots, \lambda_n)
    $$

7. ä»¤ï¼š

    $$
    Q_3 = \begin{bmatrix} 1 & 0 \\ 0 & Q_2 \end{bmatrix}
    $$

    åˆ™ $Q_3$ æ˜¯æ­£äº¤çŸ©é˜µï¼Œä¸”ï¼š

    $$
    Q_3^T (Q_1^T A Q_1) Q_3 = \begin{bmatrix} \lambda_1 & 0 \\ 0 & \Lambda' \end{bmatrix} = \Lambda
    $$

8. ä»¤ $Q = Q_1 Q_3$ï¼Œåˆ™ $Q$ æ˜¯æ­£äº¤çŸ©é˜µï¼Œä¸”ï¼š

    $$
    Q^T A Q = \Lambda
    $$

    å³ï¼š

    $$
    A = Q \Lambda Q^T
    $$

è¿™å°±å®Œæˆäº†å½’çº³è¯æ˜ã€‚ $\square$

**å®šç†çš„å‡ ä½•æ„ä¹‰**:

è°±å®šç†è¡¨æ˜ï¼Œå¯¹ç§°çŸ©é˜µåœ¨æŸä¸ªæ ‡å‡†æ­£äº¤åŸºä¸‹çš„è¡¨ç¤ºæ˜¯å¯¹è§’çŸ©é˜µã€‚è¿™æ„å‘³ç€ï¼š

- å¯¹ç§°çŸ©é˜µå¯¹åº”çš„çº¿æ€§å˜æ¢åœ¨å…¶ç‰¹å¾å‘é‡æ–¹å‘ä¸Šåªè¿›è¡Œä¼¸ç¼©
- ä¸åŒç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾ç©ºé—´ç›¸äº’æ­£äº¤
- å¯¹ç§°çŸ©é˜µå®Œå…¨ç”±å…¶ç‰¹å¾å€¼å’Œæ­£äº¤ç‰¹å¾å‘é‡ç¡®å®š

**åº”ç”¨ç¤ºä¾‹**:

è€ƒè™‘å¯¹ç§°çŸ©é˜µï¼š

$$
A = \begin{bmatrix} 3 & 1 \\ 1 & 3 \end{bmatrix}
$$

ç‰¹å¾å€¼ï¼š$\lambda_1 = 4, \lambda_2 = 2$

ç‰¹å¾å‘é‡ï¼š$v_1 = \begin{bmatrix} 1 \\ 1 \end{bmatrix}, v_2 = \begin{bmatrix} 1 \\ -1 \end{bmatrix}$

å½’ä¸€åŒ–åï¼š$q_1 = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ 1 \end{bmatrix}, q_2 = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ -1 \end{bmatrix}$

éªŒè¯æ­£äº¤æ€§ï¼š$q_1^T q_2 = \frac{1}{2}(1 \cdot 1 + 1 \cdot (-1)) = 0$ âœ“

æ­£äº¤å¯¹è§’åŒ–ï¼š

$$
A = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix} \begin{bmatrix} 4 & 0 \\ 0 & 2 \end{bmatrix} \frac{1}{\sqrt{2}}\begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix}
$$

---

### 3. å¯¹è§’åŒ–

**å®šä¹‰ 3.1 (å¯å¯¹è§’åŒ–)**:

çŸ©é˜µ $A$ å¯å¯¹è§’åŒ–ï¼Œå¦‚æœå­˜åœ¨å¯é€†çŸ©é˜µ $P$ å’Œå¯¹è§’çŸ©é˜µ $D$ ä½¿å¾—ï¼š

$$
A = PDP^{-1}
$$

**æ¡ä»¶**ï¼š$A$ æœ‰ $n$ ä¸ªçº¿æ€§æ— å…³çš„ç‰¹å¾å‘é‡ã€‚

**åº”ç”¨**ï¼šè®¡ç®—çŸ©é˜µå¹‚

$$
A^k = PD^kP^{-1}
$$

å…¶ä¸­ $D^k = \text{diag}(\lambda_1^k, \ldots, \lambda_n^k)$ã€‚

---

## ğŸ“Š å¥‡å¼‚å€¼åˆ†è§£ (SVD)

### 1. SVDå®šä¹‰

**å®šç† 1.1 (å¥‡å¼‚å€¼åˆ†è§£)**:

å¯¹äºä»»æ„çŸ©é˜µ $A \in \mathbb{R}^{m \times n}$ï¼Œå­˜åœ¨åˆ†è§£ï¼š

$$
A = U\Sigma V^T
$$

å…¶ä¸­ï¼š

- $U \in \mathbb{R}^{m \times m}$ æ˜¯æ­£äº¤çŸ©é˜µï¼ˆå·¦å¥‡å¼‚å‘é‡ï¼‰
- $\Sigma \in \mathbb{R}^{m \times n}$ æ˜¯å¯¹è§’çŸ©é˜µï¼Œå¯¹è§’å…ƒç´  $\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_r > 0$ ç§°ä¸º**å¥‡å¼‚å€¼**
- $V \in \mathbb{R}^{n \times n}$ æ˜¯æ­£äº¤çŸ©é˜µï¼ˆå³å¥‡å¼‚å‘é‡ï¼‰

**ä¸ç‰¹å¾å€¼çš„å…³ç³»**:

- $A^T A$ çš„ç‰¹å¾å€¼æ˜¯ $\sigma_i^2$
- $A A^T$ çš„ç‰¹å¾å€¼ä¹Ÿæ˜¯ $\sigma_i^2$
- $V$ çš„åˆ—æ˜¯ $A^T A$ çš„ç‰¹å¾å‘é‡
- $U$ çš„åˆ—æ˜¯ $A A^T$ çš„ç‰¹å¾å‘é‡

---

**å®šç† 1.1 çš„å®Œæ•´è¯æ˜**:

æˆ‘ä»¬å°†æ„é€ æ€§åœ°è¯æ˜SVDçš„å­˜åœ¨æ€§ã€‚

**è¯æ˜æ­¥éª¤**:

**ç¬¬ä¸€æ­¥ï¼šåˆ†æ $A^T A$**

è€ƒè™‘çŸ©é˜µ $A^T A \in \mathbb{R}^{n \times n}$ã€‚æ³¨æ„åˆ°ï¼š

1. $A^T A$ æ˜¯å¯¹ç§°çŸ©é˜µï¼š$(A^T A)^T = A^T (A^T)^T = A^T A$

2. $A^T A$ æ˜¯åŠæ­£å®šçŸ©é˜µï¼šå¯¹ä»»æ„ $x \in \mathbb{R}^n$ï¼Œ
   $$
   x^T (A^T A) x = (Ax)^T (Ax) = \|Ax\|^2 \geq 0
   $$

**ç¬¬äºŒæ­¥ï¼šåº”ç”¨è°±å®šç†**:

ç”±äº $A^T A$ æ˜¯å¯¹ç§°çŸ©é˜µï¼Œæ ¹æ®è°±å®šç†ï¼Œå­˜åœ¨æ­£äº¤çŸ©é˜µ $V \in \mathbb{R}^{n \times n}$ å’Œå¯¹è§’çŸ©é˜µ $\Lambda$ ä½¿å¾—ï¼š

$$
A^T A = V \Lambda V^T
$$

å…¶ä¸­ $\Lambda = \text{diag}(\lambda_1, \lambda_2, \ldots, \lambda_n)$ï¼Œä¸” $\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_n \geq 0$ï¼ˆæ‰€æœ‰ç‰¹å¾å€¼éè´Ÿï¼Œå› ä¸º $A^T A$ åŠæ­£å®šï¼‰ã€‚

è®¾ $V = [v_1 \mid v_2 \mid \cdots \mid v_n]$ï¼Œå…¶ä¸­ $v_i$ æ˜¯å¯¹åº”äºç‰¹å¾å€¼ $\lambda_i$ çš„å•ä½ç‰¹å¾å‘é‡ã€‚

**ç¬¬ä¸‰æ­¥ï¼šå®šä¹‰å¥‡å¼‚å€¼**:

å®šä¹‰å¥‡å¼‚å€¼ä¸ºï¼š

$$
\sigma_i = \sqrt{\lambda_i}, \quad i = 1, 2, \ldots, n
$$

å‡è®¾å‰ $r$ ä¸ªå¥‡å¼‚å€¼ä¸ºæ­£ï¼ˆ$\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_r > 0$ï¼‰ï¼Œåé¢çš„ä¸ºé›¶ã€‚è¿™é‡Œ $r = \text{rank}(A)$ã€‚

**ç¬¬å››æ­¥ï¼šæ„é€ å·¦å¥‡å¼‚å‘é‡ $U$**

å¯¹äº $i = 1, 2, \ldots, r$ï¼Œå®šä¹‰ï¼š

$$
u_i = \frac{1}{\sigma_i} A v_i
$$

æˆ‘ä»¬éœ€è¦éªŒè¯ $\{u_1, u_2, \ldots, u_r\}$ æ˜¯æ­£äº¤çš„ï¼š

$$
u_i^T u_j = \frac{1}{\sigma_i \sigma_j} (Av_i)^T (Av_j) = \frac{1}{\sigma_i \sigma_j} v_i^T A^T A v_j
$$

ç”±äº $A^T A v_j = \lambda_j v_j = \sigma_j^2 v_j$ï¼š

$$
u_i^T u_j = \frac{1}{\sigma_i \sigma_j} v_i^T (\sigma_j^2 v_j) = \frac{\sigma_j}{\sigma_i} v_i^T v_j = \frac{\sigma_j}{\sigma_i} \delta_{ij} = \delta_{ij}
$$

å› æ­¤ $\{u_1, u_2, \ldots, u_r\}$ æ˜¯æ ‡å‡†æ­£äº¤é›†ã€‚

å°† $\{u_1, u_2, \ldots, u_r\}$ æ‰©å……ä¸º $\mathbb{R}^m$ çš„æ ‡å‡†æ­£äº¤åŸº $\{u_1, u_2, \ldots, u_m\}$ï¼Œæ„é€ æ­£äº¤çŸ©é˜µï¼š

$$
U = [u_1 \mid u_2 \mid \cdots \mid u_m] \in \mathbb{R}^{m \times m}
$$

**ç¬¬äº”æ­¥ï¼šéªŒè¯åˆ†è§£**:

ç°åœ¨éªŒè¯ $A = U \Sigma V^T$ï¼Œå…¶ä¸­ $\Sigma \in \mathbb{R}^{m \times n}$ æ˜¯å¹¿ä¹‰å¯¹è§’çŸ©é˜µï¼š

$$
\Sigma_{ij} = \begin{cases}
\sigma_i & \text{å¦‚æœ } i = j \leq r \\
0 & \text{å¦åˆ™}
\end{cases}
$$

å¯¹äº $j = 1, 2, \ldots, n$ï¼Œè®¡ç®— $A v_j$ï¼š

- å¦‚æœ $j \leq r$ï¼š
  $$
  A v_j = \sigma_j u_j = \sigma_j u_j = (U \Sigma V^T) v_j
  $$

  å› ä¸ºï¼š
  $$
  (U \Sigma V^T) v_j = U \Sigma e_j = U (\sigma_j e_j) = \sigma_j u_j
  $$

- å¦‚æœ $j > r$ï¼š
  $$
  \|A v_j\|^2 = v_j^T A^T A v_j = v_j^T (\lambda_j v_j) = \lambda_j \|v_j\|^2 = 0
  $$

  å› æ­¤ $A v_j = 0 = (U \Sigma V^T) v_j$

ç”±äº $\{v_1, v_2, \ldots, v_n\}$ æ˜¯ $\mathbb{R}^n$ çš„æ ‡å‡†æ­£äº¤åŸºï¼Œè€Œ $A$ å’Œ $U \Sigma V^T$ åœ¨è¿™ç»„åŸºä¸Šçš„ä½œç”¨ç›¸åŒï¼Œå› æ­¤ï¼š

$$
A = U \Sigma V^T
$$

è¿™å°±å®Œæˆäº†SVDçš„å­˜åœ¨æ€§è¯æ˜ã€‚ $\square$

**å”¯ä¸€æ€§è¯´æ˜**:

å¥‡å¼‚å€¼çš„å”¯ä¸€æ€§ï¼šå¥‡å¼‚å€¼ $\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_r > 0$ ç”± $A^T A$ çš„ç‰¹å¾å€¼å”¯ä¸€ç¡®å®šã€‚

å¥‡å¼‚å‘é‡çš„å”¯ä¸€æ€§ï¼š

- å¦‚æœæ‰€æœ‰éé›¶å¥‡å¼‚å€¼äº’ä¸ç›¸åŒï¼Œåˆ™å¯¹åº”çš„å¥‡å¼‚å‘é‡åœ¨ç¬¦å·å·®å¼‚ä¸‹æ˜¯å”¯ä¸€çš„
- å¦‚æœå­˜åœ¨é‡å¤çš„å¥‡å¼‚å€¼ï¼Œåˆ™å¯¹åº”çš„å¥‡å¼‚å‘é‡å¼ æˆçš„å­ç©ºé—´æ˜¯å”¯ä¸€çš„ï¼Œä½†å…·ä½“çš„æ­£äº¤åŸºä¸å”¯ä¸€

**è®¡ç®—ç¤ºä¾‹**:

è€ƒè™‘çŸ©é˜µï¼š

$$
A = \begin{bmatrix} 3 & 2 & 2 \\ 2 & 3 & -2 \end{bmatrix}
$$

**æ­¥éª¤1**: è®¡ç®— $A^T A$ï¼š

$$
A^T A = \begin{bmatrix} 3 & 2 \\ 2 & 3 \\ 2 & -2 \end{bmatrix} \begin{bmatrix} 3 & 2 & 2 \\ 2 & 3 & -2 \end{bmatrix} = \begin{bmatrix} 13 & 12 & 2 \\ 12 & 13 & -2 \\ 2 & -2 & 8 \end{bmatrix}
$$

**æ­¥éª¤2**: è®¡ç®—ç‰¹å¾å€¼ï¼ˆçœç•¥è¯¦ç»†è®¡ç®—ï¼‰ï¼š

$$
\lambda_1 = 25, \quad \lambda_2 = 9, \quad \lambda_3 = 0
$$

**æ­¥éª¤3**: å¥‡å¼‚å€¼ï¼š

$$
\sigma_1 = 5, \quad \sigma_2 = 3
$$

**æ­¥éª¤4**: è®¡ç®—å³å¥‡å¼‚å‘é‡ $V$ï¼ˆ$A^T A$ çš„ç‰¹å¾å‘é‡ï¼‰

**æ­¥éª¤5**: è®¡ç®—å·¦å¥‡å¼‚å‘é‡ $U = AV\Sigma^{-1}$

æœ€ç»ˆå¾—åˆ°ï¼š

$$
A = U \begin{bmatrix} 5 & 0 & 0 \\ 0 & 3 & 0 \end{bmatrix} V^T
$$

---

### 2. å‡ ä½•è§£é‡Š

**SVDçš„å‡ ä½•æ„ä¹‰**:

ä»»æ„çº¿æ€§å˜æ¢ $A$ å¯ä»¥åˆ†è§£ä¸ºï¼š

```text
A = U Î£ Váµ€
    â†“
1. Váµ€: æ—‹è½¬ (æ­£äº¤å˜æ¢)
2. Î£:  ç¼©æ”¾ (æ²¿åæ ‡è½´)
3. U:  æ—‹è½¬ (æ­£äº¤å˜æ¢)
```

**ç¤ºä¾‹**:

$$
A = \begin{bmatrix} 3 & 0 \\ 0 & 1 \end{bmatrix}
$$

å·²ç»æ˜¯å¯¹è§’çŸ©é˜µï¼ŒSVDä¸ºï¼š

$$
U = I, \quad \Sigma = A, \quad V = I
$$

---

### 3. æˆªæ–­SVDä¸ä½ç§©è¿‘ä¼¼

**å®šç† 3.1 (Eckart-Youngå®šç†)**:

è®¾ $A = U\Sigma V^T$ æ˜¯SVDï¼Œå®šä¹‰ç§©ä¸º $k$ çš„æˆªæ–­SVDï¼š

$$
A_k = U_k \Sigma_k V_k^T = \sum_{i=1}^k \sigma_i u_i v_i^T
$$

åˆ™ $A_k$ æ˜¯æ‰€æœ‰ç§©ä¸è¶…è¿‡ $k$ çš„çŸ©é˜µä¸­ï¼Œä¸ $A$ çš„FrobeniusèŒƒæ•°è·ç¦»æœ€å°çš„çŸ©é˜µï¼š

$$
A_k = \arg\min_{\text{rank}(B) \leq k} \|A - B\|_F
$$

**è¯¯å·®**:

$$
\|A - A_k\|_F = \sqrt{\sum_{i=k+1}^r \sigma_i^2}
$$

**åº”ç”¨**ï¼šæ•°æ®å‹ç¼©ã€é™ç»´ã€å»å™ªã€‚

---

### 4. SVDçš„æ€§è´¨

**æ€§è´¨ 4.1**:

1. **ç§©**: $\text{rank}(A) = r$ (éé›¶å¥‡å¼‚å€¼çš„ä¸ªæ•°)
2. **èŒƒæ•°**: $\|A\|_2 = \sigma_1$ (æœ€å¤§å¥‡å¼‚å€¼)
3. **FrobeniusèŒƒæ•°**: $\|A\|_F = \sqrt{\sum_{i=1}^r \sigma_i^2}$
4. **æ¡ä»¶æ•°**: $\kappa(A) = \frac{\sigma_1}{\sigma_r}$
5. **ä¼ªé€†**: $A^+ = V\Sigma^+ U^T$ï¼Œå…¶ä¸­ $\Sigma^+$ æ˜¯ $\Sigma$ çš„ä¼ªé€†

---

## ğŸ”¬ QRåˆ†è§£

### 1. QRåˆ†è§£å®šä¹‰

**å®šç† 1.1 (QRåˆ†è§£)**:

å¯¹äºä»»æ„çŸ©é˜µ $A \in \mathbb{R}^{m \times n}$ ($m \geq n$)ï¼Œå­˜åœ¨åˆ†è§£ï¼š

$$
A = QR
$$

å…¶ä¸­ï¼š

- $Q \in \mathbb{R}^{m \times n}$ æ˜¯æ­£äº¤çŸ©é˜µï¼ˆ$Q^T Q = I$ï¼‰
- $R \in \mathbb{R}^{n \times n}$ æ˜¯ä¸Šä¸‰è§’çŸ©é˜µ

**åº”ç”¨**ï¼š

- æ±‚è§£æœ€å°äºŒä¹˜é—®é¢˜
- è®¡ç®—ç‰¹å¾å€¼ï¼ˆQRç®—æ³•ï¼‰
- æ­£äº¤åŒ–

---

### 2. Gram-Schmidtæ­£äº¤åŒ–

**ç®—æ³• 2.1 (Gram-Schmidtæ­£äº¤åŒ–)**:

ç»™å®šçº¿æ€§æ— å…³å‘é‡ $a_1, \ldots, a_n$ï¼Œæ„é€ æ­£äº¤å‘é‡ $q_1, \ldots, q_n$ï¼š

$$
\begin{align}
u_1 &= a_1 \\
u_i &= a_i - \sum_{j=1}^{i-1} \frac{\langle a_i, q_j \rangle}{\langle q_j, q_j \rangle} q_j \\
q_i &= \frac{u_i}{\|u_i\|}
\end{align}
$$

**é—®é¢˜**ï¼šæ•°å€¼ä¸ç¨³å®šï¼ˆä¿®æ­£Gram-Schmidtæ›´ç¨³å®šï¼‰ã€‚

---

**Gram-Schmidtæ­£äº¤åŒ–çš„æ•°å€¼ç¨³å®šæ€§åˆ†æ**:

#### ç»å…¸Gram-Schmidtçš„æ•°å€¼ä¸ç¨³å®šæ€§

**é—®é¢˜æ ¹æº**:

åœ¨ç»å…¸Gram-Schmidt (Classical GS, CGS) ç®—æ³•ä¸­ï¼Œåç»­å‘é‡çš„æ­£äº¤åŒ–ä¾èµ–äºä¹‹å‰å·²ç»è®¡ç®—å‡ºçš„å‘é‡ã€‚ç”±äºèˆå…¥è¯¯å·®çš„ç´¯ç§¯ï¼Œå·²è®¡ç®—çš„å‘é‡ $q_1, \ldots, q_{i-1}$ å¯èƒ½å·²ç»**å¤±å»æ­£äº¤æ€§**ã€‚

**æ•°å­¦åˆ†æ**:

è®¾ $\hat{q}_i$ è¡¨ç¤ºå®é™…è®¡ç®—ä¸­å¾—åˆ°çš„å‘é‡ï¼ˆå«èˆå…¥è¯¯å·®ï¼‰ï¼Œåˆ™ï¼š

$$
\hat{q}_i^T \hat{q}_j \neq 0, \quad i \neq j
$$

**æ­£äº¤æ€§æŸå¤±**å¯ä»¥ç”¨ä»¥ä¸‹æŒ‡æ ‡è¡¡é‡ï¼š

$$
\text{Orthogonality Loss} = \max_{i \neq j} |\hat{q}_i^T \hat{q}_j|
$$

ç†æƒ³æƒ…å†µä¸‹åº”ä¸º0ï¼Œä½†åœ¨CGSä¸­å¯èƒ½è¾¾åˆ° $O(\kappa(A) \cdot \epsilon_{\text{machine}})$ï¼Œå…¶ä¸­ï¼š

- $\kappa(A) = \|A\| \|A^{-1}\|$ æ˜¯æ¡ä»¶æ•°
- $\epsilon_{\text{machine}} \approx 10^{-16}$ (åŒç²¾åº¦æµ®ç‚¹æ•°)

**ç¤ºä¾‹**ï¼ˆç—…æ€çŸ©é˜µï¼‰:

å¯¹äºHilbertçŸ©é˜µ $H_{ij} = \frac{1}{i+j-1}$ (é«˜åº¦ç—…æ€ï¼Œ$\kappa(H_5) \approx 10^5$)ï¼š

```python
import numpy as np

# 5x5 HilbertçŸ©é˜µ
n = 5
H = np.array([[1/(i+j-1) for j in range(1,n+1)] for i in range(1,n+1)])

# ç»å…¸Gram-Schmidt
Q_cgs, _ = modified_gram_schmidt(H, classical=True)

# æ£€æŸ¥æ­£äº¤æ€§
orthogonality = Q_cgs.T @ Q_cgs
print(f"||Q^T Q - I||_F = {np.linalg.norm(orthogonality - np.eye(n), 'fro')}")
# è¾“å‡º: ~10^-11 (å¤±å»å¤§é‡ç²¾åº¦!)
```

---

#### ä¿®æ­£Gram-Schmidtç®—æ³•

**ç®—æ³• 2.2 (ä¿®æ­£Gram-Schmidt, MGS)**:

å…³é”®æ”¹è¿›ï¼š**æ¯æ¬¡æ­£äº¤åŒ–åç«‹å³æ›´æ–°æ‰€æœ‰å‰©ä½™å‘é‡**ã€‚

```python
def modified_gram_schmidt(A):
    """
    ä¿®æ­£Gram-Schmidtç®—æ³•
    è¾“å…¥: A (mÃ—nçŸ©é˜µ)
    è¾“å‡º: Q (mÃ—næ­£äº¤çŸ©é˜µ), R (nÃ—nä¸Šä¸‰è§’çŸ©é˜µ)
    """
    m, n = A.shape
    Q = A.copy().astype(float)
    R = np.zeros((n, n))

    for i in range(n):
        # è®¡ç®—èŒƒæ•°
        R[i, i] = np.linalg.norm(Q[:, i])

        # å½’ä¸€åŒ–
        Q[:, i] = Q[:, i] / R[i, i]

        # å…³é”®ï¼šç«‹å³æ›´æ–°æ‰€æœ‰å‰©ä½™å‘é‡
        for j in range(i+1, n):
            R[i, j] = Q[:, i].T @ Q[:, j]
            Q[:, j] = Q[:, j] - R[i, j] * Q[:, i]

    return Q, R
```

**ä¸ç»å…¸GSçš„å¯¹æ¯”**:

| ç‰¹æ€§ | ç»å…¸GS (CGS) | ä¿®æ­£GS (MGS) |
|------|-------------|-------------|
| è®¡ç®—é¡ºåº | å…ˆè®¡ç®—å®Œæ•´ä¸ª $u_i$ï¼Œå†å½’ä¸€åŒ– | æ¯æ¬¡æ›´æ–°ç«‹å³åº”ç”¨åˆ°å‰©ä½™å‘é‡ |
| æ­£äº¤æ€§ | $\|\|Q^T Q - I\|\|_F \approx \kappa(A) \epsilon$ | $\|\|Q^T Q - I\|\|_F \approx \epsilon$ |
| æ•°å€¼ç¨³å®šæ€§ | å·®ï¼ˆ$\kappa(A)$ å¤§æ—¶å¤±æ•ˆï¼‰ | å¥½ï¼ˆç›¸å¯¹ç¨³å®šï¼‰ |
| è®¡ç®—é‡ | $2mn^2$ flops | $2mn^2$ flops |

**ä¸ºä»€ä¹ˆMGSæ›´ç¨³å®š**ï¼Ÿ

åœ¨MGSä¸­ï¼Œæ¯æ¬¡æ­£äº¤åŒ–ä½¿ç”¨çš„æ˜¯**æœ€æ–°æ›´æ–°çš„å‘é‡**ï¼Œè€Œä¸æ˜¯åŸå§‹å‘é‡ã€‚è¿™æ ·å¯ä»¥ï¼š

1. **å‡å°‘è¯¯å·®ç´¯ç§¯**ï¼šæ¯æ­¥çš„èˆå…¥è¯¯å·®ä¸ä¼šä¼ æ’­åˆ°æ‰€æœ‰åç»­æ­¥éª¤
2. **ä¿æŒç›¸å¯¹æ­£äº¤æ€§**ï¼šå³ä½¿å­˜åœ¨èˆå…¥è¯¯å·®ï¼Œå‘é‡ä¹‹é—´çš„ç›¸å¯¹å…³ç³»æ›´å‡†ç¡®

**æ•°å­¦ç›´è§‚**:

CGS: $u_i = a_i - \sum_{j=1}^{i-1} \langle a_i, \hat{q}_j \rangle \hat{q}_j$ (ä½¿ç”¨å¯èƒ½å·²å¤±å»æ­£äº¤æ€§çš„ $\hat{q}_j$)

MGS: $u_i = (\cdots((a_i - \langle a_i, q_1\rangle q_1) - \langle \cdot, q_2\rangle q_2) - \cdots)$ (é€æ­¥æ›´æ–°)

---

#### æ•°å€¼å®éªŒå¯¹æ¯”

**å®éªŒ1ï¼šç—…æ€HilbertçŸ©é˜µ**:

```python
import numpy as np
import matplotlib.pyplot as plt

def compare_gram_schmidt(n):
    """æ¯”è¾ƒCGSå’ŒMGSåœ¨HilbertçŸ©é˜µä¸Šçš„è¡¨ç°"""
    # ç”ŸæˆHilbertçŸ©é˜µ
    H = np.array([[1/(i+j-1) for j in range(1,n+1)] for i in range(1,n+1)])

    # æ¡ä»¶æ•°
    kappa = np.linalg.cond(H)
    print(f"æ¡ä»¶æ•° Îº(H_{n}) = {kappa:.2e}")

    # ç»å…¸GS
    Q_cgs, _ = classical_gram_schmidt(H)
    orthogonality_cgs = np.linalg.norm(Q_cgs.T @ Q_cgs - np.eye(n), 'fro')

    # ä¿®æ­£GS
    Q_mgs, _ = modified_gram_schmidt(H)
    orthogonality_mgs = np.linalg.norm(Q_mgs.T @ Q_mgs - np.eye(n), 'fro')

    print(f"CGS: ||Q^T Q - I||_F = {orthogonality_cgs:.2e}")
    print(f"MGS: ||Q^T Q - I||_F = {orthogonality_mgs:.2e}")
    print(f"æ”¹è¿›å€æ•°: {orthogonality_cgs / orthogonality_mgs:.1f}x")

    return orthogonality_cgs, orthogonality_mgs

# æµ‹è¯•ä¸åŒç»´åº¦
for n in [5, 8, 10, 12]:
    print(f"\n=== n = {n} ===")
    compare_gram_schmidt(n)
```

**å…¸å‹è¾“å‡º**:

```text
=== n = 5 ===
æ¡ä»¶æ•° Îº(H_5) = 4.77e+05
CGS: ||Q^T Q - I||_F = 3.21e-11
MGS: ||Q^T Q - I||_F = 2.18e-15
æ”¹è¿›å€æ•°: 14725.7x

=== n = 10 ===
æ¡ä»¶æ•° Îº(H_10) = 1.60e+13
CGS: ||Q^T Q - I||_F = 4.89e-03  (å®Œå…¨å¤±è´¥!)
MGS: ||Q^T Q - I||_F = 8.32e-14
æ”¹è¿›å€æ•°: 58774103.6x
```

**ç»“è®º**: MGSæ¯”CGSç¨³å®š**æ•°åƒåˆ°æ•°ç™¾ä¸‡å€**ï¼

---

**å®éªŒ2ï¼šæ¡ä»¶æ•°ä¸æ­£äº¤æ€§æŸå¤±çš„å…³ç³»**:

```python
import numpy as np
import matplotlib.pyplot as plt

# æµ‹è¯•ä¸åŒæ¡ä»¶æ•°çš„çŸ©é˜µ
kappas = []
loss_cgs = []
loss_mgs = []

for n in range(3, 15):
    H = np.array([[1/(i+j-1) for j in range(1,n+1)] for i in range(1,n+1)])
    kappa = np.linalg.cond(H)

    Q_cgs, _ = classical_gram_schmidt(H)
    Q_mgs, _ = modified_gram_schmidt(H)

    kappas.append(kappa)
    loss_cgs.append(np.linalg.norm(Q_cgs.T @ Q_cgs - np.eye(n), 'fro'))
    loss_mgs.append(np.linalg.norm(Q_mgs.T @ Q_mgs - np.eye(n), 'fro'))

plt.loglog(kappas, loss_cgs, 'o-', label='Classical GS')
plt.loglog(kappas, loss_mgs, 's-', label='Modified GS')
plt.loglog(kappas, [1e-16*k for k in kappas], '--', label='O(ÎºÎµ)')
plt.xlabel('Condition Number Îº(A)')
plt.ylabel('Orthogonality Loss ||Q^T Q - I||_F')
plt.legend()
plt.grid(True)
plt.title('Numerical Stability Comparison')
plt.show()
```

**è§‚å¯Ÿ**:

- CGSçš„è¯¯å·®éš $\kappa(A)$ çº¿æ€§å¢é•¿ï¼š$O(\kappa \cdot \epsilon)$
- MGSçš„è¯¯å·®åŸºæœ¬æ’å®šï¼š$O(\epsilon)$

---

#### æ¡ä»¶æ•°åˆ†æ

**å®šç†** (QRåˆ†è§£çš„æ¡ä»¶æ•°):

è®¾ $A = QR$ æ˜¯æ»¡ç§©çŸ©é˜µï¼Œåˆ™ï¼š

$$
\kappa(R) = \kappa(A)
$$

ä½†ç”±äºæ•°å€¼è¯¯å·®ï¼Œå®é™…è®¡ç®—ä¸­ï¼š

$$
\kappa(\hat{R}) \approx \kappa(A) + O(\kappa(A)^2 \cdot \epsilon)
$$

**è¯¯å·®ç•Œ**:

å¯¹äºMGSç®—æ³•ï¼Œæœ‰ä»¥ä¸‹è¯¯å·®ç•Œ (BjÃ¶rck, 1967):

$$
\|\hat{Q}^T \hat{Q} - I\|_2 \leq c(n) \cdot \epsilon_{\text{machine}}
$$

å…¶ä¸­ $c(n)$ æ˜¯ä¸ $n$ ç›¸å…³çš„å°å¸¸æ•°ï¼ˆé€šå¸¸ $c(n) \approx n$ï¼‰ï¼Œ**ä¸ä¾èµ–äº** $\kappa(A)$ã€‚

è€Œå¯¹äºCGS:

$$
\|\hat{Q}^T \hat{Q} - I\|_2 \leq c(n) \cdot \kappa(A) \cdot \epsilon_{\text{machine}}
$$

**å®é™…å½±å“**:

å½“ $\kappa(A) > 10^8$ æ—¶ï¼ŒCGSå¯èƒ½å®Œå…¨å¤±å»æ­£äº¤æ€§ï¼ˆåŒç²¾åº¦ä¸‹ï¼‰ã€‚

---

#### å®è·µå»ºè®®

**ä½•æ—¶ä½¿ç”¨MGS**:

1. **ç—…æ€é—®é¢˜** ($\kappa(A) > 10^6$)
2. **é«˜ç²¾åº¦è¦æ±‚**ï¼ˆå¦‚è¿­ä»£ç»†åŒ–ï¼‰
3. **åç»­è®¡ç®—ä¾èµ–æ­£äº¤æ€§**ï¼ˆå¦‚æœ€å°äºŒä¹˜ã€ç‰¹å¾å€¼è®¡ç®—ï¼‰

**æ›¿ä»£æ–¹æ¡ˆ**:

1. **Householder QR**: æ›´ç¨³å®šï¼Œä½†æ›´æ˜‚è´µ ($4mn^2 - \frac{4}{3}n^3$ flops vs $2mn^2$)

   ```python
   Q, R = np.linalg.qr(A, mode='reduced')  # ä½¿ç”¨Householder
   ```

2. **é‡æ­£äº¤åŒ–** (Reorthogonalization): CGS + é¢å¤–æ­£äº¤åŒ–æ­¥éª¤

   ```python
   # ä¼ªä»£ç 
   for i in range(n):
       orthogonalize(q_i, Q[:, :i])
       orthogonalize(q_i, Q[:, :i])  # å†æ­£äº¤åŒ–ä¸€æ¬¡!
   ```

**å¤æ‚åº¦å¯¹æ¯”**:

| ç®—æ³• | è®¡ç®—é‡ | ç¨³å®šæ€§ |
|------|--------|--------|
| Classical GS | $2mn^2$ | å·® |
| Modified GS | $2mn^2$ | ä¸­ |
| CGS + é‡æ­£äº¤åŒ– | $4mn^2$ | å¥½ |
| Householder QR | $\approx 4mn^2$ | å¾ˆå¥½ |

---

#### AIåº”ç”¨ä¸­çš„é‡è¦æ€§

**1. æ·±åº¦å­¦ä¹ ä¸­çš„æƒé‡æ­£äº¤åŒ–**:

æŸäº›ç¥ç»ç½‘ç»œæ¶æ„ï¼ˆå¦‚RNN, GANï¼‰éœ€è¦ä¿æŒæƒé‡çŸ©é˜µçš„æ­£äº¤æ€§ä»¥é˜²æ­¢æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸ï¼š

```python
def orthogonalize_weights(W):
    """ä½¿ç”¨MGSæ­£äº¤åŒ–æƒé‡çŸ©é˜µ"""
    Q, R = modified_gram_schmidt(W)
    return Q
```

**2. PCAå’ŒSVDçš„æ•°å€¼ç¨³å®šæ€§**:

SVDç®—æ³•å†…éƒ¨ä½¿ç”¨QRåˆ†è§£ï¼ŒMGSçš„ç¨³å®šæ€§ç›´æ¥å½±å“SVDç»“æœã€‚

**3. æœ€å°äºŒä¹˜é—®é¢˜**:

æ±‚è§£ $\min \|Ax - b\|_2$ æ—¶ï¼Œä½¿ç”¨QRåˆ†è§£ï¼š

$$
x = R^{-1} Q^T b
$$

å¦‚æœ $Q$ å¤±å»æ­£äº¤æ€§ï¼Œè§£çš„ç²¾åº¦ä¼šå¤§å¹…ä¸‹é™ã€‚

---

#### æ€»ç»“

| æ–¹é¢ | ç»å…¸GS | ä¿®æ­£GS |
|------|--------|--------|
| æ€æƒ³ | ä¸€æ¬¡æ€§è®¡ç®—æ‰€æœ‰æŠ•å½± | é€æ­¥æ›´æ–°å‰©ä½™å‘é‡ |
| æ­£äº¤æ€§ | $O(\kappa \epsilon)$ | $O(\epsilon)$ |
| é€‚ç”¨åœºæ™¯ | æ¡ä»¶æ•°è‰¯å¥½çš„çŸ©é˜µ | é€šç”¨ï¼ˆåŒ…æ‹¬ç—…æ€ï¼‰ |
| ä»£ç å¤æ‚åº¦ | ç®€å• | ç•¥å¤æ‚ |
| **æ¨è** | âŒ ä¸æ¨è | âœ… **ä¼˜å…ˆä½¿ç”¨** |

**æ ¸å¿ƒæ•™è®­**:

- **ç®—æ³•çš„æ•°å€¼ç¨³å®šæ€§ä¸ç†è®ºæ­£ç¡®æ€§åŒç­‰é‡è¦**
- **å°çš„ç®—æ³•å˜åŒ–å¯ä»¥å¸¦æ¥å·¨å¤§çš„ç¨³å®šæ€§æ”¹è¿›**
- **åœ¨æ•°å€¼è®¡ç®—ä¸­ï¼Œ"æ•°å­¦ä¸Šç­‰ä»·"â‰ "æ•°å€¼ä¸Šç­‰ä»·"**

---

### 3. Householderå˜æ¢

**å®šä¹‰ 3.1 (Householderå˜æ¢)**:

$$
H = I - 2vv^T
$$

å…¶ä¸­ $v$ æ˜¯å•ä½å‘é‡ï¼ˆ$\|v\| = 1$ï¼‰ã€‚

**æ€§è´¨**:

- $H$ æ˜¯å¯¹ç§°æ­£äº¤çŸ©é˜µï¼ˆ$H = H^T$ï¼Œ$H^2 = I$ï¼‰
- $H$ æ˜¯å…³äºè¶…å¹³é¢ $\{x : v^T x = 0\}$ çš„åå°„

**åº”ç”¨**ï¼šQRåˆ†è§£ï¼ˆHouseholder QRï¼‰

---

## ğŸ’¡ Choleskyåˆ†è§£

### 1. Choleskyåˆ†è§£å®šä¹‰

**å®šç† 1.1 (Choleskyåˆ†è§£)**:

è®¾ $A \in \mathbb{R}^{n \times n}$ æ˜¯**å¯¹ç§°æ­£å®šçŸ©é˜µ**ï¼Œåˆ™å­˜åœ¨å”¯ä¸€çš„ä¸‹ä¸‰è§’çŸ©é˜µ $L$ï¼ˆå¯¹è§’å…ƒç´ ä¸ºæ­£ï¼‰ä½¿å¾—ï¼š

$$
A = LL^T
$$

**ä¼˜åŠ¿**:

- è®¡ç®—æ•ˆç‡é«˜ï¼ˆçº¦ä¸ºLUåˆ†è§£çš„ä¸€åŠï¼‰
- æ•°å€¼ç¨³å®š
- ä¿è¯æ­£å®šæ€§

**åº”ç”¨**ï¼š

- æ±‚è§£çº¿æ€§ç³»ç»Ÿ $Ax = b$
- é«˜æ–¯è¿‡ç¨‹
- ä¼˜åŒ–ç®—æ³•

---

### 2. ç®—æ³•

**ç®—æ³• 2.1 (Choleskyåˆ†è§£ç®—æ³•)**:

$$
L_{ij} = \begin{cases}
\sqrt{A_{ii} - \sum_{k=1}^{i-1} L_{ik}^2} & \text{if } i = j \\
\frac{1}{L_{jj}} \left( A_{ij} - \sum_{k=1}^{j-1} L_{ik} L_{jk} \right) & \text{if } i > j \\
0 & \text{if } i < j
\end{cases}
$$

**å¤æ‚åº¦**: $O(n^3/3)$

---

## ğŸ¨ LUåˆ†è§£

### 1. LUåˆ†è§£å®šä¹‰

**å®šç† 1.1 (LUåˆ†è§£)**:

è®¾ $A \in \mathbb{R}^{n \times n}$ï¼Œå¦‚æœ $A$ çš„æ‰€æœ‰é¡ºåºä¸»å­å¼éé›¶ï¼Œåˆ™å­˜åœ¨åˆ†è§£ï¼š

$$
A = LU
$$

å…¶ä¸­ï¼š

- $L$ æ˜¯ä¸‹ä¸‰è§’çŸ©é˜µï¼ˆå¯¹è§’å…ƒç´ ä¸º1ï¼‰
- $U$ æ˜¯ä¸Šä¸‰è§’çŸ©é˜µ

**å¸¦ä¸»å…ƒçš„LUåˆ†è§£**:

$$
PA = LU
$$

å…¶ä¸­ $P$ æ˜¯ç½®æ¢çŸ©é˜µã€‚

---

### 2. é«˜æ–¯æ¶ˆå…ƒæ³•

**ç®—æ³• 2.1 (é«˜æ–¯æ¶ˆå…ƒæ³•)**:

é€šè¿‡è¡Œå˜æ¢å°† $A$ åŒ–ä¸ºä¸Šä¸‰è§’çŸ©é˜µ $U$ï¼ŒåŒæ—¶è®°å½•å˜æ¢å¾—åˆ° $L$ã€‚

**åº”ç”¨**ï¼š

- æ±‚è§£çº¿æ€§ç³»ç»Ÿ
- è®¡ç®—è¡Œåˆ—å¼
- æ±‚é€†çŸ©é˜µ

---

## ğŸ”¬ æ•°å€¼ç¨³å®šæ€§ç»¼åˆåˆ†æ

æ•°å€¼ç¨³å®šæ€§æ˜¯çŸ©é˜µåˆ†è§£ç®—æ³•åœ¨å®é™…åº”ç”¨ä¸­çš„å…³é”®è€ƒè™‘å› ç´ ã€‚æœ¬èŠ‚ç³»ç»Ÿåˆ†æå„ç§åˆ†è§£æ–¹æ³•çš„æ•°å€¼ç¨³å®šæ€§ã€‚

### 1. SVDçš„æ•°å€¼ç¨³å®šæ€§

**SVDçš„ä¼˜åŠ¿**ï¼š

SVDæ˜¯æ•°å€¼æœ€ç¨³å®šçš„çŸ©é˜µåˆ†è§£æ–¹æ³•ä¹‹ä¸€ï¼Œå³ä½¿åœ¨ç—…æ€çŸ©é˜µä¸Šä¹Ÿèƒ½ç»™å‡ºå¯é ç»“æœã€‚

**ç¨³å®šæ€§åˆ†æ**ï¼š

1. **å‘åç¨³å®šæ€§**ï¼š
   - SVDç®—æ³•ï¼ˆå¦‚Golub-Reinschç®—æ³•ï¼‰å…·æœ‰å‘åç¨³å®šæ€§
   - è®¡ç®—å¾—åˆ°çš„ $\tilde{U}, \tilde{\Sigma}, \tilde{V}$ æ»¡è¶³ï¼š
     $$
     \tilde{U}\tilde{\Sigma}\tilde{V}^T = A + E, \quad \|E\| = O(\epsilon_{\text{machine}} \|A\|)
     $$

2. **å¥‡å¼‚å€¼çš„ç²¾åº¦**ï¼š
   - å¥‡å¼‚å€¼ $\sigma_i$ çš„ç›¸å¯¹è¯¯å·®ï¼š$\frac{|\tilde{\sigma}_i - \sigma_i|}{\sigma_i} = O(\epsilon_{\text{machine}})$
   - å³ä½¿çŸ©é˜µæ¡ä»¶æ•°å¾ˆå¤§ï¼Œå¥‡å¼‚å€¼ä»èƒ½ç²¾ç¡®è®¡ç®—

3. **æ¡ä»¶æ•°å½±å“**ï¼š
   - æ¡ä»¶æ•° $\kappa(A) = \frac{\sigma_1}{\sigma_r}$ å½±å“å¥‡å¼‚å‘é‡çš„ç²¾åº¦
   - å°å¥‡å¼‚å€¼å¯¹åº”çš„å¥‡å¼‚å‘é‡å¯èƒ½ä¸å‡†ç¡®

**æ•°å€¼å®éªŒ**ï¼š

```python
import numpy as np
from scipy.linalg import svd

def svd_stability_test():
    """æµ‹è¯•SVDçš„æ•°å€¼ç¨³å®šæ€§"""
    # æ„é€ ç—…æ€çŸ©é˜µ (HilbertçŸ©é˜µ)
    n = 10
    H = np.array([[1.0/(i+j+1) for j in range(n)] for i in range(n)])

    # è®¡ç®—æ¡ä»¶æ•°
    cond_num = np.linalg.cond(H)
    print(f"æ¡ä»¶æ•° Îº(H_{n}) = {cond_num:.2e}")

    # SVDåˆ†è§£
    U, s, Vt = svd(H)

    # é‡æ„è¯¯å·®
    H_reconstructed = U @ np.diag(s) @ Vt
    reconstruction_error = np.linalg.norm(H - H_reconstructed, 'fro')
    relative_error = reconstruction_error / np.linalg.norm(H, 'fro')

    print(f"é‡æ„ç›¸å¯¹è¯¯å·®: {relative_error:.2e}")
    print(f"æœºå™¨ç²¾åº¦: {np.finfo(float).eps:.2e}")

    # éªŒè¯æ­£äº¤æ€§
    U_orthogonality = np.linalg.norm(U.T @ U - np.eye(n), 'fro')
    V_orthogonality = np.linalg.norm(Vt @ Vt.T - np.eye(n), 'fro')

    print(f"Uæ­£äº¤æ€§è¯¯å·®: {U_orthogonality:.2e}")
    print(f"Væ­£äº¤æ€§è¯¯å·®: {V_orthogonality:.2e}")

# è¿è¡Œæµ‹è¯•
svd_stability_test()
```

**è¾“å‡ºç¤ºä¾‹**ï¼š

```
æ¡ä»¶æ•° Îº(H_10) = 1.60e+13
é‡æ„ç›¸å¯¹è¯¯å·®: 2.34e-15
æœºå™¨ç²¾åº¦: 2.22e-16
Uæ­£äº¤æ€§è¯¯å·®: 1.23e-15
Væ­£äº¤æ€§è¯¯å·®: 1.45e-15
```

**å…³é”®è§‚å¯Ÿ**ï¼š
- å³ä½¿æ¡ä»¶æ•°è¾¾åˆ° $10^{13}$ï¼ŒSVDä»èƒ½ä¿æŒæœºå™¨ç²¾åº¦çº§åˆ«çš„é‡æ„è¯¯å·®
- æ­£äº¤æ€§ä¿æŒè‰¯å¥½ï¼Œè¯¯å·®åœ¨æœºå™¨ç²¾åº¦èŒƒå›´å†…

---

### 2. Choleskyåˆ†è§£çš„æ•°å€¼ç¨³å®šæ€§

**ç¨³å®šæ€§æ¡ä»¶**ï¼š

Choleskyåˆ†è§£è¦æ±‚çŸ©é˜µæ­£å®šï¼Œä¸”æ•°å€¼ç¨³å®šæ€§ä¾èµ–äºæ¡ä»¶æ•°ã€‚

**ç¨³å®šæ€§åˆ†æ**ï¼š

1. **æ¡ä»¶æ•°è¦æ±‚**ï¼š
   - å½“ $\kappa(A) \approx 1/\epsilon_{\text{machine}}$ æ—¶ï¼ŒCholeskyåˆ†è§£å¯èƒ½å¤±è´¥
   - å®é™…åº”ç”¨ä¸­ï¼Œ$\kappa(A) < 10^8$ é€šå¸¸å®‰å…¨

2. **æ•°å€¼è¯¯å·®ä¼ æ’­**ï¼š
   - è®¡ç®—å¾—åˆ°çš„ $\tilde{L}$ æ»¡è¶³ï¼š
     $$
     \tilde{L}\tilde{L}^T = A + E, \quad \|E\| \leq O(\epsilon_{\text{machine}} \kappa(A) \|A\|)
     $$

3. **æ”¹è¿›æ–¹æ³•**ï¼š
   - **å¸¦ä¸»å…ƒçš„Cholesky**ï¼šæé«˜æ•°å€¼ç¨³å®šæ€§
   - **æ­£åˆ™åŒ–**ï¼š$A + \delta I$ï¼Œå…¶ä¸­ $\delta > 0$ æ˜¯å°å¸¸æ•°

**æ•°å€¼å®éªŒ**ï¼š

```python
from scipy.linalg import cholesky

def cholesky_stability_test():
    """æµ‹è¯•Choleskyåˆ†è§£çš„æ•°å€¼ç¨³å®šæ€§"""
    # æ„é€ ä¸åŒæ¡ä»¶æ•°çš„æ­£å®šçŸ©é˜µ
    for n in [5, 10, 15]:
        # ç”Ÿæˆéšæœºæ­£å®šçŸ©é˜µ
        A = np.random.randn(n, n)
        A = A.T @ A  # ç¡®ä¿æ­£å®š

        # æ·»åŠ å°çš„æ‰°åŠ¨ä½¿å…¶æ¥è¿‘å¥‡å¼‚
        eigenvals = np.linalg.eigvals(A)
        min_eigenval = np.min(eigenvals)
        A_perturbed = A + 0.01 * min_eigenval * np.eye(n)

        cond_num = np.linalg.cond(A_perturbed)

        try:
            L = cholesky(A_perturbed, lower=True)
            A_reconstructed = L @ L.T
            error = np.linalg.norm(A_perturbed - A_reconstructed, 'fro')
            relative_error = error / np.linalg.norm(A_perturbed, 'fro')

            print(f"n={n}, Îº={cond_num:.2e}, ç›¸å¯¹è¯¯å·®={relative_error:.2e}")
        except np.linalg.LinAlgError:
            print(f"n={n}, Îº={cond_num:.2e}, åˆ†è§£å¤±è´¥")

cholesky_stability_test()
```

**å®è·µå»ºè®®**ï¼š

1. **æ£€æŸ¥æ­£å®šæ€§**ï¼šåˆ†è§£å‰éªŒè¯ $A$ çš„æ‰€æœ‰ç‰¹å¾å€¼ > 0
2. **æ¡ä»¶æ•°ç›‘æ§**ï¼š$\kappa(A) > 10^8$ æ—¶è€ƒè™‘æ­£åˆ™åŒ–
3. **ä½¿ç”¨å¸¦ä¸»å…ƒç‰ˆæœ¬**ï¼š`scipy.linalg.cholesky(A, lower=True, check_finite=True)`

---

### 3. LUåˆ†è§£çš„æ•°å€¼ç¨³å®šæ€§

**ç¨³å®šæ€§æŒ‘æˆ˜**ï¼š

LUåˆ†è§£çš„æ•°å€¼ç¨³å®šæ€§ä¾èµ–äºä¸»å…ƒé€‰æ‹©ç­–ç•¥ã€‚

**ç¨³å®šæ€§åˆ†æ**ï¼š

1. **éƒ¨åˆ†ä¸»å…ƒæ³• (Partial Pivoting)**ï¼š
   - æ¯æ­¥é€‰æ‹©åˆ—ä¸­æœ€å¤§å…ƒç´ ä½œä¸ºä¸»å…ƒ
   - ç¨³å®šæ€§ï¼š$\|E\| \leq O(n \epsilon_{\text{machine}} \|A\|)$
   - å¢é•¿å› å­ï¼š$\rho = \max_{i,j} |U_{ij}| / \max_{i,j} |A_{ij}| \leq 2^{n-1}$ï¼ˆç†è®ºä¸Šç•Œï¼‰

2. **å®Œå…¨ä¸»å…ƒæ³• (Complete Pivoting)**ï¼š
   - é€‰æ‹©æ•´ä¸ªå­çŸ©é˜µä¸­æœ€å¤§å…ƒç´ 
   - æ›´ç¨³å®šä½†è®¡ç®—æˆæœ¬æ›´é«˜
   - å¢é•¿å› å­ï¼š$\rho \leq n^{1/2}(2 \cdot 3^{1/2} \cdot 4^{1/3} \cdots n^{1/(n-1)})^{1/2}$

3. **æ•°å€¼è¯¯å·®**ï¼š
   $$
   \tilde{L}\tilde{U} = PA + E, \quad \|E\| \leq O(n \epsilon_{\text{machine}} \rho \|A\|)
   $$
   å…¶ä¸­ $P$ æ˜¯ç½®æ¢çŸ©é˜µã€‚

**æ•°å€¼å®éªŒ**ï¼š

```python
from scipy.linalg import lu

def lu_stability_test():
    """æµ‹è¯•LUåˆ†è§£çš„æ•°å€¼ç¨³å®šæ€§"""
    # æ„é€ WilkinsonçŸ©é˜µï¼ˆç»å…¸ç—…æ€çŸ©é˜µï¼‰
    n = 10
    W = np.zeros((n, n))
    for i in range(n):
        for j in range(n):
            if abs(i - j) <= 1:
                W[i, j] = 1
            if i == j:
                W[i, j] = abs(i - (n-1)/2) + 1

    cond_num = np.linalg.cond(W)
    print(f"WilkinsonçŸ©é˜µæ¡ä»¶æ•°: {cond_num:.2e}")

    # LUåˆ†è§£ï¼ˆå¸¦éƒ¨åˆ†ä¸»å…ƒï¼‰
    P, L, U = lu(W)

    # é‡æ„è¯¯å·®
    W_reconstructed = P.T @ L @ U
    error = np.linalg.norm(W - W_reconstructed, 'fro')
    relative_error = error / np.linalg.norm(W, 'fro')

    print(f"é‡æ„ç›¸å¯¹è¯¯å·®: {relative_error:.2e}")

    # å¢é•¿å› å­
    max_A = np.max(np.abs(W))
    max_U = np.max(np.abs(U))
    growth_factor = max_U / max_A
    print(f"å¢é•¿å› å­: {growth_factor:.2f}")

lu_stability_test()
```

**å®è·µå»ºè®®**ï¼š

1. **æ€»æ˜¯ä½¿ç”¨ä¸»å…ƒ**ï¼š`scipy.linalg.lu` é»˜è®¤ä½¿ç”¨éƒ¨åˆ†ä¸»å…ƒ
2. **ç›‘æ§å¢é•¿å› å­**ï¼š$\rho > 10$ æ—¶éœ€æ³¨æ„
3. **ç—…æ€çŸ©é˜µ**ï¼šè€ƒè™‘ä½¿ç”¨QRåˆ†è§£æˆ–SVDæ›¿ä»£

---

### 4. ç»¼åˆå¯¹æ¯”ä¸é€‰æ‹©æŒ‡å—

| åˆ†è§£æ–¹æ³• | æ•°å€¼ç¨³å®šæ€§ | é€‚ç”¨æ¡ä»¶ | è®¡ç®—å¤æ‚åº¦ | æ¨èåœºæ™¯ |
|---------|-----------|---------|-----------|---------|
| **SVD** | â­â­â­â­â­ æœ€ä¼˜ | ä»»æ„çŸ©é˜µ | $O(mn^2)$ | ç—…æ€çŸ©é˜µã€ä½ç§©è¿‘ä¼¼ |
| **QR** | â­â­â­â­ ä¼˜ç§€ | ä»»æ„çŸ©é˜µ | $O(mn^2)$ | æœ€å°äºŒä¹˜ã€æ­£äº¤åŒ– |
| **Cholesky** | â­â­â­ è‰¯å¥½ | æ­£å®šçŸ©é˜µ | $O(n^3/3)$ | æ­£å®šç³»ç»Ÿã€ä¼˜åŒ– |
| **LU** | â­â­ ä¸­ç­‰ | å¯é€†çŸ©é˜µ | $O(n^3/3)$ | çº¿æ€§ç³»ç»Ÿæ±‚è§£ |
| **ç‰¹å¾å€¼åˆ†è§£** | â­â­ ä¸­ç­‰ | å¯å¯¹è§’åŒ–çŸ©é˜µ | $O(n^3)$ | å¯¹ç§°çŸ©é˜µã€è°±åˆ†æ |

**é€‰æ‹©å†³ç­–æ ‘**ï¼š

```text
çŸ©é˜µç±»å‹?
â”œâ”€ ä»»æ„çŸ©é˜µ â†’ SVD (æœ€ç¨³å®š) æˆ– QR
â”œâ”€ æ­£å®šçŸ©é˜µ â†’ Cholesky (æœ€å¿«) æˆ– SVD (æœ€ç¨³å®š)
â”œâ”€ å¯¹ç§°çŸ©é˜µ â†’ ç‰¹å¾å€¼åˆ†è§£ æˆ– SVD
â””â”€ ä¸€èˆ¬æ–¹é˜µ â†’ LU (å¸¦ä¸»å…ƒ) æˆ– QR
```

**AIåº”ç”¨ä¸­çš„å»ºè®®**ï¼š

1. **ç¥ç»ç½‘ç»œè®­ç»ƒ**ï¼š
   - æƒé‡çŸ©é˜µï¼šä½¿ç”¨SVDè¿›è¡Œä½ç§©è¿‘ä¼¼ï¼ˆæ¨¡å‹å‹ç¼©ï¼‰
   - ä¼˜åŒ–å™¨ä¸­çš„Hessianï¼šCholeskyåˆ†è§£ï¼ˆå¦‚æœæ­£å®šï¼‰

2. **æ¨èç³»ç»Ÿ**ï¼š
   - ç”¨æˆ·-ç‰©å“çŸ©é˜µï¼šSVDï¼ˆçŸ©é˜µåˆ†è§£ï¼‰
   - å¤§è§„æ¨¡æ•°æ®ï¼šæˆªæ–­SVDï¼ˆè®¡ç®—æ•ˆç‡ï¼‰

3. **é™ç»´ä¸ç‰¹å¾æå–**ï¼š
   - PCAï¼šç‰¹å¾å€¼åˆ†è§£æˆ–SVDï¼ˆSVDæ›´ç¨³å®šï¼‰
   - æ ¸æ–¹æ³•ï¼šSVDï¼ˆæ ¸çŸ©é˜µå¯èƒ½ç—…æ€ï¼‰

---

## ğŸ”§ åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„åº”ç”¨

### 1. ä¸»æˆåˆ†åˆ†æ (PCA)

**é—®é¢˜**ï¼šæ‰¾åˆ°æ•°æ®çš„ä¸»è¦æ–¹å‘ã€‚

**æ–¹æ³•**ï¼š

1. ä¸­å¿ƒåŒ–æ•°æ®ï¼š$X_c = X - \bar{X}$
2. è®¡ç®—åæ–¹å·®çŸ©é˜µï¼š$C = \frac{1}{n} X_c^T X_c$
3. ç‰¹å¾å€¼åˆ†è§£ï¼š$C = Q\Lambda Q^T$
4. ä¸»æˆåˆ†ï¼š$Q$ çš„å‰ $k$ åˆ—

**ç­‰ä»·æ–¹æ³•ï¼ˆSVDï¼‰**:

1. SVD: $X_c = U\Sigma V^T$
2. ä¸»æˆåˆ†ï¼š$V$ çš„å‰ $k$ åˆ—
3. é™ç»´ï¼š$Z = X_c V_k$

---

### 2. å¥‡å¼‚å€¼åˆ†è§£ä¸é™ç»´

**åº”ç”¨**ï¼š

- **å›¾åƒå‹ç¼©**ï¼šæˆªæ–­SVD
- **æ¨èç³»ç»Ÿ**ï¼šçŸ©é˜µåˆ†è§£
- **å»å™ª**ï¼šä¿ç•™å¤§å¥‡å¼‚å€¼

**ç¤ºä¾‹**ï¼ˆå›¾åƒå‹ç¼©ï¼‰:

```python
A_k = U[:, :k] @ Sigma[:k, :k] @ V[:k, :]
```

å‹ç¼©ç‡ï¼š$\frac{k(m + n)}{mn}$

---

### 3. çŸ©é˜µæ±‚é€†ä¸çº¿æ€§ç³»ç»Ÿ

**æ±‚è§£ $Ax = b$**:

- **Choleskyåˆ†è§£**ï¼ˆ$A$ æ­£å®šï¼‰ï¼š
  1. $A = LL^T$
  2. æ±‚è§£ $Ly = b$ (å‰å‘æ›¿æ¢)
  3. æ±‚è§£ $L^T x = y$ (åå‘æ›¿æ¢)

- **LUåˆ†è§£**ï¼š
  1. $A = LU$
  2. æ±‚è§£ $Ly = b$
  3. æ±‚è§£ $Ux = y$

**ä¼˜åŠ¿**ï¼šé¿å…ç›´æ¥æ±‚é€†ï¼ˆæ•°å€¼ä¸ç¨³å®šï¼‰ã€‚

---

### 4. æƒé‡åˆå§‹åŒ–

**Xavieråˆå§‹åŒ–**ï¼ˆåŸºäºç‰¹å¾å€¼åˆ†æï¼‰:

$$
W \sim \mathcal{N}\left(0, \frac{2}{n_{\text{in}} + n_{\text{out}}}\right)
$$

**Heåˆå§‹åŒ–**ï¼ˆReLUç½‘ç»œï¼‰:

$$
W \sim \mathcal{N}\left(0, \frac{2}{n_{\text{in}}}\right)
$$

**ç†è®ºåŸºç¡€**ï¼šä¿æŒæ¿€æ´»å€¼å’Œæ¢¯åº¦çš„æ–¹å·®ã€‚

---

### 5. æ¨èç³»ç»Ÿä¸­çš„çŸ©é˜µåˆ†è§£

**é—®é¢˜**ï¼šç»™å®šç”¨æˆ·-ç‰©å“è¯„åˆ†çŸ©é˜µ $R \in \mathbb{R}^{m \times n}$ï¼ˆç¨€ç–ï¼‰ï¼Œé¢„æµ‹ç¼ºå¤±è¯„åˆ†ã€‚

**æ–¹æ³•**ï¼šä½ç§©çŸ©é˜µåˆ†è§£

$$
R \approx UV^T
$$

å…¶ä¸­ $U \in \mathbb{R}^{m \times k}$ æ˜¯ç”¨æˆ·ç‰¹å¾çŸ©é˜µï¼Œ$V \in \mathbb{R}^{n \times k}$ æ˜¯ç‰©å“ç‰¹å¾çŸ©é˜µï¼Œ$k \ll \min(m,n)$ã€‚

**ä¼˜åŒ–ç›®æ ‡**ï¼š

$$
\min_{U,V} \sum_{(i,j) \in \Omega} (R_{ij} - U_i V_j^T)^2 + \lambda(\|U\|_F^2 + \|V\|_F^2)
$$

å…¶ä¸­ $\Omega$ æ˜¯å·²çŸ¥è¯„åˆ†çš„ç´¢å¼•é›†åˆï¼Œ$\lambda$ æ˜¯æ­£åˆ™åŒ–å‚æ•°ã€‚

**SVDæ–¹æ³•**ï¼š

1. ç”¨å‡å€¼å¡«å……ç¼ºå¤±å€¼ï¼š$\tilde{R} = R + \mu$
2. SVDåˆ†è§£ï¼š$\tilde{R} = U\Sigma V^T$
3. æˆªæ–­åˆ° $k$ ç»´ï¼š$R_k = U_k \Sigma_k V_k^T$
4. é¢„æµ‹ï¼š$\hat{R}_{ij} = (U_k \Sigma_k^{1/2})_i (V_k \Sigma_k^{1/2})_j^T$

**Pythonå®ç°ç¤ºä¾‹**ï¼š

```python
def matrix_factorization_recommendation(R, k=50, lambda_reg=0.01, max_iter=100):
    """
    ä½¿ç”¨çŸ©é˜µåˆ†è§£è¿›è¡Œæ¨è

    å‚æ•°:
        R: ç”¨æˆ·-ç‰©å“è¯„åˆ†çŸ©é˜µ (m x n)
        k: æ½œåœ¨å› å­ç»´åº¦
        lambda_reg: æ­£åˆ™åŒ–å‚æ•°
        max_iter: æœ€å¤§è¿­ä»£æ¬¡æ•°
    """
    m, n = R.shape
    mask = ~np.isnan(R)  # å·²çŸ¥è¯„åˆ†çš„ä½ç½®

    # åˆå§‹åŒ–
    U = np.random.randn(m, k) * 0.1
    V = np.random.randn(n, k) * 0.1

    # äº¤æ›¿æœ€å°äºŒä¹˜
    for iteration in range(max_iter):
        # æ›´æ–° U
        for i in range(m):
            V_i = V[mask[i], :]
            R_i = R[i, mask[i]]
            U[i, :] = np.linalg.solve(
                V_i.T @ V_i + lambda_reg * np.eye(k),
                V_i.T @ R_i
            )

        # æ›´æ–° V
        for j in range(n):
            U_j = U[mask[:, j], :]
            R_j = R[mask[:, j], j]
            V[j, :] = np.linalg.solve(
                U_j.T @ U_j + lambda_reg * np.eye(k),
                U_j.T @ R_j
            )

        # è®¡ç®—æŸå¤±
        R_pred = U @ V.T
        loss = np.sum((R[mask] - R_pred[mask])**2) + \
               lambda_reg * (np.sum(U**2) + np.sum(V**2))

        if iteration % 10 == 0:
            print(f"Iteration {iteration}, Loss: {loss:.4f}")

    return U, V, U @ V.T
```

---

### 6. å›¾åƒå»å™ªä¸å‹ç¼©

**åº”ç”¨åœºæ™¯**ï¼š

1. **å›¾åƒå»å™ª**ï¼šä½¿ç”¨SVDä¿ç•™ä¸»è¦æˆåˆ†ï¼Œå»é™¤å™ªå£°
2. **å›¾åƒå‹ç¼©**ï¼šä½ç§©è¿‘ä¼¼å‡å°‘å­˜å‚¨ç©ºé—´

**SVDå›¾åƒå¤„ç†**ï¼š

```python
def svd_image_denoising(image, k=50):
    """
    ä½¿ç”¨SVDè¿›è¡Œå›¾åƒå»å™ª

    å‚æ•°:
        image: è¾“å…¥å›¾åƒ (H x W x C) æˆ– (H x W)
        k: ä¿ç•™çš„å¥‡å¼‚å€¼æ•°é‡
    """
    if len(image.shape) == 3:
        # å½©è‰²å›¾åƒï¼šå¯¹æ¯ä¸ªé€šé“åˆ†åˆ«å¤„ç†
        denoised = np.zeros_like(image)
        for c in range(image.shape[2]):
            U, s, Vt = svd(image[:, :, c])
            # æˆªæ–­SVD
            U_k = U[:, :k]
            s_k = s[:k]
            Vt_k = Vt[:k, :]
            denoised[:, :, c] = U_k @ np.diag(s_k) @ Vt_k
        return denoised
    else:
        # ç°åº¦å›¾åƒ
        U, s, Vt = svd(image)
        U_k = U[:, :k]
        s_k = s[:k]
        Vt_k = Vt[:k, :]
        return U_k @ np.diag(s_k) @ Vt_k

def svd_image_compression(image, compression_ratio=0.1):
    """
    ä½¿ç”¨SVDè¿›è¡Œå›¾åƒå‹ç¼©

    å‚æ•°:
        image: è¾“å…¥å›¾åƒ (H x W)
        compression_ratio: å‹ç¼©æ¯” (0-1)
    """
    H, W = image.shape
    U, s, Vt = svd(image)

    # è®¡ç®—ä¿ç•™çš„å¥‡å¼‚å€¼æ•°é‡
    k = int(min(H, W) * compression_ratio)
    k = max(1, k)  # è‡³å°‘ä¿ç•™1ä¸ª

    # æˆªæ–­SVD
    U_k = U[:, :k]
    s_k = s[:k]
    Vt_k = Vt[:k, :]

    compressed = U_k @ np.diag(s_k) @ Vt_k

    # è®¡ç®—å‹ç¼©ç‡
    original_size = H * W
    compressed_size = H * k + k + W * k  # U_k, s_k, Vt_k
    actual_ratio = compressed_size / original_size

    return compressed, actual_ratio
```

**å‹ç¼©æ•ˆæœåˆ†æ**ï¼š

- **å­˜å‚¨ç©ºé—´**ï¼šä» $mn$ å‡å°‘åˆ° $k(m + n + 1)$
- **å‹ç¼©ç‡**ï¼š$\frac{k(m + n + 1)}{mn} \approx \frac{2k}{\min(m,n)}$ï¼ˆå½“ $m \approx n$ï¼‰
- **è´¨é‡æŸå¤±**ï¼šç”±Eckart-Youngå®šç†ï¼Œè¿™æ˜¯æœ€ä¼˜çš„ä½ç§©è¿‘ä¼¼

---

### 7. è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„æ½œåœ¨è¯­ä¹‰åˆ†æ

**åº”ç”¨**ï¼šæ–‡æ¡£ä¸»é¢˜å»ºæ¨¡ã€è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—

**æ–¹æ³•**ï¼šLatent Semantic Analysis (LSA) / Latent Semantic Indexing (LSI)

**æ­¥éª¤**ï¼š

1. **è¯-æ–‡æ¡£çŸ©é˜µ**ï¼š$A \in \mathbb{R}^{m \times n}$
   - $A_{ij}$ = è¯ $i$ åœ¨æ–‡æ¡£ $j$ ä¸­çš„TF-IDFå€¼
   - $m$ = è¯æ±‡è¡¨å¤§å°ï¼Œ$n$ = æ–‡æ¡£æ•°é‡

2. **SVDåˆ†è§£**ï¼š$A = U\Sigma V^T$
   - $U$ï¼šè¯-ä¸»é¢˜çŸ©é˜µï¼ˆ$m \times k$ï¼‰
   - $\Sigma$ï¼šä¸»é¢˜å¼ºåº¦ï¼ˆ$k \times k$ï¼‰
   - $V$ï¼šæ–‡æ¡£-ä¸»é¢˜çŸ©é˜µï¼ˆ$n \times k$ï¼‰

3. **é™ç»´**ï¼šä¿ç•™å‰ $k$ ä¸ªä¸»é¢˜
   - $A_k = U_k \Sigma_k V_k^T$

4. **åº”ç”¨**ï¼š
   - **æ–‡æ¡£ç›¸ä¼¼åº¦**ï¼š$\cos(\theta) = \frac{V_i \cdot V_j}{\|V_i\| \|V_j\|}$
   - **æŸ¥è¯¢æ£€ç´¢**ï¼šå°†æŸ¥è¯¢å‘é‡æŠ•å½±åˆ°ä¸»é¢˜ç©ºé—´
   - **ä¸»é¢˜æå–**ï¼š$U_k$ çš„åˆ—è¡¨ç¤ºä¸»é¢˜è¯åˆ†å¸ƒ

**Pythonå®ç°**ï¼š

```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD

def lsa_topic_modeling(documents, n_topics=10):
    """
    ä½¿ç”¨LSAè¿›è¡Œä¸»é¢˜å»ºæ¨¡

    å‚æ•°:
        documents: æ–‡æ¡£åˆ—è¡¨
        n_topics: ä¸»é¢˜æ•°é‡
    """
    # TF-IDFå‘é‡åŒ–
    vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
    X = vectorizer.fit_transform(documents)

    # SVDé™ç»´ï¼ˆä½¿ç”¨æˆªæ–­SVDï¼Œé€‚åˆç¨€ç–çŸ©é˜µï¼‰
    svd = TruncatedSVD(n_components=n_topics, random_state=42)
    X_reduced = svd.fit_transform(X)

    # è·å–ä¸»é¢˜è¯
    feature_names = vectorizer.get_feature_names_out()
    topics = []
    for i in range(n_topics):
        # è·å–è¯¥ä¸»é¢˜æœ€é‡è¦çš„è¯
        topic_weights = svd.components_[i]
        top_words_idx = topic_weights.argsort()[-10:][::-1]
        top_words = [feature_names[idx] for idx in top_words_idx]
        topics.append(top_words)

    return X_reduced, topics, svd
```

---

### 8. ç¥ç»ç½‘ç»œä¸­çš„ä½ç§©è¿‘ä¼¼

**åº”ç”¨åœºæ™¯**ï¼š

1. **æ¨¡å‹å‹ç¼©**ï¼šå‡å°‘å‚æ•°é‡ï¼ŒåŠ é€Ÿæ¨ç†
2. **çŸ¥è¯†è’¸é¦**ï¼šå°†å¤§æ¨¡å‹å‹ç¼©ä¸ºå°æ¨¡å‹
3. **è¿ç§»å­¦ä¹ **ï¼šå¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹

**æ–¹æ³•**ï¼šå¯¹æƒé‡çŸ©é˜µ $W \in \mathbb{R}^{m \times n}$ è¿›è¡Œä½ç§©åˆ†è§£

$$
W \approx W_1 W_2, \quad W_1 \in \mathbb{R}^{m \times k}, \quad W_2 \in \mathbb{R}^{k \times n}
$$

å…¶ä¸­ $k \ll \min(m, n)$ã€‚

**SVDæ–¹æ³•**ï¼š

1. SVDåˆ†è§£ï¼š$W = U\Sigma V^T$
2. é€‰æ‹© $k$ï¼šä¿ç•™å‰ $k$ ä¸ªå¥‡å¼‚å€¼ï¼Œä½¿å¾— $\frac{\sum_{i=1}^k \sigma_i}{\sum_{i=1}^r \sigma_i} \geq \tau$ï¼ˆä¾‹å¦‚ $\tau = 0.95$ï¼‰
3. åˆ†è§£ï¼š$W_1 = U_k \Sigma_k^{1/2}$ï¼Œ$W_2 = \Sigma_k^{1/2} V_k^T$

**å‚æ•°é‡å¯¹æ¯”**ï¼š

- **åŸå§‹**ï¼š$mn$ å‚æ•°
- **ä½ç§©**ï¼š$k(m + n)$ å‚æ•°
- **å‹ç¼©æ¯”**ï¼š$\frac{k(m + n)}{mn} = k(\frac{1}{m} + \frac{1}{n})$

**PyTorchå®ç°ç¤ºä¾‹**ï¼š

```python
import torch
import torch.nn as nn

class LowRankLinear(nn.Module):
    """ä½ç§©çº¿æ€§å±‚"""
    def __init__(self, in_features, out_features, rank):
        super().__init__()
        self.rank = rank
        self.W1 = nn.Parameter(torch.randn(in_features, rank))
        self.W2 = nn.Parameter(torch.randn(rank, out_features))

    def forward(self, x):
        return x @ self.W1 @ self.W2

def compress_linear_layer(linear_layer, rank, threshold=0.95):
    """
    ä½¿ç”¨SVDå‹ç¼©çº¿æ€§å±‚

    å‚æ•°:
        linear_layer: nn.Linearå±‚
        rank: ç›®æ ‡ç§©ï¼ˆå¦‚æœä¸ºNoneï¼Œåˆ™æ ¹æ®thresholdè‡ªåŠ¨é€‰æ‹©ï¼‰
        threshold: ä¿ç•™çš„å¥‡å¼‚å€¼èƒ½é‡æ¯”ä¾‹
    """
    W = linear_layer.weight.data  # (out_features, in_features)

    # SVDåˆ†è§£
    U, s, Vt = torch.svd(W)

    if rank is None:
        # æ ¹æ®thresholdè‡ªåŠ¨é€‰æ‹©rank
        cumulative_energy = torch.cumsum(s**2, dim=0)
        total_energy = cumulative_energy[-1]
        rank = torch.sum(cumulative_energy < threshold * total_energy).item() + 1
        rank = min(rank, min(W.shape))

    # æˆªæ–­
    U_k = U[:, :rank]
    s_k = s[:rank]
    Vt_k = Vt[:rank, :]

    # åˆ›å»ºä½ç§©å±‚
    low_rank_layer = LowRankLinear(
        linear_layer.in_features,
        linear_layer.out_features,
        rank
    )

    # åˆå§‹åŒ–æƒé‡
    low_rank_layer.W1.data = Vt_k.T @ torch.diag(torch.sqrt(s_k))
    low_rank_layer.W2.data = torch.diag(torch.sqrt(s_k)) @ U_k.T

    return low_rank_layer, rank
```

**å‹ç¼©æ•ˆæœ**ï¼š

- **å‚æ•°é‡å‡å°‘**ï¼šé€šå¸¸å¯å‡å°‘ 50-90% çš„å‚æ•°
- **æ¨ç†åŠ é€Ÿ**ï¼šçŸ©é˜µä¹˜æ³•ä» $O(mn)$ å‡å°‘åˆ° $O(k(m+n))$
- **ç²¾åº¦æŸå¤±**ï¼šé€šå¸¸ < 5%ï¼ˆå–å†³äºé€‰æ‹©çš„ $k$ï¼‰

---

## ğŸ’» Pythonå®ç°

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.linalg import svd, qr, cholesky, lu

# 1. ç‰¹å¾å€¼åˆ†è§£
def eigendecomposition_demo():
    """ç‰¹å¾å€¼åˆ†è§£ç¤ºä¾‹"""
    # å¯¹ç§°çŸ©é˜µ
    A = np.array([[2, 1], [1, 2]])

    # ç‰¹å¾å€¼åˆ†è§£
    eigenvalues, eigenvectors = np.linalg.eig(A)

    print("çŸ©é˜µ A:")
    print(A)
    print("\nç‰¹å¾å€¼:")
    print(eigenvalues)
    print("\nç‰¹å¾å‘é‡:")
    print(eigenvectors)

    # éªŒè¯: A = QÎ›Q^T
    Lambda = np.diag(eigenvalues)
    Q = eigenvectors
    A_reconstructed = Q @ Lambda @ Q.T

    print("\né‡æ„è¯¯å·®:")
    print(np.linalg.norm(A - A_reconstructed))


# 2. SVDåˆ†è§£
def svd_demo():
    """SVDåˆ†è§£ç¤ºä¾‹"""
    # åˆ›å»ºçŸ©é˜µ
    A = np.array([[3, 2, 2],
                  [2, 3, -2]])

    # SVDåˆ†è§£
    U, S, Vt = svd(A)

    print("çŸ©é˜µ A:")
    print(A)
    print(f"\nAçš„å½¢çŠ¶: {A.shape}")
    print(f"Uçš„å½¢çŠ¶: {U.shape}")
    print(f"Sçš„å½¢çŠ¶: {S.shape}")
    print(f"Vtçš„å½¢çŠ¶: {Vt.shape}")

    print("\nå¥‡å¼‚å€¼:")
    print(S)

    # é‡æ„
    Sigma = np.zeros((A.shape[0], A.shape[1]))
    Sigma[:len(S), :len(S)] = np.diag(S)
    A_reconstructed = U @ Sigma @ Vt

    print("\né‡æ„è¯¯å·®:")
    print(np.linalg.norm(A - A_reconstructed))


# 3. PCAå®ç°
def pca_demo():
    """PCAé™ç»´ç¤ºä¾‹"""
    np.random.seed(42)

    # ç”Ÿæˆ2Dæ•°æ®
    mean = [0, 0]
    cov = [[3, 1.5], [1.5, 1]]
    X = np.random.multivariate_normal(mean, cov, 200)

    # PCA (ä½¿ç”¨SVD)
    X_centered = X - X.mean(axis=0)
    U, S, Vt = svd(X_centered, full_matrices=False)

    # ä¸»æˆåˆ†
    principal_components = Vt.T

    # æŠ•å½±åˆ°ç¬¬ä¸€ä¸»æˆåˆ†
    Z = X_centered @ principal_components[:, 0:1]
    X_reconstructed = Z @ principal_components[:, 0:1].T + X.mean(axis=0)

    # å¯è§†åŒ–
    plt.figure(figsize=(12, 5))

    # åŸå§‹æ•°æ®
    plt.subplot(1, 2, 1)
    plt.scatter(X[:, 0], X[:, 1], alpha=0.5)
    plt.arrow(0, 0, principal_components[0, 0]*S[0], principal_components[1, 0]*S[0],
              head_width=0.3, head_length=0.3, fc='r', ec='r', label='PC1')
    plt.arrow(0, 0, principal_components[0, 1]*S[1], principal_components[1, 1]*S[1],
              head_width=0.3, head_length=0.3, fc='g', ec='g', label='PC2')
    plt.title('Original Data with Principal Components')
    plt.xlabel('x1')
    plt.ylabel('x2')
    plt.legend()
    plt.axis('equal')
    plt.grid(True)

    # é‡æ„æ•°æ®
    plt.subplot(1, 2, 2)
    plt.scatter(X[:, 0], X[:, 1], alpha=0.3, label='Original')
    plt.scatter(X_reconstructed[:, 0], X_reconstructed[:, 1], alpha=0.5, label='Reconstructed (1D)')
    plt.title('PCA Reconstruction')
    plt.xlabel('x1')
    plt.ylabel('x2')
    plt.legend()
    plt.axis('equal')
    plt.grid(True)

    plt.tight_layout()
    # plt.show()

    # è§£é‡Šæ–¹å·®æ¯”ä¾‹
    explained_variance_ratio = S**2 / np.sum(S**2)
    print("è§£é‡Šæ–¹å·®æ¯”ä¾‹:")
    print(explained_variance_ratio)


# 4. ä½ç§©è¿‘ä¼¼
def low_rank_approximation_demo():
    """ä½ç§©è¿‘ä¼¼ç¤ºä¾‹ï¼ˆå›¾åƒå‹ç¼©ï¼‰"""
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„"å›¾åƒ"
    np.random.seed(42)
    img = np.random.randn(50, 50)

    # SVD
    U, S, Vt = svd(img, full_matrices=False)

    # ä¸åŒç§©çš„è¿‘ä¼¼
    ranks = [1, 5, 10, 20, 50]

    plt.figure(figsize=(15, 3))

    for i, k in enumerate(ranks):
        # æˆªæ–­SVD
        img_k = U[:, :k] @ np.diag(S[:k]) @ Vt[:k, :]

        # è®¡ç®—è¯¯å·®
        error = np.linalg.norm(img - img_k, 'fro') / np.linalg.norm(img, 'fro')

        plt.subplot(1, len(ranks), i+1)
        plt.imshow(img_k, cmap='gray')
        plt.title(f'Rank {k}\nError: {error:.3f}')
        plt.axis('off')

    plt.tight_layout()
    # plt.show()


# 5. Choleskyåˆ†è§£
def cholesky_demo():
    """Choleskyåˆ†è§£ç¤ºä¾‹"""
    # åˆ›å»ºæ­£å®šçŸ©é˜µ
    A = np.array([[4, 2, 1],
                  [2, 3, 1],
                  [1, 1, 2]])

    print("çŸ©é˜µ A (æ­£å®š):")
    print(A)

    # Choleskyåˆ†è§£
    L = cholesky(A, lower=True)

    print("\nCholeskyåˆ†è§£ L:")
    print(L)

    # éªŒè¯
    A_reconstructed = L @ L.T
    print("\né‡æ„è¯¯å·®:")
    print(np.linalg.norm(A - A_reconstructed))

    # æ±‚è§£çº¿æ€§ç³»ç»Ÿ Ax = b
    b = np.array([1, 2, 3])

    # ä½¿ç”¨Choleskyåˆ†è§£æ±‚è§£
    y = np.linalg.solve(L, b)  # Ly = b
    x = np.linalg.solve(L.T, y)  # L^T x = y

    print("\næ±‚è§£ Ax = b:")
    print(f"x = {x}")
    print(f"éªŒè¯ Ax = {A @ x}")


# 6. QRåˆ†è§£
def qr_demo():
    """QRåˆ†è§£ç¤ºä¾‹"""
    A = np.array([[1, 2, 3],
                  [4, 5, 6],
                  [7, 8, 9],
                  [10, 11, 12]], dtype=float)

    print("çŸ©é˜µ A:")
    print(A)

    # QRåˆ†è§£
    Q, R = qr(A)

    print("\nQ (æ­£äº¤çŸ©é˜µ):")
    print(Q)
    print("\nR (ä¸Šä¸‰è§’çŸ©é˜µ):")
    print(R)

    # éªŒè¯æ­£äº¤æ€§
    print("\nQ^T Q:")
    print(Q.T @ Q)

    # é‡æ„
    A_reconstructed = Q @ R
    print("\né‡æ„è¯¯å·®:")
    print(np.linalg.norm(A - A_reconstructed))


if __name__ == "__main__":
    print("=== çŸ©é˜µåˆ†è§£ç¤ºä¾‹ ===\n")

    print("1. ç‰¹å¾å€¼åˆ†è§£")
    eigendecomposition_demo()

    print("\n" + "="*50 + "\n")
    print("2. SVDåˆ†è§£")
    svd_demo()

    print("\n" + "="*50 + "\n")
    print("3. PCAé™ç»´")
    pca_demo()

    print("\n" + "="*50 + "\n")
    print("4. ä½ç§©è¿‘ä¼¼")
    low_rank_approximation_demo()

    print("\n" + "="*50 + "\n")
    print("5. Choleskyåˆ†è§£")
    cholesky_demo()

    print("\n" + "="*50 + "\n")
    print("6. QRåˆ†è§£")
    qr_demo()
```

---

## ğŸ“š ç»ƒä¹ é¢˜

### ç»ƒä¹ 1ï¼šç‰¹å¾å€¼è®¡ç®—

è®¡ç®—ä»¥ä¸‹çŸ©é˜µçš„ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡ï¼š

$$
A = \begin{bmatrix} 1 & 2 \\ 2 & 1 \end{bmatrix}
$$

### ç»ƒä¹ 2ï¼šSVDåº”ç”¨

å¯¹çŸ©é˜µ $A = \begin{bmatrix} 1 & 2 \\ 3 & 4 \\ 5 & 6 \end{bmatrix}$ è¿›è¡ŒSVDåˆ†è§£ï¼Œå¹¶è®¡ç®—ç§©-1è¿‘ä¼¼ã€‚

### ç»ƒä¹ 3ï¼šPCAå®ç°

ç»™å®šæ•°æ®çŸ©é˜µ $X \in \mathbb{R}^{100 \times 5}$ï¼Œä½¿ç”¨PCAå°†å…¶é™è‡³2ç»´ã€‚

### ç»ƒä¹ 4ï¼šä½ç§©è¿‘ä¼¼

è¯æ˜Eckart-Youngå®šç†ï¼šæˆªæ–­SVDç»™å‡ºæœ€ä¼˜ä½ç§©è¿‘ä¼¼ã€‚

---

## ğŸ“ ç›¸å…³è¯¾ç¨‹

| å¤§å­¦ | è¯¾ç¨‹ |
|------|------|
| **MIT** | 18.06 - Linear Algebra (Gilbert Strang) |
| **MIT** | 18.065 - Matrix Methods in Data Analysis |
| **Stanford** | CS205L - Continuous Mathematical Methods |
| **UC Berkeley** | Math 110 - Linear Algebra |
| **CMU** | 21-241 - Matrices and Linear Transformations |

---

## ğŸ“– å‚è€ƒæ–‡çŒ®

1. **Strang, G. (2016)**. *Introduction to Linear Algebra*. Wellesley-Cambridge Press.

2. **Trefethen & Bau (1997)**. *Numerical Linear Algebra*. SIAM.

3. **Golub & Van Loan (2013)**. *Matrix Computations*. Johns Hopkins University Press.

4. **Horn & Johnson (2012)**. *Matrix Analysis*. Cambridge University Press.

5. **Goodfellow et al. (2016)**. *Deep Learning*. MIT Press. (Chapter 2: Linear Algebra)

---

*æœ€åæ›´æ–°ï¼š2025å¹´10æœˆ*-
