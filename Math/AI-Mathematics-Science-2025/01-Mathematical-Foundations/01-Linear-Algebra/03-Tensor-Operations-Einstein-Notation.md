# å¼ é‡è¿ç®—ä¸Einsteinæ±‚å’Œçº¦å®š (Tensor Operations & Einstein Notation)

> **The Language of Modern Deep Learning**
>
> ç°ä»£æ·±åº¦å­¦ä¹ çš„è¯­è¨€

---

## ç›®å½•

- [å¼ é‡è¿ç®—ä¸Einsteinæ±‚å’Œçº¦å®š (Tensor Operations \& Einstein Notation)](#å¼ é‡è¿ç®—ä¸einsteinæ±‚å’Œçº¦å®š-tensor-operations--einstein-notation)
  - [ç›®å½•](#ç›®å½•)
  - [ğŸ“‹ æ ¸å¿ƒæ€æƒ³](#-æ ¸å¿ƒæ€æƒ³)
  - [ğŸ¯ å¼ é‡åŸºç¡€](#-å¼ é‡åŸºç¡€)
    - [1. å¼ é‡å®šä¹‰](#1-å¼ é‡å®šä¹‰)
    - [2. å¼ é‡çš„ç§©ä¸å½¢çŠ¶](#2-å¼ é‡çš„ç§©ä¸å½¢çŠ¶)
    - [3. å¼ é‡çš„ç´¢å¼•](#3-å¼ é‡çš„ç´¢å¼•)
  - [ğŸ“Š Einsteinæ±‚å’Œçº¦å®š](#-einsteinæ±‚å’Œçº¦å®š)
    - [1. åŸºæœ¬è§„åˆ™](#1-åŸºæœ¬è§„åˆ™)
    - [2. å¸¸è§è¿ç®—](#2-å¸¸è§è¿ç®—)
    - [3. ä¼˜åŠ¿](#3-ä¼˜åŠ¿)
  - [ğŸ”¬ å¼ é‡è¿ç®—](#-å¼ é‡è¿ç®—)
    - [1. åŸºæœ¬è¿ç®—](#1-åŸºæœ¬è¿ç®—)
    - [2. å¼ é‡ç§¯](#2-å¼ é‡ç§¯)
    - [3. å¼ é‡ç¼©å¹¶ (Contraction)](#3-å¼ é‡ç¼©å¹¶-contraction)
  - [ğŸ’¡ åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„åº”ç”¨](#-åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„åº”ç”¨)
    - [1. å…¨è¿æ¥å±‚](#1-å…¨è¿æ¥å±‚)
    - [2. å·ç§¯å±‚](#2-å·ç§¯å±‚)
    - [3. æ³¨æ„åŠ›æœºåˆ¶](#3-æ³¨æ„åŠ›æœºåˆ¶)
    - [4. æ‰¹å¤„ç†](#4-æ‰¹å¤„ç†)
  - [ğŸ¨ å¼ é‡åˆ†è§£](#-å¼ é‡åˆ†è§£)
    - [1. CPåˆ†è§£ (CANDECOMP/PARAFAC)](#1-cpåˆ†è§£-candecompparafac)
    - [2. Tuckeråˆ†è§£](#2-tuckeråˆ†è§£)
    - [3. å¼ é‡ç½‘ç»œ](#3-å¼ é‡ç½‘ç»œ)
  - [ğŸ”§ é«˜çº§å¼ é‡è¿ç®—](#-é«˜çº§å¼ é‡è¿ç®—)
    - [1. å¼ é‡é‡å¡‘ (Reshape)](#1-å¼ é‡é‡å¡‘-reshape)
    - [2. è½¬ç½®ä¸ç½®æ¢](#2-è½¬ç½®ä¸ç½®æ¢)
    - [3. å¹¿æ’­ (Broadcasting)](#3-å¹¿æ’­-broadcasting)
    - [4. å¼ é‡åˆ‡ç‰‡](#4-å¼ é‡åˆ‡ç‰‡)
  - [ğŸ’» Pythonå®ç°](#-pythonå®ç°)
  - [ğŸ“š ç»ƒä¹ é¢˜](#-ç»ƒä¹ é¢˜)
    - [ç»ƒä¹ 1ï¼šEinsteinæ±‚å’Œçº¦å®š](#ç»ƒä¹ 1einsteinæ±‚å’Œçº¦å®š)
    - [ç»ƒä¹ 2ï¼šå¼ é‡ç¼©å¹¶](#ç»ƒä¹ 2å¼ é‡ç¼©å¹¶)
    - [ç»ƒä¹ 3ï¼šå·ç§¯è¿ç®—](#ç»ƒä¹ 3å·ç§¯è¿ç®—)
    - [ç»ƒä¹ 4ï¼šæ³¨æ„åŠ›æœºåˆ¶](#ç»ƒä¹ 4æ³¨æ„åŠ›æœºåˆ¶)
  - [ğŸ“ ç›¸å…³è¯¾ç¨‹](#-ç›¸å…³è¯¾ç¨‹)
  - [ğŸ“– å‚è€ƒæ–‡çŒ®](#-å‚è€ƒæ–‡çŒ®)

---

## ğŸ“‹ æ ¸å¿ƒæ€æƒ³

**å¼ é‡**æ˜¯å‘é‡å’ŒçŸ©é˜µçš„é«˜ç»´æ¨å¹¿ï¼Œæ˜¯ç°ä»£æ·±åº¦å­¦ä¹ çš„æ ¸å¿ƒæ•°æ®ç»“æ„ã€‚**Einsteinæ±‚å’Œçº¦å®š**æä¾›äº†ä¸€ç§ç®€æ´ä¼˜é›…çš„å¼ é‡è¿ç®—è¡¨ç¤ºæ–¹æ³•ã€‚

**ä¸ºä»€ä¹ˆå¼ é‡é‡è¦**:

```text
æ·±åº¦å­¦ä¹ ä¸­çš„å¼ é‡:
â”œâ”€ æ•°æ®è¡¨ç¤º: å›¾åƒã€è§†é¢‘ã€æ–‡æœ¬
â”œâ”€ æ¨¡å‹å‚æ•°: æƒé‡ã€åç½®
â”œâ”€ ä¸­é—´æ¿€æ´»: ç‰¹å¾å›¾
â””â”€ æ¢¯åº¦: åå‘ä¼ æ’­

Einsteinçº¦å®šä¼˜åŠ¿:
â”œâ”€ ç®€æ´è¡¨ç¤º
â”œâ”€ é¿å…æ±‚å’Œç¬¦å·
â”œâ”€ æ¸…æ™°çš„ç´¢å¼•ç»“æ„
â””â”€ æ˜“äºæ¨å¯¼
```

---

## ğŸ¯ å¼ é‡åŸºç¡€

### 1. å¼ é‡å®šä¹‰

**å®šä¹‰ 1.1 (å¼ é‡)**:

å¼ é‡æ˜¯å¤šç»´æ•°ç»„ï¼Œæ˜¯æ ‡é‡ã€å‘é‡ã€çŸ©é˜µçš„æ¨å¹¿ã€‚

**æ•°å­¦å®šä¹‰**:

ä¸€ä¸ª $n$ é˜¶å¼ é‡ $\mathcal{T}$ æ˜¯ä¸€ä¸ªå¤šçº¿æ€§æ˜ å°„ï¼š

$$
\mathcal{T}: V_1^* \times V_2^* \times \cdots \times V_n^* \to \mathbb{R}
$$

å…¶ä¸­ $V_i^*$ æ˜¯å‘é‡ç©ºé—´ $V_i$ çš„å¯¹å¶ç©ºé—´ã€‚

**å®é™…ç†è§£**:

- 0é˜¶å¼ é‡ï¼šæ ‡é‡ (scalar)
- 1é˜¶å¼ é‡ï¼šå‘é‡ (vector)
- 2é˜¶å¼ é‡ï¼šçŸ©é˜µ (matrix)
- 3é˜¶å¼ é‡ï¼šç«‹æ–¹ä½“ (cube)
- né˜¶å¼ é‡ï¼šnç»´æ•°ç»„

---

### 2. å¼ é‡çš„ç§©ä¸å½¢çŠ¶

**ç§© (Rank/Order)**:

å¼ é‡çš„ç»´æ•°ï¼Œå³ç´¢å¼•çš„æ•°é‡ã€‚

**å½¢çŠ¶ (Shape)**:

æ¯ä¸ªç»´åº¦çš„å¤§å°ã€‚

**ç¤ºä¾‹**:

- æ ‡é‡: ç§©=0, å½¢çŠ¶=()
- å‘é‡ $\mathbf{v} \in \mathbb{R}^n$: ç§©=1, å½¢çŠ¶=(n,)
- çŸ©é˜µ $A \in \mathbb{R}^{m \times n}$: ç§©=2, å½¢çŠ¶=(m, n)
- RGBå›¾åƒ: ç§©=3, å½¢çŠ¶=(H, W, 3)
- æ‰¹é‡å›¾åƒ: ç§©=4, å½¢çŠ¶=(B, H, W, C)

---

### 3. å¼ é‡çš„ç´¢å¼•

**ç´¢å¼•è¡¨ç¤º**:

- å‘é‡: $v_i$
- çŸ©é˜µ: $A_{ij}$
- 3é˜¶å¼ é‡: $T_{ijk}$
- né˜¶å¼ é‡: $T_{i_1 i_2 \cdots i_n}$

**ç´¢å¼•çº¦å®š**:

- ä¸Šæ ‡ï¼šé€†å˜ç´¢å¼• (contravariant)
- ä¸‹æ ‡ï¼šåå˜ç´¢å¼• (covariant)
- æ·±åº¦å­¦ä¹ ä¸­é€šå¸¸ä½¿ç”¨ä¸‹æ ‡

---

## ğŸ“Š Einsteinæ±‚å’Œçº¦å®š

### 1. åŸºæœ¬è§„åˆ™

**è§„åˆ™ 1.1 (Einsteinæ±‚å’Œçº¦å®š)**:

å½“ä¸€ä¸ªç´¢å¼•åœ¨è¡¨è¾¾å¼ä¸­å‡ºç°ä¸¤æ¬¡ï¼ˆä¸€æ¬¡ä¸Šæ ‡ï¼Œä¸€æ¬¡ä¸‹æ ‡ï¼Œæˆ–ä¸¤æ¬¡ä¸‹æ ‡ï¼‰ï¼Œåˆ™å¯¹è¯¥ç´¢å¼•æ±‚å’Œï¼Œä¸”æ±‚å’Œç¬¦å· $\sum$ å¯ä»¥çœç•¥ã€‚

**ç¤ºä¾‹**:

ä¼ ç»Ÿè¡¨ç¤ºï¼š

$$
\sum_{i=1}^n a_i b_i
$$

Einsteinçº¦å®šï¼š

$$
a_i b_i
$$

**é‡å¤ç´¢å¼•**ç§°ä¸º**å“‘æŒ‡æ ‡ (dummy index)**ï¼Œéé‡å¤ç´¢å¼•ç§°ä¸º**è‡ªç”±æŒ‡æ ‡ (free index)**ã€‚

---

### 2. å¸¸è§è¿ç®—

**å‘é‡å†…ç§¯**:

ä¼ ç»Ÿï¼š$\mathbf{a}^T \mathbf{b} = \sum_{i=1}^n a_i b_i$

Einsteinï¼š$a_i b_i$

**çŸ©é˜µ-å‘é‡ä¹˜æ³•**:

ä¼ ç»Ÿï¼š$(\mathbf{A}\mathbf{x})_i = \sum_{j=1}^n A_{ij} x_j$

Einsteinï¼š$(Ax)_i = A_{ij} x_j$

**çŸ©é˜µä¹˜æ³•**:

ä¼ ç»Ÿï¼š$(AB)_{ij} = \sum_{k=1}^n A_{ik} B_{kj}$

Einsteinï¼š$(AB)_{ij} = A_{ik} B_{kj}$

**çŸ©é˜µè¿¹**:

ä¼ ç»Ÿï¼š$\text{tr}(A) = \sum_{i=1}^n A_{ii}$

Einsteinï¼š$\text{tr}(A) = A_{ii}$

**Frobeniuså†…ç§¯**:

ä¼ ç»Ÿï¼š$\langle A, B \rangle = \sum_{i=1}^m \sum_{j=1}^n A_{ij} B_{ij}$

Einsteinï¼š$\langle A, B \rangle = A_{ij} B_{ij}$

---

### 3. ä¼˜åŠ¿

**ç®€æ´æ€§**:

- é¿å…ç¹ççš„æ±‚å’Œç¬¦å·
- è¡¨è¾¾å¼æ›´ç´§å‡‘

**æ¸…æ™°æ€§**:

- ç´¢å¼•ç»“æ„ä¸€ç›®äº†ç„¶
- è‡ªç”±æŒ‡æ ‡æ˜ç¡®

**æ˜“äºæ¨å¯¼**:

- é“¾å¼æ³•åˆ™ç®€æ´
- æ¢¯åº¦è®¡ç®—ç›´è§‚

---

## ğŸ”¬ å¼ é‡è¿ç®—

### 1. åŸºæœ¬è¿ç®—

**é€å…ƒç´ è¿ç®—**:

- åŠ æ³•ï¼š$C_{ijk} = A_{ijk} + B_{ijk}$
- ä¹˜æ³•ï¼š$C_{ijk} = A_{ijk} \cdot B_{ijk}$

**æ ‡é‡ä¹˜æ³•**:

$$
C_{ijk} = \alpha A_{ijk}
$$

---

### 2. å¼ é‡ç§¯

**å¤–ç§¯ (Outer Product)**:

å‘é‡å¤–ç§¯ï¼š

$$
C_{ij} = a_i b_j
$$

å¼ é‡å¤–ç§¯ï¼š

$$
D_{ijkl} = A_{ij} B_{kl}
$$

**ç¤ºä¾‹**:

$\mathbf{a} \in \mathbb{R}^m$, $\mathbf{b} \in \mathbb{R}^n$ï¼Œåˆ™ $\mathbf{a} \otimes \mathbf{b} \in \mathbb{R}^{m \times n}$ã€‚

---

### 3. å¼ é‡ç¼©å¹¶ (Contraction)

**å®šä¹‰ 3.1 (ç¼©å¹¶)**:

å¯¹å¼ é‡çš„ä¸¤ä¸ªç´¢å¼•æ±‚å’Œï¼Œé™ä½å¼ é‡çš„ç§©ã€‚

**ç¤ºä¾‹**:

çŸ©é˜µä¹˜æ³•æ˜¯ç¼©å¹¶ï¼š

$$
C_{ij} = A_{ik} B_{kj}
$$

å¼ é‡ç¼©å¹¶ï¼š

$$
C_{ij} = T_{ikj} \quad \text{(å¯¹ } k \text{ æ±‚å’Œ)}
$$

**æ€§è´¨**:

- ç¼©å¹¶é™ä½ç§©ï¼š$(p, q)$ å‹å¼ é‡ç¼©å¹¶åå˜ä¸º $(p-1, q-1)$ å‹
- çŸ©é˜µè¿¹æ˜¯å®Œå…¨ç¼©å¹¶ï¼š$\text{tr}(A) = A_{ii}$

---

## ğŸ’¡ åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„åº”ç”¨

### 1. å…¨è¿æ¥å±‚

**å‰å‘ä¼ æ’­**:

$$
y_i = W_{ij} x_j + b_i
$$

**æ‰¹é‡å¤„ç†**:

$$
Y_{bi} = W_{ij} X_{bj} + b_i
$$

å…¶ä¸­ $b$ æ˜¯æ‰¹é‡ç´¢å¼•ã€‚

**æ¢¯åº¦**:

$$
\frac{\partial L}{\partial W_{ij}} = \frac{\partial L}{\partial y_k} \frac{\partial y_k}{\partial W_{ij}} = \frac{\partial L}{\partial y_i} x_j
$$

Einsteinçº¦å®šï¼š

$$
\frac{\partial L}{\partial W_{ij}} = \delta_i x_j
$$

å…¶ä¸­ $\delta_i = \frac{\partial L}{\partial y_i}$ã€‚

---

### 2. å·ç§¯å±‚

**2Då·ç§¯**:

$$
Y_{bchw} = W_{ckhw'} X_{bk(h+h')(w+w')}
$$

å…¶ä¸­ï¼š

- $b$: æ‰¹é‡ç´¢å¼•
- $c$: è¾“å‡ºé€šé“
- $k$: è¾“å…¥é€šé“
- $h, w$: ç©ºé—´ä½ç½®
- $h', w'$: å·ç§¯æ ¸ä½ç½®

**ç®€åŒ–è¡¨ç¤º**:

$$
Y = W * X
$$

---

### 3. æ³¨æ„åŠ›æœºåˆ¶

**Scaled Dot-Product Attention**:

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V
$$

**Einsteinçº¦å®š**:

$$
A_{ij} = \frac{Q_{ik} K_{jk}}{\sqrt{d_k}}
$$

$$
\text{Output}_{ik} = \text{softmax}(A)_{ij} V_{jk}
$$

**æ‰¹é‡å¤šå¤´æ³¨æ„åŠ›**:

$$
\text{Output}_{bhik} = \text{softmax}(A)_{bhij} V_{bhjk}
$$

å…¶ä¸­ï¼š

- $b$: æ‰¹é‡
- $h$: å¤´æ•°
- $i, j$: åºåˆ—ä½ç½®
- $k$: ç‰¹å¾ç»´åº¦

---

### 4. æ‰¹å¤„ç†

**æ‰¹é‡çŸ©é˜µä¹˜æ³•**:

$$
Y_{bij} = X_{bik} W_{bkj}
$$

**æ‰¹é‡å½’ä¸€åŒ–**:

$$
\hat{x}_{bi} = \frac{x_{bi} - \mu_i}{\sqrt{\sigma_i^2 + \epsilon}}
$$

å…¶ä¸­ï¼š

$$
\mu_i = \frac{1}{B} x_{bi}
$$

$$
\sigma_i^2 = \frac{1}{B} (x_{bi} - \mu_i)^2
$$

---

## ğŸ¨ å¼ é‡åˆ†è§£

### 1. CPåˆ†è§£ (CANDECOMP/PARAFAC)

**å®šä¹‰**:

å°†3é˜¶å¼ é‡ $\mathcal{T} \in \mathbb{R}^{I \times J \times K}$ åˆ†è§£ä¸ºç§©1å¼ é‡çš„å’Œï¼š

$$
T_{ijk} = \sum_{r=1}^R a_{ir} b_{jr} c_{kr}
$$

çŸ©é˜µå½¢å¼ï¼š

$$
\mathcal{T} = \sum_{r=1}^R \mathbf{a}_r \circ \mathbf{b}_r \circ \mathbf{c}_r
$$

å…¶ä¸­ $\circ$ è¡¨ç¤ºå¤–ç§¯ã€‚

**åº”ç”¨**:

- æ¨¡å‹å‹ç¼©
- ç‰¹å¾æå–
- æ¨èç³»ç»Ÿ

---

### 2. Tuckeråˆ†è§£

**å®šä¹‰**:

$$
T_{ijk} = G_{pqr} A_{ip} B_{jq} C_{kr}
$$

å…¶ä¸­ $G$ æ˜¯æ ¸å¿ƒå¼ é‡ï¼Œ$A, B, C$ æ˜¯å› å­çŸ©é˜µã€‚

**çŸ©é˜µå½¢å¼**:

$$
\mathcal{T} = \mathcal{G} \times_1 A \times_2 B \times_3 C
$$

**ä¸SVDçš„å…³ç³»**:

Tuckeråˆ†è§£æ˜¯SVDçš„é«˜é˜¶æ¨å¹¿ã€‚

---

### 3. å¼ é‡ç½‘ç»œ

**å¼ é‡ç½‘ç»œè¡¨ç¤º**:

å¤æ‚çš„å¼ é‡è¿ç®—å¯ä»¥ç”¨ç½‘ç»œå›¾è¡¨ç¤ºï¼š

```text
    i       j
    |       |
  â”Œâ”€â”´â”€â”   â”Œâ”€â”´â”€â”
  â”‚ A â”‚â”€â”€â”€â”‚ B â”‚
  â””â”€â”¬â”€â”˜   â””â”€â”¬â”€â”˜
    k       l
```

è¡¨ç¤ºï¼š$C_{ijkl} = A_{ik} B_{jl}$

**åº”ç”¨**:

- é‡å­è®¡ç®—
- æ·±åº¦å­¦ä¹ æ¨¡å‹å‹ç¼©
- ç‰©ç†æ¨¡æ‹Ÿ

---

## ğŸ”§ é«˜çº§å¼ é‡è¿ç®—

### 1. å¼ é‡é‡å¡‘ (Reshape)

**å®šä¹‰**:

æ”¹å˜å¼ é‡çš„å½¢çŠ¶ï¼Œä½†ä¿æŒå…ƒç´ æ€»æ•°ä¸å˜ã€‚

**ç¤ºä¾‹**:

$(2, 3, 4) \to (6, 4)$ æˆ– $(24,)$

**åº”ç”¨**:

- å±•å¹³ (Flatten): $(B, H, W, C) \to (B, H \times W \times C)$
- é‡å¡‘: $(B, L, D) \to (B, L, H, D/H)$ (å¤šå¤´æ³¨æ„åŠ›)

---

### 2. è½¬ç½®ä¸ç½®æ¢

**è½¬ç½® (Transpose)**:

äº¤æ¢ä¸¤ä¸ªç»´åº¦ï¼š

$$
B_{ji} = A_{ij}
$$

**ç½®æ¢ (Permute)**:

ä»»æ„é‡æ’ç»´åº¦ï¼š

$$
B_{ikj} = A_{ijk}
$$

**åº”ç”¨**:

- çŸ©é˜µè½¬ç½®: $(m, n) \to (n, m)$
- é€šé“é¡ºåºè½¬æ¢: $(B, H, W, C) \to (B, C, H, W)$

---

### 3. å¹¿æ’­ (Broadcasting)

**å®šä¹‰**:

è‡ªåŠ¨æ‰©å±•å¼ é‡çš„ç»´åº¦ä»¥åŒ¹é…è¿ç®—ã€‚

**è§„åˆ™**:

1. å¦‚æœä¸¤ä¸ªå¼ é‡ç»´æ•°ä¸åŒï¼Œåœ¨è¾ƒå°çš„å¼ é‡å‰é¢è¡¥1
2. å¦‚æœæŸç»´åº¦å¤§å°ä¸º1ï¼Œåˆ™æ²¿è¯¥ç»´åº¦å¤åˆ¶

**ç¤ºä¾‹**:

```python
A: (3, 1)
B: (1, 4)
A + B: (3, 4)  # å¹¿æ’­
```

**åº”ç”¨**:

- åç½®åŠ æ³•: $(B, N) + (N,) \to (B, N)$
- æ‰¹é‡å½’ä¸€åŒ–

---

### 4. å¼ é‡åˆ‡ç‰‡

**åˆ‡ç‰‡æ“ä½œ**:

æå–å¼ é‡çš„å­é›†ï¼š

$$
B = A[i_1:i_2, j_1:j_2, :]
$$

**åº”ç”¨**:

- æå–æ‰¹é‡å­é›†
- æå–ç‰¹å¾å­é›†
- çª—å£æ“ä½œ

---

## ğŸ’» Pythonå®ç°

```python
import numpy as np
import torch
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# 1. Einsteinæ±‚å’Œçº¦å®šç¤ºä¾‹
def einstein_examples():
    """Einsteinæ±‚å’Œçº¦å®šç¤ºä¾‹"""
    print("=== Einsteinæ±‚å’Œçº¦å®š ===\n")
    
    # å‘é‡å†…ç§¯
    a = np.array([1, 2, 3])
    b = np.array([4, 5, 6])
    
    # ä¼ ç»Ÿæ–¹æ³•
    dot_traditional = np.sum(a * b)
    # Einsteinçº¦å®š (numpy)
    dot_einstein = np.einsum('i,i->', a, b)
    
    print(f"å‘é‡å†…ç§¯:")
    print(f"  ä¼ ç»Ÿ: {dot_traditional}")
    print(f"  Einstein: {dot_einstein}\n")
    
    # çŸ©é˜µ-å‘é‡ä¹˜æ³•
    A = np.array([[1, 2], [3, 4]])
    x = np.array([1, 2])
    
    # ä¼ ç»Ÿæ–¹æ³•
    y_traditional = A @ x
    # Einsteinçº¦å®š
    y_einstein = np.einsum('ij,j->i', A, x)
    
    print(f"çŸ©é˜µ-å‘é‡ä¹˜æ³•:")
    print(f"  ä¼ ç»Ÿ: {y_traditional}")
    print(f"  Einstein: {y_einstein}\n")
    
    # çŸ©é˜µä¹˜æ³•
    B = np.array([[5, 6], [7, 8]])
    
    # ä¼ ç»Ÿæ–¹æ³•
    C_traditional = A @ B
    # Einsteinçº¦å®š
    C_einstein = np.einsum('ik,kj->ij', A, B)
    
    print(f"çŸ©é˜µä¹˜æ³•:")
    print(f"  ä¼ ç»Ÿ:\n{C_traditional}")
    print(f"  Einstein:\n{C_einstein}\n")
    
    # çŸ©é˜µè¿¹
    trace_traditional = np.trace(A)
    trace_einstein = np.einsum('ii->', A)
    
    print(f"çŸ©é˜µè¿¹:")
    print(f"  ä¼ ç»Ÿ: {trace_traditional}")
    print(f"  Einstein: {trace_einstein}\n")
    
    # å¤–ç§¯
    outer_traditional = np.outer(a, b)
    outer_einstein = np.einsum('i,j->ij', a, b)
    
    print(f"å¤–ç§¯:")
    print(f"  ä¼ ç»Ÿ:\n{outer_traditional}")
    print(f"  Einstein:\n{outer_einstein}\n")


# 2. å¼ é‡è¿ç®—
def tensor_operations():
    """å¼ é‡åŸºæœ¬è¿ç®—"""
    print("=== å¼ é‡è¿ç®— ===\n")
    
    # åˆ›å»ºå¼ é‡
    T = np.random.randn(2, 3, 4)
    print(f"å¼ é‡å½¢çŠ¶: {T.shape}")
    print(f"å¼ é‡ç§©: {T.ndim}\n")
    
    # å¼ é‡ç¼©å¹¶
    # å¯¹ç¬¬äºŒä¸ªç»´åº¦æ±‚å’Œ
    T_contract = np.einsum('ijk->ik', T)
    print(f"ç¼©å¹¶åå½¢çŠ¶: {T_contract.shape}\n")
    
    # å¼ é‡è½¬ç½®
    T_transpose = np.transpose(T, (2, 0, 1))
    print(f"è½¬ç½®åå½¢çŠ¶: {T_transpose.shape}\n")
    
    # å¼ é‡é‡å¡‘
    T_reshape = T.reshape(6, 4)
    print(f"é‡å¡‘åå½¢çŠ¶: {T_reshape.shape}\n")


# 3. æ‰¹é‡çŸ©é˜µä¹˜æ³•
def batch_matrix_multiply():
    """æ‰¹é‡çŸ©é˜µä¹˜æ³•"""
    print("=== æ‰¹é‡çŸ©é˜µä¹˜æ³• ===\n")
    
    # æ‰¹é‡å¤§å°ä¸º4ï¼ŒçŸ©é˜µå¤§å°ä¸º3x2å’Œ2x5
    A = np.random.randn(4, 3, 2)
    B = np.random.randn(4, 2, 5)
    
    # æ–¹æ³•1: å¾ªç¯
    C_loop = np.zeros((4, 3, 5))
    for i in range(4):
        C_loop[i] = A[i] @ B[i]
    
    # æ–¹æ³•2: Einsteinçº¦å®š
    C_einstein = np.einsum('bij,bjk->bik', A, B)
    
    # æ–¹æ³•3: PyTorch bmm
    A_torch = torch.from_numpy(A)
    B_torch = torch.from_numpy(B)
    C_torch = torch.bmm(A_torch, B_torch).numpy()
    
    print(f"ç»“æœå½¢çŠ¶: {C_einstein.shape}")
    print(f"æ–¹æ³•ä¸€è‡´æ€§: {np.allclose(C_loop, C_einstein, C_torch)}\n")


# 4. æ³¨æ„åŠ›æœºåˆ¶
def attention_mechanism():
    """æ³¨æ„åŠ›æœºåˆ¶å®ç°"""
    print("=== æ³¨æ„åŠ›æœºåˆ¶ ===\n")
    
    # å‚æ•°
    batch_size = 2
    seq_len = 4
    d_model = 8
    
    # Q, K, V
    Q = np.random.randn(batch_size, seq_len, d_model)
    K = np.random.randn(batch_size, seq_len, d_model)
    V = np.random.randn(batch_size, seq_len, d_model)
    
    # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
    # scores = Q @ K^T / sqrt(d_model)
    scores = np.einsum('bik,bjk->bij', Q, K) / np.sqrt(d_model)
    
    # Softmax
    exp_scores = np.exp(scores - np.max(scores, axis=-1, keepdims=True))
    attention_weights = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)
    
    # åŠ æƒæ±‚å’Œ
    # output = attention_weights @ V
    output = np.einsum('bij,bjk->bik', attention_weights, V)
    
    print(f"Qå½¢çŠ¶: {Q.shape}")
    print(f"Kå½¢çŠ¶: {K.shape}")
    print(f"Vå½¢çŠ¶: {V.shape}")
    print(f"æ³¨æ„åŠ›æƒé‡å½¢çŠ¶: {attention_weights.shape}")
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}\n")


# 5. å¼ é‡åˆ†è§£ - CPåˆ†è§£
def cp_decomposition(T, rank):
    """CPåˆ†è§£ (ç®€åŒ–ç‰ˆï¼Œä½¿ç”¨ALSç®—æ³•)"""
    I, J, K = T.shape
    
    # åˆå§‹åŒ–å› å­çŸ©é˜µ
    A = np.random.randn(I, rank)
    B = np.random.randn(J, rank)
    C = np.random.randn(K, rank)
    
    # ALSè¿­ä»£
    n_iter = 10
    for _ in range(n_iter):
        # æ›´æ–°A
        V = np.einsum('jr,kr->jkr', B, C)
        V_flat = V.reshape(J*K, rank)
        T_flat = T.reshape(I, J*K)
        A = T_flat @ V_flat @ np.linalg.inv(V_flat.T @ V_flat + 1e-8*np.eye(rank))
        
        # æ›´æ–°B (ç±»ä¼¼)
        V = np.einsum('ir,kr->ikr', A, C)
        V_flat = V.reshape(I*K, rank)
        T_flat = T.transpose(1, 0, 2).reshape(J, I*K)
        B = T_flat @ V_flat @ np.linalg.inv(V_flat.T @ V_flat + 1e-8*np.eye(rank))
        
        # æ›´æ–°C (ç±»ä¼¼)
        V = np.einsum('ir,jr->ijr', A, B)
        V_flat = V.reshape(I*J, rank)
        T_flat = T.transpose(2, 0, 1).reshape(K, I*J)
        C = T_flat @ V_flat @ np.linalg.inv(V_flat.T @ V_flat + 1e-8*np.eye(rank))
    
    # é‡æ„å¼ é‡
    T_reconstructed = np.einsum('ir,jr,kr->ijk', A, B, C)
    
    return A, B, C, T_reconstructed


def test_cp_decomposition():
    """æµ‹è¯•CPåˆ†è§£"""
    print("=== CPåˆ†è§£ ===\n")
    
    # åˆ›å»ºä½ç§©å¼ é‡
    rank = 3
    I, J, K = 10, 12, 8
    
    A_true = np.random.randn(I, rank)
    B_true = np.random.randn(J, rank)
    C_true = np.random.randn(K, rank)
    
    T = np.einsum('ir,jr,kr->ijk', A_true, B_true, C_true)
    
    # CPåˆ†è§£
    A, B, C, T_reconstructed = cp_decomposition(T, rank)
    
    # è®¡ç®—è¯¯å·®
    error = np.linalg.norm(T - T_reconstructed) / np.linalg.norm(T)
    
    print(f"åŸå§‹å¼ é‡å½¢çŠ¶: {T.shape}")
    print(f"åˆ†è§£ç§©: {rank}")
    print(f"é‡æ„è¯¯å·®: {error:.6f}\n")


# 6. å¹¿æ’­ç¤ºä¾‹
def broadcasting_examples():
    """å¹¿æ’­ç¤ºä¾‹"""
    print("=== å¹¿æ’­ ===\n")
    
    # ç¤ºä¾‹1: å‘é‡åŠ åˆ°çŸ©é˜µæ¯ä¸€è¡Œ
    A = np.array([[1, 2, 3],
                  [4, 5, 6]])
    b = np.array([10, 20, 30])
    
    C = A + b  # å¹¿æ’­
    print(f"çŸ©é˜µ + å‘é‡ (å¹¿æ’­):")
    print(f"Aå½¢çŠ¶: {A.shape}")
    print(f"bå½¢çŠ¶: {b.shape}")
    print(f"Cå½¢çŠ¶: {C.shape}")
    print(f"C:\n{C}\n")
    
    # ç¤ºä¾‹2: æ‰¹é‡å½’ä¸€åŒ–
    X = np.random.randn(32, 10)  # (batch, features)
    
    # è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
    mean = np.mean(X, axis=0, keepdims=True)  # (1, 10)
    std = np.std(X, axis=0, keepdims=True)    # (1, 10)
    
    # å½’ä¸€åŒ– (å¹¿æ’­)
    X_normalized = (X - mean) / (std + 1e-8)
    
    print(f"æ‰¹é‡å½’ä¸€åŒ–:")
    print(f"Xå½¢çŠ¶: {X.shape}")
    print(f"meanå½¢çŠ¶: {mean.shape}")
    print(f"stdå½¢çŠ¶: {std.shape}")
    print(f"X_normalizedå½¢çŠ¶: {X_normalized.shape}\n")


# 7. å¯è§†åŒ–3Då¼ é‡
def visualize_3d_tensor():
    """å¯è§†åŒ–3Då¼ é‡"""
    # åˆ›å»ºç®€å•çš„3Då¼ é‡
    T = np.zeros((5, 5, 5))
    T[2, 2, 2] = 1  # ä¸­å¿ƒç‚¹
    T[1:4, 1:4, 1:4] = 0.5  # å†…éƒ¨ç«‹æ–¹ä½“
    
    fig = plt.figure(figsize=(10, 8))
    ax = fig.add_subplot(111, projection='3d')
    
    # æ‰¾åˆ°éé›¶å…ƒç´ 
    x, y, z = np.where(T > 0)
    colors = T[x, y, z]
    
    # ç»˜åˆ¶
    scatter = ax.scatter(x, y, z, c=colors, cmap='viridis', 
                        s=100, alpha=0.6, edgecolors='k')
    
    ax.set_xlabel('X')
    ax.set_ylabel('Y')
    ax.set_zlabel('Z')
    ax.set_title('3D Tensor Visualization')
    
    plt.colorbar(scatter, ax=ax, label='Value')
    # plt.show()


if __name__ == "__main__":
    print("=" * 60)
    print("å¼ é‡è¿ç®—ä¸Einsteinæ±‚å’Œçº¦å®šç¤ºä¾‹")
    print("=" * 60 + "\n")
    
    einstein_examples()
    tensor_operations()
    batch_matrix_multiply()
    attention_mechanism()
    test_cp_decomposition()
    broadcasting_examples()
    
    print("\nå¯è§†åŒ–3Då¼ é‡...")
    visualize_3d_tensor()
    
    print("\næ‰€æœ‰ç¤ºä¾‹å®Œæˆï¼")
```

---

## ğŸ“š ç»ƒä¹ é¢˜

### ç»ƒä¹ 1ï¼šEinsteinæ±‚å’Œçº¦å®š

ä½¿ç”¨Einsteinæ±‚å’Œçº¦å®šè¡¨ç¤ºä»¥ä¸‹è¿ç®—ï¼š

1. å‘é‡å¤–ç§¯ï¼š$\mathbf{a} \otimes \mathbf{b}$
2. çŸ©é˜µFrobeniusèŒƒæ•°ï¼š$\|A\|_F$
3. æ‰¹é‡çŸ©é˜µä¹˜æ³•

### ç»ƒä¹ 2ï¼šå¼ é‡ç¼©å¹¶

ç»™å®š3é˜¶å¼ é‡ $T \in \mathbb{R}^{3 \times 4 \times 5}$ï¼Œè®¡ç®—ï¼š

1. å¯¹ç¬¬ä¸€ä¸ªç´¢å¼•ç¼©å¹¶
2. å¯¹ç¬¬äºŒä¸ªç´¢å¼•ç¼©å¹¶
3. å®Œå…¨ç¼©å¹¶ï¼ˆæ‰€æœ‰ç´¢å¼•ï¼‰

### ç»ƒä¹ 3ï¼šå·ç§¯è¿ç®—

ä½¿ç”¨Einsteinçº¦å®šè¡¨ç¤º2Då·ç§¯è¿ç®—ï¼ŒåŒ…æ‹¬ï¼š

1. å•é€šé“å·ç§¯
2. å¤šé€šé“å·ç§¯
3. æ‰¹é‡å·ç§¯

### ç»ƒä¹ 4ï¼šæ³¨æ„åŠ›æœºåˆ¶

å®ç°å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼Œä½¿ç”¨Einsteinçº¦å®šè¡¨ç¤ºæ‰€æœ‰è¿ç®—ã€‚

---

## ğŸ“ ç›¸å…³è¯¾ç¨‹

| å¤§å­¦ | è¯¾ç¨‹ |
|------|------|
| **MIT** | 18.065 - Matrix Methods in Data Analysis |
| **Stanford** | CS231n - Convolutional Neural Networks |
| **CMU** | 10-708 - Probabilistic Graphical Models |
| **UC Berkeley** | CS189 - Introduction to Machine Learning |

---

## ğŸ“– å‚è€ƒæ–‡çŒ®

1. **Kolda & Bader (2009)**. *Tensor Decompositions and Applications*. SIAM Review.

2. **Cichocki et al. (2015)**. *Tensor Decompositions for Signal Processing Applications*. IEEE Signal Processing Magazine.

3. **Goodfellow et al. (2016)**. *Deep Learning*. MIT Press. (Chapter 2)

4. **Novikov et al. (2015)**. *Tensorizing Neural Networks*. NeurIPS.

5. **Paszke et al. (2019)**. *PyTorch: An Imperative Style, High-Performance Deep Learning Library*. NeurIPS.

---

*æœ€åæ›´æ–°ï¼š2025å¹´10æœˆ*-
