# å¼ é‡è¿ç®—ä¸Einsteinæ±‚å’Œçº¦å®š (Tensor Operations & Einstein Notation)

> **The Language of Modern Deep Learning**
>
> ç°ä»£æ·±åº¦å­¦ä¹ çš„è¯­è¨€

---

## ç›®å½•

- [å¼ é‡è¿ç®—ä¸Einsteinæ±‚å’Œçº¦å®š (Tensor Operations \& Einstein Notation)](#å¼ é‡è¿ç®—ä¸einsteinæ±‚å’Œçº¦å®š-tensor-operations--einstein-notation)
  - [ç›®å½•](#ç›®å½•)
  - [ğŸ“‹ æ ¸å¿ƒæ€æƒ³](#-æ ¸å¿ƒæ€æƒ³)
  - [ğŸ¯ å¼ é‡åŸºç¡€](#-å¼ é‡åŸºç¡€)
    - [1. å¼ é‡å®šä¹‰](#1-å¼ é‡å®šä¹‰)
    - [2. å¼ é‡çš„ç§©ä¸å½¢çŠ¶](#2-å¼ é‡çš„ç§©ä¸å½¢çŠ¶)
    - [3. å¼ é‡çš„ç´¢å¼•](#3-å¼ é‡çš„ç´¢å¼•)
  - [ğŸ“Š Einsteinæ±‚å’Œçº¦å®š](#-einsteinæ±‚å’Œçº¦å®š)
    - [1. åŸºæœ¬è§„åˆ™](#1-åŸºæœ¬è§„åˆ™)
    - [2. å¸¸è§è¿ç®—](#2-å¸¸è§è¿ç®—)
    - [3. ä¼˜åŠ¿](#3-ä¼˜åŠ¿)
    - [4. å¸¸è§é”™è¯¯ä¸é™·é˜±](#4-å¸¸è§é”™è¯¯ä¸é™·é˜±)
      - [é”™è¯¯1ï¼šç´¢å¼•å‡ºç°ä¸‰æ¬¡æˆ–æ›´å¤š](#é”™è¯¯1ç´¢å¼•å‡ºç°ä¸‰æ¬¡æˆ–æ›´å¤š)
      - [é”™è¯¯2ï¼šæ··æ·†è‡ªç”±æŒ‡æ ‡ä¸å“‘æŒ‡æ ‡](#é”™è¯¯2æ··æ·†è‡ªç”±æŒ‡æ ‡ä¸å“‘æŒ‡æ ‡)
      - [é”™è¯¯3ï¼šåœ¨æ·±åº¦å­¦ä¹ ä¸­è¯¯ç”¨æ‰¹æ¬¡ç»´åº¦](#é”™è¯¯3åœ¨æ·±åº¦å­¦ä¹ ä¸­è¯¯ç”¨æ‰¹æ¬¡ç»´åº¦)
      - [é”™è¯¯4ï¼šè½¬ç½®æ—¶ç´¢å¼•é¡ºåºé”™è¯¯](#é”™è¯¯4è½¬ç½®æ—¶ç´¢å¼•é¡ºåºé”™è¯¯)
      - [é”™è¯¯5ï¼šæ··æ·†ç‚¹ç§¯ä¸å¤–ç§¯](#é”™è¯¯5æ··æ·†ç‚¹ç§¯ä¸å¤–ç§¯)
      - [é”™è¯¯6ï¼šæ¢¯åº¦è®¡ç®—ä¸­çš„ç´¢å¼•é”™è¯¯](#é”™è¯¯6æ¢¯åº¦è®¡ç®—ä¸­çš„ç´¢å¼•é”™è¯¯)
      - [é”™è¯¯7ï¼šæ·±åº¦å­¦ä¹ ä¸­çš„ç»´åº¦å¹¿æ’­æ··æ·†](#é”™è¯¯7æ·±åº¦å­¦ä¹ ä¸­çš„ç»´åº¦å¹¿æ’­æ··æ·†)
      - [å®è·µå»ºè®®æ€»ç»“](#å®è·µå»ºè®®æ€»ç»“)
      - [è°ƒè¯•æŠ€å·§](#è°ƒè¯•æŠ€å·§)
      - [AIåº”ç”¨ä¸­çš„é‡è¦æ€§](#aiåº”ç”¨ä¸­çš„é‡è¦æ€§)
  - [ğŸ”¬ å¼ é‡è¿ç®—](#-å¼ é‡è¿ç®—)
    - [1. åŸºæœ¬è¿ç®—](#1-åŸºæœ¬è¿ç®—)
    - [2. å¼ é‡ç§¯](#2-å¼ é‡ç§¯)
    - [3. å¼ é‡ç¼©å¹¶ (Contraction)](#3-å¼ é‡ç¼©å¹¶-contraction)
  - [ğŸ’¡ åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„åº”ç”¨](#-åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„åº”ç”¨)
    - [1. å…¨è¿æ¥å±‚](#1-å…¨è¿æ¥å±‚)
    - [2. å·ç§¯å±‚](#2-å·ç§¯å±‚)
    - [3. æ³¨æ„åŠ›æœºåˆ¶](#3-æ³¨æ„åŠ›æœºåˆ¶)
    - [4. æ‰¹å¤„ç†](#4-æ‰¹å¤„ç†)
  - [ğŸ¨ å¼ é‡åˆ†è§£](#-å¼ é‡åˆ†è§£)
    - [1. CPåˆ†è§£ (CANDECOMP/PARAFAC)](#1-cpåˆ†è§£-candecompparafac)
    - [2. Tuckeråˆ†è§£](#2-tuckeråˆ†è§£)
    - [3. å¼ é‡ç½‘ç»œ](#3-å¼ é‡ç½‘ç»œ)
  - [ğŸ”§ é«˜çº§å¼ é‡è¿ç®—](#-é«˜çº§å¼ é‡è¿ç®—)
    - [1. å¼ é‡é‡å¡‘ (Reshape)](#1-å¼ é‡é‡å¡‘-reshape)
    - [2. è½¬ç½®ä¸ç½®æ¢](#2-è½¬ç½®ä¸ç½®æ¢)
    - [3. å¹¿æ’­ (Broadcasting)](#3-å¹¿æ’­-broadcasting)
    - [4. å¼ é‡åˆ‡ç‰‡](#4-å¼ é‡åˆ‡ç‰‡)
  - [ğŸ’» Pythonå®ç°](#-pythonå®ç°)
  - [ğŸ“š ç»ƒä¹ é¢˜](#-ç»ƒä¹ é¢˜)
    - [ç»ƒä¹ 1ï¼šEinsteinæ±‚å’Œçº¦å®š](#ç»ƒä¹ 1einsteinæ±‚å’Œçº¦å®š)
    - [ç»ƒä¹ 2ï¼šå¼ é‡ç¼©å¹¶](#ç»ƒä¹ 2å¼ é‡ç¼©å¹¶)
    - [ç»ƒä¹ 3ï¼šå·ç§¯è¿ç®—](#ç»ƒä¹ 3å·ç§¯è¿ç®—)
    - [ç»ƒä¹ 4ï¼šæ³¨æ„åŠ›æœºåˆ¶](#ç»ƒä¹ 4æ³¨æ„åŠ›æœºåˆ¶)
  - [ğŸ“ ç›¸å…³è¯¾ç¨‹](#-ç›¸å…³è¯¾ç¨‹)
  - [ğŸ“– å‚è€ƒæ–‡çŒ®](#-å‚è€ƒæ–‡çŒ®)

---

## ğŸ“‹ æ ¸å¿ƒæ€æƒ³

**å¼ é‡**æ˜¯å‘é‡å’ŒçŸ©é˜µçš„é«˜ç»´æ¨å¹¿ï¼Œæ˜¯ç°ä»£æ·±åº¦å­¦ä¹ çš„æ ¸å¿ƒæ•°æ®ç»“æ„ã€‚**Einsteinæ±‚å’Œçº¦å®š**æä¾›äº†ä¸€ç§ç®€æ´ä¼˜é›…çš„å¼ é‡è¿ç®—è¡¨ç¤ºæ–¹æ³•ã€‚

**ä¸ºä»€ä¹ˆå¼ é‡é‡è¦**:

```text
æ·±åº¦å­¦ä¹ ä¸­çš„å¼ é‡:
â”œâ”€ æ•°æ®è¡¨ç¤º: å›¾åƒã€è§†é¢‘ã€æ–‡æœ¬
â”œâ”€ æ¨¡å‹å‚æ•°: æƒé‡ã€åç½®
â”œâ”€ ä¸­é—´æ¿€æ´»: ç‰¹å¾å›¾
â””â”€ æ¢¯åº¦: åå‘ä¼ æ’­

Einsteinçº¦å®šä¼˜åŠ¿:
â”œâ”€ ç®€æ´è¡¨ç¤º
â”œâ”€ é¿å…æ±‚å’Œç¬¦å·
â”œâ”€ æ¸…æ™°çš„ç´¢å¼•ç»“æ„
â””â”€ æ˜“äºæ¨å¯¼
```

---

## ğŸ¯ å¼ é‡åŸºç¡€

### 1. å¼ é‡å®šä¹‰

**å®šä¹‰ 1.1 (å¼ é‡)**:

å¼ é‡æ˜¯å¤šç»´æ•°ç»„ï¼Œæ˜¯æ ‡é‡ã€å‘é‡ã€çŸ©é˜µçš„æ¨å¹¿ã€‚

**æ•°å­¦å®šä¹‰**:

ä¸€ä¸ª $n$ é˜¶å¼ é‡ $\mathcal{T}$ æ˜¯ä¸€ä¸ªå¤šçº¿æ€§æ˜ å°„ï¼š

$$
\mathcal{T}: V_1^* \times V_2^* \times \cdots \times V_n^* \to \mathbb{R}
$$

å…¶ä¸­ $V_i^*$ æ˜¯å‘é‡ç©ºé—´ $V_i$ çš„å¯¹å¶ç©ºé—´ã€‚

**å®é™…ç†è§£**:

- 0é˜¶å¼ é‡ï¼šæ ‡é‡ (scalar)
- 1é˜¶å¼ é‡ï¼šå‘é‡ (vector)
- 2é˜¶å¼ é‡ï¼šçŸ©é˜µ (matrix)
- 3é˜¶å¼ é‡ï¼šç«‹æ–¹ä½“ (cube)
- né˜¶å¼ é‡ï¼šnç»´æ•°ç»„

---

### 2. å¼ é‡çš„ç§©ä¸å½¢çŠ¶

**ç§© (Rank/Order)**:

å¼ é‡çš„ç»´æ•°ï¼Œå³ç´¢å¼•çš„æ•°é‡ã€‚

**å½¢çŠ¶ (Shape)**:

æ¯ä¸ªç»´åº¦çš„å¤§å°ã€‚

**ç¤ºä¾‹**:

- æ ‡é‡: ç§©=0, å½¢çŠ¶=()
- å‘é‡ $\mathbf{v} \in \mathbb{R}^n$: ç§©=1, å½¢çŠ¶=(n,)
- çŸ©é˜µ $A \in \mathbb{R}^{m \times n}$: ç§©=2, å½¢çŠ¶=(m, n)
- RGBå›¾åƒ: ç§©=3, å½¢çŠ¶=(H, W, 3)
- æ‰¹é‡å›¾åƒ: ç§©=4, å½¢çŠ¶=(B, H, W, C)

---

### 3. å¼ é‡çš„ç´¢å¼•

**ç´¢å¼•è¡¨ç¤º**:

- å‘é‡: $v_i$
- çŸ©é˜µ: $A_{ij}$
- 3é˜¶å¼ é‡: $T_{ijk}$
- né˜¶å¼ é‡: $T_{i_1 i_2 \cdots i_n}$

**ç´¢å¼•çº¦å®š**:

- ä¸Šæ ‡ï¼šé€†å˜ç´¢å¼• (contravariant)
- ä¸‹æ ‡ï¼šåå˜ç´¢å¼• (covariant)
- æ·±åº¦å­¦ä¹ ä¸­é€šå¸¸ä½¿ç”¨ä¸‹æ ‡

---

## ğŸ“Š Einsteinæ±‚å’Œçº¦å®š

### 1. åŸºæœ¬è§„åˆ™

**è§„åˆ™ 1.1 (Einsteinæ±‚å’Œçº¦å®š)**:

å½“ä¸€ä¸ªç´¢å¼•åœ¨è¡¨è¾¾å¼ä¸­å‡ºç°ä¸¤æ¬¡ï¼ˆä¸€æ¬¡ä¸Šæ ‡ï¼Œä¸€æ¬¡ä¸‹æ ‡ï¼Œæˆ–ä¸¤æ¬¡ä¸‹æ ‡ï¼‰ï¼Œåˆ™å¯¹è¯¥ç´¢å¼•æ±‚å’Œï¼Œä¸”æ±‚å’Œç¬¦å· $\sum$ å¯ä»¥çœç•¥ã€‚

**ç¤ºä¾‹**:

ä¼ ç»Ÿè¡¨ç¤ºï¼š

$$
\sum_{i=1}^n a_i b_i
$$

Einsteinçº¦å®šï¼š

$$
a_i b_i
$$

**é‡å¤ç´¢å¼•**ç§°ä¸º**å“‘æŒ‡æ ‡ (dummy index)**ï¼Œéé‡å¤ç´¢å¼•ç§°ä¸º**è‡ªç”±æŒ‡æ ‡ (free index)**ã€‚

---

### 2. å¸¸è§è¿ç®—

**å‘é‡å†…ç§¯**:

ä¼ ç»Ÿï¼š$\mathbf{a}^T \mathbf{b} = \sum_{i=1}^n a_i b_i$

Einsteinï¼š$a_i b_i$

**çŸ©é˜µ-å‘é‡ä¹˜æ³•**:

ä¼ ç»Ÿï¼š$(\mathbf{A}\mathbf{x})_i = \sum_{j=1}^n A_{ij} x_j$

Einsteinï¼š$(Ax)_i = A_{ij} x_j$

**çŸ©é˜µä¹˜æ³•**:

ä¼ ç»Ÿï¼š$(AB)_{ij} = \sum_{k=1}^n A_{ik} B_{kj}$

Einsteinï¼š$(AB)_{ij} = A_{ik} B_{kj}$

**çŸ©é˜µè¿¹**:

ä¼ ç»Ÿï¼š$\text{tr}(A) = \sum_{i=1}^n A_{ii}$

Einsteinï¼š$\text{tr}(A) = A_{ii}$

**Frobeniuså†…ç§¯**:

ä¼ ç»Ÿï¼š$\langle A, B \rangle = \sum_{i=1}^m \sum_{j=1}^n A_{ij} B_{ij}$

Einsteinï¼š$\langle A, B \rangle = A_{ij} B_{ij}$

---

### 3. ä¼˜åŠ¿

**ç®€æ´æ€§**:

- é¿å…ç¹ççš„æ±‚å’Œç¬¦å·
- è¡¨è¾¾å¼æ›´ç´§å‡‘

**æ¸…æ™°æ€§**:

- ç´¢å¼•ç»“æ„ä¸€ç›®äº†ç„¶
- è‡ªç”±æŒ‡æ ‡æ˜ç¡®

**æ˜“äºæ¨å¯¼**:

- é“¾å¼æ³•åˆ™ç®€æ´
- æ¢¯åº¦è®¡ç®—ç›´è§‚

---

### 4. å¸¸è§é”™è¯¯ä¸é™·é˜±

Einsteinæ±‚å’Œçº¦å®šè™½ç„¶ç®€æ´ï¼Œä½†å®¹æ˜“è¢«è¯¯ç”¨ã€‚ä»¥ä¸‹æ˜¯å¸¸è§çš„é”™è¯¯ç±»å‹åŠæ­£ç¡®ç”¨æ³•ï¼š

#### é”™è¯¯1ï¼šç´¢å¼•å‡ºç°ä¸‰æ¬¡æˆ–æ›´å¤š

**é”™è¯¯ç¤ºä¾‹**ï¼š

$$
A_{ij} B_{ij} C_{ij} \quad \text{(âŒ é”™è¯¯)}
$$

**é—®é¢˜åˆ†æ**ï¼š

- Einsteinçº¦å®šä»…å¯¹**æ°å¥½å‡ºç°ä¸¤æ¬¡**çš„ç´¢å¼•æ±‚å’Œ
- å‡ºç°ä¸‰æ¬¡çš„ç´¢å¼•ä¸ç¬¦åˆçº¦å®šï¼Œè¡¨è¾¾å¼æ— æ„ä¹‰

**æ­£ç¡®å†™æ³•**ï¼š

å¦‚æœæ„å›¾æ˜¯é€å…ƒç´ ä¹˜æ³•åæ±‚å’Œï¼š

$$
\sum_{ijk} A_{ij} B_{ij} C_{ij} = A_{ij} B_{ij} C_{ij} \quad \text{(éœ€è¦æ˜¾å¼å†™å‡º}\sum\text{)}
$$

æˆ–è€…ä½¿ç”¨æ–°çš„å“‘æŒ‡æ ‡ï¼š

$$
D_{ij} = B_{ij} C_{ij}, \quad S = A_{ij} D_{ij}
$$

#### é”™è¯¯2ï¼šæ··æ·†è‡ªç”±æŒ‡æ ‡ä¸å“‘æŒ‡æ ‡

**é”™è¯¯ç¤ºä¾‹**ï¼š

$$
y_i = A_{ij} x_j + b_j \quad \text{(âŒ é”™è¯¯)}
$$

**é—®é¢˜åˆ†æ**ï¼š

- å·¦è¾¹è‡ªç”±æŒ‡æ ‡æ˜¯ $i$
- å³è¾¹ç¬¬ä¸€é¡¹ï¼š$j$ æ˜¯å“‘æŒ‡æ ‡ï¼ˆè¢«æ±‚å’Œï¼‰ï¼Œ$i$ æ˜¯è‡ªç”±æŒ‡æ ‡ âœ“
- å³è¾¹ç¬¬äºŒé¡¹ï¼š$j$ æ˜¯è‡ªç”±æŒ‡æ ‡ âœ—
- **è‡ªç”±æŒ‡æ ‡ä¸ä¸€è‡´**ï¼

**æ­£ç¡®å†™æ³•**ï¼š

$$
y_i = A_{ij} x_j + b_i
$$

**é€šç”¨è§„åˆ™**ï¼š

> **Einsteinçº¦å®šçš„"ç´¢å¼•ä¸€è‡´æ€§åŸåˆ™"**ï¼š
> è¡¨è¾¾å¼ä¸¤è¾¹çš„è‡ªç”±æŒ‡æ ‡å¿…é¡»å®Œå…¨ä¸€è‡´ï¼ˆåŒ…æ‹¬æ•°é‡å’Œä½ç½®ï¼‰ã€‚

#### é”™è¯¯3ï¼šåœ¨æ·±åº¦å­¦ä¹ ä¸­è¯¯ç”¨æ‰¹æ¬¡ç»´åº¦

**é”™è¯¯ç¤ºä¾‹**ï¼ˆæ‰¹é‡çŸ©é˜µä¹˜æ³•ï¼‰ï¼š

$$
Y_i = W_{ij} X_j \quad \text{(âŒ ç¼ºå°‘æ‰¹æ¬¡ç´¢å¼•)}
$$

**é—®é¢˜åˆ†æ**ï¼š

- æ·±åº¦å­¦ä¹ ä¸­é€šå¸¸æœ‰æ‰¹æ¬¡ç»´åº¦ (batch dimension)
- ä¸Šå¼æ²¡æœ‰è€ƒè™‘æ‰¹æ¬¡ç´¢å¼•ï¼Œåªå¯¹å•ä¸ªæ ·æœ¬æœ‰æ•ˆ

**æ­£ç¡®å†™æ³•**ï¼š

$$
Y_{bi} = W_{ij} X_{bj}
$$

å…¶ä¸­ $b$ æ˜¯æ‰¹æ¬¡ç´¢å¼•ï¼ˆä¸æ±‚å’Œçš„è‡ªç”±æŒ‡æ ‡ï¼‰ã€‚

**PyTorchç¤ºä¾‹**ï¼š

```python
# é”™è¯¯ï¼šå¿½ç•¥æ‰¹æ¬¡ç»´åº¦
# y = W @ x  # å‡è®¾ x.shape = (batch, n), W.shape = (m, n)

# æ­£ç¡®ï¼šä½¿ç”¨ einsum æ˜ç¡®å¤„ç†æ‰¹æ¬¡
y = torch.einsum('ij,bj->bi', W, x)  # b: æ‰¹æ¬¡, i: è¾“å‡º, j: æ±‚å’Œ
```

#### é”™è¯¯4ï¼šè½¬ç½®æ—¶ç´¢å¼•é¡ºåºé”™è¯¯

**é”™è¯¯ç¤ºä¾‹**ï¼š

$$
A_{ji}^T = A_{ij} \quad \text{(âŒ è¡¨ç¤ºæ³•æ··ä¹±)}
$$

**é—®é¢˜åˆ†æ**ï¼š

- è½¬ç½®åº”è¯¥äº¤æ¢ç´¢å¼•**ä½ç½®**ï¼Œè€Œä¸æ˜¯åœ¨ç¬¦å·ä¸Šæ·»åŠ  $T$
- å·²ç»å†™ $A_{ji}$ å°±ä»£è¡¨è½¬ç½®ï¼Œä¸éœ€è¦å†åŠ  $^T$

**æ­£ç¡®å†™æ³•**ï¼š

$$
A^T_{ij} = A_{ji} \quad \text{æˆ–ç®€å†™ä¸º} \quad (A^T)_{ij} = A_{ji}
$$

**çŸ©é˜µä¹˜æ³•ç¤ºä¾‹**ï¼š

$$
(AB^T)_{ij} = A_{ik} B^T_{kj} = A_{ik} B_{jk}
$$

#### é”™è¯¯5ï¼šæ··æ·†ç‚¹ç§¯ä¸å¤–ç§¯

**é”™è¯¯ç¤ºä¾‹**ï¼š

$$
\mathbf{a} \otimes \mathbf{b} = a_i b_i \quad \text{(âŒ è¿™æ˜¯å†…ç§¯ï¼Œä¸æ˜¯å¤–ç§¯)}
$$

**æ­£ç¡®åŒºåˆ†**ï¼š

| è¿ç®— | Einsteinè¡¨ç¤º | ç»“æœç»´åº¦ | ç¤ºä¾‹ |
|------|--------------|----------|------|
| **å†…ç§¯** (ç‚¹ç§¯) | $a_i b_i$ | æ ‡é‡ | $\mathbb{R}^n \times \mathbb{R}^n \to \mathbb{R}$ |
| **å¤–ç§¯** | $a_i b_j$ | çŸ©é˜µ | $\mathbb{R}^n \times \mathbb{R}^m \to \mathbb{R}^{n \times m}$ |

**è®°å¿†æ³•åˆ™**ï¼š

- **é‡å¤ç´¢å¼• â†’ æ±‚å’Œ â†’ é™ç»´**ï¼ˆå†…ç§¯ï¼‰
- **ä¸é‡å¤ç´¢å¼• â†’ æ‰€æœ‰ç»„åˆ â†’ å‡ç»´**ï¼ˆå¤–ç§¯ï¼‰

#### é”™è¯¯6ï¼šæ¢¯åº¦è®¡ç®—ä¸­çš„ç´¢å¼•é”™è¯¯

**åœºæ™¯**ï¼šè®¡ç®— $f(W) = \|WX - Y\|^2$ å…³äº $W$ çš„æ¢¯åº¦

**é”™è¯¯ç¤ºä¾‹**ï¼š

$$
\frac{\partial f}{\partial W_{ij}} = 2(W_{ij} X_j - Y_i) X_j \quad \text{(âŒ ç´¢å¼•ä¸ä¸€è‡´)}
$$

**æ­£ç¡®æ¨å¯¼**ï¼š

$$
\begin{aligned}
f &= (W_{ik} X_k - Y_i)(W_{ij} X_j - Y_i) \\
\frac{\partial f}{\partial W_{pq}} &= \delta_{ip} \delta_{kq} X_k (W_{ij} X_j - Y_i) + (W_{ik} X_k - Y_i) \delta_{ip} \delta_{jq} X_j \\
&= 2(W_{pj} X_j - Y_p) X_q
\end{aligned}
$$

ç®€æ´å†™æ³•ï¼š

$$
\frac{\partial f}{\partial W} = 2(WX - Y) \otimes X
$$

æˆ–ä½¿ç”¨Einsteinçº¦å®šï¼š

$$
[\nabla_W f]_{ij} = 2(W_{ik} X_k - Y_i) X_j
$$

#### é”™è¯¯7ï¼šæ·±åº¦å­¦ä¹ ä¸­çš„ç»´åº¦å¹¿æ’­æ··æ·†

**é”™è¯¯ç¤ºä¾‹**ï¼ˆTransformerä¸­çš„æ³¨æ„åŠ›æœºåˆ¶ï¼‰ï¼š

$$
\text{Attention}_{ij} = \frac{\exp(Q_i K_j)}{\sum_k \exp(Q_i K_k)} \quad \text{(âŒ ç»´åº¦ä¸åŒ¹é…)}
$$

**é—®é¢˜åˆ†æ**ï¼š

- $Q_i$ å’Œ $K_j$ éƒ½æ˜¯å‘é‡ï¼ˆç»´åº¦ä¸º $d$ï¼‰
- ä¸èƒ½ç›´æ¥ç›¸ä¹˜å¾—åˆ°æ ‡é‡

**æ­£ç¡®å†™æ³•**ï¼š

$$
\text{Attention}_{ij} = \frac{\exp(Q_{ia} K_{ja})}{\sum_k \exp(Q_{ia} K_{ka})}
$$

å…¶ä¸­ $a$ æ˜¯ç‰¹å¾ç»´åº¦çš„å“‘æŒ‡æ ‡ã€‚

**PyTorchå®ç°**ï¼š

```python
# Q: (batch, seq, d_k), K: (batch, seq, d_k)
scores = torch.einsum('bqa,bka->bqk', Q, K)  # (batch, seq, seq)
attention = torch.softmax(scores / np.sqrt(d_k), dim=-1)
```

#### å®è·µå»ºè®®æ€»ç»“

| è§„åˆ™ | è¯´æ˜ | æ£€æŸ¥æ–¹æ³• |
|------|------|----------|
| **ç´¢å¼•å‡ºç°æ¬¡æ•°** | å“‘æŒ‡æ ‡æ°å¥½2æ¬¡ï¼Œè‡ªç”±æŒ‡æ ‡æ°å¥½1æ¬¡ | æ•°æ¯ä¸ªç´¢å¼•å­—æ¯çš„å‡ºç°æ¬¡æ•° |
| **è‡ªç”±æŒ‡æ ‡ä¸€è‡´æ€§** | ç­‰å·ä¸¤è¾¹è‡ªç”±æŒ‡æ ‡å¿…é¡»ç›¸åŒ | åˆ—å‡ºå·¦å³ä¸¤è¾¹çš„è‡ªç”±æŒ‡æ ‡å¯¹æ¯” |
| **ç»´åº¦åŒ¹é…** | æ±‚å’Œç´¢å¼•çš„èŒƒå›´å¿…é¡»ä¸€è‡´ | æ£€æŸ¥ $i=1,\ldots,n$ åœ¨æ‰€æœ‰é¡¹ä¸­ç›¸åŒ |
| **æ˜¾å¼æ‰¹æ¬¡ç»´åº¦** | æ·±åº¦å­¦ä¹ ä¸­å§‹ç»ˆè€ƒè™‘æ‰¹æ¬¡ | æ·»åŠ  $b$ ä½œä¸ºç¬¬ä¸€ä¸ªç´¢å¼• |
| **ä½¿ç”¨å·¥å…·éªŒè¯** | ç”¨ `np.einsum` æˆ– `torch.einsum` éªŒè¯ | å…ˆå†™å‡ºEinsteinå½¢å¼ï¼Œå†è½¬ä¸ºä»£ç  |

#### è°ƒè¯•æŠ€å·§

**æŠ€å·§1ï¼šç”»ç´¢å¼•å›¾**:

```text
ç¤ºä¾‹: Y_{bi} = W_{ij} X_{bj}

   j      j
   â†“      â†“
W [i,j] Ã— X[b,j]  â†’  Y[b,i]
   â†‘ free    â†‘ free     â†‘ free
       (æ±‚å’Œ)            (ç»“æœ)
```

**æŠ€å·§2ï¼šä½¿ç”¨`einsum`æ¨¡å¼å­—ç¬¦ä¸²**

```python
# å°†Einsteinçº¦å®šç›´æ¥è½¬ä¸ºeinsum
# Y_{bi} = W_{ij} X_{bj}  â†’  'ij,bj->bi'

import torch
Y = torch.einsum('ij,bj->bi', W, X)
```

**æŠ€å·§3ï¼šç»´åº¦æ£€æŸ¥æ¸…å•**:

```python
def check_einstein(equation, shapes):
    """
    æ£€æŸ¥Einsteinæ±‚å’Œçº¦å®šçš„æ­£ç¡®æ€§
    
    ç¤ºä¾‹:
    equation = "ij,bj->bi"
    shapes = [(m, n), (batch, n)]
    """
    inputs, output = equation.split('->')
    input_terms = inputs.split(',')
    
    # æ£€æŸ¥1ï¼šç»Ÿè®¡ç´¢å¼•å‡ºç°æ¬¡æ•°
    index_counts = {}
    for term in input_terms:
        for idx in term:
            index_counts[idx] = index_counts.get(idx, 0) + 1
    
    # æ£€æŸ¥2ï¼šéªŒè¯å“‘æŒ‡æ ‡ï¼ˆå‡ºç°2æ¬¡ï¼‰
    dummy = [idx for idx, cnt in index_counts.items() if cnt == 2]
    
    # æ£€æŸ¥3ï¼šéªŒè¯è‡ªç”±æŒ‡æ ‡ï¼ˆå‡ºç°1æ¬¡ï¼‰
    free = [idx for idx, cnt in index_counts.items() if cnt == 1]
    
    # æ£€æŸ¥4ï¼šè¾“å‡ºç´¢å¼•å¿…é¡»æ˜¯è‡ªç”±æŒ‡æ ‡
    assert set(output) == set(free), f"è¾“å‡ºç´¢å¼• {output} å¿…é¡»ç­‰äºè‡ªç”±æŒ‡æ ‡ {free}"
    
    print(f"âœ“ å“‘æŒ‡æ ‡ (æ±‚å’Œ): {dummy}")
    print(f"âœ“ è‡ªç”±æŒ‡æ ‡ (ä¿ç•™): {free}")
    print(f"âœ“ è¾“å‡ºå½¢çŠ¶: {output}")
    
# ä½¿ç”¨ç¤ºä¾‹
check_einstein("ij,bj->bi", [(10, 20), (32, 20)])
# è¾“å‡º:
# âœ“ å“‘æŒ‡æ ‡ (æ±‚å’Œ): ['j']
# âœ“ è‡ªç”±æŒ‡æ ‡ (ä¿ç•™): ['i', 'b']
# âœ“ è¾“å‡ºå½¢çŠ¶: bi
```

#### AIåº”ç”¨ä¸­çš„é‡è¦æ€§

åœ¨ç°ä»£æ·±åº¦å­¦ä¹ æ¡†æ¶ä¸­ï¼Œæ­£ç¡®ä½¿ç”¨Einsteinçº¦å®šè‡³å…³é‡è¦ï¼š

**1. Transformeræ¨¡å‹**ï¼š

- å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶æ¶‰åŠ4-5ä¸ªç´¢å¼•ï¼ˆbatch, head, sequence, featureï¼‰
- é”™è¯¯çš„ç´¢å¼•é¡ºåºä¼šå¯¼è‡´éš¾ä»¥è°ƒè¯•çš„ç»´åº¦é”™è¯¯

**2. å›¾ç¥ç»ç½‘ç»œ**ï¼š

- é‚»æ¥çŸ©é˜µä¸èŠ‚ç‚¹ç‰¹å¾çš„ä¹˜æ³•éœ€è¦ç²¾ç¡®çš„ç´¢å¼•ç®¡ç†
- é”™è¯¯ç¤ºä¾‹ï¼š$H_i' = A_{ij} H_j$ vs æ­£ç¡®ï¼š$H_i' = A_{ij} H_j W_{jk}$

**3. å¼ é‡åˆ†è§£**ï¼š

- CPåˆ†è§£ã€Tuckeråˆ†è§£ä¾èµ–å¤æ‚çš„å¤šç´¢å¼•ç¼©å¹¶
- ä¸€ä¸ªç´¢å¼•é”™è¯¯ä¼šå¯¼è‡´å®Œå…¨é”™è¯¯çš„åˆ†è§£ç»“æœ

**æ ¸å¿ƒæ•™è®­**ï¼š

> **åœ¨ç¼–å†™å¤æ‚çš„å¼ é‡è¿ç®—å‰ï¼ŒåŠ¡å¿…å…ˆç”¨Einsteinçº¦å®šå†™å‡ºæ•°å­¦è¡¨è¾¾å¼ï¼ŒéªŒè¯ç´¢å¼•ä¸€è‡´æ€§ï¼Œå†è½¬æ¢ä¸ºä»£ç ã€‚**

---

## ğŸ”¬ å¼ é‡è¿ç®—

### 1. åŸºæœ¬è¿ç®—

**é€å…ƒç´ è¿ç®—**:

- åŠ æ³•ï¼š$C_{ijk} = A_{ijk} + B_{ijk}$
- ä¹˜æ³•ï¼š$C_{ijk} = A_{ijk} \cdot B_{ijk}$

**æ ‡é‡ä¹˜æ³•**:

$$
C_{ijk} = \alpha A_{ijk}
$$

---

### 2. å¼ é‡ç§¯

**å¤–ç§¯ (Outer Product)**:

å‘é‡å¤–ç§¯ï¼š

$$
C_{ij} = a_i b_j
$$

å¼ é‡å¤–ç§¯ï¼š

$$
D_{ijkl} = A_{ij} B_{kl}
$$

**ç¤ºä¾‹**:

$\mathbf{a} \in \mathbb{R}^m$, $\mathbf{b} \in \mathbb{R}^n$ï¼Œåˆ™ $\mathbf{a} \otimes \mathbf{b} \in \mathbb{R}^{m \times n}$ã€‚

---

### 3. å¼ é‡ç¼©å¹¶ (Contraction)

**å®šä¹‰ 3.1 (ç¼©å¹¶)**:

å¯¹å¼ é‡çš„ä¸¤ä¸ªç´¢å¼•æ±‚å’Œï¼Œé™ä½å¼ é‡çš„ç§©ã€‚

**ç¤ºä¾‹**:

çŸ©é˜µä¹˜æ³•æ˜¯ç¼©å¹¶ï¼š

$$
C_{ij} = A_{ik} B_{kj}
$$

å¼ é‡ç¼©å¹¶ï¼š

$$
C_{ij} = T_{ikj} \quad \text{(å¯¹ } k \text{ æ±‚å’Œ)}
$$

**æ€§è´¨**:

- ç¼©å¹¶é™ä½ç§©ï¼š$(p, q)$ å‹å¼ é‡ç¼©å¹¶åå˜ä¸º $(p-1, q-1)$ å‹
- çŸ©é˜µè¿¹æ˜¯å®Œå…¨ç¼©å¹¶ï¼š$\text{tr}(A) = A_{ii}$

---

**å¼ é‡ç¼©å¹¶çš„è¯¦ç»†æ¨å¯¼**:

**æ¨å¯¼1ï¼šçŸ©é˜µä¹˜æ³•ä½œä¸ºç¼©å¹¶**:

è€ƒè™‘ä¸¤ä¸ªçŸ©é˜µ $A \in \mathbb{R}^{m \times n}$ å’Œ $B \in \mathbb{R}^{n \times p}$ã€‚

ä½¿ç”¨Einsteinè®°å·ï¼ŒçŸ©é˜µä¹˜æ³•å¯ä»¥å†™ä¸ºï¼š

$$
C_{ij} = A_{ik} B_{kj}
$$

è¿™é‡Œï¼š

- $i$ æ˜¯è‡ªç”±æŒ‡æ ‡ï¼ˆèŒƒå›´ $1$ åˆ° $m$ï¼‰
- $j$ æ˜¯è‡ªç”±æŒ‡æ ‡ï¼ˆèŒƒå›´ $1$ åˆ° $p$ï¼‰
- $k$ æ˜¯å“‘æŒ‡æ ‡ï¼ˆèŒƒå›´ $1$ åˆ° $n$ï¼Œè‡ªåŠ¨æ±‚å’Œï¼‰

**å±•å¼€å½¢å¼**ï¼š

$$
C_{ij} = \sum_{k=1}^{n} A_{ik} B_{kj}
$$

**å‡ ä½•è§£é‡Š**ï¼š

- æˆ‘ä»¬ä»ä¸¤ä¸ªäºŒé˜¶å¼ é‡ $A_{ik}$ å’Œ $B_{kj}$ å¼€å§‹
- å¯¹å…±åŒçš„ç´¢å¼• $k$ è¿›è¡Œç¼©å¹¶ï¼ˆæ±‚å’Œï¼‰
- å¾—åˆ°æ–°çš„äºŒé˜¶å¼ é‡ $C_{ij}$

**æ¨å¯¼2ï¼šé«˜é˜¶å¼ é‡ç¼©å¹¶**:

è€ƒè™‘ä¸‰é˜¶å¼ é‡ $T_{ijk} \in \mathbb{R}^{m \times n \times p}$ã€‚

å¯¹ç¬¬ä¸€å’Œç¬¬ä¸‰ä¸ªç´¢å¼•è¿›è¡Œç¼©å¹¶ï¼ˆè¦æ±‚ $m = p$ï¼‰ï¼š

$$
S_{j} = T_{iji}
$$

**å±•å¼€å½¢å¼**ï¼š

$$
S_{j} = \sum_{i=1}^{m} T_{iji}
$$

è¿™å°†ä¸‰é˜¶å¼ é‡é™ä¸ºä¸€é˜¶å¼ é‡ï¼ˆå‘é‡ï¼‰ã€‚

**æ¨å¯¼3ï¼šå¼ é‡ç§¯åçš„ç¼©å¹¶**:

è®¾ $\mathbf{u} \in \mathbb{R}^m$ï¼Œ$\mathbf{v} \in \mathbb{R}^n$ï¼Œå®ƒä»¬çš„å¤–ç§¯ä¸ºï¼š

$$
T_{ij} = u_i v_j
$$

ç°åœ¨è€ƒè™‘ $T$ ä¸å¦ä¸€ä¸ªçŸ©é˜µ $A_{jk}$ çš„ç¼©å¹¶ï¼š

$$
R_{ik} = T_{ij} A_{jk} = (u_i v_j) A_{jk} = u_i (v_j A_{jk})
$$

**å±•å¼€**ï¼š

$$
R_{ik} = u_i \sum_{j=1}^{n} v_j A_{jk}
$$

è¿™å¯ä»¥ç†è§£ä¸ºï¼š

1. å…ˆè®¡ç®— $\mathbf{v}^T A$ï¼ˆå¾—åˆ°è¡Œå‘é‡ï¼‰
2. å†ä¸ $\mathbf{u}$ åšå¤–ç§¯

**æ¨å¯¼4ï¼šæ‰¹é‡çŸ©é˜µä¹˜æ³•**:

åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬ç»å¸¸å¤„ç†æ‰¹é‡æ•°æ®ã€‚è€ƒè™‘ï¼š

- æ‰¹é‡è¾“å…¥ï¼š$X \in \mathbb{R}^{B \times d_{in}}$ï¼ˆ$B$ æ˜¯æ‰¹é‡å¤§å°ï¼‰
- æƒé‡çŸ©é˜µï¼š$W \in \mathbb{R}^{d_{in} \times d_{out}}$

æ‰¹é‡çŸ©é˜µä¹˜æ³•ï¼š

$$
Y_{bi} = X_{bj} W_{ji}
$$

**å±•å¼€**ï¼š

$$
Y_{bi} = \sum_{j=1}^{d_{in}} X_{bj} W_{ji}
$$

è¿™é‡Œï¼š

- $b$ ç´¢å¼•éå†æ‰¹é‡ä¸­çš„æ¯ä¸ªæ ·æœ¬
- $j$ æ˜¯è¢«ç¼©å¹¶çš„ç´¢å¼•ï¼ˆè¾“å…¥ç»´åº¦ï¼‰
- $i$ æ˜¯è¾“å‡ºç»´åº¦

**æ¨å¯¼5ï¼šé“¾å¼æ³•åˆ™ä¸ç¼©å¹¶**:

è€ƒè™‘å¤åˆå‡½æ•° $f(g(x))$ï¼Œå…¶ä¸­ï¼š

- $y_i = g_i(x_j)$
- $z = f(y_i)$

ä½¿ç”¨é“¾å¼æ³•åˆ™è®¡ç®— $\frac{\partial z}{\partial x_j}$ï¼š

$$
\frac{\partial z}{\partial x_j} = \frac{\partial z}{\partial y_i} \frac{\partial y_i}{\partial x_j}
$$

åœ¨Einsteinè®°å·ä¸­ï¼š

$$
\frac{\partial z}{\partial x_j} = \frac{\partial z}{\partial y_i} J_{ij}
$$

å…¶ä¸­ $J_{ij} = \frac{\partial y_i}{\partial x_j}$ æ˜¯JacobiançŸ©é˜µã€‚

è¿™æ˜¯ä¸€ä¸ªçŸ©é˜µ-å‘é‡ä¹˜æ³•ï¼Œé€šè¿‡å¯¹ $i$ ç´¢å¼•çš„ç¼©å¹¶å®ç°ã€‚

**æ¨å¯¼6ï¼šäºŒæ¬¡å‹**:

è€ƒè™‘äºŒæ¬¡å‹ $q(\mathbf{x}) = \mathbf{x}^T A \mathbf{x}$ã€‚

ä½¿ç”¨Einsteinè®°å·ï¼š

$$
q = x_i A_{ij} x_j
$$

**å±•å¼€**ï¼š

$$
q = \sum_{i=1}^{n} \sum_{j=1}^{n} x_i A_{ij} x_j
$$

è¿™æ¶‰åŠä¸¤æ¬¡ç¼©å¹¶ï¼š

1. ç¬¬ä¸€æ¬¡ç¼©å¹¶ï¼š$y_i = A_{ij} x_j$ï¼ˆçŸ©é˜µ-å‘é‡ä¹˜æ³•ï¼‰
2. ç¬¬äºŒæ¬¡ç¼©å¹¶ï¼š$q = x_i y_i$ï¼ˆå†…ç§¯ï¼‰

**æ¢¯åº¦æ¨å¯¼**ï¼š

$$
\frac{\partial q}{\partial x_k} = \frac{\partial}{\partial x_k}(x_i A_{ij} x_j)
$$

ä½¿ç”¨ä¹˜ç§¯æ³•åˆ™ï¼š

$$
\frac{\partial q}{\partial x_k} = \delta_{ik} A_{ij} x_j + x_i A_{ij} \delta_{jk} = A_{kj} x_j + x_i A_{ik}
$$

å¦‚æœ $A$ æ˜¯å¯¹ç§°çŸ©é˜µï¼ˆ$A_{ij} = A_{ji}$ï¼‰ï¼š

$$
\frac{\partial q}{\partial x_k} = 2 A_{kj} x_j
$$

å‘é‡å½¢å¼ï¼š$\nabla q = 2A\mathbf{x}$

**å®é™…åº”ç”¨ç¤ºä¾‹**ï¼š

åœ¨ç¥ç»ç½‘ç»œä¸­ï¼Œè€ƒè™‘å…¨è¿æ¥å±‚çš„åå‘ä¼ æ’­ï¼š

å‰å‘ï¼š$\mathbf{y} = W\mathbf{x} + \mathbf{b}$

Einsteinè®°å·ï¼š$y_i = W_{ij} x_j + b_i$

æŸå¤±å¯¹æƒé‡çš„æ¢¯åº¦ï¼š

$$
\frac{\partial L}{\partial W_{ij}} = \frac{\partial L}{\partial y_k} \frac{\partial y_k}{\partial W_{ij}}
$$

ç”±äº $\frac{\partial y_k}{\partial W_{ij}} = \delta_{ki} x_j$ï¼š

$$
\frac{\partial L}{\partial W_{ij}} = \frac{\partial L}{\partial y_k} \delta_{ki} x_j = \frac{\partial L}{\partial y_i} x_j
$$

è¿™æ­£æ˜¯å¤–ç§¯ $\frac{\partial L}{\partial \mathbf{y}} \otimes \mathbf{x}$ã€‚

---

## ğŸ’¡ åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„åº”ç”¨

### 1. å…¨è¿æ¥å±‚

**å‰å‘ä¼ æ’­**:

$$
y_i = W_{ij} x_j + b_i
$$

**æ‰¹é‡å¤„ç†**:

$$
Y_{bi} = W_{ij} X_{bj} + b_i
$$

å…¶ä¸­ $b$ æ˜¯æ‰¹é‡ç´¢å¼•ã€‚

**æ¢¯åº¦**:

$$
\frac{\partial L}{\partial W_{ij}} = \frac{\partial L}{\partial y_k} \frac{\partial y_k}{\partial W_{ij}} = \frac{\partial L}{\partial y_i} x_j
$$

Einsteinçº¦å®šï¼š

$$
\frac{\partial L}{\partial W_{ij}} = \delta_i x_j
$$

å…¶ä¸­ $\delta_i = \frac{\partial L}{\partial y_i}$ã€‚

---

### 2. å·ç§¯å±‚

**2Då·ç§¯**:

$$
Y_{bchw} = W_{ckhw'} X_{bk(h+h')(w+w')}
$$

å…¶ä¸­ï¼š

- $b$: æ‰¹é‡ç´¢å¼•
- $c$: è¾“å‡ºé€šé“
- $k$: è¾“å…¥é€šé“
- $h, w$: ç©ºé—´ä½ç½®
- $h', w'$: å·ç§¯æ ¸ä½ç½®

**ç®€åŒ–è¡¨ç¤º**:

$$
Y = W * X
$$

---

### 3. æ³¨æ„åŠ›æœºåˆ¶

**Scaled Dot-Product Attention**:

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V
$$

**Einsteinçº¦å®š**:

$$
A_{ij} = \frac{Q_{ik} K_{jk}}{\sqrt{d_k}}
$$

$$
\text{Output}_{ik} = \text{softmax}(A)_{ij} V_{jk}
$$

**æ‰¹é‡å¤šå¤´æ³¨æ„åŠ›**:

$$
\text{Output}_{bhik} = \text{softmax}(A)_{bhij} V_{bhjk}
$$

å…¶ä¸­ï¼š

- $b$: æ‰¹é‡
- $h$: å¤´æ•°
- $i, j$: åºåˆ—ä½ç½®
- $k$: ç‰¹å¾ç»´åº¦

---

### 4. æ‰¹å¤„ç†

**æ‰¹é‡çŸ©é˜µä¹˜æ³•**:

$$
Y_{bij} = X_{bik} W_{bkj}
$$

**æ‰¹é‡å½’ä¸€åŒ–**:

$$
\hat{x}_{bi} = \frac{x_{bi} - \mu_i}{\sqrt{\sigma_i^2 + \epsilon}}
$$

å…¶ä¸­ï¼š

$$
\mu_i = \frac{1}{B} x_{bi}
$$

$$
\sigma_i^2 = \frac{1}{B} (x_{bi} - \mu_i)^2
$$

---

## ğŸ¨ å¼ é‡åˆ†è§£

### 1. CPåˆ†è§£ (CANDECOMP/PARAFAC)

**å®šä¹‰**:

å°†3é˜¶å¼ é‡ $\mathcal{T} \in \mathbb{R}^{I \times J \times K}$ åˆ†è§£ä¸ºç§©1å¼ é‡çš„å’Œï¼š

$$
T_{ijk} = \sum_{r=1}^R a_{ir} b_{jr} c_{kr}
$$

çŸ©é˜µå½¢å¼ï¼š

$$
\mathcal{T} = \sum_{r=1}^R \mathbf{a}_r \circ \mathbf{b}_r \circ \mathbf{c}_r
$$

å…¶ä¸­ $\circ$ è¡¨ç¤ºå¤–ç§¯ã€‚

**åº”ç”¨**:

- æ¨¡å‹å‹ç¼©
- ç‰¹å¾æå–
- æ¨èç³»ç»Ÿ

---

### 2. Tuckeråˆ†è§£

**å®šä¹‰**:

$$
T_{ijk} = G_{pqr} A_{ip} B_{jq} C_{kr}
$$

å…¶ä¸­ $G$ æ˜¯æ ¸å¿ƒå¼ é‡ï¼Œ$A, B, C$ æ˜¯å› å­çŸ©é˜µã€‚

**çŸ©é˜µå½¢å¼**:

$$
\mathcal{T} = \mathcal{G} \times_1 A \times_2 B \times_3 C
$$

**ä¸SVDçš„å…³ç³»**:

Tuckeråˆ†è§£æ˜¯SVDçš„é«˜é˜¶æ¨å¹¿ã€‚

---

### 3. å¼ é‡ç½‘ç»œ

**å¼ é‡ç½‘ç»œè¡¨ç¤º**:

å¤æ‚çš„å¼ é‡è¿ç®—å¯ä»¥ç”¨ç½‘ç»œå›¾è¡¨ç¤ºï¼š

```text
    i       j
    |       |
  â”Œâ”€â”´â”€â”   â”Œâ”€â”´â”€â”
  â”‚ A â”‚â”€â”€â”€â”‚ B â”‚
  â””â”€â”¬â”€â”˜   â””â”€â”¬â”€â”˜
    k       l
```

è¡¨ç¤ºï¼š$C_{ijkl} = A_{ik} B_{jl}$

**åº”ç”¨**:

- é‡å­è®¡ç®—
- æ·±åº¦å­¦ä¹ æ¨¡å‹å‹ç¼©
- ç‰©ç†æ¨¡æ‹Ÿ

---

## ğŸ”§ é«˜çº§å¼ é‡è¿ç®—

### 1. å¼ é‡é‡å¡‘ (Reshape)

**å®šä¹‰**:

æ”¹å˜å¼ é‡çš„å½¢çŠ¶ï¼Œä½†ä¿æŒå…ƒç´ æ€»æ•°ä¸å˜ã€‚

**ç¤ºä¾‹**:

$(2, 3, 4) \to (6, 4)$ æˆ– $(24,)$

**åº”ç”¨**:

- å±•å¹³ (Flatten): $(B, H, W, C) \to (B, H \times W \times C)$
- é‡å¡‘: $(B, L, D) \to (B, L, H, D/H)$ (å¤šå¤´æ³¨æ„åŠ›)

---

### 2. è½¬ç½®ä¸ç½®æ¢

**è½¬ç½® (Transpose)**:

äº¤æ¢ä¸¤ä¸ªç»´åº¦ï¼š

$$
B_{ji} = A_{ij}
$$

**ç½®æ¢ (Permute)**:

ä»»æ„é‡æ’ç»´åº¦ï¼š

$$
B_{ikj} = A_{ijk}
$$

**åº”ç”¨**:

- çŸ©é˜µè½¬ç½®: $(m, n) \to (n, m)$
- é€šé“é¡ºåºè½¬æ¢: $(B, H, W, C) \to (B, C, H, W)$

---

### 3. å¹¿æ’­ (Broadcasting)

**å®šä¹‰**:

è‡ªåŠ¨æ‰©å±•å¼ é‡çš„ç»´åº¦ä»¥åŒ¹é…è¿ç®—ã€‚

**è§„åˆ™**:

1. å¦‚æœä¸¤ä¸ªå¼ é‡ç»´æ•°ä¸åŒï¼Œåœ¨è¾ƒå°çš„å¼ é‡å‰é¢è¡¥1
2. å¦‚æœæŸç»´åº¦å¤§å°ä¸º1ï¼Œåˆ™æ²¿è¯¥ç»´åº¦å¤åˆ¶

**ç¤ºä¾‹**:

```python
A: (3, 1)
B: (1, 4)
A + B: (3, 4)  # å¹¿æ’­
```

**åº”ç”¨**:

- åç½®åŠ æ³•: $(B, N) + (N,) \to (B, N)$
- æ‰¹é‡å½’ä¸€åŒ–

---

### 4. å¼ é‡åˆ‡ç‰‡

**åˆ‡ç‰‡æ“ä½œ**:

æå–å¼ é‡çš„å­é›†ï¼š

$$
B = A[i_1:i_2, j_1:j_2, :]
$$

**åº”ç”¨**:

- æå–æ‰¹é‡å­é›†
- æå–ç‰¹å¾å­é›†
- çª—å£æ“ä½œ

---

## ğŸ’» Pythonå®ç°

```python
import numpy as np
import torch
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# 1. Einsteinæ±‚å’Œçº¦å®šç¤ºä¾‹
def einstein_examples():
    """Einsteinæ±‚å’Œçº¦å®šç¤ºä¾‹"""
    print("=== Einsteinæ±‚å’Œçº¦å®š ===\n")
    
    # å‘é‡å†…ç§¯
    a = np.array([1, 2, 3])
    b = np.array([4, 5, 6])
    
    # ä¼ ç»Ÿæ–¹æ³•
    dot_traditional = np.sum(a * b)
    # Einsteinçº¦å®š (numpy)
    dot_einstein = np.einsum('i,i->', a, b)
    
    print(f"å‘é‡å†…ç§¯:")
    print(f"  ä¼ ç»Ÿ: {dot_traditional}")
    print(f"  Einstein: {dot_einstein}\n")
    
    # çŸ©é˜µ-å‘é‡ä¹˜æ³•
    A = np.array([[1, 2], [3, 4]])
    x = np.array([1, 2])
    
    # ä¼ ç»Ÿæ–¹æ³•
    y_traditional = A @ x
    # Einsteinçº¦å®š
    y_einstein = np.einsum('ij,j->i', A, x)
    
    print(f"çŸ©é˜µ-å‘é‡ä¹˜æ³•:")
    print(f"  ä¼ ç»Ÿ: {y_traditional}")
    print(f"  Einstein: {y_einstein}\n")
    
    # çŸ©é˜µä¹˜æ³•
    B = np.array([[5, 6], [7, 8]])
    
    # ä¼ ç»Ÿæ–¹æ³•
    C_traditional = A @ B
    # Einsteinçº¦å®š
    C_einstein = np.einsum('ik,kj->ij', A, B)
    
    print(f"çŸ©é˜µä¹˜æ³•:")
    print(f"  ä¼ ç»Ÿ:\n{C_traditional}")
    print(f"  Einstein:\n{C_einstein}\n")
    
    # çŸ©é˜µè¿¹
    trace_traditional = np.trace(A)
    trace_einstein = np.einsum('ii->', A)
    
    print(f"çŸ©é˜µè¿¹:")
    print(f"  ä¼ ç»Ÿ: {trace_traditional}")
    print(f"  Einstein: {trace_einstein}\n")
    
    # å¤–ç§¯
    outer_traditional = np.outer(a, b)
    outer_einstein = np.einsum('i,j->ij', a, b)
    
    print(f"å¤–ç§¯:")
    print(f"  ä¼ ç»Ÿ:\n{outer_traditional}")
    print(f"  Einstein:\n{outer_einstein}\n")


# 2. å¼ é‡è¿ç®—
def tensor_operations():
    """å¼ é‡åŸºæœ¬è¿ç®—"""
    print("=== å¼ é‡è¿ç®— ===\n")
    
    # åˆ›å»ºå¼ é‡
    T = np.random.randn(2, 3, 4)
    print(f"å¼ é‡å½¢çŠ¶: {T.shape}")
    print(f"å¼ é‡ç§©: {T.ndim}\n")
    
    # å¼ é‡ç¼©å¹¶
    # å¯¹ç¬¬äºŒä¸ªç»´åº¦æ±‚å’Œ
    T_contract = np.einsum('ijk->ik', T)
    print(f"ç¼©å¹¶åå½¢çŠ¶: {T_contract.shape}\n")
    
    # å¼ é‡è½¬ç½®
    T_transpose = np.transpose(T, (2, 0, 1))
    print(f"è½¬ç½®åå½¢çŠ¶: {T_transpose.shape}\n")
    
    # å¼ é‡é‡å¡‘
    T_reshape = T.reshape(6, 4)
    print(f"é‡å¡‘åå½¢çŠ¶: {T_reshape.shape}\n")


# 3. æ‰¹é‡çŸ©é˜µä¹˜æ³•
def batch_matrix_multiply():
    """æ‰¹é‡çŸ©é˜µä¹˜æ³•"""
    print("=== æ‰¹é‡çŸ©é˜µä¹˜æ³• ===\n")
    
    # æ‰¹é‡å¤§å°ä¸º4ï¼ŒçŸ©é˜µå¤§å°ä¸º3x2å’Œ2x5
    A = np.random.randn(4, 3, 2)
    B = np.random.randn(4, 2, 5)
    
    # æ–¹æ³•1: å¾ªç¯
    C_loop = np.zeros((4, 3, 5))
    for i in range(4):
        C_loop[i] = A[i] @ B[i]
    
    # æ–¹æ³•2: Einsteinçº¦å®š
    C_einstein = np.einsum('bij,bjk->bik', A, B)
    
    # æ–¹æ³•3: PyTorch bmm
    A_torch = torch.from_numpy(A)
    B_torch = torch.from_numpy(B)
    C_torch = torch.bmm(A_torch, B_torch).numpy()
    
    print(f"ç»“æœå½¢çŠ¶: {C_einstein.shape}")
    print(f"æ–¹æ³•ä¸€è‡´æ€§: {np.allclose(C_loop, C_einstein, C_torch)}\n")


# 4. æ³¨æ„åŠ›æœºåˆ¶
def attention_mechanism():
    """æ³¨æ„åŠ›æœºåˆ¶å®ç°"""
    print("=== æ³¨æ„åŠ›æœºåˆ¶ ===\n")
    
    # å‚æ•°
    batch_size = 2
    seq_len = 4
    d_model = 8
    
    # Q, K, V
    Q = np.random.randn(batch_size, seq_len, d_model)
    K = np.random.randn(batch_size, seq_len, d_model)
    V = np.random.randn(batch_size, seq_len, d_model)
    
    # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
    # scores = Q @ K^T / sqrt(d_model)
    scores = np.einsum('bik,bjk->bij', Q, K) / np.sqrt(d_model)
    
    # Softmax
    exp_scores = np.exp(scores - np.max(scores, axis=-1, keepdims=True))
    attention_weights = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)
    
    # åŠ æƒæ±‚å’Œ
    # output = attention_weights @ V
    output = np.einsum('bij,bjk->bik', attention_weights, V)
    
    print(f"Qå½¢çŠ¶: {Q.shape}")
    print(f"Kå½¢çŠ¶: {K.shape}")
    print(f"Vå½¢çŠ¶: {V.shape}")
    print(f"æ³¨æ„åŠ›æƒé‡å½¢çŠ¶: {attention_weights.shape}")
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}\n")


# 5. å¼ é‡åˆ†è§£ - CPåˆ†è§£
def cp_decomposition(T, rank):
    """CPåˆ†è§£ (ç®€åŒ–ç‰ˆï¼Œä½¿ç”¨ALSç®—æ³•)"""
    I, J, K = T.shape
    
    # åˆå§‹åŒ–å› å­çŸ©é˜µ
    A = np.random.randn(I, rank)
    B = np.random.randn(J, rank)
    C = np.random.randn(K, rank)
    
    # ALSè¿­ä»£
    n_iter = 10
    for _ in range(n_iter):
        # æ›´æ–°A
        V = np.einsum('jr,kr->jkr', B, C)
        V_flat = V.reshape(J*K, rank)
        T_flat = T.reshape(I, J*K)
        A = T_flat @ V_flat @ np.linalg.inv(V_flat.T @ V_flat + 1e-8*np.eye(rank))
        
        # æ›´æ–°B (ç±»ä¼¼)
        V = np.einsum('ir,kr->ikr', A, C)
        V_flat = V.reshape(I*K, rank)
        T_flat = T.transpose(1, 0, 2).reshape(J, I*K)
        B = T_flat @ V_flat @ np.linalg.inv(V_flat.T @ V_flat + 1e-8*np.eye(rank))
        
        # æ›´æ–°C (ç±»ä¼¼)
        V = np.einsum('ir,jr->ijr', A, B)
        V_flat = V.reshape(I*J, rank)
        T_flat = T.transpose(2, 0, 1).reshape(K, I*J)
        C = T_flat @ V_flat @ np.linalg.inv(V_flat.T @ V_flat + 1e-8*np.eye(rank))
    
    # é‡æ„å¼ é‡
    T_reconstructed = np.einsum('ir,jr,kr->ijk', A, B, C)
    
    return A, B, C, T_reconstructed


def test_cp_decomposition():
    """æµ‹è¯•CPåˆ†è§£"""
    print("=== CPåˆ†è§£ ===\n")
    
    # åˆ›å»ºä½ç§©å¼ é‡
    rank = 3
    I, J, K = 10, 12, 8
    
    A_true = np.random.randn(I, rank)
    B_true = np.random.randn(J, rank)
    C_true = np.random.randn(K, rank)
    
    T = np.einsum('ir,jr,kr->ijk', A_true, B_true, C_true)
    
    # CPåˆ†è§£
    A, B, C, T_reconstructed = cp_decomposition(T, rank)
    
    # è®¡ç®—è¯¯å·®
    error = np.linalg.norm(T - T_reconstructed) / np.linalg.norm(T)
    
    print(f"åŸå§‹å¼ é‡å½¢çŠ¶: {T.shape}")
    print(f"åˆ†è§£ç§©: {rank}")
    print(f"é‡æ„è¯¯å·®: {error:.6f}\n")


# 6. å¹¿æ’­ç¤ºä¾‹
def broadcasting_examples():
    """å¹¿æ’­ç¤ºä¾‹"""
    print("=== å¹¿æ’­ ===\n")
    
    # ç¤ºä¾‹1: å‘é‡åŠ åˆ°çŸ©é˜µæ¯ä¸€è¡Œ
    A = np.array([[1, 2, 3],
                  [4, 5, 6]])
    b = np.array([10, 20, 30])
    
    C = A + b  # å¹¿æ’­
    print(f"çŸ©é˜µ + å‘é‡ (å¹¿æ’­):")
    print(f"Aå½¢çŠ¶: {A.shape}")
    print(f"bå½¢çŠ¶: {b.shape}")
    print(f"Cå½¢çŠ¶: {C.shape}")
    print(f"C:\n{C}\n")
    
    # ç¤ºä¾‹2: æ‰¹é‡å½’ä¸€åŒ–
    X = np.random.randn(32, 10)  # (batch, features)
    
    # è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
    mean = np.mean(X, axis=0, keepdims=True)  # (1, 10)
    std = np.std(X, axis=0, keepdims=True)    # (1, 10)
    
    # å½’ä¸€åŒ– (å¹¿æ’­)
    X_normalized = (X - mean) / (std + 1e-8)
    
    print(f"æ‰¹é‡å½’ä¸€åŒ–:")
    print(f"Xå½¢çŠ¶: {X.shape}")
    print(f"meanå½¢çŠ¶: {mean.shape}")
    print(f"stdå½¢çŠ¶: {std.shape}")
    print(f"X_normalizedå½¢çŠ¶: {X_normalized.shape}\n")


# 7. å¯è§†åŒ–3Då¼ é‡
def visualize_3d_tensor():
    """å¯è§†åŒ–3Då¼ é‡"""
    # åˆ›å»ºç®€å•çš„3Då¼ é‡
    T = np.zeros((5, 5, 5))
    T[2, 2, 2] = 1  # ä¸­å¿ƒç‚¹
    T[1:4, 1:4, 1:4] = 0.5  # å†…éƒ¨ç«‹æ–¹ä½“
    
    fig = plt.figure(figsize=(10, 8))
    ax = fig.add_subplot(111, projection='3d')
    
    # æ‰¾åˆ°éé›¶å…ƒç´ 
    x, y, z = np.where(T > 0)
    colors = T[x, y, z]
    
    # ç»˜åˆ¶
    scatter = ax.scatter(x, y, z, c=colors, cmap='viridis', 
                        s=100, alpha=0.6, edgecolors='k')
    
    ax.set_xlabel('X')
    ax.set_ylabel('Y')
    ax.set_zlabel('Z')
    ax.set_title('3D Tensor Visualization')
    
    plt.colorbar(scatter, ax=ax, label='Value')
    # plt.show()


if __name__ == "__main__":
    print("=" * 60)
    print("å¼ é‡è¿ç®—ä¸Einsteinæ±‚å’Œçº¦å®šç¤ºä¾‹")
    print("=" * 60 + "\n")
    
    einstein_examples()
    tensor_operations()
    batch_matrix_multiply()
    attention_mechanism()
    test_cp_decomposition()
    broadcasting_examples()
    
    print("\nå¯è§†åŒ–3Då¼ é‡...")
    visualize_3d_tensor()
    
    print("\næ‰€æœ‰ç¤ºä¾‹å®Œæˆï¼")
```

---

## ğŸ“š ç»ƒä¹ é¢˜

### ç»ƒä¹ 1ï¼šEinsteinæ±‚å’Œçº¦å®š

ä½¿ç”¨Einsteinæ±‚å’Œçº¦å®šè¡¨ç¤ºä»¥ä¸‹è¿ç®—ï¼š

1. å‘é‡å¤–ç§¯ï¼š$\mathbf{a} \otimes \mathbf{b}$
2. çŸ©é˜µFrobeniusèŒƒæ•°ï¼š$\|A\|_F$
3. æ‰¹é‡çŸ©é˜µä¹˜æ³•

### ç»ƒä¹ 2ï¼šå¼ é‡ç¼©å¹¶

ç»™å®š3é˜¶å¼ é‡ $T \in \mathbb{R}^{3 \times 4 \times 5}$ï¼Œè®¡ç®—ï¼š

1. å¯¹ç¬¬ä¸€ä¸ªç´¢å¼•ç¼©å¹¶
2. å¯¹ç¬¬äºŒä¸ªç´¢å¼•ç¼©å¹¶
3. å®Œå…¨ç¼©å¹¶ï¼ˆæ‰€æœ‰ç´¢å¼•ï¼‰

### ç»ƒä¹ 3ï¼šå·ç§¯è¿ç®—

ä½¿ç”¨Einsteinçº¦å®šè¡¨ç¤º2Då·ç§¯è¿ç®—ï¼ŒåŒ…æ‹¬ï¼š

1. å•é€šé“å·ç§¯
2. å¤šé€šé“å·ç§¯
3. æ‰¹é‡å·ç§¯

### ç»ƒä¹ 4ï¼šæ³¨æ„åŠ›æœºåˆ¶

å®ç°å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼Œä½¿ç”¨Einsteinçº¦å®šè¡¨ç¤ºæ‰€æœ‰è¿ç®—ã€‚

---

## ğŸ“ ç›¸å…³è¯¾ç¨‹

| å¤§å­¦ | è¯¾ç¨‹ |
|------|------|
| **MIT** | 18.065 - Matrix Methods in Data Analysis |
| **Stanford** | CS231n - Convolutional Neural Networks |
| **CMU** | 10-708 - Probabilistic Graphical Models |
| **UC Berkeley** | CS189 - Introduction to Machine Learning |

---

## ğŸ“– å‚è€ƒæ–‡çŒ®

1. **Kolda & Bader (2009)**. *Tensor Decompositions and Applications*. SIAM Review.

2. **Cichocki et al. (2015)**. *Tensor Decompositions for Signal Processing Applications*. IEEE Signal Processing Magazine.

3. **Goodfellow et al. (2016)**. *Deep Learning*. MIT Press. (Chapter 2)

4. **Novikov et al. (2015)**. *Tensorizing Neural Networks*. NeurIPS.

5. **Paszke et al. (2019)**. *PyTorch: An Imperative Style, High-Performance Deep Learning Library*. NeurIPS.

---

*æœ€åæ›´æ–°ï¼š2025å¹´10æœˆ*-
