# çŸ©é˜µå¾®åˆ†ä¸Jacobian/Hessian (Matrix Calculus & Jacobian/Hessian)

> **The Mathematics of Backpropagation**
>
> åå‘ä¼ æ’­çš„æ•°å­¦åŸºç¡€

---

## ç›®å½•

- [çŸ©é˜µå¾®åˆ†ä¸Jacobian/Hessian (Matrix Calculus \& Jacobian/Hessian)](#çŸ©é˜µå¾®åˆ†ä¸jacobianhessian-matrix-calculus--jacobianhessian)
  - [ç›®å½•](#ç›®å½•)
  - [ğŸ“‹ æ ¸å¿ƒæ€æƒ³](#-æ ¸å¿ƒæ€æƒ³)
  - [ğŸ¯ æ ‡é‡å¯¹å‘é‡/çŸ©é˜µçš„å¯¼æ•°](#-æ ‡é‡å¯¹å‘é‡çŸ©é˜µçš„å¯¼æ•°)
    - [1. æ ‡é‡å¯¹å‘é‡çš„å¯¼æ•°](#1-æ ‡é‡å¯¹å‘é‡çš„å¯¼æ•°)
    - [2. æ ‡é‡å¯¹çŸ©é˜µçš„å¯¼æ•°](#2-æ ‡é‡å¯¹çŸ©é˜µçš„å¯¼æ•°)
    - [3. å¸¸è§å¯¼æ•°å…¬å¼](#3-å¸¸è§å¯¼æ•°å…¬å¼)
  - [ğŸ“Š å‘é‡å¯¹å‘é‡çš„å¯¼æ•° - JacobiançŸ©é˜µ](#-å‘é‡å¯¹å‘é‡çš„å¯¼æ•°---jacobiançŸ©é˜µ)
    - [1. Jacobianå®šä¹‰](#1-jacobianå®šä¹‰)
    - [2. Jacobiançš„æ€§è´¨](#2-jacobiançš„æ€§è´¨)
    - [3. é“¾å¼æ³•åˆ™](#3-é“¾å¼æ³•åˆ™)
  - [ğŸ”¬ äºŒé˜¶å¯¼æ•° - HessiançŸ©é˜µ](#-äºŒé˜¶å¯¼æ•°---hessiançŸ©é˜µ)
    - [1. Hessianå®šä¹‰](#1-hessianå®šä¹‰)
    - [2. Hessiançš„æ€§è´¨](#2-hessiançš„æ€§è´¨)
    - [3. äºŒé˜¶Taylorå±•å¼€](#3-äºŒé˜¶taylorå±•å¼€)
  - [ğŸ’¡ çŸ©é˜µå¾®åˆ†æŠ€å·§](#-çŸ©é˜µå¾®åˆ†æŠ€å·§)
    - [1. å¾®åˆ†æ³•åˆ™](#1-å¾®åˆ†æ³•åˆ™)
    - [2. è¿¹æŠ€å·§](#2-è¿¹æŠ€å·§)
    - [3. Kroneckerç§¯](#3-kroneckerç§¯)
  - [ğŸ¨ åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„åº”ç”¨](#-åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„åº”ç”¨)
    - [1. åå‘ä¼ æ’­](#1-åå‘ä¼ æ’­)
    - [2. æ¢¯åº¦ä¸‹é™](#2-æ¢¯åº¦ä¸‹é™)
    - [3. äºŒé˜¶ä¼˜åŒ–æ–¹æ³•](#3-äºŒé˜¶ä¼˜åŒ–æ–¹æ³•)
    - [4. ç¥ç»ç½‘ç»œä¸­çš„å¸¸è§å¯¼æ•°](#4-ç¥ç»ç½‘ç»œä¸­çš„å¸¸è§å¯¼æ•°)
  - [ğŸ”§ é«˜çº§ä¸»é¢˜](#-é«˜çº§ä¸»é¢˜)
    - [1. å‘é‡åŒ–æŠ€å·§](#1-å‘é‡åŒ–æŠ€å·§)
    - [2. è‡ªåŠ¨å¾®åˆ†](#2-è‡ªåŠ¨å¾®åˆ†)
    - [3. é«˜é˜¶å¯¼æ•°](#3-é«˜é˜¶å¯¼æ•°)
  - [ğŸ’» Pythonå®ç°](#-pythonå®ç°)
  - [ğŸ“š ç»ƒä¹ é¢˜](#-ç»ƒä¹ é¢˜)
    - [ç»ƒä¹ 1ï¼šæ¢¯åº¦è®¡ç®—](#ç»ƒä¹ 1æ¢¯åº¦è®¡ç®—)
    - [ç»ƒä¹ 2ï¼šJacobiançŸ©é˜µ](#ç»ƒä¹ 2jacobiançŸ©é˜µ)
    - [ç»ƒä¹ 3ï¼šHessiançŸ©é˜µ](#ç»ƒä¹ 3hessiançŸ©é˜µ)
    - [ç»ƒä¹ 4ï¼šåå‘ä¼ æ’­](#ç»ƒä¹ 4åå‘ä¼ æ’­)
  - [ğŸ“ ç›¸å…³è¯¾ç¨‹](#-ç›¸å…³è¯¾ç¨‹)
  - [ğŸ“– å‚è€ƒæ–‡çŒ®](#-å‚è€ƒæ–‡çŒ®)

---

## ğŸ“‹ æ ¸å¿ƒæ€æƒ³

**çŸ©é˜µå¾®åˆ†**æ˜¯å¤„ç†å¤šå…ƒå‡½æ•°å¯¼æ•°çš„å¼ºå¤§å·¥å…·ï¼Œæ˜¯æ·±åº¦å­¦ä¹ ä¸­åå‘ä¼ æ’­ç®—æ³•çš„æ•°å­¦åŸºç¡€ã€‚**JacobiançŸ©é˜µ**å’Œ**HessiançŸ©é˜µ**åˆ†åˆ«è¡¨ç¤ºä¸€é˜¶å’ŒäºŒé˜¶å¯¼æ•°ä¿¡æ¯ã€‚

**ä¸ºä»€ä¹ˆçŸ©é˜µå¾®åˆ†é‡è¦**:

```text
æ·±åº¦å­¦ä¹ ä¸­çš„åº”ç”¨:
â”œâ”€ åå‘ä¼ æ’­: è®¡ç®—æ¢¯åº¦
â”œâ”€ æ¢¯åº¦ä¸‹é™: å‚æ•°æ›´æ–°
â”œâ”€ äºŒé˜¶ä¼˜åŒ–: Newtonæ³•ã€æ‹ŸNewtonæ³•
â””â”€ æ•æ„Ÿæ€§åˆ†æ: å‚æ•°é‡è¦æ€§

æ ¸å¿ƒå·¥å…·:
â”œâ”€ æ¢¯åº¦ (Gradient): âˆ‡f
â”œâ”€ JacobiançŸ©é˜µ: J
â”œâ”€ HessiançŸ©é˜µ: H
â””â”€ é“¾å¼æ³•åˆ™: å¤åˆå‡½æ•°æ±‚å¯¼
```

---

## ğŸ¯ æ ‡é‡å¯¹å‘é‡/çŸ©é˜µçš„å¯¼æ•°

### 1. æ ‡é‡å¯¹å‘é‡çš„å¯¼æ•°

**å®šä¹‰ 1.1 (æ¢¯åº¦)**:

è®¾ $f: \mathbb{R}^n \to \mathbb{R}$ï¼Œåˆ™ $f$ å¯¹ $\mathbf{x}$ çš„**æ¢¯åº¦**å®šä¹‰ä¸ºï¼š

$$
\nabla f(\mathbf{x}) = \begin{bmatrix}
\frac{\partial f}{\partial x_1} \\
\frac{\partial f}{\partial x_2} \\
\vdots \\
\frac{\partial f}{\partial x_n}
\end{bmatrix} \in \mathbb{R}^n
$$

**å¸ƒå±€çº¦å®š**:

- **åˆ†å­å¸ƒå±€ (Numerator layout)**: æ¢¯åº¦ä¸ºåˆ—å‘é‡
- **åˆ†æ¯å¸ƒå±€ (Denominator layout)**: æ¢¯åº¦ä¸ºè¡Œå‘é‡

æœ¬æ–‡æ¡£é‡‡ç”¨**åˆ†å­å¸ƒå±€**ã€‚

---

### 2. æ ‡é‡å¯¹çŸ©é˜µçš„å¯¼æ•°

**å®šä¹‰ 2.1 (çŸ©é˜µå¯¼æ•°)**:

è®¾ $f: \mathbb{R}^{m \times n} \to \mathbb{R}$ï¼Œåˆ™ $f$ å¯¹çŸ©é˜µ $X$ çš„å¯¼æ•°å®šä¹‰ä¸ºï¼š

$$
\frac{\partial f}{\partial X} = \begin{bmatrix}
\frac{\partial f}{\partial X_{11}} & \cdots & \frac{\partial f}{\partial X_{1n}} \\
\vdots & \ddots & \vdots \\
\frac{\partial f}{\partial X_{m1}} & \cdots & \frac{\partial f}{\partial X_{mn}}
\end{bmatrix} \in \mathbb{R}^{m \times n}
$$

---

### 3. å¸¸è§å¯¼æ•°å…¬å¼

**çº¿æ€§å‡½æ•°**:

$$
f(\mathbf{x}) = \mathbf{a}^T \mathbf{x} \quad \Rightarrow \quad \nabla f = \mathbf{a}
$$

**äºŒæ¬¡å‹**:

$$
f(\mathbf{x}) = \mathbf{x}^T A \mathbf{x} \quad \Rightarrow \quad \nabla f = (A + A^T) \mathbf{x}
$$

å¦‚æœ $A$ æ˜¯å¯¹ç§°çš„ï¼Œåˆ™ï¼š

$$
\nabla f = 2A\mathbf{x}
$$

**èŒƒæ•°**:

$$
f(\mathbf{x}) = \|\mathbf{x}\|^2 = \mathbf{x}^T \mathbf{x} \quad \Rightarrow \quad \nabla f = 2\mathbf{x}
$$

**çŸ©é˜µè¿¹**:

$$
f(X) = \text{tr}(AX) \quad \Rightarrow \quad \frac{\partial f}{\partial X} = A^T
$$

$$
f(X) = \text{tr}(X^T AX) \quad \Rightarrow \quad \frac{\partial f}{\partial X} = (A + A^T)X
$$

**è¡Œåˆ—å¼**:

$$
f(X) = \log \det(X) \quad \Rightarrow \quad \frac{\partial f}{\partial X} = X^{-T}
$$

---

## ğŸ“Š å‘é‡å¯¹å‘é‡çš„å¯¼æ•° - JacobiançŸ©é˜µ

### 1. Jacobianå®šä¹‰

**å®šä¹‰ 1.1 (JacobiançŸ©é˜µ)**:

è®¾ $\mathbf{f}: \mathbb{R}^n \to \mathbb{R}^m$ï¼Œ$\mathbf{f}(\mathbf{x}) = [f_1(\mathbf{x}), \ldots, f_m(\mathbf{x})]^T$ï¼Œåˆ™**JacobiançŸ©é˜µ**å®šä¹‰ä¸ºï¼š

$$
J = \frac{\partial \mathbf{f}}{\partial \mathbf{x}} = \begin{bmatrix}
\frac{\partial f_1}{\partial x_1} & \cdots & \frac{\partial f_1}{\partial x_n} \\
\vdots & \ddots & \vdots \\
\frac{\partial f_m}{\partial x_1} & \cdots & \frac{\partial f_m}{\partial x_n}
\end{bmatrix} \in \mathbb{R}^{m \times n}
$$

**ç¬¬ $i$ è¡Œ**æ˜¯ $f_i$ çš„æ¢¯åº¦çš„è½¬ç½®ï¼š

$$
J_{i,:} = (\nabla f_i)^T
$$

---

### 2. Jacobiançš„æ€§è´¨

**çº¿æ€§æ€§**:

$$
\frac{\partial (A\mathbf{f} + B\mathbf{g})}{\partial \mathbf{x}} = A \frac{\partial \mathbf{f}}{\partial \mathbf{x}} + B \frac{\partial \mathbf{g}}{\partial \mathbf{x}}
$$

**ä¹˜ç§¯æ³•åˆ™**:

$$
\frac{\partial (\mathbf{f}^T \mathbf{g})}{\partial \mathbf{x}} = \mathbf{f}^T \frac{\partial \mathbf{g}}{\partial \mathbf{x}} + \mathbf{g}^T \frac{\partial \mathbf{f}}{\partial \mathbf{x}}
$$

---

### 3. é“¾å¼æ³•åˆ™

**å®šç† 3.1 (é“¾å¼æ³•åˆ™)**:

è®¾ $\mathbf{f}: \mathbb{R}^n \to \mathbb{R}^m$ï¼Œ$\mathbf{g}: \mathbb{R}^m \to \mathbb{R}^k$ï¼Œåˆ™ï¼š

$$
\frac{\partial (\mathbf{g} \circ \mathbf{f})}{\partial \mathbf{x}} = \frac{\partial \mathbf{g}}{\partial \mathbf{y}} \cdot \frac{\partial \mathbf{f}}{\partial \mathbf{x}}
$$

å…¶ä¸­ $\mathbf{y} = \mathbf{f}(\mathbf{x})$ã€‚

**çŸ©é˜µå½¢å¼**:

$$
J_{\mathbf{g} \circ \mathbf{f}} = J_{\mathbf{g}} \cdot J_{\mathbf{f}}
$$

**ç¤ºä¾‹**:

è®¾ $\mathbf{y} = A\mathbf{x}$ï¼Œ$\mathbf{z} = B\mathbf{y}$ï¼Œåˆ™ï¼š

$$
\frac{\partial \mathbf{z}}{\partial \mathbf{x}} = B \cdot A = BA
$$

---

## ğŸ”¬ äºŒé˜¶å¯¼æ•° - HessiançŸ©é˜µ

### 1. Hessianå®šä¹‰

**å®šä¹‰ 1.1 (HessiançŸ©é˜µ)**:

è®¾ $f: \mathbb{R}^n \to \mathbb{R}$ï¼Œåˆ™**HessiançŸ©é˜µ**å®šä¹‰ä¸ºï¼š

$$
H = \nabla^2 f = \frac{\partial^2 f}{\partial \mathbf{x}^2} = \begin{bmatrix}
\frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
\frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & \cdots & \frac{\partial^2 f}{\partial x_2 \partial x_n} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial^2 f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_n \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_n^2}
\end{bmatrix} \in \mathbb{R}^{n \times n}
$$

**å…ƒç´ è¡¨ç¤º**:

$$
H_{ij} = \frac{\partial^2 f}{\partial x_i \partial x_j}
$$

---

### 2. Hessiançš„æ€§è´¨

**å¯¹ç§°æ€§ (Schwarzå®šç†)**:

å¦‚æœ $f$ æ˜¯ $C^2$ å‡½æ•°ï¼ˆäºŒé˜¶è¿ç»­å¯å¾®ï¼‰ï¼Œåˆ™ï¼š

$$
H_{ij} = H_{ji} \quad \Rightarrow \quad H = H^T
$$

**æ­£å®šæ€§ä¸å‡¸æ€§**:

- $H \succ 0$ (æ­£å®š) $\Rightarrow$ $f$ æ˜¯**ä¸¥æ ¼å‡¸å‡½æ•°**
- $H \succeq 0$ (åŠæ­£å®š) $\Rightarrow$ $f$ æ˜¯**å‡¸å‡½æ•°**
- $H \prec 0$ (è´Ÿå®š) $\Rightarrow$ $f$ æ˜¯**ä¸¥æ ¼å‡¹å‡½æ•°**

**ä¸´ç•Œç‚¹åˆ¤å®š**:

è®¾ $\nabla f(\mathbf{x}^*) = \mathbf{0}$ï¼Œåˆ™ï¼š

- $H(\mathbf{x}^*) \succ 0$ $\Rightarrow$ $\mathbf{x}^*$ æ˜¯**å±€éƒ¨æå°å€¼**
- $H(\mathbf{x}^*) \prec 0$ $\Rightarrow$ $\mathbf{x}^*$ æ˜¯**å±€éƒ¨æå¤§å€¼**
- $H(\mathbf{x}^*)$ ä¸å®š $\Rightarrow$ $\mathbf{x}^*$ æ˜¯**éç‚¹**

---

### 3. äºŒé˜¶Taylorå±•å¼€

**å®šç† 3.1 (äºŒé˜¶Taylorå±•å¼€)**:

$$
f(\mathbf{x} + \mathbf{h}) \approx f(\mathbf{x}) + \nabla f(\mathbf{x})^T \mathbf{h} + \frac{1}{2} \mathbf{h}^T H(\mathbf{x}) \mathbf{h}
$$

**åº”ç”¨**:

- å‡½æ•°é€¼è¿‘
- Newtonæ³•
- ä¿¡èµ–åŸŸæ–¹æ³•

---

## ğŸ’¡ çŸ©é˜µå¾®åˆ†æŠ€å·§

### 1. å¾®åˆ†æ³•åˆ™

**åŸºæœ¬å¾®åˆ†**:

$$
d(\mathbf{x}^T \mathbf{a}) = \mathbf{a}^T d\mathbf{x}
$$

$$
d(\mathbf{x}^T A \mathbf{x}) = \mathbf{x}^T (A + A^T) d\mathbf{x}
$$

$$
d(\text{tr}(AX)) = \text{tr}(A \, dX)
$$

**ä¹˜ç§¯æ³•åˆ™**:

$$
d(XY) = (dX)Y + X(dY)
$$

**é€†çŸ©é˜µ**:

$$
d(X^{-1}) = -X^{-1} (dX) X^{-1}
$$

**è¡Œåˆ—å¼**:

$$
d(\det(X)) = \det(X) \cdot \text{tr}(X^{-1} dX)
$$

---

### 2. è¿¹æŠ€å·§

**æŠ€å·§ 2.1 (è¿¹çš„å¾ªç¯æ€§)**:

$$
\text{tr}(ABC) = \text{tr}(CAB) = \text{tr}(BCA)
$$

**åº”ç”¨**:

å°†æ ‡é‡è¡¨ç¤ºä¸ºè¿¹ï¼Œä¾¿äºæ±‚å¯¼ï¼š

$$
\mathbf{a}^T \mathbf{b} = \text{tr}(\mathbf{a}^T \mathbf{b}) = \text{tr}(\mathbf{b} \mathbf{a}^T)
$$

**ç¤ºä¾‹**:

$$
\frac{\partial (\mathbf{a}^T X \mathbf{b})}{\partial X} = \frac{\partial \text{tr}(\mathbf{a}^T X \mathbf{b})}{\partial X} = \frac{\partial \text{tr}(\mathbf{b} \mathbf{a}^T X)}{\partial X} = \mathbf{a} \mathbf{b}^T
$$

---

### 3. Kroneckerç§¯

**å®šä¹‰ 3.1 (Kroneckerç§¯)**:

$$
A \otimes B = \begin{bmatrix}
a_{11}B & \cdots & a_{1n}B \\
\vdots & \ddots & \vdots \\
a_{m1}B & \cdots & a_{mn}B
\end{bmatrix}
$$

**å‘é‡åŒ–**:

$$
\text{vec}(AXB) = (B^T \otimes A) \text{vec}(X)
$$

**åº”ç”¨**:

ç®€åŒ–çŸ©é˜µå¯¹çŸ©é˜µçš„å¯¼æ•°è®¡ç®—ã€‚

---

## ğŸ¨ åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„åº”ç”¨

### 1. åå‘ä¼ æ’­

**å‰å‘ä¼ æ’­**:

$$
\mathbf{z}^{(l)} = W^{(l)} \mathbf{a}^{(l-1)} + \mathbf{b}^{(l)}
$$

$$
\mathbf{a}^{(l)} = \sigma(\mathbf{z}^{(l)})
$$

**åå‘ä¼ æ’­**:

$$
\delta^{(l)} = \frac{\partial L}{\partial \mathbf{z}^{(l)}}
$$

**é“¾å¼æ³•åˆ™**:

$$
\delta^{(l-1)} = (W^{(l)})^T \delta^{(l)} \odot \sigma'(\mathbf{z}^{(l-1)})
$$

**æ¢¯åº¦**:

$$
\frac{\partial L}{\partial W^{(l)}} = \delta^{(l)} (\mathbf{a}^{(l-1)})^T
$$

$$
\frac{\partial L}{\partial \mathbf{b}^{(l)}} = \delta^{(l)}
$$

---

### 2. æ¢¯åº¦ä¸‹é™

**å‚æ•°æ›´æ–°**:

$$
\theta_{t+1} = \theta_t - \eta \nabla L(\theta_t)
$$

**æ‰¹é‡æ¢¯åº¦ä¸‹é™**:

$$
\nabla L(\theta) = \frac{1}{n} \sum_{i=1}^n \nabla L_i(\theta)
$$

**éšæœºæ¢¯åº¦ä¸‹é™**:

$$
\theta_{t+1} = \theta_t - \eta \nabla L_i(\theta_t)
$$

---

### 3. äºŒé˜¶ä¼˜åŒ–æ–¹æ³•

**Newtonæ³•**:

$$
\theta_{t+1} = \theta_t - H^{-1} \nabla L(\theta_t)
$$

**æ‹ŸNewtonæ³• (BFGS)**:

ä½¿ç”¨è¿‘ä¼¼Hessian $B_t$ï¼š

$$
\theta_{t+1} = \theta_t - \eta B_t^{-1} \nabla L(\theta_t)
$$

**Gauss-Newtonæ³•**:

å¯¹äºæœ€å°äºŒä¹˜é—®é¢˜ $L = \frac{1}{2} \|\mathbf{r}(\theta)\|^2$ï¼š

$$
H \approx J^T J
$$

å…¶ä¸­ $J$ æ˜¯æ®‹å·®çš„Jacobianã€‚

---

### 4. ç¥ç»ç½‘ç»œä¸­çš„å¸¸è§å¯¼æ•°

**Sigmoidæ¿€æ´»**:

$$
\sigma(z) = \frac{1}{1 + e^{-z}} \quad \Rightarrow \quad \sigma'(z) = \sigma(z)(1 - \sigma(z))
$$

**Tanhæ¿€æ´»**:

$$
\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}} \quad \Rightarrow \quad \tanh'(z) = 1 - \tanh^2(z)
$$

**ReLUæ¿€æ´»**:

$$
\text{ReLU}(z) = \max(0, z) \quad \Rightarrow \quad \text{ReLU}'(z) = \begin{cases}
1 & z > 0 \\
0 & z \leq 0
\end{cases}
$$

**Softmax**:

$$
\text{softmax}(\mathbf{z})_i = \frac{e^{z_i}}{\sum_j e^{z_j}}
$$

$$
\frac{\partial \text{softmax}(\mathbf{z})_i}{\partial z_j} = \text{softmax}(\mathbf{z})_i (\delta_{ij} - \text{softmax}(\mathbf{z})_j)
$$

**äº¤å‰ç†µæŸå¤±**:

$$
L = -\sum_i y_i \log \hat{y}_i
$$

$$
\frac{\partial L}{\partial \mathbf{z}} = \hat{\mathbf{y}} - \mathbf{y}
$$

ï¼ˆå½“ä½¿ç”¨softmaxæ¿€æ´»æ—¶ï¼‰

---

## ğŸ”§ é«˜çº§ä¸»é¢˜

### 1. å‘é‡åŒ–æŠ€å·§

**æ‰¹é‡å¤„ç†**:

å°†æ‰¹é‡æ•°æ®ç»„ç»‡ä¸ºçŸ©é˜µï¼Œä½¿ç”¨çŸ©é˜µè¿ç®—ä»£æ›¿å¾ªç¯ï¼š

$$
Y = XW + \mathbf{b} \mathbf{1}^T
$$

å…¶ä¸­ $X \in \mathbb{R}^{B \times n}$ï¼Œ$W \in \mathbb{R}^{n \times m}$ã€‚

**æ¢¯åº¦**:

$$
\frac{\partial L}{\partial W} = X^T \delta
$$

---

### 2. è‡ªåŠ¨å¾®åˆ†

**å‰å‘æ¨¡å¼ (Forward Mode)**:

è®¡ç®— $\frac{\partial \mathbf{y}}{\partial x_i}$ï¼Œé€‚åˆè¾“å…¥ç»´åº¦å°çš„æƒ…å†µã€‚

**åå‘æ¨¡å¼ (Reverse Mode)**:

è®¡ç®— $\frac{\partial y_i}{\partial \mathbf{x}}$ï¼Œé€‚åˆè¾“å‡ºç»´åº¦å°çš„æƒ…å†µï¼ˆæ·±åº¦å­¦ä¹ å¸¸ç”¨ï¼‰ã€‚

**è®¡ç®—å›¾**:

```text
x â†’ fâ‚ â†’ yâ‚ â†’ fâ‚‚ â†’ yâ‚‚ â†’ ... â†’ L
```

åå‘ä¼ æ’­æ²¿è®¡ç®—å›¾åå‘è®¡ç®—æ¢¯åº¦ã€‚

---

### 3. é«˜é˜¶å¯¼æ•°

**ä¸‰é˜¶å¼ é‡**:

$$
\frac{\partial^3 f}{\partial x_i \partial x_j \partial x_k}
$$

**åº”ç”¨**:

- é«˜é˜¶ä¼˜åŒ–æ–¹æ³•
- æ•æ„Ÿæ€§åˆ†æ
- ä¸ç¡®å®šæ€§é‡åŒ–

---

## ğŸ’» Pythonå®ç°

```python
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# 1. æ¢¯åº¦è®¡ç®—ç¤ºä¾‹
def gradient_examples():
    """æ¢¯åº¦è®¡ç®—ç¤ºä¾‹"""
    print("=== æ¢¯åº¦è®¡ç®— ===\n")
    
    # çº¿æ€§å‡½æ•°: f(x) = a^T x
    a = np.array([1, 2, 3])
    x = np.array([4, 5, 6])
    
    f = np.dot(a, x)
    grad_f = a  # æ¢¯åº¦å°±æ˜¯ a
    
    print(f"çº¿æ€§å‡½æ•° f(x) = a^T x:")
    print(f"  f = {f}")
    print(f"  âˆ‡f = {grad_f}\n")
    
    # äºŒæ¬¡å‹: f(x) = x^T A x
    A = np.array([[2, 1], [1, 3]])
    x = np.array([1, 2])
    
    f = x @ A @ x
    grad_f = (A + A.T) @ x  # æ¢¯åº¦ = (A + A^T)x
    
    print(f"äºŒæ¬¡å‹ f(x) = x^T A x:")
    print(f"  f = {f}")
    print(f"  âˆ‡f = {grad_f}\n")
    
    # èŒƒæ•°: f(x) = ||x||^2
    x = np.array([3, 4])
    f = np.dot(x, x)
    grad_f = 2 * x
    
    print(f"èŒƒæ•°å¹³æ–¹ f(x) = ||x||^2:")
    print(f"  f = {f}")
    print(f"  âˆ‡f = {grad_f}\n")


# 2. JacobiançŸ©é˜µè®¡ç®—
def jacobian_example():
    """JacobiançŸ©é˜µè®¡ç®—"""
    print("=== JacobiançŸ©é˜µ ===\n")
    
    # å®šä¹‰å‡½æ•° f: R^2 -> R^3
    def f(x):
        return np.array([
            x[0]**2 + x[1],
            x[0] * x[1],
            x[0] + x[1]**2
        ])
    
    # è§£æJacobian
    def jacobian_f(x):
        return np.array([
            [2*x[0], 1],
            [x[1], x[0]],
            [1, 2*x[1]]
        ])
    
    # æ•°å€¼Jacobian (æœ‰é™å·®åˆ†)
    def numerical_jacobian(f, x, eps=1e-7):
        n = len(x)
        m = len(f(x))
        J = np.zeros((m, n))
        
        for i in range(n):
            x_plus = x.copy()
            x_plus[i] += eps
            x_minus = x.copy()
            x_minus[i] -= eps
            
            J[:, i] = (f(x_plus) - f(x_minus)) / (2 * eps)
        
        return J
    
    # æµ‹è¯•ç‚¹
    x = np.array([1.0, 2.0])
    
    J_analytical = jacobian_f(x)
    J_numerical = numerical_jacobian(f, x)
    
    print(f"æµ‹è¯•ç‚¹ x = {x}")
    print(f"\nè§£æJacobian:\n{J_analytical}")
    print(f"\næ•°å€¼Jacobian:\n{J_numerical}")
    print(f"\nè¯¯å·®: {np.max(np.abs(J_analytical - J_numerical)):.2e}\n")


# 3. HessiançŸ©é˜µè®¡ç®—
def hessian_example():
    """HessiançŸ©é˜µè®¡ç®—"""
    print("=== HessiançŸ©é˜µ ===\n")
    
    # å®šä¹‰å‡½æ•° f: R^2 -> R
    def f(x):
        return x[0]**2 + 2*x[0]*x[1] + 3*x[1]**2
    
    # è§£æHessian
    def hessian_f(x):
        return np.array([
            [2, 2],
            [2, 6]
        ])
    
    # æ•°å€¼Hessian
    def numerical_hessian(f, x, eps=1e-5):
        n = len(x)
        H = np.zeros((n, n))
        
        for i in range(n):
            for j in range(n):
                x_pp = x.copy()
                x_pp[i] += eps
                x_pp[j] += eps
                
                x_pm = x.copy()
                x_pm[i] += eps
                x_pm[j] -= eps
                
                x_mp = x.copy()
                x_mp[i] -= eps
                x_mp[j] += eps
                
                x_mm = x.copy()
                x_mm[i] -= eps
                x_mm[j] -= eps
                
                H[i, j] = (f(x_pp) - f(x_pm) - f(x_mp) + f(x_mm)) / (4 * eps**2)
        
        return H
    
    x = np.array([1.0, 1.0])
    
    H_analytical = hessian_f(x)
    H_numerical = numerical_hessian(f, x)
    
    print(f"æµ‹è¯•ç‚¹ x = {x}")
    print(f"\nè§£æHessian:\n{H_analytical}")
    print(f"\næ•°å€¼Hessian:\n{H_numerical}")
    print(f"\nå¯¹ç§°æ€§æ£€æŸ¥: {np.allclose(H_analytical, H_analytical.T)}")
    
    # æ­£å®šæ€§æ£€æŸ¥
    eigenvalues = np.linalg.eigvalsh(H_analytical)
    print(f"\nç‰¹å¾å€¼: {eigenvalues}")
    if np.all(eigenvalues > 0):
        print("Hessianæ˜¯æ­£å®šçš„ â†’ å‡½æ•°æ˜¯ä¸¥æ ¼å‡¸çš„\n")


# 4. åå‘ä¼ æ’­ç¤ºä¾‹
class SimpleNN:
    """ç®€å•ç¥ç»ç½‘ç»œ"""
    
    def __init__(self, input_dim, hidden_dim, output_dim):
        # åˆå§‹åŒ–æƒé‡
        self.W1 = np.random.randn(input_dim, hidden_dim) * 0.01
        self.b1 = np.zeros(hidden_dim)
        self.W2 = np.random.randn(hidden_dim, output_dim) * 0.01
        self.b2 = np.zeros(output_dim)
        
    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))
    
    def sigmoid_derivative(self, a):
        return a * (1 - a)
    
    def forward(self, X):
        """å‰å‘ä¼ æ’­"""
        self.z1 = X @ self.W1 + self.b1
        self.a1 = self.sigmoid(self.z1)
        self.z2 = self.a1 @ self.W2 + self.b2
        self.a2 = self.sigmoid(self.z2)
        return self.a2
    
    def backward(self, X, y, output):
        """åå‘ä¼ æ’­"""
        m = X.shape[0]
        
        # è¾“å‡ºå±‚æ¢¯åº¦
        dz2 = output - y
        dW2 = (self.a1.T @ dz2) / m
        db2 = np.sum(dz2, axis=0) / m
        
        # éšè—å±‚æ¢¯åº¦
        da1 = dz2 @ self.W2.T
        dz1 = da1 * self.sigmoid_derivative(self.a1)
        dW1 = (X.T @ dz1) / m
        db1 = np.sum(dz1, axis=0) / m
        
        return dW1, db1, dW2, db2
    
    def train(self, X, y, epochs=1000, lr=0.1):
        """è®­ç»ƒ"""
        losses = []
        
        for epoch in range(epochs):
            # å‰å‘ä¼ æ’­
            output = self.forward(X)
            
            # è®¡ç®—æŸå¤±
            loss = np.mean((output - y)**2)
            losses.append(loss)
            
            # åå‘ä¼ æ’­
            dW1, db1, dW2, db2 = self.backward(X, y, output)
            
            # æ›´æ–°å‚æ•°
            self.W1 -= lr * dW1
            self.b1 -= lr * db1
            self.W2 -= lr * dW2
            self.b2 -= lr * db2
            
            if epoch % 100 == 0:
                print(f"Epoch {epoch}, Loss: {loss:.6f}")
        
        return losses


def backpropagation_demo():
    """åå‘ä¼ æ’­æ¼”ç¤º"""
    print("=== åå‘ä¼ æ’­ ===\n")
    
    # ç”ŸæˆXORæ•°æ®
    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
    y = np.array([[0], [1], [1], [0]])
    
    # åˆ›å»ºå¹¶è®­ç»ƒç½‘ç»œ
    nn = SimpleNN(input_dim=2, hidden_dim=4, output_dim=1)
    losses = nn.train(X, y, epochs=5000, lr=1.0)
    
    # æµ‹è¯•
    print("\næµ‹è¯•ç»“æœ:")
    predictions = nn.forward(X)
    for i in range(len(X)):
        print(f"  è¾“å…¥: {X[i]}, é¢„æµ‹: {predictions[i][0]:.4f}, çœŸå®: {y[i][0]}")
    
    # ç»˜åˆ¶æŸå¤±æ›²çº¿
    plt.figure(figsize=(10, 6))
    plt.plot(losses)
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training Loss')
    plt.grid(True, alpha=0.3)
    plt.yscale('log')
    # plt.show()


# 5. æ¢¯åº¦ä¸‹é™å¯è§†åŒ–
def gradient_descent_visualization():
    """æ¢¯åº¦ä¸‹é™å¯è§†åŒ–"""
    # å®šä¹‰å‡½æ•°: f(x, y) = x^2 + 2y^2
    def f(x, y):
        return x**2 + 2*y**2
    
    def grad_f(x, y):
        return np.array([2*x, 4*y])
    
    # æ¢¯åº¦ä¸‹é™
    x, y = 3.0, 2.0
    lr = 0.1
    trajectory = [(x, y)]
    
    for _ in range(20):
        grad = grad_f(x, y)
        x -= lr * grad[0]
        y -= lr * grad[1]
        trajectory.append((x, y))
    
    trajectory = np.array(trajectory)
    
    # ç»˜åˆ¶
    fig = plt.figure(figsize=(12, 5))
    
    # 2Dç­‰é«˜çº¿å›¾
    ax1 = fig.add_subplot(121)
    x_range = np.linspace(-4, 4, 100)
    y_range = np.linspace(-3, 3, 100)
    X, Y = np.meshgrid(x_range, y_range)
    Z = f(X, Y)
    
    contour = ax1.contour(X, Y, Z, levels=20, cmap='viridis')
    ax1.clabel(contour, inline=True, fontsize=8)
    ax1.plot(trajectory[:, 0], trajectory[:, 1], 'ro-', linewidth=2, markersize=6)
    ax1.plot(trajectory[0, 0], trajectory[0, 1], 'go', markersize=10, label='Start')
    ax1.plot(trajectory[-1, 0], trajectory[-1, 1], 'r*', markersize=15, label='End')
    ax1.set_xlabel('x')
    ax1.set_ylabel('y')
    ax1.set_title('Gradient Descent (2D)')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 3Dæ›²é¢å›¾
    ax2 = fig.add_subplot(122, projection='3d')
    ax2.plot_surface(X, Y, Z, alpha=0.6, cmap='viridis')
    ax2.plot(trajectory[:, 0], trajectory[:, 1], 
             [f(x, y) for x, y in trajectory], 
             'ro-', linewidth=2, markersize=6)
    ax2.set_xlabel('x')
    ax2.set_ylabel('y')
    ax2.set_zlabel('f(x, y)')
    ax2.set_title('Gradient Descent (3D)')
    
    plt.tight_layout()
    # plt.show()


# 6. Newtonæ³• vs æ¢¯åº¦ä¸‹é™
def newton_vs_gradient_descent():
    """Newtonæ³• vs æ¢¯åº¦ä¸‹é™"""
    print("=== Newtonæ³• vs æ¢¯åº¦ä¸‹é™ ===\n")
    
    # å®šä¹‰å‡½æ•°
    def f(x):
        return x[0]**2 + 2*x[1]**2
    
    def grad_f(x):
        return np.array([2*x[0], 4*x[1]])
    
    def hessian_f(x):
        return np.array([[2, 0], [0, 4]])
    
    # æ¢¯åº¦ä¸‹é™
    x_gd = np.array([3.0, 2.0])
    trajectory_gd = [x_gd.copy()]
    lr = 0.1
    
    for _ in range(20):
        x_gd -= lr * grad_f(x_gd)
        trajectory_gd.append(x_gd.copy())
    
    # Newtonæ³•
    x_newton = np.array([3.0, 2.0])
    trajectory_newton = [x_newton.copy()]
    
    for _ in range(5):
        grad = grad_f(x_newton)
        H = hessian_f(x_newton)
        x_newton -= np.linalg.solve(H, grad)
        trajectory_newton.append(x_newton.copy())
    
    print(f"æ¢¯åº¦ä¸‹é™ (20æ­¥): ç»ˆç‚¹ = {trajectory_gd[-1]}")
    print(f"Newtonæ³• (5æ­¥): ç»ˆç‚¹ = {trajectory_newton[-1]}\n")


if __name__ == "__main__":
    print("=" * 60)
    print("çŸ©é˜µå¾®åˆ†ä¸Jacobian/Hessianç¤ºä¾‹")
    print("=" * 60 + "\n")
    
    gradient_examples()
    jacobian_example()
    hessian_example()
    backpropagation_demo()
    
    print("\nå¯è§†åŒ–...")
    gradient_descent_visualization()
    newton_vs_gradient_descent()
    
    print("\næ‰€æœ‰ç¤ºä¾‹å®Œæˆï¼")
```

---

## ğŸ“š ç»ƒä¹ é¢˜

### ç»ƒä¹ 1ï¼šæ¢¯åº¦è®¡ç®—

è®¡ç®—ä»¥ä¸‹å‡½æ•°çš„æ¢¯åº¦ï¼š

1. $f(\mathbf{x}) = \mathbf{a}^T \mathbf{x} + b$
2. $f(\mathbf{x}) = \|\mathbf{x} - \mathbf{a}\|^2$
3. $f(\mathbf{x}) = \mathbf{x}^T A \mathbf{x} + \mathbf{b}^T \mathbf{x} + c$

### ç»ƒä¹ 2ï¼šJacobiançŸ©é˜µ

è®¡ç®—ä»¥ä¸‹å‡½æ•°çš„JacobiançŸ©é˜µï¼š

1. $\mathbf{f}(\mathbf{x}) = A\mathbf{x} + \mathbf{b}$
2. $\mathbf{f}(\mathbf{x}) = \text{softmax}(\mathbf{x})$

### ç»ƒä¹ 3ï¼šHessiançŸ©é˜µ

è®¡ç®—ä»¥ä¸‹å‡½æ•°çš„HessiançŸ©é˜µï¼Œå¹¶åˆ¤æ–­å‡¸æ€§ï¼š

1. $f(\mathbf{x}) = \mathbf{x}^T A \mathbf{x}$ï¼ˆ$A$ å¯¹ç§°ï¼‰
2. $f(\mathbf{x}) = \log(\sum_i e^{x_i})$

### ç»ƒä¹ 4ï¼šåå‘ä¼ æ’­

æ‰‹åŠ¨æ¨å¯¼ä¸¤å±‚ç¥ç»ç½‘ç»œçš„åå‘ä¼ æ’­å…¬å¼ã€‚

---

## ğŸ“ ç›¸å…³è¯¾ç¨‹

| å¤§å­¦ | è¯¾ç¨‹ |
|------|------|
| **MIT** | 18.065 - Matrix Methods in Data Analysis |
| **Stanford** | CS229 - Machine Learning |
| **CMU** | 10-701 - Machine Learning |
| **UC Berkeley** | CS189 - Introduction to Machine Learning |

---

## ğŸ“– å‚è€ƒæ–‡çŒ®

1. **Petersen & Pedersen (2012)**. *The Matrix Cookbook*. Technical University of Denmark.

2. **Goodfellow et al. (2016)**. *Deep Learning*. MIT Press. (Chapter 4)

3. **Boyd & Vandenberghe (2004)**. *Convex Optimization*. Cambridge University Press.

4. **Magnus & Neudecker (2019)**. *Matrix Differential Calculus with Applications in Statistics and Econometrics*. Wiley.

5. **Griewank & Walther (2008)**. *Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation*. SIAM.

---

*æœ€åæ›´æ–°ï¼š2025å¹´10æœˆ*-
