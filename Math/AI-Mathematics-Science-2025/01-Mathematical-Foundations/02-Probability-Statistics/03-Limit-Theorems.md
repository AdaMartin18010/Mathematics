# æé™å®šç† (Limit Theorems)

> **The Foundation of Statistical Inference**
>
> ç»Ÿè®¡æ¨æ–­çš„ç†è®ºåŸºç¡€

---

## ç›®å½•

- [æé™å®šç† (Limit Theorems)](#æé™å®šç†-limit-theorems)
  - [ç›®å½•](#ç›®å½•)
  - [ğŸ“‹ æ ¸å¿ƒæ€æƒ³](#-æ ¸å¿ƒæ€æƒ³)
  - [ğŸ¯ æ”¶æ•›æ€§æ¦‚å¿µ](#-æ”¶æ•›æ€§æ¦‚å¿µ)
    - [1. ä¾æ¦‚ç‡æ”¶æ•›](#1-ä¾æ¦‚ç‡æ”¶æ•›)
    - [2. å‡ ä¹å¿…ç„¶æ”¶æ•›](#2-å‡ ä¹å¿…ç„¶æ”¶æ•›)
    - [3. ä¾åˆ†å¸ƒæ”¶æ•›](#3-ä¾åˆ†å¸ƒæ”¶æ•›)
    - [4. æ”¶æ•›æ€§ä¹‹é—´çš„å…³ç³»](#4-æ”¶æ•›æ€§ä¹‹é—´çš„å…³ç³»)
  - [ğŸ“Š å¤§æ•°å®šå¾‹ (Law of Large Numbers)](#-å¤§æ•°å®šå¾‹-law-of-large-numbers)
    - [1. å¼±å¤§æ•°å®šå¾‹ (WLLN)](#1-å¼±å¤§æ•°å®šå¾‹-wlln)
    - [2. å¼ºå¤§æ•°å®šå¾‹ (SLLN)](#2-å¼ºå¤§æ•°å®šå¾‹-slln)
    - [3. åº”ç”¨](#3-åº”ç”¨)
  - [ğŸ”¬ ä¸­å¿ƒæé™å®šç† (Central Limit Theorem)](#-ä¸­å¿ƒæé™å®šç†-central-limit-theorem)
    - [1. ç»å…¸ä¸­å¿ƒæé™å®šç†](#1-ç»å…¸ä¸­å¿ƒæé™å®šç†)
    - [2. Lindeberg-LÃ©vyå®šç†](#2-lindeberg-lÃ©vyå®šç†)
    - [3. Lyapunovå®šç†](#3-lyapunovå®šç†)
    - [4. Berry-Esseenå®šç†](#4-berry-esseenå®šç†)
  - [ğŸ’¡ å¤šå…ƒä¸­å¿ƒæé™å®šç†](#-å¤šå…ƒä¸­å¿ƒæé™å®šç†)
    - [1. å¤šå…ƒCLT](#1-å¤šå…ƒclt)
    - [2. Deltaæ–¹æ³•](#2-deltaæ–¹æ³•)
  - [ğŸ¨ åœ¨æœºå™¨å­¦ä¹ ä¸­çš„åº”ç”¨](#-åœ¨æœºå™¨å­¦ä¹ ä¸­çš„åº”ç”¨)
    - [1. ç»éªŒé£é™©æœ€å°åŒ–](#1-ç»éªŒé£é™©æœ€å°åŒ–)
    - [2. å‚æ•°ä¼°è®¡](#2-å‚æ•°ä¼°è®¡)
    - [3. ç½®ä¿¡åŒºé—´](#3-ç½®ä¿¡åŒºé—´)
    - [4. å‡è®¾æ£€éªŒ](#4-å‡è®¾æ£€éªŒ)
    - [5. Bootstrapæ–¹æ³•](#5-bootstrapæ–¹æ³•)
  - [ğŸ”§ é«˜çº§ä¸»é¢˜](#-é«˜çº§ä¸»é¢˜)
    - [1. å¤§åå·®ç†è®º](#1-å¤§åå·®ç†è®º)
    - [2. å‡½æ•°å‹ä¸­å¿ƒæé™å®šç†](#2-å‡½æ•°å‹ä¸­å¿ƒæé™å®šç†)
    - [3. ä¾èµ–æ•°æ®çš„æé™å®šç†](#3-ä¾èµ–æ•°æ®çš„æé™å®šç†)
  - [ğŸ’» Pythonå®ç°](#-pythonå®ç°)
  - [ğŸ“š ç»ƒä¹ é¢˜](#-ç»ƒä¹ é¢˜)
    - [ç»ƒä¹ 1ï¼šå¤§æ•°å®šå¾‹éªŒè¯](#ç»ƒä¹ 1å¤§æ•°å®šå¾‹éªŒè¯)
    - [ç»ƒä¹ 2ï¼šä¸­å¿ƒæé™å®šç†](#ç»ƒä¹ 2ä¸­å¿ƒæé™å®šç†)
    - [ç»ƒä¹ 3ï¼šç½®ä¿¡åŒºé—´](#ç»ƒä¹ 3ç½®ä¿¡åŒºé—´)
    - [ç»ƒä¹ 4ï¼šBootstrap](#ç»ƒä¹ 4bootstrap)
  - [ğŸ“ ç›¸å…³è¯¾ç¨‹](#-ç›¸å…³è¯¾ç¨‹)
  - [ğŸ“– å‚è€ƒæ–‡çŒ®](#-å‚è€ƒæ–‡çŒ®)

---

## ğŸ“‹ æ ¸å¿ƒæ€æƒ³

**æé™å®šç†**æè¿°äº†å¤§é‡éšæœºå˜é‡çš„å’Œæˆ–å¹³å‡çš„æ¸è¿‘è¡Œä¸ºï¼Œæ˜¯ç»Ÿè®¡æ¨æ–­å’Œæœºå™¨å­¦ä¹ çš„ç†è®ºåŸºç¡€ã€‚

**ä¸ºä»€ä¹ˆæé™å®šç†é‡è¦**:

```text
ç»Ÿè®¡æ¨æ–­çš„åŸºç¡€:
â”œâ”€ å¤§æ•°å®šå¾‹: æ ·æœ¬å‡å€¼æ”¶æ•›åˆ°æ€»ä½“å‡å€¼
â”œâ”€ ä¸­å¿ƒæé™å®šç†: æ ·æœ¬å‡å€¼çš„åˆ†å¸ƒè¶‹äºæ­£æ€
â”œâ”€ ç½®ä¿¡åŒºé—´: å‚æ•°ä¼°è®¡çš„ä¸ç¡®å®šæ€§
â””â”€ å‡è®¾æ£€éªŒ: ç»Ÿè®¡æ˜¾è‘—æ€§

æœºå™¨å­¦ä¹ ä¸­çš„åº”ç”¨:
â”œâ”€ ç»éªŒé£é™©æœ€å°åŒ–
â”œâ”€ æ³›åŒ–è¯¯å·®ä¼°è®¡
â”œâ”€ æ¨¡å‹é€‰æ‹©
â””â”€ ä¸ç¡®å®šæ€§é‡åŒ–
```

---

## ğŸ¯ æ”¶æ•›æ€§æ¦‚å¿µ

### 1. ä¾æ¦‚ç‡æ”¶æ•›

**å®šä¹‰ 1.1 (ä¾æ¦‚ç‡æ”¶æ•›)**:

éšæœºå˜é‡åºåˆ— $\{X_n\}$ **ä¾æ¦‚ç‡æ”¶æ•›**åˆ° $X$ï¼Œè®°ä½œ $X_n \xrightarrow{P} X$ï¼Œå¦‚æœï¼š

$$
\forall \epsilon > 0, \quad \lim_{n \to \infty} P(|X_n - X| > \epsilon) = 0
$$

**ç­‰ä»·è¡¨è¿°**:

$$
\lim_{n \to \infty} P(|X_n - X| \leq \epsilon) = 1
$$

**ç›´è§‰**ï¼š$X_n$ ä»¥å¾ˆå¤§çš„æ¦‚ç‡æ¥è¿‘ $X$ã€‚

---

### 2. å‡ ä¹å¿…ç„¶æ”¶æ•›

**å®šä¹‰ 2.1 (å‡ ä¹å¿…ç„¶æ”¶æ•›)**:

éšæœºå˜é‡åºåˆ— $\{X_n\}$ **å‡ ä¹å¿…ç„¶æ”¶æ•›**ï¼ˆæˆ–**ä»¥æ¦‚ç‡1æ”¶æ•›**ï¼‰åˆ° $X$ï¼Œè®°ä½œ $X_n \xrightarrow{a.s.} X$ï¼Œå¦‚æœï¼š

$$
P\left(\lim_{n \to \infty} X_n = X\right) = 1
$$

**ç­‰ä»·è¡¨è¿°**:

$$
P\left(\{\omega : \lim_{n \to \infty} X_n(\omega) = X(\omega)\}\right) = 1
$$

**ç›´è§‰**ï¼šé™¤äº†é›¶æ¦‚ç‡äº‹ä»¶å¤–ï¼Œ$X_n$ çš„æ¯ä¸ªæ ·æœ¬è·¯å¾„éƒ½æ”¶æ•›åˆ° $X$ã€‚

---

### 3. ä¾åˆ†å¸ƒæ”¶æ•›

**å®šä¹‰ 3.1 (ä¾åˆ†å¸ƒæ”¶æ•›)**:

éšæœºå˜é‡åºåˆ— $\{X_n\}$ **ä¾åˆ†å¸ƒæ”¶æ•›**åˆ° $X$ï¼Œè®°ä½œ $X_n \xrightarrow{d} X$ï¼Œå¦‚æœï¼š

$$
\lim_{n \to \infty} F_n(x) = F(x)
$$

å¯¹äº $F$ çš„æ‰€æœ‰è¿ç»­ç‚¹ $x$ï¼Œå…¶ä¸­ $F_n$ å’Œ $F$ åˆ†åˆ«æ˜¯ $X_n$ å’Œ $X$ çš„ç´¯ç§¯åˆ†å¸ƒå‡½æ•°ã€‚

**è®°å·**ï¼šä¹Ÿè®°ä½œ $X_n \Rightarrow X$ã€‚

---

### 4. æ”¶æ•›æ€§ä¹‹é—´çš„å…³ç³»

**å®šç† 4.1 (æ”¶æ•›æ€§å±‚æ¬¡)**:

$$
X_n \xrightarrow{a.s.} X \quad \Rightarrow \quad X_n \xrightarrow{P} X \quad \Rightarrow \quad X_n \xrightarrow{d} X
$$

**åå‘ä¸æˆç«‹**ï¼š

- ä¾åˆ†å¸ƒæ”¶æ•› $\not\Rightarrow$ ä¾æ¦‚ç‡æ”¶æ•›
- ä¾æ¦‚ç‡æ”¶æ•› $\not\Rightarrow$ å‡ ä¹å¿…ç„¶æ”¶æ•›

**ç‰¹æ®Šæƒ…å†µ**ï¼šå¦‚æœ $X$ æ˜¯å¸¸æ•° $c$ï¼Œåˆ™ï¼š

$$
X_n \xrightarrow{d} c \quad \Leftrightarrow \quad X_n \xrightarrow{P} c
$$

---

## ğŸ“Š å¤§æ•°å®šå¾‹ (Law of Large Numbers)

### 1. å¼±å¤§æ•°å®šå¾‹ (WLLN)

**å®šç† 1.1 (Khinchinå¼±å¤§æ•°å®šå¾‹)**:

è®¾ $X_1, X_2, \ldots$ æ˜¯ç‹¬ç«‹åŒåˆ†å¸ƒçš„éšæœºå˜é‡ï¼Œä¸” $E[X_i] = \mu$ å­˜åœ¨ã€‚ä»¤ï¼š

$$
\bar{X}_n = \frac{1}{n} \sum_{i=1}^n X_i
$$

åˆ™ï¼š

$$
\bar{X}_n \xrightarrow{P} \mu
$$

**è¯æ˜æ€è·¯** (Chebyshevä¸ç­‰å¼):

å‡è®¾ $\text{Var}(X_i) = \sigma^2 < \infty$ï¼Œåˆ™ï¼š

$$
\text{Var}(\bar{X}_n) = \frac{\sigma^2}{n}
$$

ç”±Chebyshevä¸ç­‰å¼ï¼š

$$
P(|\bar{X}_n - \mu| > \epsilon) \leq \frac{\text{Var}(\bar{X}_n)}{\epsilon^2} = \frac{\sigma^2}{n\epsilon^2} \to 0
$$

---

### 2. å¼ºå¤§æ•°å®šå¾‹ (SLLN)

**å®šç† 2.1 (Kolmogorovå¼ºå¤§æ•°å®šå¾‹)**:

è®¾ $X_1, X_2, \ldots$ æ˜¯ç‹¬ç«‹åŒåˆ†å¸ƒçš„éšæœºå˜é‡ï¼Œä¸” $E[|X_i|] < \infty$ï¼Œ$E[X_i] = \mu$ã€‚åˆ™ï¼š

$$
\bar{X}_n \xrightarrow{a.s.} \mu
$$

**æ„ä¹‰**ï¼šæ ·æœ¬å‡å€¼ä»¥æ¦‚ç‡1æ”¶æ•›åˆ°æ€»ä½“å‡å€¼ã€‚

---

### 3. åº”ç”¨

**è’™ç‰¹å¡æ´›ç§¯åˆ†**:

ä¼°è®¡ $\theta = E[g(X)]$ï¼š

$$
\hat{\theta}_n = \frac{1}{n} \sum_{i=1}^n g(X_i) \xrightarrow{a.s.} \theta
$$

**ç»éªŒé£é™©**:

$$
\hat{R}_n(f) = \frac{1}{n} \sum_{i=1}^n L(f(x_i), y_i) \xrightarrow{P} R(f) = E[L(f(X), Y)]
$$

---

## ğŸ”¬ ä¸­å¿ƒæé™å®šç† (Central Limit Theorem)

### 1. ç»å…¸ä¸­å¿ƒæé™å®šç†

**å®šç† 1.1 (ç»å…¸CLT)**:

è®¾ $X_1, X_2, \ldots$ æ˜¯ç‹¬ç«‹åŒåˆ†å¸ƒçš„éšæœºå˜é‡ï¼Œ$E[X_i] = \mu$ï¼Œ$\text{Var}(X_i) = \sigma^2 < \infty$ã€‚ä»¤ï¼š

$$
Z_n = \frac{\bar{X}_n - \mu}{\sigma / \sqrt{n}} = \frac{\sum_{i=1}^n X_i - n\mu}{\sigma\sqrt{n}}
$$

åˆ™ï¼š

$$
Z_n \xrightarrow{d} N(0, 1)
$$

**ç­‰ä»·è¡¨è¿°**:

$$
\sqrt{n}(\bar{X}_n - \mu) \xrightarrow{d} N(0, \sigma^2)
$$

æˆ–ï¼š

$$
\bar{X}_n \sim \text{AN}\left(\mu, \frac{\sigma^2}{n}\right)
$$

å…¶ä¸­ AN è¡¨ç¤º"æ¸è¿‘æ­£æ€"(Asymptotically Normal)ã€‚

---

### 2. Lindeberg-LÃ©vyå®šç†

**å®šç† 2.1 (Lindeberg-LÃ©vy CLT)**:

è®¾ $X_1, X_2, \ldots$ ç‹¬ç«‹åŒåˆ†å¸ƒï¼Œ$E[X_i] = \mu$ï¼Œ$\text{Var}(X_i) = \sigma^2 < \infty$ã€‚åˆ™å¯¹äºä»»æ„ $x \in \mathbb{R}$ï¼š

$$
\lim_{n \to \infty} P\left(\frac{\sum_{i=1}^n X_i - n\mu}{\sigma\sqrt{n}} \leq x\right) = \Phi(x)
$$

å…¶ä¸­ $\Phi$ æ˜¯æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„ç´¯ç§¯åˆ†å¸ƒå‡½æ•°ã€‚

---

### 3. Lyapunovå®šç†

**å®šç† 3.1 (Lyapunov CLT)**:

è®¾ $X_1, X_2, \ldots$ ç‹¬ç«‹ï¼ˆä¸å¿…åŒåˆ†å¸ƒï¼‰ï¼Œ$E[X_i] = \mu_i$ï¼Œ$\text{Var}(X_i) = \sigma_i^2$ã€‚ä»¤ï¼š

$$
s_n^2 = \sum_{i=1}^n \sigma_i^2
$$

å¦‚æœå­˜åœ¨ $\delta > 0$ ä½¿å¾—ï¼š

$$
\lim_{n \to \infty} \frac{1}{s_n^{2+\delta}} \sum_{i=1}^n E[|X_i - \mu_i|^{2+\delta}] = 0
$$

åˆ™ï¼š

$$
\frac{\sum_{i=1}^n (X_i - \mu_i)}{s_n} \xrightarrow{d} N(0, 1)
$$

**æ„ä¹‰**ï¼šå…è®¸éåŒåˆ†å¸ƒçš„æƒ…å†µã€‚

---

### 4. Berry-Esseenå®šç†

**å®šç† 4.1 (Berry-Esseenå®šç†)**:

è®¾ $X_1, \ldots, X_n$ ç‹¬ç«‹åŒåˆ†å¸ƒï¼Œ$E[X_i] = \mu$ï¼Œ$\text{Var}(X_i) = \sigma^2$ï¼Œ$E[|X_i - \mu|^3] = \rho < \infty$ã€‚åˆ™ï¼š

$$
\sup_x \left|P\left(\frac{\bar{X}_n - \mu}{\sigma/\sqrt{n}} \leq x\right) - \Phi(x)\right| \leq \frac{C\rho}{\sigma^3 \sqrt{n}}
$$

å…¶ä¸­ $C$ æ˜¯ç»å¯¹å¸¸æ•°ï¼ˆ$C \leq 0.4748$ï¼‰ã€‚

**æ„ä¹‰**ï¼šç»™å‡ºäº†æ”¶æ•›é€Ÿåº¦çš„ç•Œï¼Œé€šå¸¸æ˜¯ $O(n^{-1/2})$ã€‚

---

## ğŸ’¡ å¤šå…ƒä¸­å¿ƒæé™å®šç†

### 1. å¤šå…ƒCLT

**å®šç† 1.1 (å¤šå…ƒCLT)**:

è®¾ $\mathbf{X}_1, \mathbf{X}_2, \ldots$ æ˜¯ç‹¬ç«‹åŒåˆ†å¸ƒçš„ $d$ ç»´éšæœºå‘é‡ï¼Œ$E[\mathbf{X}_i] = \boldsymbol{\mu}$ï¼Œ$\text{Cov}(\mathbf{X}_i) = \Sigma$ï¼ˆæ­£å®šï¼‰ã€‚åˆ™ï¼š

$$
\sqrt{n}(\bar{\mathbf{X}}_n - \boldsymbol{\mu}) \xrightarrow{d} N(\mathbf{0}, \Sigma)
$$

å…¶ä¸­ $\bar{\mathbf{X}}_n = \frac{1}{n} \sum_{i=1}^n \mathbf{X}_i$ã€‚

---

### 2. Deltaæ–¹æ³•

**å®šç† 2.1 (Deltaæ–¹æ³•)**:

è®¾ $\sqrt{n}(X_n - \theta) \xrightarrow{d} N(0, \sigma^2)$ï¼Œ$g$ æ˜¯å¯å¾®å‡½æ•°ä¸” $g'(\theta) \neq 0$ã€‚åˆ™ï¼š

$$
\sqrt{n}(g(X_n) - g(\theta)) \xrightarrow{d} N(0, [g'(\theta)]^2 \sigma^2)
$$

**å¤šå…ƒDeltaæ–¹æ³•**:

è®¾ $\sqrt{n}(\mathbf{X}_n - \boldsymbol{\theta}) \xrightarrow{d} N(\mathbf{0}, \Sigma)$ï¼Œ$g: \mathbb{R}^d \to \mathbb{R}$ å¯å¾®ã€‚åˆ™ï¼š

$$
\sqrt{n}(g(\mathbf{X}_n) - g(\boldsymbol{\theta})) \xrightarrow{d} N(0, \nabla g(\boldsymbol{\theta})^T \Sigma \nabla g(\boldsymbol{\theta}))
$$

**åº”ç”¨**ï¼šéçº¿æ€§å˜æ¢çš„æ¸è¿‘åˆ†å¸ƒã€‚

---

## ğŸ¨ åœ¨æœºå™¨å­¦ä¹ ä¸­çš„åº”ç”¨

### 1. ç»éªŒé£é™©æœ€å°åŒ–

**æ³›åŒ–è¯¯å·®**:

è®­ç»ƒè¯¯å·®ï¼š

$$
\hat{R}_n(f) = \frac{1}{n} \sum_{i=1}^n L(f(x_i), y_i)
$$

çœŸå®è¯¯å·®ï¼š

$$
R(f) = E[L(f(X), Y)]
$$

**å¤§æ•°å®šå¾‹**ï¼š$\hat{R}_n(f) \xrightarrow{P} R(f)$

**ä¸­å¿ƒæé™å®šç†**ï¼š

$$
\sqrt{n}(\hat{R}_n(f) - R(f)) \xrightarrow{d} N(0, \sigma^2)
$$

å…¶ä¸­ $\sigma^2 = \text{Var}(L(f(X), Y))$ã€‚

---

### 2. å‚æ•°ä¼°è®¡

**æœ€å¤§ä¼¼ç„¶ä¼°è®¡ (MLE)**:

$$
\hat{\theta}_n = \arg\max_{\theta} \frac{1}{n} \sum_{i=1}^n \log p(x_i; \theta)
$$

**æ¸è¿‘æ­£æ€æ€§**:

åœ¨æ­£åˆ™æ¡ä»¶ä¸‹ï¼š

$$
\sqrt{n}(\hat{\theta}_n - \theta_0) \xrightarrow{d} N(0, I(\theta_0)^{-1})
$$

å…¶ä¸­ $I(\theta)$ æ˜¯Fisherä¿¡æ¯çŸ©é˜µã€‚

---

### 3. ç½®ä¿¡åŒºé—´

**æ„é€ ç½®ä¿¡åŒºé—´**:

ç”±CLTï¼Œå¯¹äºæ ·æœ¬å‡å€¼ï¼š

$$
P\left(\mu - z_{\alpha/2} \frac{\sigma}{\sqrt{n}} \leq \bar{X}_n \leq \mu + z_{\alpha/2} \frac{\sigma}{\sqrt{n}}\right) \approx 1 - \alpha
$$

**95%ç½®ä¿¡åŒºé—´**:

$$
\bar{X}_n \pm 1.96 \frac{\sigma}{\sqrt{n}}
$$

**å®è·µä¸­**ï¼šç”¨æ ·æœ¬æ ‡å‡†å·® $s$ ä¼°è®¡ $\sigma$ã€‚

---

### 4. å‡è®¾æ£€éªŒ

**Zæ£€éªŒ**:

æ£€éªŒ $H_0: \mu = \mu_0$ vs $H_1: \mu \neq \mu_0$ã€‚

æ£€éªŒç»Ÿè®¡é‡ï¼š

$$
Z = \frac{\bar{X}_n - \mu_0}{\sigma / \sqrt{n}}
$$

åœ¨ $H_0$ ä¸‹ï¼Œ$Z \sim N(0, 1)$ï¼ˆæ¸è¿‘åœ°ï¼‰ã€‚

**æ‹’ç»åŸŸ**ï¼š$|Z| > z_{\alpha/2}$ã€‚

---

### 5. Bootstrapæ–¹æ³•

**BootstrapåŸç†**:

ä»æ ·æœ¬ $\{x_1, \ldots, x_n\}$ ä¸­æœ‰æ”¾å›åœ°æŠ½å– $n$ ä¸ªæ ·æœ¬ï¼Œé‡å¤ $B$ æ¬¡ã€‚

**Bootstrapåˆ†å¸ƒ**:

$$
\hat{F}_n^* \approx F
$$

**åº”ç”¨**ï¼šä¼°è®¡ç»Ÿè®¡é‡çš„åˆ†å¸ƒã€æ„é€ ç½®ä¿¡åŒºé—´ã€‚

---

## ğŸ”§ é«˜çº§ä¸»é¢˜

### 1. å¤§åå·®ç†è®º

**CramÃ©rå®šç†**:

è®¾ $X_1, X_2, \ldots$ ç‹¬ç«‹åŒåˆ†å¸ƒï¼ŒçŸ©æ¯å‡½æ•° $M(t) = E[e^{tX_i}]$ å­˜åœ¨ã€‚åˆ™å¯¹äº $a > \mu$ï¼š

$$
\lim_{n \to \infty} \frac{1}{n} \log P(\bar{X}_n \geq a) = -I(a)
$$

å…¶ä¸­ $I(a) = \sup_t \{ta - \log M(t)\}$ æ˜¯**é€Ÿç‡å‡½æ•°**ã€‚

**æ„ä¹‰**ï¼šæè¿°å°¾éƒ¨æ¦‚ç‡çš„æŒ‡æ•°è¡°å‡é€Ÿç‡ã€‚

---

### 2. å‡½æ•°å‹ä¸­å¿ƒæé™å®šç†

**Donskerå®šç†**:

ç»éªŒè¿‡ç¨‹ï¼š

$$
G_n(t) = \sqrt{n}(\hat{F}_n(t) - F(t))
$$

å…¶ä¸­ $\hat{F}_n$ æ˜¯ç»éªŒåˆ†å¸ƒå‡½æ•°ã€‚

**Donskerå®šç†**ï¼š

$$
G_n \Rightarrow B \circ F
$$

å…¶ä¸­ $B$ æ˜¯å¸ƒæœ—æ¡¥ï¼ˆBrownian bridgeï¼‰ã€‚

---

### 3. ä¾èµ–æ•°æ®çš„æé™å®šç†

**æ—¶é—´åºåˆ—**:

å¯¹äºå¼±ä¾èµ–çš„æ—¶é—´åºåˆ—ï¼ˆå¦‚æ··åˆåºåˆ—ï¼‰ï¼Œåœ¨é€‚å½“æ¡ä»¶ä¸‹ï¼ŒCLTä»ç„¶æˆç«‹ï¼Œä½†æ–¹å·®éœ€è¦è°ƒæ•´ï¼š

$$
\sqrt{n}(\bar{X}_n - \mu) \xrightarrow{d} N(0, \sigma^2 + 2\sum_{k=1}^\infty \gamma(k))
$$

å…¶ä¸­ $\gamma(k) = \text{Cov}(X_0, X_k)$ æ˜¯è‡ªåæ–¹å·®å‡½æ•°ã€‚

---

## ğŸ’» Pythonå®ç°

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
from scipy.stats import norm, expon, uniform, chi2

# 1. å¤§æ•°å®šå¾‹æ¼”ç¤º
def law_of_large_numbers_demo():
    """å¤§æ•°å®šå¾‹æ¼”ç¤º"""
    print("=== å¤§æ•°å®šå¾‹ ===\n")
    
    # å‚æ•°
    mu = 5  # çœŸå®å‡å€¼
    n_max = 10000
    n_trials = 5
    
    plt.figure(figsize=(12, 5))
    
    # ä¸åŒåˆ†å¸ƒ
    distributions = {
        'Uniform(0, 10)': lambda n: np.random.uniform(0, 10, n),
        'Exponential(Î»=0.2)': lambda n: np.random.exponential(5, n),
        'Normal(5, 2)': lambda n: np.random.normal(5, 2, n)
    }
    
    for idx, (name, dist) in enumerate(distributions.items(), 1):
        plt.subplot(1, 3, idx)
        
        for _ in range(n_trials):
            samples = dist(n_max)
            cumulative_mean = np.cumsum(samples) / np.arange(1, n_max + 1)
            plt.plot(cumulative_mean, alpha=0.5, linewidth=0.8)
        
        plt.axhline(y=mu, color='r', linestyle='--', linewidth=2, label=f'True mean = {mu}')
        plt.xlabel('Sample size n')
        plt.ylabel('Sample mean')
        plt.title(f'LLN: {name}')
        plt.xscale('log')
        plt.grid(True, alpha=0.3)
        plt.legend()
    
    plt.tight_layout()
    # plt.show()
    
    print("å¤§æ•°å®šå¾‹ï¼šæ ·æœ¬å‡å€¼æ”¶æ•›åˆ°çœŸå®å‡å€¼\n")


# 2. ä¸­å¿ƒæé™å®šç†æ¼”ç¤º
def central_limit_theorem_demo():
    """ä¸­å¿ƒæé™å®šç†æ¼”ç¤º"""
    print("=== ä¸­å¿ƒæé™å®šç† ===\n")
    
    # ä¸åŒåˆ†å¸ƒ
    distributions = {
        'Uniform(0, 1)': (lambda n: np.random.uniform(0, 1, n), 0.5, 1/12),
        'Exponential(Î»=1)': (lambda n: np.random.exponential(1, n), 1, 1),
        'Chi-square(df=2)': (lambda n: np.random.chisquare(2, n), 2, 4)
    }
    
    sample_sizes = [1, 5, 30, 100]
    n_samples = 10000
    
    fig, axes = plt.subplots(len(distributions), len(sample_sizes), 
                             figsize=(16, 10))
    
    for i, (dist_name, (dist_func, mu, var)) in enumerate(distributions.items()):
        for j, n in enumerate(sample_sizes):
            # ç”Ÿæˆæ ·æœ¬å‡å€¼
            sample_means = np.array([
                np.mean(dist_func(n)) for _ in range(n_samples)
            ])
            
            # æ ‡å‡†åŒ–
            z_scores = (sample_means - mu) / np.sqrt(var / n)
            
            # ç»˜åˆ¶ç›´æ–¹å›¾
            ax = axes[i, j]
            ax.hist(z_scores, bins=50, density=True, alpha=0.7, 
                   edgecolor='black', label='Sample')
            
            # å åŠ æ ‡å‡†æ­£æ€åˆ†å¸ƒ
            x = np.linspace(-4, 4, 100)
            ax.plot(x, norm.pdf(x), 'r-', linewidth=2, label='N(0,1)')
            
            ax.set_xlim(-4, 4)
            ax.set_title(f'{dist_name}\nn={n}')
            ax.grid(True, alpha=0.3)
            
            if j == 0:
                ax.set_ylabel('Density')
            if i == len(distributions) - 1:
                ax.set_xlabel('Standardized mean')
            if i == 0 and j == 0:
                ax.legend()
    
    plt.tight_layout()
    # plt.show()
    
    print("ä¸­å¿ƒæé™å®šç†ï¼šæ ‡å‡†åŒ–çš„æ ·æœ¬å‡å€¼è¶‹äºæ ‡å‡†æ­£æ€åˆ†å¸ƒ\n")


# 3. æ”¶æ•›é€Ÿåº¦ - Berry-Esseen
def berry_esseen_demo():
    """Berry-Esseenå®šç†ï¼šæ”¶æ•›é€Ÿåº¦"""
    print("=== Berry-Esseenå®šç† ===\n")
    
    sample_sizes = [10, 30, 100, 300, 1000]
    n_samples = 10000
    
    # ä½¿ç”¨æŒ‡æ•°åˆ†å¸ƒ
    mu = 1
    sigma = 1
    
    max_errors = []
    theoretical_bounds = []
    
    for n in sample_sizes:
        # ç”Ÿæˆæ ·æœ¬å‡å€¼
        sample_means = np.array([
            np.mean(np.random.exponential(1, n)) for _ in range(n_samples)
        ])
        
        # æ ‡å‡†åŒ–
        z_scores = (sample_means - mu) / (sigma / np.sqrt(n))
        
        # è®¡ç®—ç»éªŒCDF
        z_sorted = np.sort(z_scores)
        empirical_cdf = np.arange(1, len(z_sorted) + 1) / len(z_sorted)
        
        # ç†è®ºCDF
        theoretical_cdf = norm.cdf(z_sorted)
        
        # æœ€å¤§è¯¯å·®
        max_error = np.max(np.abs(empirical_cdf - theoretical_cdf))
        max_errors.append(max_error)
        
        # Berry-Esseenç•Œ
        rho = 2  # E[|X - mu|^3] for Exp(1)
        C = 0.4748
        bound = C * rho / (sigma**3 * np.sqrt(n))
        theoretical_bounds.append(bound)
        
        print(f"n={n:4d}: æœ€å¤§è¯¯å·®={max_error:.4f}, Berry-Esseenç•Œ={bound:.4f}")
    
    # ç»˜å›¾
    plt.figure(figsize=(10, 6))
    plt.loglog(sample_sizes, max_errors, 'bo-', linewidth=2, 
              markersize=8, label='Observed error')
    plt.loglog(sample_sizes, theoretical_bounds, 'r--', linewidth=2, 
              label='Berry-Esseen bound')
    plt.loglog(sample_sizes, [1/np.sqrt(n) for n in sample_sizes], 
              'g:', linewidth=2, label='O(1/âˆšn)')
    plt.xlabel('Sample size n')
    plt.ylabel('Maximum error')
    plt.title('Berry-Esseen Theorem: Convergence Rate')
    plt.legend()
    plt.grid(True, alpha=0.3)
    # plt.show()
    
    print()


# 4. ç½®ä¿¡åŒºé—´
def confidence_interval_demo():
    """ç½®ä¿¡åŒºé—´æ¼”ç¤º"""
    print("=== ç½®ä¿¡åŒºé—´ ===\n")
    
    # çœŸå®å‚æ•°
    mu_true = 10
    sigma_true = 2
    
    # æ ·æœ¬å¤§å°
    n = 30
    
    # ç½®ä¿¡æ°´å¹³
    confidence_level = 0.95
    alpha = 1 - confidence_level
    z_critical = norm.ppf(1 - alpha/2)
    
    # ç”Ÿæˆå¤šä¸ªæ ·æœ¬å¹¶è®¡ç®—ç½®ä¿¡åŒºé—´
    n_experiments = 100
    coverage = 0
    
    plt.figure(figsize=(12, 8))
    
    for i in range(n_experiments):
        # ç”Ÿæˆæ ·æœ¬
        sample = np.random.normal(mu_true, sigma_true, n)
        
        # æ ·æœ¬ç»Ÿè®¡é‡
        x_bar = np.mean(sample)
        s = np.std(sample, ddof=1)
        
        # ç½®ä¿¡åŒºé—´
        margin = z_critical * s / np.sqrt(n)
        ci_lower = x_bar - margin
        ci_upper = x_bar + margin
        
        # æ£€æŸ¥æ˜¯å¦è¦†ç›–çœŸå®å€¼
        covers = ci_lower <= mu_true <= ci_upper
        coverage += covers
        
        # ç»˜åˆ¶
        color = 'green' if covers else 'red'
        plt.plot([ci_lower, ci_upper], [i, i], color=color, alpha=0.6)
        plt.plot(x_bar, i, 'o', color=color, markersize=3)
    
    # çœŸå®å‡å€¼
    plt.axvline(mu_true, color='blue', linestyle='--', linewidth=2, 
               label=f'True mean = {mu_true}')
    
    plt.xlabel('Value')
    plt.ylabel('Experiment')
    plt.title(f'95% Confidence Intervals\nCoverage: {coverage}/{n_experiments} = {coverage/n_experiments:.2%}')
    plt.legend()
    plt.grid(True, alpha=0.3, axis='x')
    # plt.show()
    
    print(f"ç†è®ºè¦†ç›–ç‡: {confidence_level:.2%}")
    print(f"å®é™…è¦†ç›–ç‡: {coverage/n_experiments:.2%}\n")


# 5. Bootstrapæ–¹æ³•
def bootstrap_demo():
    """Bootstrapæ–¹æ³•æ¼”ç¤º"""
    print("=== Bootstrapæ–¹æ³• ===\n")
    
    # åŸå§‹æ ·æœ¬
    np.random.seed(42)
    n = 50
    original_sample = np.random.exponential(2, n)
    
    # ç»Ÿè®¡é‡ï¼šä¸­ä½æ•°
    def statistic(data):
        return np.median(data)
    
    observed_stat = statistic(original_sample)
    
    # Bootstrap
    n_bootstrap = 10000
    bootstrap_stats = []
    
    for _ in range(n_bootstrap):
        # æœ‰æ”¾å›æŠ½æ ·
        bootstrap_sample = np.random.choice(original_sample, size=n, replace=True)
        bootstrap_stats.append(statistic(bootstrap_sample))
    
    bootstrap_stats = np.array(bootstrap_stats)
    
    # Bootstrapç½®ä¿¡åŒºé—´
    ci_lower = np.percentile(bootstrap_stats, 2.5)
    ci_upper = np.percentile(bootstrap_stats, 97.5)
    
    # ç»˜å›¾
    plt.figure(figsize=(12, 5))
    
    # åŸå§‹æ ·æœ¬
    plt.subplot(1, 2, 1)
    plt.hist(original_sample, bins=20, edgecolor='black', alpha=0.7)
    plt.axvline(observed_stat, color='r', linestyle='--', linewidth=2,
               label=f'Median = {observed_stat:.2f}')
    plt.xlabel('Value')
    plt.ylabel('Frequency')
    plt.title('Original Sample')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Bootstrapåˆ†å¸ƒ
    plt.subplot(1, 2, 2)
    plt.hist(bootstrap_stats, bins=50, edgecolor='black', alpha=0.7, density=True)
    plt.axvline(observed_stat, color='r', linestyle='--', linewidth=2,
               label=f'Observed = {observed_stat:.2f}')
    plt.axvline(ci_lower, color='g', linestyle=':', linewidth=2,
               label=f'95% CI: [{ci_lower:.2f}, {ci_upper:.2f}]')
    plt.axvline(ci_upper, color='g', linestyle=':', linewidth=2)
    plt.xlabel('Median')
    plt.ylabel('Density')
    plt.title('Bootstrap Distribution of Median')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    # plt.show()
    
    print(f"è§‚æµ‹ä¸­ä½æ•°: {observed_stat:.2f}")
    print(f"Bootstrapå‡å€¼: {np.mean(bootstrap_stats):.2f}")
    print(f"Bootstrapæ ‡å‡†è¯¯: {np.std(bootstrap_stats):.2f}")
    print(f"95% Bootstrap CI: [{ci_lower:.2f}, {ci_upper:.2f}]\n")


# 6. Deltaæ–¹æ³•
def delta_method_demo():
    """Deltaæ–¹æ³•æ¼”ç¤º"""
    print("=== Deltaæ–¹æ³• ===\n")
    
    # å‚æ•°
    n = 100
    n_samples = 10000
    mu = 2
    sigma = 1
    
    # éçº¿æ€§å˜æ¢: g(x) = x^2
    def g(x):
        return x**2
    
    def g_prime(x):
        return 2*x
    
    # ç”Ÿæˆæ ·æœ¬å‡å€¼
    sample_means = np.array([
        np.mean(np.random.normal(mu, sigma, n)) for _ in range(n_samples)
    ])
    
    # å˜æ¢åçš„ç»Ÿè®¡é‡
    transformed = g(sample_means)
    
    # Deltaæ–¹æ³•çš„æ¸è¿‘åˆ†å¸ƒ
    asymptotic_mean = g(mu)
    asymptotic_var = (g_prime(mu)**2) * (sigma**2 / n)
    
    # ç»˜å›¾
    plt.figure(figsize=(12, 5))
    
    # æ ·æœ¬å‡å€¼åˆ†å¸ƒ
    plt.subplot(1, 2, 1)
    plt.hist(sample_means, bins=50, density=True, alpha=0.7, 
            edgecolor='black', label='Sample')
    x = np.linspace(sample_means.min(), sample_means.max(), 100)
    plt.plot(x, norm.pdf(x, mu, sigma/np.sqrt(n)), 'r-', 
            linewidth=2, label='Theoretical')
    plt.xlabel('Sample mean')
    plt.ylabel('Density')
    plt.title('Distribution of Sample Mean')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # å˜æ¢åçš„åˆ†å¸ƒ
    plt.subplot(1, 2, 2)
    plt.hist(transformed, bins=50, density=True, alpha=0.7, 
            edgecolor='black', label='Sample')
    x = np.linspace(transformed.min(), transformed.max(), 100)
    plt.plot(x, norm.pdf(x, asymptotic_mean, np.sqrt(asymptotic_var)), 
            'r-', linewidth=2, label='Delta method')
    plt.xlabel('g(Sample mean)')
    plt.ylabel('Density')
    plt.title('Distribution of g(Sample Mean)\ng(x) = xÂ²')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    # plt.show()
    
    print(f"å˜æ¢åçš„å‡å€¼:")
    print(f"  è§‚æµ‹: {np.mean(transformed):.4f}")
    print(f"  ç†è®º: {asymptotic_mean:.4f}")
    print(f"\nå˜æ¢åçš„æ–¹å·®:")
    print(f"  è§‚æµ‹: {np.var(transformed):.4f}")
    print(f"  Deltaæ–¹æ³•: {asymptotic_var:.4f}\n")


if __name__ == "__main__":
    print("=" * 60)
    print("æé™å®šç†ç¤ºä¾‹")
    print("=" * 60 + "\n")
    
    law_of_large_numbers_demo()
    central_limit_theorem_demo()
    berry_esseen_demo()
    confidence_interval_demo()
    bootstrap_demo()
    delta_method_demo()
    
    print("\næ‰€æœ‰ç¤ºä¾‹å®Œæˆï¼")
```

---

## ğŸ“š ç»ƒä¹ é¢˜

### ç»ƒä¹ 1ï¼šå¤§æ•°å®šå¾‹éªŒè¯

ä½¿ç”¨è’™ç‰¹å¡æ´›æ–¹æ³•ä¼°è®¡ $\pi$ï¼ŒéªŒè¯å¤§æ•°å®šå¾‹ã€‚

### ç»ƒä¹ 2ï¼šä¸­å¿ƒæé™å®šç†

å¯¹äº $\text{Uniform}(0, 1)$ åˆ†å¸ƒï¼ŒéªŒè¯ä¸åŒæ ·æœ¬å¤§å°ä¸‹çš„CLTã€‚

### ç»ƒä¹ 3ï¼šç½®ä¿¡åŒºé—´

æ„é€ æ€»ä½“å‡å€¼çš„95%ç½®ä¿¡åŒºé—´ï¼ŒéªŒè¯è¦†ç›–ç‡ã€‚

### ç»ƒä¹ 4ï¼šBootstrap

ä½¿ç”¨Bootstrapæ–¹æ³•ä¼°è®¡æ ·æœ¬ä¸­ä½æ•°çš„æ ‡å‡†è¯¯å’Œç½®ä¿¡åŒºé—´ã€‚

---

## ğŸ“ ç›¸å…³è¯¾ç¨‹

| å¤§å­¦ | è¯¾ç¨‹ |
|------|------|
| **MIT** | 18.650 - Statistics for Applications |
| **Stanford** | STATS200 - Introduction to Statistical Inference |
| **UC Berkeley** | STAT134 - Concepts of Probability |
| **CMU** | 36-705 - Intermediate Statistics |

---

## ğŸ“– å‚è€ƒæ–‡çŒ®

1. **Billingsley, P. (1995)**. *Probability and Measure*. Wiley.

2. **Durrett, R. (2019)**. *Probability: Theory and Examples*. Cambridge University Press.

3. **Van der Vaart, A. W. (1998)**. *Asymptotic Statistics*. Cambridge University Press.

4. **Casella & Berger (2002)**. *Statistical Inference*. Duxbury Press.

5. **Efron & Tibshirani (1994)**. *An Introduction to the Bootstrap*. Chapman & Hall.

---

*æœ€åæ›´æ–°ï¼š2025å¹´10æœˆ*
