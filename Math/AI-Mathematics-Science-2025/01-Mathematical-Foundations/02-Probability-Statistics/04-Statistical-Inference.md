# ç»Ÿè®¡æ¨æ–­ (Statistical Inference)

> **From Data to Knowledge: Estimation, Testing, and Decision Making**
>
> ä»æ•°æ®åˆ°çŸ¥è¯†ï¼šä¼°è®¡ã€æ£€éªŒä¸å†³ç­–

---

## ç›®å½•

- [ç»Ÿè®¡æ¨æ–­ (Statistical Inference)](#ç»Ÿè®¡æ¨æ–­-statistical-inference)
  - [ç›®å½•](#ç›®å½•)
  - [ğŸ“‹ æ ¸å¿ƒæ€æƒ³](#-æ ¸å¿ƒæ€æƒ³)
  - [ğŸ¯ ç‚¹ä¼°è®¡](#-ç‚¹ä¼°è®¡)
    - [1. æœ€å¤§ä¼¼ç„¶ä¼°è®¡ (MLE)](#1-æœ€å¤§ä¼¼ç„¶ä¼°è®¡-mle)
    - [2. çŸ©ä¼°è®¡ (Method of Moments)](#2-çŸ©ä¼°è®¡-method-of-moments)
    - [3. ä¼°è®¡é‡çš„æ€§è´¨](#3-ä¼°è®¡é‡çš„æ€§è´¨)
  - [ğŸ“Š åŒºé—´ä¼°è®¡](#-åŒºé—´ä¼°è®¡)
    - [1. ç½®ä¿¡åŒºé—´](#1-ç½®ä¿¡åŒºé—´)
    - [2. æ¸è¿‘ç½®ä¿¡åŒºé—´](#2-æ¸è¿‘ç½®ä¿¡åŒºé—´)
    - [3. Bootstrapç½®ä¿¡åŒºé—´](#3-bootstrapç½®ä¿¡åŒºé—´)
  - [ğŸ”¬ å‡è®¾æ£€éªŒ](#-å‡è®¾æ£€éªŒ)
    - [1. åŸºæœ¬æ¦‚å¿µ](#1-åŸºæœ¬æ¦‚å¿µ)
    - [2. ç»å…¸æ£€éªŒ](#2-ç»å…¸æ£€éªŒ)
    - [3. på€¼ä¸å¤šé‡æ£€éªŒ](#3-på€¼ä¸å¤šé‡æ£€éªŒ)
  - [ğŸ’¡ è´å¶æ–¯æ¨æ–­](#-è´å¶æ–¯æ¨æ–­)
    - [1. è´å¶æ–¯å®šç†](#1-è´å¶æ–¯å®šç†)
    - [2. å…±è½­å…ˆéªŒ](#2-å…±è½­å…ˆéªŒ)
    - [3. åéªŒè®¡ç®—](#3-åéªŒè®¡ç®—)
  - [ğŸ¨ å˜åˆ†æ¨æ–­](#-å˜åˆ†æ¨æ–­)
    - [1. è¯æ®ä¸‹ç•Œ (ELBO)](#1-è¯æ®ä¸‹ç•Œ-elbo)
    - [2. å¹³å‡åœºå˜åˆ†](#2-å¹³å‡åœºå˜åˆ†)
    - [3. å˜åˆ†è‡ªç¼–ç å™¨ (VAE)](#3-å˜åˆ†è‡ªç¼–ç å™¨-vae)
  - [ğŸ”§ è’™ç‰¹å¡æ´›æ–¹æ³•](#-è’™ç‰¹å¡æ´›æ–¹æ³•)
    - [1. é‡è¦æ€§é‡‡æ ·](#1-é‡è¦æ€§é‡‡æ ·)
    - [2. Markové“¾è’™ç‰¹å¡æ´› (MCMC)](#2-markové“¾è’™ç‰¹å¡æ´›-mcmc)
    - [3. Hamiltonianè’™ç‰¹å¡æ´› (HMC)](#3-hamiltonianè’™ç‰¹å¡æ´›-hmc)
  - [ğŸ’» Pythonå®ç°](#-pythonå®ç°)
  - [ğŸ“š ç»ƒä¹ é¢˜](#-ç»ƒä¹ é¢˜)
    - [ç»ƒä¹ 1ï¼šMLE](#ç»ƒä¹ 1mle)
    - [ç»ƒä¹ 2ï¼šå‡è®¾æ£€éªŒ](#ç»ƒä¹ 2å‡è®¾æ£€éªŒ)
    - [ç»ƒä¹ 3ï¼šè´å¶æ–¯æ¨æ–­](#ç»ƒä¹ 3è´å¶æ–¯æ¨æ–­)
    - [ç»ƒä¹ 4ï¼šå˜åˆ†æ¨æ–­](#ç»ƒä¹ 4å˜åˆ†æ¨æ–­)
  - [ğŸ“ ç›¸å…³è¯¾ç¨‹](#-ç›¸å…³è¯¾ç¨‹)
  - [ğŸ“– å‚è€ƒæ–‡çŒ®](#-å‚è€ƒæ–‡çŒ®)

---

## ğŸ“‹ æ ¸å¿ƒæ€æƒ³

**ç»Ÿè®¡æ¨æ–­**æ˜¯ä»æ•°æ®ä¸­æå–ä¿¡æ¯ã€åšå‡ºå†³ç­–çš„ç§‘å­¦ã€‚

**ä¸¤å¤§å­¦æ´¾**:

```text
é¢‘ç‡å­¦æ´¾ (Frequentist):
â”œâ”€ å‚æ•°æ˜¯å›ºå®šä½†æœªçŸ¥çš„
â”œâ”€ æ•°æ®æ˜¯éšæœºçš„
â”œâ”€ æ–¹æ³•: MLE, å‡è®¾æ£€éªŒ, ç½®ä¿¡åŒºé—´
â””â”€ è§£é‡Š: é•¿æœŸé¢‘ç‡

è´å¶æ–¯å­¦æ´¾ (Bayesian):
â”œâ”€ å‚æ•°æ˜¯éšæœºçš„ (æœ‰å…ˆéªŒåˆ†å¸ƒ)
â”œâ”€ æ•°æ®æ˜¯è§‚æµ‹åˆ°çš„
â”œâ”€ æ–¹æ³•: åéªŒåˆ†å¸ƒ, è´å¶æ–¯å› å­
â””â”€ è§£é‡Š: ä¸»è§‚æ¦‚ç‡
```

---

## ğŸ¯ ç‚¹ä¼°è®¡

### 1. æœ€å¤§ä¼¼ç„¶ä¼°è®¡ (MLE)

**å®šä¹‰**:

ç»™å®šæ•°æ® $X_1, \ldots, X_n \sim p(x; \theta)$ï¼Œ**ä¼¼ç„¶å‡½æ•°**ä¸ºï¼š

$$
L(\theta) = \prod_{i=1}^n p(X_i; \theta)
$$

**å¯¹æ•°ä¼¼ç„¶**:

$$
\ell(\theta) = \log L(\theta) = \sum_{i=1}^n \log p(X_i; \theta)
$$

**æœ€å¤§ä¼¼ç„¶ä¼°è®¡**:

$$
\hat{\theta}_{MLE} = \arg\max_{\theta} \ell(\theta)
$$

---

**ä¾‹ 1.1 (é«˜æ–¯åˆ†å¸ƒçš„MLE)**:

è®¾ $X_1, \ldots, X_n \sim \mathcal{N}(\mu, \sigma^2)$ã€‚

**å¯¹æ•°ä¼¼ç„¶**:

$$
\ell(\mu, \sigma^2) = -\frac{n}{2} \log(2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^n (X_i - \mu)^2
$$

**MLE**:

$$
\hat{\mu}_{MLE} = \frac{1}{n} \sum_{i=1}^n X_i = \bar{X}
$$

$$
\hat{\sigma}^2_{MLE} = \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})^2
$$

---

**å®šç† 1.1 (MLEçš„æ¸è¿‘æ€§è´¨)**:

åœ¨æ­£åˆ™æ¡ä»¶ä¸‹ï¼ŒMLEå…·æœ‰ä»¥ä¸‹æ€§è´¨ï¼š

1. **ç›¸åˆæ€§** (Consistency):
   $$
   \hat{\theta}_n \xrightarrow{P} \theta_0
   $$

2. **æ¸è¿‘æ­£æ€æ€§** (Asymptotic Normality):
   $$
   \sqrt{n}(\hat{\theta}_n - \theta_0) \xrightarrow{d} \mathcal{N}(0, I(\theta_0)^{-1})
   $$
   å…¶ä¸­ $I(\theta)$ æ˜¯Fisherä¿¡æ¯çŸ©é˜µã€‚

3. **æ¸è¿‘æœ‰æ•ˆæ€§** (Asymptotic Efficiency):
   MLEè¾¾åˆ°CramÃ©r-Raoä¸‹ç•Œã€‚

---

**Fisherä¿¡æ¯çŸ©é˜µ**:

$$
I(\theta) = -\mathbb{E}\left[\frac{\partial^2 \ell(\theta)}{\partial \theta \partial \theta^T}\right] = \mathbb{E}\left[\left(\frac{\partial \ell(\theta)}{\partial \theta}\right)\left(\frac{\partial \ell(\theta)}{\partial \theta}\right)^T\right]
$$

**CramÃ©r-Raoä¸‹ç•Œ**:

$$
\text{Var}(\hat{\theta}) \geq \frac{1}{n I(\theta)}
$$

---

**å®šç† 1.1 çš„å®Œæ•´è¯æ˜**:

æˆ‘ä»¬å°†åˆ†åˆ«è¯æ˜MLEçš„ä¸‰ä¸ªæ¸è¿‘æ€§è´¨ã€‚ä¸ºç®€åŒ–è®°å·ï¼Œè€ƒè™‘æ ‡é‡å‚æ•° $\theta \in \mathbb{R}$ï¼ˆå¤šç»´æƒ…å†µç±»ä¼¼ï¼‰ã€‚

**æ­£åˆ™æ¡ä»¶**:

1. å‚æ•°ç©ºé—´ $\Theta$ åŒ…å«çœŸå®å‚æ•° $\theta_0$ çš„ä¸€ä¸ªé‚»åŸŸ
2. æ”¯æ’‘é›† $\{x : p(x; \theta) > 0\}$ ä¸ä¾èµ–äº $\theta$
3. $\log p(x; \theta)$ å…³äº $\theta$ ä¸‰æ¬¡å¯å¾®
4. å¯ä»¥äº¤æ¢æ±‚å¯¼å’Œç§¯åˆ†çš„é¡ºåº
5. Fisherä¿¡æ¯ $I(\theta_0) > 0$ ä¸”æœ‰é™

---

**è¯æ˜ (1): ç›¸åˆæ€§**:

**è¯æ˜æ€è·¯**: è¯æ˜ $\hat{\theta}_n$ æœ€å¤§åŒ–æ ·æœ¬å¯¹æ•°ä¼¼ç„¶ï¼Œè€Œæ ·æœ¬å¯¹æ•°ä¼¼ç„¶ä¾æ¦‚ç‡æ”¶æ•›åˆ°æœŸæœ›å¯¹æ•°ä¼¼ç„¶ï¼Œåè€…åœ¨ $\theta_0$ å¤„å”¯ä¸€æœ€å¤§ã€‚

**ç¬¬ä¸€æ­¥ï¼šå®šä¹‰ç›®æ ‡å‡½æ•°**:

ä»¤ï¼š

- $\ell_n(\theta) = \frac{1}{n} \sum_{i=1}^n \log p(X_i; \theta)$ ï¼ˆæ ·æœ¬å¹³å‡å¯¹æ•°ä¼¼ç„¶ï¼‰
- $\ell(\theta) = \mathbb{E}[\log p(X; \theta)]$ ï¼ˆæœŸæœ›å¯¹æ•°ä¼¼ç„¶ï¼‰

**ç¬¬äºŒæ­¥ï¼šå¤§æ•°å®šå¾‹**:

ç”±å¼ºå¤§æ•°å®šå¾‹ï¼Œå¯¹æ¯ä¸ªå›ºå®šçš„ $\theta$ï¼š

$$
\ell_n(\theta) \xrightarrow{a.s.} \ell(\theta)
$$

**ç¬¬ä¸‰æ­¥ï¼šè¯†åˆ«æœ€å¤§å€¼ç‚¹**:

**å¼•ç†**: $\ell(\theta)$ åœ¨ $\theta = \theta_0$ å¤„å”¯ä¸€æœ€å¤§ã€‚

**è¯æ˜å¼•ç†**: ä½¿ç”¨Kullback-Leibleræ•£åº¦ã€‚å¯¹äº $\theta \neq \theta_0$ï¼š

$$
\ell(\theta) - \ell(\theta_0) = \mathbb{E}[\log p(X; \theta)] - \mathbb{E}[\log p(X; \theta_0)]
$$

$$
= \mathbb{E}\left[\log \frac{p(X; \theta)}{p(X; \theta_0)}\right] = -\text{KL}(p_{\theta_0} \| p_\theta) < 0
$$

ï¼ˆKLæ•£åº¦éè´Ÿï¼Œä¸”ä»…å½“ $\theta = \theta_0$ æ—¶ä¸ºé›¶ï¼‰

**ç¬¬å››æ­¥ï¼šä¸€è‡´æ”¶æ•›**:

åœ¨ç´§é›†ä¸Šï¼Œ$\ell_n(\theta)$ ä¸€è‡´æ”¶æ•›åˆ° $\ell(\theta)$ï¼ˆéœ€è¦æ›´å¼ºçš„æ¡ä»¶ï¼Œå¦‚å¯æ§æ€§ï¼‰ã€‚

**ç¬¬äº”æ­¥ï¼šç»“è®º**:

ç”±äº $\hat{\theta}_n$ æœ€å¤§åŒ– $\ell_n(\theta)$ï¼Œè€Œ $\ell_n(\theta)$ ä¸€è‡´æ”¶æ•›åˆ°åœ¨ $\theta_0$ å¤„å”¯ä¸€æœ€å¤§çš„ $\ell(\theta)$ï¼Œå› æ­¤ï¼š

$$
\hat{\theta}_n \xrightarrow{P} \theta_0
$$

$\square$

---

**è¯æ˜ (2): æ¸è¿‘æ­£æ€æ€§**:

è¿™æ˜¯æœ€é‡è¦ä¹Ÿæœ€æŠ€æœ¯æ€§çš„éƒ¨åˆ†ã€‚

**ç¬¬ä¸€æ­¥ï¼šScoreå‡½æ•°**:

å®šä¹‰**Scoreå‡½æ•°**ï¼š

$$
s(\theta) = \frac{\partial \log p(X; \theta)}{\partial \theta}
$$

**å…³é”®æ€§è´¨**:

- $\mathbb{E}[s(\theta_0)] = 0$ ï¼ˆåœ¨çœŸå®å‚æ•°ä¸‹ï¼ŒScoreçš„æœŸæœ›ä¸ºé›¶ï¼‰
- $\text{Var}(s(\theta_0)) = I(\theta_0)$ ï¼ˆFisherä¿¡æ¯ï¼‰

**è¯æ˜ç¬¬ä¸€ä¸ªæ€§è´¨**:

$$
\mathbb{E}[s(\theta_0)] = \mathbb{E}\left[\frac{\partial \log p(X; \theta_0)}{\partial \theta}\right] = \int \frac{\partial p(x; \theta_0)}{\partial \theta} dx
$$

$$
= \frac{\partial}{\partial \theta} \int p(x; \theta_0) dx = \frac{\partial}{\partial \theta} 1 = 0
$$

**ç¬¬äºŒæ­¥ï¼šMLEçš„ä¸€é˜¶æ¡ä»¶**:

MLE $\hat{\theta}_n$ æ»¡è¶³ï¼š

$$
\frac{\partial \ell_n(\theta)}{\partial \theta}\bigg|_{\theta = \hat{\theta}_n} = \frac{1}{n} \sum_{i=1}^n s_i(\hat{\theta}_n) = 0
$$

å…¶ä¸­ $s_i(\theta) = \frac{\partial \log p(X_i; \theta)}{\partial \theta}$ã€‚

**ç¬¬ä¸‰æ­¥ï¼šTaylorå±•å¼€**:

åœ¨ $\theta_0$ é™„è¿‘å¯¹Scoreå‡½æ•°æ±‚å’Œè¿›è¡ŒTaylorå±•å¼€ï¼š

$$
0 = \frac{1}{n} \sum_{i=1}^n s_i(\hat{\theta}_n) = \frac{1}{n} \sum_{i=1}^n s_i(\theta_0) + \frac{1}{n} \sum_{i=1}^n s_i'(\tilde{\theta}_n)(\hat{\theta}_n - \theta_0)
$$

å…¶ä¸­ $\tilde{\theta}_n$ åœ¨ $\theta_0$ å’Œ $\hat{\theta}_n$ ä¹‹é—´ã€‚

é‡æ–°æ•´ç†ï¼š

$$
\sqrt{n}(\hat{\theta}_n - \theta_0) = -\left[\frac{1}{n} \sum_{i=1}^n s_i'(\tilde{\theta}_n)\right]^{-1} \cdot \frac{1}{\sqrt{n}} \sum_{i=1}^n s_i(\theta_0)
$$

**ç¬¬å››æ­¥ï¼šåˆ†æåˆ†å­**:

ç”±ä¸­å¿ƒæé™å®šç†ï¼Œ$\{s_i(\theta_0)\}$ æ˜¯ç‹¬ç«‹åŒåˆ†å¸ƒçš„ï¼Œ$\mathbb{E}[s_i(\theta_0)] = 0$ï¼Œ$\text{Var}(s_i(\theta_0)) = I(\theta_0)$ï¼Œå› æ­¤ï¼š

$$
\frac{1}{\sqrt{n}} \sum_{i=1}^n s_i(\theta_0) \xrightarrow{d} \mathcal{N}(0, I(\theta_0))
$$

**ç¬¬äº”æ­¥ï¼šåˆ†æåˆ†æ¯**:

$$
s_i'(\theta) = \frac{\partial^2 \log p(X_i; \theta)}{\partial \theta^2}
$$

ç”±å¤§æ•°å®šå¾‹ï¼š

$$
\frac{1}{n} \sum_{i=1}^n s_i'(\tilde{\theta}_n) \xrightarrow{P} \mathbb{E}[s'(\theta_0)] = -I(\theta_0)
$$

ï¼ˆè¿™é‡Œä½¿ç”¨äº† $\tilde{\theta}_n \xrightarrow{P} \theta_0$ å’Œè¿ç»­æ€§ï¼‰

**ç¬¬å…­æ­¥ï¼šSlutskyå®šç†**:

ç»“åˆç¬¬å››æ­¥å’Œç¬¬äº”æ­¥ï¼Œåº”ç”¨Slutskyå®šç†ï¼š

$$
\sqrt{n}(\hat{\theta}_n - \theta_0) = \frac{1}{\sqrt{n}} \sum_{i=1}^n s_i(\theta_0) \cdot \left[-\frac{1}{n} \sum_{i=1}^n s_i'(\tilde{\theta}_n)\right]^{-1}
$$

$$
\xrightarrow{d} \mathcal{N}(0, I(\theta_0)) \cdot \frac{1}{I(\theta_0)} = \mathcal{N}(0, I(\theta_0)^{-1})
$$

å› æ­¤ï¼š

$$
\sqrt{n}(\hat{\theta}_n - \theta_0) \xrightarrow{d} \mathcal{N}(0, I(\theta_0)^{-1})
$$

$\square$

---

**è¯æ˜ (3): æ¸è¿‘æœ‰æ•ˆæ€§**:

**CramÃ©r-Raoä¸‹ç•Œ**:

å¯¹äºä»»ä½•æ— åä¼°è®¡é‡ $\hat{\theta}$ï¼š

$$
\text{Var}(\hat{\theta}) \geq \frac{1}{n I(\theta)}
$$

**è¯æ˜CramÃ©r-Raoä¸‹ç•Œ**:

è®¾ $\hat{\theta}$ æ˜¯ $\theta$ çš„æ— åä¼°è®¡é‡ï¼Œå³ $\mathbb{E}[\hat{\theta}] = \theta$ã€‚

å¯¹ $\theta$ æ±‚å¯¼ï¼š

$$
\frac{\partial}{\partial \theta} \mathbb{E}[\hat{\theta}] = 1
$$

$$
\int \hat{\theta}(x) \frac{\partial p(x; \theta)}{\partial \theta} dx = 1
$$

$$
\int \hat{\theta}(x) p(x; \theta) \frac{\partial \log p(x; \theta)}{\partial \theta} dx = 1
$$

$$
\mathbb{E}[\hat{\theta} \cdot s(\theta)] = 1
$$

ç”±Cauchy-Schwarzä¸ç­‰å¼ï¼š

$$
1 = \mathbb{E}[\hat{\theta} \cdot s(\theta)]^2 \leq \mathbb{E}[\hat{\theta}^2] \cdot \mathbb{E}[s(\theta)^2] = \text{Var}(\hat{\theta}) \cdot I(\theta)
$$

ï¼ˆä½¿ç”¨äº† $\mathbb{E}[\hat{\theta}] = \theta$, $\mathbb{E}[s(\theta)] = 0$ï¼‰

å› æ­¤ï¼š

$$
\text{Var}(\hat{\theta}) \geq \frac{1}{I(\theta)}
$$

å¯¹äº $n$ ä¸ªæ ·æœ¬ï¼š

$$
\text{Var}(\hat{\theta}) \geq \frac{1}{n I(\theta)}
$$

$\square$

**MLEè¾¾åˆ°ä¸‹ç•Œ**:

ä»æ¸è¿‘æ­£æ€æ€§ï¼š

$$
\text{Var}(\hat{\theta}_n) \sim \frac{I(\theta_0)^{-1}}{n} = \frac{1}{n I(\theta_0)}
$$

å› æ­¤MLEæ¸è¿‘åœ°è¾¾åˆ°CramÃ©r-Raoä¸‹ç•Œï¼Œæ˜¯æ¸è¿‘æœ‰æ•ˆçš„ã€‚ $\square$

---

**è¯æ˜çš„å…³é”®è¦ç‚¹**ï¼š

1. **ç›¸åˆæ€§**: åŸºäºå¤§æ•°å®šå¾‹å’ŒKLæ•£åº¦çš„å”¯ä¸€æ€§
2. **æ¸è¿‘æ­£æ€æ€§**: åŸºäºCLTå’ŒTaylorå±•å¼€ï¼Œè¿™æ˜¯æœ€æ ¸å¿ƒçš„ç»“æœ
3. **æ¸è¿‘æœ‰æ•ˆæ€§**: MLEçš„æ–¹å·®è¾¾åˆ°ç†è®ºä¸‹ç•Œ

**å‡ ä½•ç›´è§‚**ï¼š

- **Scoreå‡½æ•°** $s(\theta)$ æ˜¯å¯¹æ•°ä¼¼ç„¶çš„æ¢¯åº¦ï¼ŒæŒ‡å‘ä¼¼ç„¶å¢åŠ çš„æ–¹å‘
- **Fisherä¿¡æ¯** $I(\theta)$ åº¦é‡äº†Scoreå‡½æ•°çš„å˜å¼‚æ€§ï¼Œåæ˜ äº†æ•°æ®æä¾›çš„ä¿¡æ¯é‡
- **MLE** é€šè¿‡è®¾ç½®Scoreä¸ºé›¶æ‰¾åˆ°ä¼¼ç„¶çš„æœ€å¤§å€¼ç‚¹

**å®é™…æ„ä¹‰**ï¼š

MLEçš„æ¸è¿‘æ€§è´¨ä¿è¯äº†ï¼š

1. å¤§æ ·æœ¬ä¸‹ï¼ŒMLEæ”¶æ•›åˆ°çœŸå®å‚æ•°ï¼ˆç›¸åˆæ€§ï¼‰
2. MLEçš„åˆ†å¸ƒè¶‹äºæ­£æ€ï¼ˆå¯ä»¥æ„é€ ç½®ä¿¡åŒºé—´ï¼‰
3. MLEæ˜¯æœ€ä¼˜çš„ï¼ˆæ¸è¿‘æœ‰æ•ˆæ€§ï¼‰

è¿™äº›æ€§è´¨ä½¿å¾—MLEæˆä¸ºç»Ÿè®¡æ¨æ–­ä¸­æœ€é‡è¦çš„æ–¹æ³•ä¹‹ä¸€ã€‚

---

### 2. çŸ©ä¼°è®¡ (Method of Moments)

**æ€æƒ³**: ç”¨æ ·æœ¬çŸ©ä¼°è®¡æ€»ä½“çŸ©ã€‚

**$k$é˜¶æ ·æœ¬çŸ©**:

$$
\hat{m}_k = \frac{1}{n} \sum_{i=1}^n X_i^k
$$

**$k$é˜¶æ€»ä½“çŸ©**:

$$
m_k(\theta) = \mathbb{E}[X^k]
$$

**çŸ©ä¼°è®¡**:

è§£æ–¹ç¨‹ç»„ $\hat{m}_k = m_k(\theta)$ å¾—åˆ° $\hat{\theta}_{MM}$ã€‚

---

**ä¾‹ 1.2 (æŒ‡æ•°åˆ†å¸ƒçš„çŸ©ä¼°è®¡)**:

è®¾ $X_1, \ldots, X_n \sim \text{Exp}(\lambda)$ï¼Œåˆ™ $\mathbb{E}[X] = 1/\lambda$ã€‚

**çŸ©ä¼°è®¡**:

$$
\hat{\lambda}_{MM} = \frac{1}{\bar{X}}
$$

---

### 3. ä¼°è®¡é‡çš„æ€§è´¨

**æ— åæ€§** (Unbiasedness):

$$
\mathbb{E}[\hat{\theta}] = \theta
$$

**ç›¸åˆæ€§** (Consistency):

$$
\hat{\theta}_n \xrightarrow{P} \theta
$$

**æœ‰æ•ˆæ€§** (Efficiency):

åœ¨æ‰€æœ‰æ— åä¼°è®¡ä¸­ï¼Œæ–¹å·®æœ€å°ã€‚

**å‡æ–¹è¯¯å·®** (MSE):

$$
\text{MSE}(\hat{\theta}) = \mathbb{E}[(\hat{\theta} - \theta)^2] = \text{Var}(\hat{\theta}) + \text{Bias}(\hat{\theta})^2
$$

---

## ğŸ“Š åŒºé—´ä¼°è®¡

### 1. ç½®ä¿¡åŒºé—´

**å®šä¹‰**:

$[L(X), U(X)]$ æ˜¯ $\theta$ çš„ $1-\alpha$ **ç½®ä¿¡åŒºé—´**ï¼Œå¦‚æœï¼š

$$
P_{\theta}(\theta \in [L(X), U(X)]) \geq 1 - \alpha, \quad \forall \theta
$$

**è§£é‡Š**: åœ¨é‡å¤æŠ½æ ·ä¸­ï¼Œçº¦ $100(1-\alpha)\%$ çš„åŒºé—´åŒ…å«çœŸå®å‚æ•°ã€‚

---

**ä¾‹ 2.1 (æ­£æ€å‡å€¼çš„ç½®ä¿¡åŒºé—´)**:

è®¾ $X_1, \ldots, X_n \sim \mathcal{N}(\mu, \sigma^2)$ï¼Œ$\sigma^2$ å·²çŸ¥ã€‚

$$
\bar{X} \sim \mathcal{N}\left(\mu, \frac{\sigma^2}{n}\right)
$$

$$
\frac{\bar{X} - \mu}{\sigma/\sqrt{n}} \sim \mathcal{N}(0, 1)
$$

**$1-\alpha$ ç½®ä¿¡åŒºé—´**:

$$
\left[\bar{X} - z_{\alpha/2} \frac{\sigma}{\sqrt{n}}, \bar{X} + z_{\alpha/2} \frac{\sigma}{\sqrt{n}}\right]
$$

å…¶ä¸­ $z_{\alpha/2}$ æ˜¯æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„ $1-\alpha/2$ åˆ†ä½æ•°ã€‚

---

**$\sigma^2$ æœªçŸ¥æ—¶**:

ä½¿ç”¨ $t$ åˆ†å¸ƒï¼š

$$
\frac{\bar{X} - \mu}{S/\sqrt{n}} \sim t_{n-1}
$$

å…¶ä¸­ $S^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \bar{X})^2$ã€‚

**ç½®ä¿¡åŒºé—´**:

$$
\left[\bar{X} - t_{\alpha/2, n-1} \frac{S}{\sqrt{n}}, \bar{X} + t_{\alpha/2, n-1} \frac{S}{\sqrt{n}}\right]
$$

---

### 2. æ¸è¿‘ç½®ä¿¡åŒºé—´

åˆ©ç”¨MLEçš„æ¸è¿‘æ­£æ€æ€§ï¼š

$$
\sqrt{n}(\hat{\theta}_n - \theta) \xrightarrow{d} \mathcal{N}(0, I(\theta)^{-1})
$$

**æ¸è¿‘ $1-\alpha$ ç½®ä¿¡åŒºé—´**:

$$
\left[\hat{\theta}_n - z_{\alpha/2} \sqrt{\frac{I(\hat{\theta}_n)^{-1}}{n}}, \hat{\theta}_n + z_{\alpha/2} \sqrt{\frac{I(\hat{\theta}_n)^{-1}}{n}}\right]
$$

---

### 3. Bootstrapç½®ä¿¡åŒºé—´

**Bootstrapæ–¹æ³•**:

1. ä»æ ·æœ¬ $\{X_1, \ldots, X_n\}$ ä¸­æœ‰æ”¾å›æŠ½å– $n$ ä¸ªæ ·æœ¬ï¼Œå¾—åˆ° $\{X_1^*, \ldots, X_n^*\}$
2. è®¡ç®— $\hat{\theta}^* = \hat{\theta}(X_1^*, \ldots, X_n^*)$
3. é‡å¤ $B$ æ¬¡ï¼Œå¾—åˆ° $\{\hat{\theta}_1^*, \ldots, \hat{\theta}_B^*\}$

**ç™¾åˆ†ä½Bootstrapç½®ä¿¡åŒºé—´**:

$$
[\hat{\theta}_{\alpha/2}^*, \hat{\theta}_{1-\alpha/2}^*]
$$

å…¶ä¸­ $\hat{\theta}_{\alpha}^*$ æ˜¯ $\{\hat{\theta}_1^*, \ldots, \hat{\theta}_B^*\}$ çš„ $\alpha$ åˆ†ä½æ•°ã€‚

---

## ğŸ”¬ å‡è®¾æ£€éªŒ

### 1. åŸºæœ¬æ¦‚å¿µ

**å‡è®¾**:

- **åŸå‡è®¾** (Null Hypothesis): $H_0$
- **å¤‡æ‹©å‡è®¾** (Alternative Hypothesis): $H_1$ æˆ– $H_a$

**ä¸¤ç±»é”™è¯¯**:

| | $H_0$ ä¸ºçœŸ | $H_0$ ä¸ºå‡ |
|---|---|---|
| **æ‹’ç» $H_0$** | ç¬¬ä¸€ç±»é”™è¯¯ (Type I) | æ­£ç¡® |
| **æ¥å— $H_0$** | æ­£ç¡® | ç¬¬äºŒç±»é”™è¯¯ (Type II) |

**æ˜¾è‘—æ€§æ°´å¹³**: $\alpha = P(\text{Type I error})$

**åŠŸæ•ˆ** (Power): $1 - \beta = 1 - P(\text{Type II error})$

---

**æ£€éªŒç»Ÿè®¡é‡**:

åŸºäºæ•°æ®è®¡ç®—çš„ç»Ÿè®¡é‡ $T(X)$ã€‚

**æ‹’ç»åŸŸ**:

$R = \{x: T(x) > c\}$ï¼Œå…¶ä¸­ $c$ æ˜¯ä¸´ç•Œå€¼ã€‚

**på€¼**:

åœ¨ $H_0$ ä¸‹ï¼Œè§‚æµ‹åˆ°å½“å‰æˆ–æ›´æç«¯ç»“æœçš„æ¦‚ç‡ã€‚

$$
p = P_{H_0}(T(X) \geq T(x_{obs}))
$$

**å†³ç­–è§„åˆ™**: å¦‚æœ $p < \alpha$ï¼Œæ‹’ç» $H_0$ã€‚

---

### 2. ç»å…¸æ£€éªŒ

**ä¾‹ 3.1 (å•æ ·æœ¬ $t$ æ£€éªŒ)**:

è®¾ $X_1, \ldots, X_n \sim \mathcal{N}(\mu, \sigma^2)$ï¼Œ$\sigma^2$ æœªçŸ¥ã€‚

**å‡è®¾**:

- $H_0: \mu = \mu_0$
- $H_1: \mu \neq \mu_0$

**æ£€éªŒç»Ÿè®¡é‡**:

$$
T = \frac{\bar{X} - \mu_0}{S/\sqrt{n}} \sim t_{n-1} \quad \text{(under } H_0\text{)}
$$

**æ‹’ç»åŸŸ**: $|T| > t_{\alpha/2, n-1}$

---

**ä¾‹ 3.2 (ä¸¤æ ·æœ¬ $t$ æ£€éªŒ)**:

è®¾ $X_1, \ldots, X_m \sim \mathcal{N}(\mu_1, \sigma^2)$ï¼Œ$Y_1, \ldots, Y_n \sim \mathcal{N}(\mu_2, \sigma^2)$ã€‚

**å‡è®¾**:

- $H_0: \mu_1 = \mu_2$
- $H_1: \mu_1 \neq \mu_2$

**æ£€éªŒç»Ÿè®¡é‡** (ç­‰æ–¹å·®):

$$
T = \frac{\bar{X} - \bar{Y}}{S_p \sqrt{1/m + 1/n}} \sim t_{m+n-2}
$$

å…¶ä¸­ $S_p^2 = \frac{(m-1)S_X^2 + (n-1)S_Y^2}{m+n-2}$ æ˜¯åˆå¹¶æ–¹å·®ã€‚

---

**ä¾‹ 3.3 (ä¼¼ç„¶æ¯”æ£€éªŒ)**:

**æ£€éªŒç»Ÿè®¡é‡**:

$$
\Lambda = \frac{\sup_{\theta \in \Theta_0} L(\theta)}{\sup_{\theta \in \Theta} L(\theta)}
$$

**Wilkså®šç†**:

åœ¨æ­£åˆ™æ¡ä»¶ä¸‹ï¼Œ

$$
-2 \log \Lambda \xrightarrow{d} \chi^2_k
$$

å…¶ä¸­ $k = \dim(\Theta) - \dim(\Theta_0)$ã€‚

---

**Wilkså®šç†çš„è¯æ˜**:

**å®šç†é™ˆè¿°ï¼ˆå®Œæ•´ç‰ˆï¼‰**:

è®¾ $X_1, \ldots, X_n$ æ˜¯æ¥è‡ªå¯†åº¦å‡½æ•° $f(x; \theta)$ çš„ç‹¬ç«‹åŒåˆ†å¸ƒæ ·æœ¬ï¼Œå…¶ä¸­ $\theta \in \Theta \subseteq \mathbb{R}^p$ã€‚

è€ƒè™‘å‡è®¾æ£€éªŒï¼š

- $H_0: \theta \in \Theta_0$ï¼ˆ$\Theta_0$ æ˜¯ $q$ ç»´å­ç©ºé—´ï¼Œ$q < p$ï¼‰
- $H_1: \theta \in \Theta$

ä¼¼ç„¶æ¯”ç»Ÿè®¡é‡ï¼š

$$
\Lambda_n = \frac{\sup_{\theta \in \Theta_0} L_n(\theta)}{\sup_{\theta \in \Theta} L_n(\theta)}
$$

**å®šç†**: åœ¨æ­£åˆ™æ¡ä»¶ä¸‹ï¼Œå½“ $H_0$ ä¸ºçœŸæ—¶ï¼š

$$
-2 \log \Lambda_n \xrightarrow{d} \chi^2_{p-q}
$$

å…¶ä¸­ $p - q$ æ˜¯è‡ªç”±åº¦ï¼ˆå‚æ•°ç©ºé—´çš„ç»´æ•°å·®ï¼‰ã€‚

---

**æ­£åˆ™æ¡ä»¶**:

1. **å‚æ•°ç©ºé—´**: $\Theta$ æ˜¯ $\mathbb{R}^p$ çš„å¼€é›†ï¼Œ$\Theta_0$ æ˜¯ $q$ ç»´å…‰æ»‘å­æµå½¢
2. **å¯è¯†åˆ«æ€§**: ä¸åŒçš„ $\theta$ å¯¹åº”ä¸åŒçš„åˆ†å¸ƒ
3. **Fisherä¿¡æ¯**: Fisherä¿¡æ¯çŸ©é˜µ $I(\theta)$ åœ¨ $\Theta$ ä¸Šè¿ç»­ä¸”æ­£å®š
4. **æ­£åˆ™æ€§**: å¯¹æ•°ä¼¼ç„¶å‡½æ•°æ»¡è¶³é€šå¸¸çš„æ­£åˆ™æ€§æ¡ä»¶ï¼ˆå¯äº¤æ¢æ±‚å¯¼ä¸ç§¯åˆ†é¡ºåºç­‰ï¼‰
5. **çœŸå‚æ•°**: çœŸå‚æ•° $\theta_0 \in \Theta_0$ï¼ˆ$H_0$ ä¸ºçœŸï¼‰

---

**è¯æ˜**:

**ç¬¬ä¸€æ­¥ï¼šMLEçš„æ¸è¿‘æ­£æ€æ€§**:

åœ¨æ­£åˆ™æ¡ä»¶ä¸‹ï¼Œæ— çº¦æŸMLE $\hat{\theta}_n$ æ»¡è¶³ï¼š

$$
\sqrt{n}(\hat{\theta}_n - \theta_0) \xrightarrow{d} N(0, I(\theta_0)^{-1})
$$

å…¶ä¸­ $I(\theta_0)$ æ˜¯Fisherä¿¡æ¯çŸ©é˜µã€‚

**ç¬¬äºŒæ­¥ï¼šçº¦æŸMLE**:

ä»¤ $\tilde{\theta}_n$ æ˜¯çº¦æŸMLEï¼ˆåœ¨ $\Theta_0$ ä¸Šçš„MLEï¼‰ã€‚

ç”±äº $\Theta_0$ æ˜¯ $q$ ç»´å­æµå½¢ï¼Œå¯ä»¥ç”¨å±€éƒ¨å‚æ•°åŒ–ï¼š

$$
\theta = h(\psi), \quad \psi \in \mathbb{R}^q
$$

å…¶ä¸­ $h: \mathbb{R}^q \to \Theta_0$ æ˜¯å…‰æ»‘åµŒå…¥ã€‚

çº¦æŸMLE $\tilde{\theta}_n$ ä¹Ÿæ»¡è¶³æ¸è¿‘æ­£æ€æ€§ï¼ˆåœ¨ $\Theta_0$ å†…ï¼‰ã€‚

**ç¬¬ä¸‰æ­¥ï¼šå¯¹æ•°ä¼¼ç„¶æ¯”çš„Taylorå±•å¼€**:

å¯¹æ•°ä¼¼ç„¶æ¯”ï¼š

$$
\log \Lambda_n = \ell_n(\tilde{\theta}_n) - \ell_n(\hat{\theta}_n)
$$

å…¶ä¸­ $\ell_n(\theta) = \log L_n(\theta) = \sum_{i=1}^n \log f(X_i; \theta)$ã€‚

åœ¨ $\theta_0$ é™„è¿‘å¯¹ $\ell_n$ è¿›è¡ŒäºŒé˜¶Taylorå±•å¼€ï¼š

$$
\ell_n(\theta) = \ell_n(\theta_0) + \nabla \ell_n(\theta_0)^T (\theta - \theta_0) + \frac{1}{2} (\theta - \theta_0)^T H_n(\theta^*) (\theta - \theta_0)
$$

å…¶ä¸­ $H_n(\theta) = \nabla^2 \ell_n(\theta)$ æ˜¯HessiançŸ©é˜µï¼Œ$\theta^*$ åœ¨ $\theta$ å’Œ $\theta_0$ ä¹‹é—´ã€‚

**ç¬¬å››æ­¥ï¼šæ— çº¦æŸMLEçš„å±•å¼€**:

ç”±äº $\hat{\theta}_n$ æ˜¯æ— çº¦æŸMLEï¼Œæœ‰ $\nabla \ell_n(\hat{\theta}_n) = 0$ã€‚

åœ¨ $\hat{\theta}_n$ é™„è¿‘å±•å¼€ï¼š

$$
\ell_n(\theta_0) = \ell_n(\hat{\theta}_n) + \frac{1}{2} (\theta_0 - \hat{\theta}_n)^T H_n(\theta_1^*) (\theta_0 - \hat{\theta}_n)
$$

**ç¬¬äº”æ­¥ï¼šçº¦æŸMLEçš„å±•å¼€**:

ç±»ä¼¼åœ°ï¼Œåœ¨ $\tilde{\theta}_n$ é™„è¿‘å±•å¼€ï¼š

$$
\ell_n(\theta_0) = \ell_n(\tilde{\theta}_n) + \nabla \ell_n(\tilde{\theta}_n)^T (\theta_0 - \tilde{\theta}_n) + \frac{1}{2} (\theta_0 - \tilde{\theta}_n)^T H_n(\theta_2^*) (\theta_0 - \tilde{\theta}_n)
$$

ä½†ç”±äº $\tilde{\theta}_n$ æ˜¯çº¦æŸMLEï¼Œæ¢¯åº¦ $\nabla \ell_n(\tilde{\theta}_n)$ åœ¨çº¦æŸæ–¹å‘ä¸Šä¸ºé›¶ï¼Œåœ¨å‚ç›´æ–¹å‘ä¸Šéé›¶ã€‚

**ç¬¬å…­æ­¥ï¼šå…³é”®è§‚å¯Ÿ**:

ç”±äº $\hat{\theta}_n$ æ˜¯å…¨å±€æœ€ä¼˜ï¼Œè€Œ $\tilde{\theta}_n$ æ˜¯çº¦æŸæœ€ä¼˜ï¼š

$$
\ell_n(\hat{\theta}_n) \geq \ell_n(\tilde{\theta}_n)
$$

å› æ­¤ï¼š

$$
-2 \log \Lambda_n = 2[\ell_n(\hat{\theta}_n) - \ell_n(\tilde{\theta}_n)] \geq 0
$$

**ç¬¬ä¸ƒæ­¥ï¼šæ¸è¿‘å±•å¼€**:

ä½¿ç”¨ $H_n(\theta) \xrightarrow{P} -n I(\theta_0)$ï¼ˆç”±å¤§æ•°å®šå¾‹ï¼‰ï¼š

$$
\ell_n(\hat{\theta}_n) - \ell_n(\theta_0) \approx -\frac{n}{2} (\hat{\theta}_n - \theta_0)^T I(\theta_0) (\hat{\theta}_n - \theta_0)
$$

$$
\ell_n(\tilde{\theta}_n) - \ell_n(\theta_0) \approx -\frac{n}{2} (\tilde{\theta}_n - \theta_0)^T I(\theta_0) (\tilde{\theta}_n - \theta_0)
$$

**ç¬¬å…«æ­¥ï¼šæŠ•å½±è§£é‡Š**:

ä»¤ $Z_n = \sqrt{n}(\hat{\theta}_n - \theta_0) \xrightarrow{d} N(0, I(\theta_0)^{-1})$ã€‚

çº¦æŸ $\theta \in \Theta_0$ ç›¸å½“äºçº¿æ€§çº¦æŸï¼ˆåœ¨ä¸€é˜¶è¿‘ä¼¼ä¸‹ï¼‰ï¼š

$$
A^T (\theta - \theta_0) = 0
$$

å…¶ä¸­ $A$ æ˜¯ $p \times (p-q)$ çŸ©é˜µï¼Œåˆ—å‘é‡å¼ æˆ $\Theta_0$ çš„æ­£äº¤è¡¥ç©ºé—´ã€‚

$\tilde{\theta}_n$ æ˜¯ $\hat{\theta}_n$ åœ¨ $\Theta_0$ ä¸Šçš„æŠ•å½±ï¼ˆåœ¨Fisheråº¦é‡ä¸‹ï¼‰ã€‚

**ç¬¬ä¹æ­¥ï¼šäºŒæ¬¡å‹**:

$$
-2 \log \Lambda_n \approx n [(\hat{\theta}_n - \theta_0)^T I(\theta_0) (\hat{\theta}_n - \theta_0) - (\tilde{\theta}_n - \theta_0)^T I(\theta_0) (\tilde{\theta}_n - \theta_0)]
$$

ä»¤ $W = I(\theta_0)^{1/2}$ï¼Œ$Y_n = W \sqrt{n}(\hat{\theta}_n - \theta_0)$ï¼Œåˆ™ $Y_n \xrightarrow{d} N(0, I_p)$ã€‚

ä»¤ $P$ æ˜¯åˆ°çº¦æŸå­ç©ºé—´çš„æ­£äº¤æŠ•å½±çŸ©é˜µï¼ˆåœ¨æ ‡å‡†å†…ç§¯ä¸‹ï¼‰ï¼Œåˆ™ï¼š

$$
-2 \log \Lambda_n \approx Y_n^T Y_n - (P Y_n)^T (P Y_n) = Y_n^T (I - P) Y_n
$$

**ç¬¬åæ­¥ï¼šæŠ•å½±çŸ©é˜µçš„æ€§è´¨**:

$I - P$ æ˜¯åˆ° $\Theta_0$ çš„æ­£äº¤è¡¥ç©ºé—´çš„æŠ•å½±çŸ©é˜µï¼Œç§©ä¸º $p - q$ã€‚

å› æ­¤ $I - P$ æœ‰ $p - q$ ä¸ªç‰¹å¾å€¼ä¸º 1ï¼Œå…¶ä½™ä¸º 0ã€‚

**ç¬¬åä¸€æ­¥ï¼šå¡æ–¹åˆ†å¸ƒ**:

å¯¹äºæ ‡å‡†æ­£æ€å‘é‡ $Y \sim N(0, I_p)$ å’Œç§©ä¸º $r$ çš„æŠ•å½±çŸ©é˜µ $P$ï¼š

$$
Y^T P Y \sim \chi^2_r
$$

å› æ­¤ï¼š

$$
-2 \log \Lambda_n \xrightarrow{d} \chi^2_{p-q}
$$

$\square$

---

**å…³é”®è¦ç‚¹**:

1. **è‡ªç”±åº¦**: $p - q$ æ˜¯å‚æ•°ç©ºé—´ç»´æ•°å·®ï¼Œè¡¨ç¤ºçº¦æŸçš„"ç´§åº¦"
2. **æŠ•å½±è§£é‡Š**: ä¼¼ç„¶æ¯”æ£€éªŒç­‰ä»·äºæ£€éªŒMLEåˆ°çº¦æŸç©ºé—´çš„"è·ç¦»"
3. **Fisheråº¦é‡**: ä½¿ç”¨Fisherä¿¡æ¯çŸ©é˜µå®šä¹‰çš„åº¦é‡
4. **æ­£åˆ™æ¡ä»¶**: ä¿è¯MLEçš„æ¸è¿‘æ­£æ€æ€§å’ŒTaylorå±•å¼€çš„æœ‰æ•ˆæ€§

---

**Wilkså®šç†çš„åº”ç”¨**:

**ä¾‹1ï¼šå•å‚æ•°æ£€éªŒ**:

æ£€éªŒ $H_0: \theta = \theta_0$ vs $H_1: \theta \neq \theta_0$ï¼ˆ$p = 1, q = 0$ï¼‰ï¼š

$$
-2 \log \Lambda_n = 2[\ell_n(\hat{\theta}_n) - \ell_n(\theta_0)] \xrightarrow{d} \chi^2_1
$$

**ä¾‹2ï¼šåµŒå¥—æ¨¡å‹æ¯”è¾ƒ**:

æ¨¡å‹1ï¼ˆå®Œæ•´ï¼‰: $p$ ä¸ªå‚æ•°  
æ¨¡å‹2ï¼ˆç®€åŒ–ï¼‰: $q$ ä¸ªå‚æ•°ï¼ˆ$q < p$ï¼‰

$$
-2 \log \Lambda_n = 2[\ell_n(\text{å®Œæ•´}) - \ell_n(\text{ç®€åŒ–})] \xrightarrow{d} \chi^2_{p-q}
$$

**ä¾‹3ï¼šæ­£æ€åˆ†å¸ƒæ–¹å·®æ£€éªŒ**:

$X_1, \ldots, X_n \sim N(\mu, \sigma^2)$ï¼Œæ£€éªŒ $H_0: \sigma^2 = \sigma_0^2$ vs $H_1: \sigma^2 \neq \sigma_0^2$ï¼š

å¯¹æ•°ä¼¼ç„¶ï¼š

$$
\ell_n(\mu, \sigma^2) = -\frac{n}{2} \log(2\pi) - \frac{n}{2} \log \sigma^2 - \frac{1}{2\sigma^2} \sum_{i=1}^n (X_i - \mu)^2
$$

æ— çº¦æŸMLE: $\hat{\mu} = \bar{X}$, $\hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})^2$

çº¦æŸMLEï¼ˆ$\sigma^2 = \sigma_0^2$ï¼‰: $\tilde{\mu} = \bar{X}$, $\tilde{\sigma}^2 = \sigma_0^2$

ä¼¼ç„¶æ¯”ç»Ÿè®¡é‡ï¼š

$$
-2 \log \Lambda_n = n \log \frac{\sigma_0^2}{\hat{\sigma}^2} + n \frac{\hat{\sigma}^2}{\sigma_0^2} - n \xrightarrow{d} \chi^2_1
$$

**ä¾‹4ï¼šçº¿æ€§å›å½’ç³»æ•°æ£€éªŒ**:

å®Œæ•´æ¨¡å‹: $Y = X\beta + \epsilon$ï¼ˆ$p$ ä¸ªç³»æ•°ï¼‰  
ç®€åŒ–æ¨¡å‹: $Y = X_0\beta_0 + \epsilon$ï¼ˆ$q$ ä¸ªç³»æ•°ï¼‰

$$
-2 \log \Lambda_n = n \log \frac{\text{RSS}_0}{\text{RSS}} \xrightarrow{d} \chi^2_{p-q}
$$

å…¶ä¸­ RSS æ˜¯æ®‹å·®å¹³æ–¹å’Œã€‚

---

**Wilkså®šç†çš„å±€é™æ€§**:

1. **æ­£åˆ™æ¡ä»¶**: éœ€è¦æ»¡è¶³ä¸¥æ ¼çš„æ­£åˆ™æ€§æ¡ä»¶
2. **è¾¹ç•Œé—®é¢˜**: å½“çœŸå‚æ•°åœ¨å‚æ•°ç©ºé—´è¾¹ç•Œä¸Šæ—¶ï¼Œç»“è®ºå¯èƒ½ä¸æˆç«‹
3. **å°æ ·æœ¬**: æ¸è¿‘ç»“æœï¼Œå°æ ·æœ¬æ—¶å¯èƒ½ä¸å‡†ç¡®
4. **æ¨¡å‹é”™è¯¯æŒ‡å®š**: å‡è®¾æ¨¡å‹æ­£ç¡®ï¼Œå¦åˆ™ç»“è®ºæ— æ•ˆ

---

**ä¸å…¶ä»–æ£€éªŒçš„å…³ç³»**:

**1. Waldæ£€éªŒ**:

$$
W_n = n(\hat{\theta}_n - \theta_0)^T I(\hat{\theta}_n) (\hat{\theta}_n - \theta_0) \xrightarrow{d} \chi^2_{p-q}
$$

**2. Scoreæ£€éªŒï¼ˆRaoæ£€éªŒï¼‰**:

$$
S_n = \frac{1}{n} \nabla \ell_n(\theta_0)^T I(\theta_0)^{-1} \nabla \ell_n(\theta_0) \xrightarrow{d} \chi^2_{p-q}
$$

**3. ä¸‰è€…çš„å…³ç³»**:

åœ¨ $H_0$ ä¸‹ï¼Œä¸‰ä¸ªç»Ÿè®¡é‡æ¸è¿‘ç­‰ä»·ï¼š

$$
W_n \sim S_n \sim -2 \log \Lambda_n \xrightarrow{d} \chi^2_{p-q}
$$

ä½†åœ¨æœ‰é™æ ·æœ¬ä¸­å¯èƒ½æœ‰å·®å¼‚ï¼š

- Wald: åªéœ€è¦æ— çº¦æŸMLE
- Score: åªéœ€è¦çº¦æŸMLE
- LRT: éœ€è¦ä¸¤ä¸ªMLEï¼Œä½†é€šå¸¸åŠŸæ•ˆæœ€å¥½

---

**æ•°å€¼ä¾‹å­**:

è®¾ $X_1, \ldots, X_{100} \sim N(\mu, 1)$ï¼Œæ£€éªŒ $H_0: \mu = 0$ vs $H_1: \mu \neq 0$ã€‚

è§‚æµ‹åˆ° $\bar{X} = 0.3$ã€‚

å¯¹æ•°ä¼¼ç„¶æ¯”ï¼š

$$
-2 \log \Lambda = 100 \times (\bar{X} - 0)^2 = 100 \times 0.09 = 9
$$

ä¸´ç•Œå€¼ï¼ˆ$\alpha = 0.05$ï¼‰: $\chi^2_1(0.95) = 3.841$

ç”±äº $9 > 3.841$ï¼Œæ‹’ç» $H_0$ã€‚

på€¼: $P(\chi^2_1 > 9) \approx 0.0027$

---

### 3. på€¼ä¸å¤šé‡æ£€éªŒ

**å¤šé‡æ£€éªŒé—®é¢˜**:

åŒæ—¶æ£€éªŒ $m$ ä¸ªå‡è®¾ $H_1, \ldots, H_m$ã€‚

**å®¶æ—é”™è¯¯ç‡** (FWER):

$$
\text{FWER} = P(\text{è‡³å°‘ä¸€ä¸ªType Ié”™è¯¯})
$$

**Bonferroniæ ¡æ­£**:

å¯¹æ¯ä¸ªæ£€éªŒä½¿ç”¨æ˜¾è‘—æ€§æ°´å¹³ $\alpha/m$ã€‚

$$
\text{FWER} \leq m \cdot \frac{\alpha}{m} = \alpha
$$

---

**é”™è¯¯å‘ç°ç‡** (FDR):

$$
\text{FDR} = \mathbb{E}\left[\frac{\text{å‡é˜³æ€§æ•°}}{\text{æ‹’ç»æ•°}}\right]
$$

**Benjamini-Hochbergè¿‡ç¨‹**:

1. å¯¹ $p$ å€¼æ’åº: $p_{(1)} \leq \cdots \leq p_{(m)}$
2. æ‰¾æœ€å¤§çš„ $k$ ä½¿å¾— $p_{(k)} \leq \frac{k}{m} \alpha$
3. æ‹’ç»å¯¹åº”äº $p_{(1)}, \ldots, p_{(k)}$ çš„å‡è®¾

**æ§åˆ¶**: $\text{FDR} \leq \alpha$

---

## ğŸ’¡ è´å¶æ–¯æ¨æ–­

### 1. è´å¶æ–¯å®šç†

**è´å¶æ–¯å…¬å¼**:

$$
p(\theta | X) = \frac{p(X | \theta) p(\theta)}{p(X)}
$$

å…¶ä¸­ï¼š

- $p(\theta)$: **å…ˆéªŒåˆ†å¸ƒ** (Prior)
- $p(X | \theta)$: **ä¼¼ç„¶** (Likelihood)
- $p(\theta | X)$: **åéªŒåˆ†å¸ƒ** (Posterior)
- $p(X) = \int p(X | \theta) p(\theta) d\theta$: **è¾¹é™…ä¼¼ç„¶** (Evidence)

---

**åéªŒæ­£æ¯”äºå…ˆéªŒä¹˜ä»¥ä¼¼ç„¶**:

$$
p(\theta | X) \propto p(X | \theta) p(\theta)
$$

---

### 2. å…±è½­å…ˆéªŒ

**å®šä¹‰**: å¦‚æœå…ˆéªŒå’ŒåéªŒå±äºåŒä¸€åˆ†å¸ƒæ—ï¼Œç§°å…ˆéªŒä¸º**å…±è½­å…ˆéªŒ**ã€‚

**ä¾‹ 4.1 (Beta-Binomialå…±è½­)**:

- **ä¼¼ç„¶**: $X \sim \text{Binomial}(n, \theta)$
- **å…ˆéªŒ**: $\theta \sim \text{Beta}(\alpha, \beta)$

$$
p(\theta) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)} \theta^{\alpha-1} (1-\theta)^{\beta-1}
$$

- **åéªŒ**: $\theta | X \sim \text{Beta}(\alpha + X, \beta + n - X)$

---

**ä¾‹ 4.2 (Gamma-Poissonå…±è½­)**:

- **ä¼¼ç„¶**: $X_1, \ldots, X_n \sim \text{Poisson}(\lambda)$
- **å…ˆéªŒ**: $\lambda \sim \text{Gamma}(\alpha, \beta)$

$$
p(\lambda) = \frac{\beta^\alpha}{\Gamma(\alpha)} \lambda^{\alpha-1} e^{-\beta\lambda}
$$

- **åéªŒ**: $\lambda | X \sim \text{Gamma}\left(\alpha + \sum X_i, \beta + n\right)$

---

**ä¾‹ 4.3 (Normal-Normalå…±è½­)**:

- **ä¼¼ç„¶**: $X_1, \ldots, X_n \sim \mathcal{N}(\mu, \sigma^2)$ï¼Œ$\sigma^2$ å·²çŸ¥
- **å…ˆéªŒ**: $\mu \sim \mathcal{N}(\mu_0, \tau^2)$

- **åéªŒ**: $\mu | X \sim \mathcal{N}(\mu_n, \tau_n^2)$

å…¶ä¸­ï¼š

$$
\tau_n^{-2} = \tau^{-2} + n\sigma^{-2}
$$

$$
\mu_n = \tau_n^2 \left(\frac{\mu_0}{\tau^2} + \frac{n\bar{X}}{\sigma^2}\right)
$$

---

### 3. åéªŒè®¡ç®—

**ç‚¹ä¼°è®¡**:

- **åéªŒå‡å€¼**: $\hat{\theta}_{Bayes} = \mathbb{E}[\theta | X]$
- **åéªŒä¸­ä½æ•°**: $\text{Median}(\theta | X)$
- **æœ€å¤§åéªŒä¼°è®¡** (MAP): $\hat{\theta}_{MAP} = \arg\max_{\theta} p(\theta | X)$

**åŒºé—´ä¼°è®¡**:

**å¯ä¿¡åŒºé—´** (Credible Interval): $[L, U]$ ä½¿å¾—

$$
P(L \leq \theta \leq U | X) = 1 - \alpha
$$

---

## ğŸ¨ å˜åˆ†æ¨æ–­

### 1. è¯æ®ä¸‹ç•Œ (ELBO)

**é—®é¢˜**: åéªŒ $p(\theta | X)$ éš¾ä»¥è®¡ç®—ã€‚

**è§£å†³**: ç”¨ç®€å•åˆ†å¸ƒ $q(\theta)$ è¿‘ä¼¼ $p(\theta | X)$ã€‚

**KLæ•£åº¦**:

$$
\text{KL}(q \| p) = \int q(\theta) \log \frac{q(\theta)}{p(\theta | X)} d\theta
$$

**ELBO** (Evidence Lower Bound):

$$
\log p(X) = \text{ELBO}(q) + \text{KL}(q \| p)
$$

å…¶ä¸­ï¼š

$$
\text{ELBO}(q) = \mathbb{E}_q[\log p(X, \theta)] - \mathbb{E}_q[\log q(\theta)]
$$

**ä¼˜åŒ–ç›®æ ‡**: $\max_q \text{ELBO}(q) \Leftrightarrow \min_q \text{KL}(q \| p)$

---

**ELBOçš„å®Œæ•´æ¨å¯¼**:

**ç›®æ ‡**: æ¨å¯¼è¯æ®ä¸‹ç•Œï¼ˆELBOï¼‰å¹¶è¯´æ˜ä¸ºä»€ä¹ˆæœ€å¤§åŒ–ELBOç­‰ä»·äºæœ€å°åŒ–KLæ•£åº¦ã€‚

**ç¬¬ä¸€æ­¥ï¼šä»KLæ•£åº¦å¼€å§‹**:

æˆ‘ä»¬æƒ³è¦æœ€å°åŒ–å˜åˆ†åˆ†å¸ƒ $q(\theta)$ ä¸åéªŒ $p(\theta | X)$ ä¹‹é—´çš„KLæ•£åº¦ï¼š

$$
\text{KL}(q \| p) = \int q(\theta) \log \frac{q(\theta)}{p(\theta | X)} d\theta
$$

å±•å¼€ï¼š

$$
\text{KL}(q \| p) = \int q(\theta) \log q(\theta) d\theta - \int q(\theta) \log p(\theta | X) d\theta
$$

$$
= \mathbb{E}_q[\log q(\theta)] - \mathbb{E}_q[\log p(\theta | X)]
$$

**ç¬¬äºŒæ­¥ï¼šä½¿ç”¨Bayeså®šç†**:

ç”±Bayeså®šç†ï¼š

$$
p(\theta | X) = \frac{p(X, \theta)}{p(X)} = \frac{p(X | \theta) p(\theta)}{p(X)}
$$

å› æ­¤ï¼š

$$
\log p(\theta | X) = \log p(X, \theta) - \log p(X)
$$

**ç¬¬ä¸‰æ­¥ï¼šä»£å…¥KLæ•£åº¦**:

$$
\text{KL}(q \| p) = \mathbb{E}_q[\log q(\theta)] - \mathbb{E}_q[\log p(X, \theta)] + \mathbb{E}_q[\log p(X)]
$$

ç”±äº $\log p(X)$ ä¸ä¾èµ–äº $\theta$ï¼Œ$\mathbb{E}_q[\log p(X)] = \log p(X)$ï¼š

$$
\text{KL}(q \| p) = \mathbb{E}_q[\log q(\theta)] - \mathbb{E}_q[\log p(X, \theta)] + \log p(X)
$$

**ç¬¬å››æ­¥ï¼šé‡æ–°æ•´ç†**:

$$
\log p(X) = \mathbb{E}_q[\log p(X, \theta)] - \mathbb{E}_q[\log q(\theta)] + \text{KL}(q \| p)
$$

å®šä¹‰ **ELBO**ï¼ˆè¯æ®ä¸‹ç•Œï¼‰ï¼š

$$
\text{ELBO}(q) = \mathbb{E}_q[\log p(X, \theta)] - \mathbb{E}_q[\log q(\theta)]
$$

å› æ­¤ï¼š

$$
\log p(X) = \text{ELBO}(q) + \text{KL}(q \| p)
$$

$\square$

**ç¬¬äº”æ­¥ï¼šELBOæ˜¯ä¸‹ç•Œ**:

ç”±äº $\text{KL}(q \| p) \geq 0$ï¼ˆKLæ•£åº¦çš„éè´Ÿæ€§ï¼‰ï¼Œæˆ‘ä»¬æœ‰ï¼š

$$
\log p(X) \geq \text{ELBO}(q)
$$

è¿™å°±æ˜¯ä¸ºä»€ä¹ˆç§°ä¸º"è¯æ®ä¸‹ç•Œ"ï¼ˆEvidence Lower Boundï¼‰ã€‚

**ç¬¬å…­æ­¥ï¼šä¼˜åŒ–ç­‰ä»·æ€§**:

ä» $\log p(X) = \text{ELBO}(q) + \text{KL}(q \| p)$ï¼Œæˆ‘ä»¬çœ‹åˆ°ï¼š

$$
\max_q \text{ELBO}(q) \Leftrightarrow \min_q \text{KL}(q \| p)
$$

å› ä¸º $\log p(X)$ ä¸ä¾èµ–äº $q$ï¼ˆå®ƒæ˜¯å›ºå®šçš„"è¯æ®"ï¼‰ã€‚

---

**ELBOçš„ä¸¤ç§ç­‰ä»·å½¢å¼**:

**å½¢å¼1**ï¼ˆæœŸæœ›å½¢å¼ï¼‰:

$$
\text{ELBO}(q) = \mathbb{E}_q[\log p(X, \theta)] - \mathbb{E}_q[\log q(\theta)]
$$

**å½¢å¼2**ï¼ˆé‡å»º + æ­£åˆ™åŒ–ï¼‰:

$$
\text{ELBO}(q) = \mathbb{E}_q[\log p(X | \theta)] - \text{KL}(q(\theta) \| p(\theta))
$$

**æ¨å¯¼å½¢å¼2**:

$$
\text{ELBO}(q) = \mathbb{E}_q[\log p(X, \theta)] - \mathbb{E}_q[\log q(\theta)]
$$

$$
= \mathbb{E}_q[\log p(X | \theta) + \log p(\theta)] - \mathbb{E}_q[\log q(\theta)]
$$

$$
= \mathbb{E}_q[\log p(X | \theta)] + \mathbb{E}_q[\log p(\theta)] - \mathbb{E}_q[\log q(\theta)]
$$

$$
= \mathbb{E}_q[\log p(X | \theta)] - \text{KL}(q(\theta) \| p(\theta))
$$

**è§£é‡Š**:

- ç¬¬ä¸€é¡¹ï¼š**é‡å»ºé¡¹**ï¼ˆreconstruction termï¼‰ï¼Œè¡¡é‡ç”Ÿæˆæ•°æ®çš„èƒ½åŠ›
- ç¬¬äºŒé¡¹ï¼š**æ­£åˆ™åŒ–é¡¹**ï¼ˆregularization termï¼‰ï¼Œä½¿ $q$ æ¥è¿‘å…ˆéªŒ $p$

---

**åœ¨VAEä¸­çš„åº”ç”¨**:

åœ¨å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ä¸­ï¼š

- $\theta$ å˜ä¸ºæ½œåœ¨å˜é‡ $z$
- $q(z | x)$ æ˜¯ç¼–ç å™¨ï¼ˆencoderï¼‰
- $p(x | z)$ æ˜¯è§£ç å™¨ï¼ˆdecoderï¼‰

ELBOå˜ä¸ºï¼š

$$
\text{ELBO} = \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x | z)] - \text{KL}(q_\phi(z|x) \| p(z))
$$

**ä¼˜åŒ–**:

- æœ€å¤§åŒ–é‡å»ºé¡¹ï¼šè§£ç å™¨å­¦ä¹ é‡å»ºè¾“å…¥
- æœ€å°åŒ–KLé¡¹ï¼šç¼–ç å™¨å­¦ä¹ æ¥è¿‘å…ˆéªŒï¼ˆé€šå¸¸æ˜¯æ ‡å‡†æ­£æ€åˆ†å¸ƒï¼‰

---

**ELBOæ¢¯åº¦çš„è®¡ç®—**:

**é—®é¢˜**: å¦‚ä½•è®¡ç®— $\nabla_\phi \mathbb{E}_{q_\phi(z)}[f(z)]$ï¼Ÿ

**æ–¹æ³•1ï¼šREINFORCEï¼ˆScore Function Estimatorï¼‰**:

$$
\nabla_\phi \mathbb{E}_{q_\phi(z)}[f(z)] = \mathbb{E}_{q_\phi(z)}[f(z) \nabla_\phi \log q_\phi(z)]
$$

**æ–¹æ³•2ï¼šé‡å‚æ•°åŒ–æŠ€å·§ï¼ˆReparameterization Trickï¼‰**:

è‹¥ $z = g(\phi, \epsilon)$ï¼Œå…¶ä¸­ $\epsilon \sim p(\epsilon)$ï¼š

$$
\nabla_\phi \mathbb{E}_{q_\phi(z)}[f(z)] = \mathbb{E}_{p(\epsilon)}[\nabla_\phi f(g(\phi, \epsilon))]
$$

**ä¾‹å­**ï¼ˆæ­£æ€åˆ†å¸ƒï¼‰:

è‹¥ $q_\phi(z) = \mathcal{N}(\mu_\phi, \sigma_\phi^2)$ï¼Œé‡å‚æ•°åŒ–ä¸ºï¼š

$$
z = \mu_\phi + \sigma_\phi \cdot \epsilon, \quad \epsilon \sim \mathcal{N}(0, 1)
$$

åˆ™ï¼š

$$
\nabla_\phi \mathbb{E}_{q_\phi(z)}[f(z)] = \mathbb{E}_{\epsilon \sim \mathcal{N}(0,1)}[\nabla_\phi f(\mu_\phi + \sigma_\phi \cdot \epsilon)]
$$

è¿™ä¸ªæ¢¯åº¦å¯ä»¥ç”¨è’™ç‰¹å¡æ´›ä¼°è®¡ï¼Œä¸”æ–¹å·®è¾ƒä½ã€‚

---

**ELBOä¸EMç®—æ³•çš„å…³ç³»**:

EMç®—æ³•å¯ä»¥çœ‹ä½œæ˜¯å˜åˆ†æ¨æ–­çš„ç‰¹æ®Šæƒ…å†µï¼š

- **Eæ­¥**: å›ºå®šå‚æ•°ï¼Œè®¡ç®—åéªŒ $q(\theta) = p(\theta | X, \hat{\theta}^{(t)})$
- **Mæ­¥**: å›ºå®šåéªŒï¼Œæœ€å¤§åŒ–ELBOå…³äºå‚æ•°

åœ¨EMä¸­ï¼ŒEæ­¥å¾—åˆ°ç²¾ç¡®åéªŒï¼Œè€Œå˜åˆ†æ¨æ–­ä¸­ $q$ æ˜¯è¿‘ä¼¼çš„ã€‚

---

**æ•°å€¼ç¤ºä¾‹**:

è€ƒè™‘ç®€å•çš„é«˜æ–¯æ··åˆæ¨¡å‹ï¼š

$$
p(x) = \sum_{k=1}^K \pi_k \mathcal{N}(x | \mu_k, \sigma_k^2)
$$

**çœŸå®åéªŒ**ï¼ˆéš¾ä»¥è®¡ç®—ï¼‰:

$$
p(z | x) = \frac{\pi_z \mathcal{N}(x | \mu_z, \sigma_z^2)}{\sum_{k=1}^K \pi_k \mathcal{N}(x | \mu_k, \sigma_k^2)}
$$

**å˜åˆ†è¿‘ä¼¼**:

$$
q(z) = \text{Categorical}(\phi_1, \ldots, \phi_K)
$$

æœ€å¤§åŒ–ELBOå¾—åˆ° $\phi$ çš„æœ€ä¼˜å€¼ï¼Œè¿‘ä¼¼çœŸå®åéªŒã€‚

---

### 2. å¹³å‡åœºå˜åˆ†

**å‡è®¾**: $q(\theta) = \prod_{i=1}^m q_i(\theta_i)$

**åæ ‡ä¸Šå‡**:

$$
q_j^*(\theta_j) \propto \exp\left(\mathbb{E}_{q_{-j}}[\log p(X, \theta)]\right)
$$

å…¶ä¸­ $q_{-j} = \prod_{i \neq j} q_i$ã€‚

---

### 3. å˜åˆ†è‡ªç¼–ç å™¨ (VAE)

**ç”Ÿæˆæ¨¡å‹**:

$$
p(x) = \int p(x | z) p(z) dz
$$

**å˜åˆ†è¿‘ä¼¼**:

$$
q_\phi(z | x) \approx p(z | x)
$$

**ELBO**:

$$
\log p(x) \geq \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x | z)] - \text{KL}(q_\phi(z|x) \| p(z))
$$

**é‡å‚æ•°åŒ–æŠ€å·§**:

$$
z = \mu_\phi(x) + \sigma_\phi(x) \odot \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)
$$

---

## ğŸ”§ è’™ç‰¹å¡æ´›æ–¹æ³•

### 1. é‡è¦æ€§é‡‡æ ·

**ç›®æ ‡**: è®¡ç®— $\mathbb{E}_p[f(X)]$ï¼Œä½†ä» $p$ é‡‡æ ·å›°éš¾ã€‚

**é‡è¦æ€§é‡‡æ ·**:

ä» $q$ é‡‡æ ·ï¼Œè®¡ç®—ï¼š

$$
\mathbb{E}_p[f(X)] = \mathbb{E}_q\left[f(X) \frac{p(X)}{q(X)}\right]
$$

**ä¼°è®¡**:

$$
\hat{\mu} = \frac{1}{n} \sum_{i=1}^n f(X_i) w(X_i), \quad X_i \sim q, \quad w(X) = \frac{p(X)}{q(X)}
$$

---

### 2. Markové“¾è’™ç‰¹å¡æ´› (MCMC)

**Metropolis-Hastingsç®—æ³•**:

1. åˆå§‹åŒ– $\theta_0$
2. å¯¹ $t = 0, 1, 2, \ldots$:
   - ä»æè®®åˆ†å¸ƒ $q(\theta' | \theta_t)$ é‡‡æ · $\theta'$
   - è®¡ç®—æ¥å—æ¦‚ç‡:
     $$
     \alpha = \min\left(1, \frac{p(\theta') q(\theta_t | \theta')}{p(\theta_t) q(\theta' | \theta_t)}\right)
     $$
   - ä»¥æ¦‚ç‡ $\alpha$ æ¥å— $\theta_{t+1} = \theta'$ï¼Œå¦åˆ™ $\theta_{t+1} = \theta_t$

---

**Gibbsé‡‡æ ·**:

å¯¹äº $\theta = (\theta_1, \ldots, \theta_m)$:

1. åˆå§‹åŒ– $\theta^{(0)}$
2. å¯¹ $t = 0, 1, 2, \ldots$:
   - $\theta_1^{(t+1)} \sim p(\theta_1 | \theta_2^{(t)}, \ldots, \theta_m^{(t)}, X)$
   - $\theta_2^{(t+1)} \sim p(\theta_2 | \theta_1^{(t+1)}, \theta_3^{(t)}, \ldots, \theta_m^{(t)}, X)$
   - $\vdots$
   - $\theta_m^{(t+1)} \sim p(\theta_m | \theta_1^{(t+1)}, \ldots, \theta_{m-1}^{(t+1)}, X)$

---

### 3. Hamiltonianè’™ç‰¹å¡æ´› (HMC)

**æ€æƒ³**: åˆ©ç”¨æ¢¯åº¦ä¿¡æ¯ï¼Œå¼•å…¥åŠ¨é‡å˜é‡ã€‚

**Hamiltonian**:

$$
H(\theta, r) = -\log p(\theta | X) + \frac{1}{2} r^T M^{-1} r
$$

å…¶ä¸­ $r$ æ˜¯åŠ¨é‡ï¼Œ$M$ æ˜¯è´¨é‡çŸ©é˜µã€‚

**Hamiltonæ–¹ç¨‹**:

$$
\frac{d\theta}{dt} = M^{-1} r, \quad \frac{dr}{dt} = \nabla_\theta \log p(\theta | X)
$$

**Leapfrogç§¯åˆ†å™¨**:

$$
r_{t+\epsilon/2} = r_t + \frac{\epsilon}{2} \nabla_\theta \log p(\theta_t | X)
$$

$$
\theta_{t+\epsilon} = \theta_t + \epsilon M^{-1} r_{t+\epsilon/2}
$$

$$
r_{t+\epsilon} = r_{t+\epsilon/2} + \frac{\epsilon}{2} \nabla_\theta \log p(\theta_{t+\epsilon} | X)
$$

---

## ğŸ’» Pythonå®ç°

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
from scipy.optimize import minimize

# 1. æœ€å¤§ä¼¼ç„¶ä¼°è®¡
def mle_normal(data):
    """æ­£æ€åˆ†å¸ƒçš„MLE"""
    mu_mle = np.mean(data)
    sigma2_mle = np.var(data, ddof=0)  # MLE (æœ‰å)
    return mu_mle, sigma2_mle


def mle_general(data, log_likelihood, theta0):
    """ä¸€èˆ¬MLE (æ•°å€¼ä¼˜åŒ–)"""
    def neg_log_lik(theta):
        return -log_likelihood(data, theta)
    
    result = minimize(neg_log_lik, theta0)
    return result.x


# 2. ç½®ä¿¡åŒºé—´
def confidence_interval_normal(data, alpha=0.05):
    """æ­£æ€å‡å€¼çš„ç½®ä¿¡åŒºé—´ (æ–¹å·®æœªçŸ¥)"""
    n = len(data)
    mean = np.mean(data)
    se = stats.sem(data)  # æ ‡å‡†è¯¯
    
    # tåˆ†å¸ƒä¸´ç•Œå€¼
    t_crit = stats.t.ppf(1 - alpha/2, n - 1)
    
    ci = (mean - t_crit * se, mean + t_crit * se)
    return ci


def bootstrap_ci(data, statistic, n_bootstrap=10000, alpha=0.05):
    """Bootstrapç½®ä¿¡åŒºé—´"""
    n = len(data)
    bootstrap_stats = []
    
    for _ in range(n_bootstrap):
        # æœ‰æ”¾å›æŠ½æ ·
        sample = np.random.choice(data, size=n, replace=True)
        bootstrap_stats.append(statistic(sample))
    
    bootstrap_stats = np.array(bootstrap_stats)
    
    # ç™¾åˆ†ä½æ³•
    ci = np.percentile(bootstrap_stats, [100*alpha/2, 100*(1-alpha/2)])
    return ci


# 3. å‡è®¾æ£€éªŒ
def t_test_one_sample(data, mu0, alpha=0.05):
    """å•æ ·æœ¬tæ£€éªŒ"""
    n = len(data)
    mean = np.mean(data)
    se = stats.sem(data)
    
    # æ£€éªŒç»Ÿè®¡é‡
    t_stat = (mean - mu0) / se
    
    # på€¼ (åŒä¾§)
    p_value = 2 * (1 - stats.t.cdf(abs(t_stat), n - 1))
    
    # å†³ç­–
    reject = p_value < alpha
    
    return {
        't_statistic': t_stat,
        'p_value': p_value,
        'reject_H0': reject,
        'confidence_interval': confidence_interval_normal(data, alpha)
    }


def t_test_two_sample(data1, data2, alpha=0.05):
    """ä¸¤æ ·æœ¬tæ£€éªŒ (ç­‰æ–¹å·®)"""
    n1, n2 = len(data1), len(data2)
    mean1, mean2 = np.mean(data1), np.mean(data2)
    var1, var2 = np.var(data1, ddof=1), np.var(data2, ddof=1)
    
    # åˆå¹¶æ–¹å·®
    sp2 = ((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2)
    
    # æ£€éªŒç»Ÿè®¡é‡
    t_stat = (mean1 - mean2) / np.sqrt(sp2 * (1/n1 + 1/n2))
    
    # på€¼
    p_value = 2 * (1 - stats.t.cdf(abs(t_stat), n1 + n2 - 2))
    
    reject = p_value < alpha
    
    return {
        't_statistic': t_stat,
        'p_value': p_value,
        'reject_H0': reject
    }


# 4. è´å¶æ–¯æ¨æ–­
class BayesianInference:
    """è´å¶æ–¯æ¨æ–­"""
    
    @staticmethod
    def beta_binomial(x, n, alpha_prior=1, beta_prior=1):
        """Beta-Binomialå…±è½­"""
        # åéªŒå‚æ•°
        alpha_post = alpha_prior + x
        beta_post = beta_prior + n - x
        
        # åéªŒå‡å€¼
        mean_post = alpha_post / (alpha_post + beta_post)
        
        # å¯ä¿¡åŒºé—´
        ci = stats.beta.ppf([0.025, 0.975], alpha_post, beta_post)
        
        return {
            'posterior_alpha': alpha_post,
            'posterior_beta': beta_post,
            'posterior_mean': mean_post,
            'credible_interval': ci
        }
    
    @staticmethod
    def normal_normal(data, mu0=0, tau2=1, sigma2=1):
        """Normal-Normalå…±è½­"""
        n = len(data)
        xbar = np.mean(data)
        
        # åéªŒå‚æ•°
        tau2_post = 1 / (1/tau2 + n/sigma2)
        mu_post = tau2_post * (mu0/tau2 + n*xbar/sigma2)
        
        # å¯ä¿¡åŒºé—´
        ci = stats.norm.ppf([0.025, 0.975], mu_post, np.sqrt(tau2_post))
        
        return {
            'posterior_mean': mu_post,
            'posterior_variance': tau2_post,
            'credible_interval': ci
        }


# 5. MCMC
class MetropolisHastings:
    """Metropolis-Hastingsç®—æ³•"""
    
    def __init__(self, log_target, proposal_std=1.0):
        self.log_target = log_target
        self.proposal_std = proposal_std
    
    def sample(self, n_samples, theta0, burn_in=1000):
        """é‡‡æ ·"""
        theta = theta0
        samples = []
        n_accept = 0
        
        for i in range(n_samples + burn_in):
            # æè®®
            theta_prop = theta + np.random.normal(0, self.proposal_std, size=theta.shape)
            
            # æ¥å—æ¦‚ç‡
            log_alpha = self.log_target(theta_prop) - self.log_target(theta)
            
            if np.log(np.random.rand()) < log_alpha:
                theta = theta_prop
                n_accept += 1
            
            if i >= burn_in:
                samples.append(theta.copy())
        
        accept_rate = n_accept / (n_samples + burn_in)
        return np.array(samples), accept_rate


class GibbsSampler:
    """Gibbsé‡‡æ ·"""
    
    def __init__(self, conditional_samplers):
        """
        conditional_samplers: åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯æ¡ä»¶é‡‡æ ·å‡½æ•°
        conditional_samplers[i](theta, i) è¿”å› theta_i | theta_{-i} çš„æ ·æœ¬
        """
        self.conditional_samplers = conditional_samplers
    
    def sample(self, n_samples, theta0, burn_in=1000):
        """é‡‡æ ·"""
        theta = theta0.copy()
        samples = []
        
        for i in range(n_samples + burn_in):
            # é€ä¸ªæ›´æ–°
            for j, sampler in enumerate(self.conditional_samplers):
                theta[j] = sampler(theta, j)
            
            if i >= burn_in:
                samples.append(theta.copy())
        
        return np.array(samples)


# 6. å˜åˆ†æ¨æ–­
class VariationalInference:
    """å˜åˆ†æ¨æ–­"""
    
    @staticmethod
    def elbo(data, q_params, log_likelihood, log_prior, log_q):
        """è®¡ç®—ELBO"""
        # ä»qé‡‡æ ·
        z_samples = log_q['sample'](q_params, n_samples=1000)
        
        # E_q[log p(x, z)]
        log_joint = log_likelihood(data, z_samples) + log_prior(z_samples)
        
        # E_q[log q(z)]
        log_q_z = log_q['log_prob'](z_samples, q_params)
        
        elbo = np.mean(log_joint - log_q_z)
        return elbo
    
    @staticmethod
    def mean_field_vi(data, log_likelihood, log_prior, q_family, 
                      n_iter=1000, lr=0.01):
        """å¹³å‡åœºå˜åˆ†æ¨æ–­"""
        # åˆå§‹åŒ–å˜åˆ†å‚æ•°
        q_params = q_family['init']()
        
        for i in range(n_iter):
            # è®¡ç®—ELBOæ¢¯åº¦ (ä½¿ç”¨è‡ªåŠ¨å¾®åˆ†æˆ–é‡å‚æ•°åŒ–)
            grad = q_family['grad_elbo'](data, q_params, log_likelihood, log_prior)
            
            # æ¢¯åº¦ä¸Šå‡
            q_params = q_family['update'](q_params, grad, lr)
            
            if i % 100 == 0:
                elbo = VariationalInference.elbo(data, q_params, 
                                                log_likelihood, log_prior, q_family)
                print(f"Iter {i}, ELBO: {elbo:.4f}")
        
        return q_params


# 7. å¯è§†åŒ–ç¤ºä¾‹
def demo_inference():
    """ç»Ÿè®¡æ¨æ–­ç¤ºä¾‹"""
    np.random.seed(42)
    
    # ç”Ÿæˆæ•°æ®
    true_mu = 5.0
    true_sigma = 2.0
    n = 100
    data = np.random.normal(true_mu, true_sigma, n)
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # 1. MLE
    mu_mle, sigma2_mle = mle_normal(data)
    ax = axes[0, 0]
    ax.hist(data, bins=30, density=True, alpha=0.7, label='Data')
    x = np.linspace(data.min(), data.max(), 100)
    ax.plot(x, stats.norm.pdf(x, mu_mle, np.sqrt(sigma2_mle)), 
            'r-', linewidth=2, label=f'MLE: Î¼={mu_mle:.2f}, ÏƒÂ²={sigma2_mle:.2f}')
    ax.axvline(true_mu, color='g', linestyle='--', label=f'True Î¼={true_mu}')
    ax.set_title('Maximum Likelihood Estimation')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # 2. ç½®ä¿¡åŒºé—´
    ax = axes[0, 1]
    ci = confidence_interval_normal(data)
    ax.errorbar(1, mu_mle, yerr=[[mu_mle-ci[0]], [ci[1]-mu_mle]], 
                fmt='o', markersize=10, capsize=10, label='95% CI')
    ax.axhline(true_mu, color='g', linestyle='--', label=f'True Î¼={true_mu}')
    ax.set_xlim(0.5, 1.5)
    ax.set_ylim(true_mu-1, true_mu+1)
    ax.set_xticks([1])
    ax.set_xticklabels(['Sample Mean'])
    ax.set_title('Confidence Interval')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # 3. Bootstrap
    ax = axes[1, 0]
    bootstrap_means = []
    for _ in range(1000):
        sample = np.random.choice(data, size=n, replace=True)
        bootstrap_means.append(np.mean(sample))
    ax.hist(bootstrap_means, bins=50, density=True, alpha=0.7, label='Bootstrap')
    ax.axvline(mu_mle, color='r', linewidth=2, label=f'Sample Mean={mu_mle:.2f}')
    ax.axvline(true_mu, color='g', linestyle='--', label=f'True Î¼={true_mu}')
    ax.set_title('Bootstrap Distribution')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # 4. è´å¶æ–¯æ¨æ–­
    ax = axes[1, 1]
    result = BayesianInference.normal_normal(data, mu0=0, tau2=100, 
                                             sigma2=true_sigma**2)
    theta = np.linspace(true_mu-2, true_mu+2, 100)
    
    # å…ˆéªŒ
    prior = stats.norm.pdf(theta, 0, 10)
    ax.plot(theta, prior, 'b--', label='Prior', linewidth=2)
    
    # åéªŒ
    posterior = stats.norm.pdf(theta, result['posterior_mean'], 
                               np.sqrt(result['posterior_variance']))
    ax.plot(theta, posterior, 'r-', label='Posterior', linewidth=2)
    
    ax.axvline(true_mu, color='g', linestyle='--', label=f'True Î¼={true_mu}')
    ax.axvline(result['posterior_mean'], color='r', linestyle=':', 
              label=f"Posterior Mean={result['posterior_mean']:.2f}")
    ax.set_title('Bayesian Inference')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    # plt.show()


if __name__ == "__main__":
    print("=" * 60)
    print("ç»Ÿè®¡æ¨æ–­ç¤ºä¾‹")
    print("=" * 60 + "\n")
    
    demo_inference()
    
    print("\næ‰€æœ‰ç¤ºä¾‹å®Œæˆï¼")
```

---

## ğŸ“š ç»ƒä¹ é¢˜

### ç»ƒä¹ 1ï¼šMLE

æ±‚æŒ‡æ•°åˆ†å¸ƒ $\text{Exp}(\lambda)$ çš„MLEï¼Œå¹¶éªŒè¯å…¶æ¸è¿‘æ­£æ€æ€§ã€‚

### ç»ƒä¹ 2ï¼šå‡è®¾æ£€éªŒ

è®¾è®¡å¹¶å®ç°ä¼¼ç„¶æ¯”æ£€éªŒï¼Œæ¯”è¾ƒä¸¤ä¸ªæ­£æ€åˆ†å¸ƒçš„æ–¹å·®æ˜¯å¦ç›¸ç­‰ã€‚

### ç»ƒä¹ 3ï¼šè´å¶æ–¯æ¨æ–­

å®ç°Poisson-Gammaå…±è½­ï¼Œå¹¶å¯è§†åŒ–å…ˆéªŒã€ä¼¼ç„¶å’ŒåéªŒã€‚

### ç»ƒä¹ 4ï¼šå˜åˆ†æ¨æ–­

å®ç°ç®€å•çš„å˜åˆ†è‡ªç¼–ç å™¨ (VAE)ï¼Œå¹¶åœ¨MNISTæ•°æ®é›†ä¸Šè®­ç»ƒã€‚

---

## ğŸ“ ç›¸å…³è¯¾ç¨‹

| å¤§å­¦ | è¯¾ç¨‹ |
|------|------|
| **MIT** | 18.650 - Statistics for Applications |
| **Stanford** | STATS200 - Introduction to Statistical Inference |
| **CMU** | 36-705 - Intermediate Statistics |
| **UC Berkeley** | STAT210A - Theoretical Statistics |

---

## ğŸ“– å‚è€ƒæ–‡çŒ®

1. **Casella & Berger (2002)**. *Statistical Inference*. Duxbury Press.

2. **Gelman et al. (2013)**. *Bayesian Data Analysis*. CRC Press.

3. **Bishop (2006)**. *Pattern Recognition and Machine Learning*. Springer.

4. **Murphy (2012)**. *Machine Learning: A Probabilistic Perspective*. MIT Press.

5. **Blei et al. (2017)**. *Variational Inference: A Review for Statisticians*. JASA.

---

*æœ€åæ›´æ–°ï¼š2025å¹´10æœˆ*-
