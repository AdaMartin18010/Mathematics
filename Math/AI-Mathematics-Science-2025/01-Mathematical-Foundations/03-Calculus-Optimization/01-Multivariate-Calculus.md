# å¤šå…ƒå¾®ç§¯åˆ† (Multivariate Calculus)

> **The Mathematical Foundation of Deep Learning Optimization**
>
> æ·±åº¦å­¦ä¹ ä¼˜åŒ–çš„æ•°å­¦åŸºçŸ³

---

## ç›®å½•

- [å¤šå…ƒå¾®ç§¯åˆ† (Multivariate Calculus)](#å¤šå…ƒå¾®ç§¯åˆ†-multivariate-calculus)
  - [ç›®å½•](#ç›®å½•)
  - [ğŸ“‹ æ ¸å¿ƒæ€æƒ³](#-æ ¸å¿ƒæ€æƒ³)
  - [ğŸ¯ åå¯¼æ•°ä¸æ¢¯åº¦](#-åå¯¼æ•°ä¸æ¢¯åº¦)
    - [1. åå¯¼æ•°](#1-åå¯¼æ•°)
    - [2. æ¢¯åº¦å‘é‡](#2-æ¢¯åº¦å‘é‡)
    - [3. æ–¹å‘å¯¼æ•°](#3-æ–¹å‘å¯¼æ•°)
  - [ğŸ“Š å¤šå…ƒå‡½æ•°çš„æ³°å‹’å±•å¼€](#-å¤šå…ƒå‡½æ•°çš„æ³°å‹’å±•å¼€)
    - [1. ä¸€é˜¶æ³°å‹’å±•å¼€](#1-ä¸€é˜¶æ³°å‹’å±•å¼€)
    - [2. äºŒé˜¶æ³°å‹’å±•å¼€](#2-äºŒé˜¶æ³°å‹’å±•å¼€)
    - [3. HessiançŸ©é˜µ](#3-hessiançŸ©é˜µ)
  - [ğŸ”¬ é“¾å¼æ³•åˆ™](#-é“¾å¼æ³•åˆ™)
    - [1. æ ‡é‡é“¾å¼æ³•åˆ™](#1-æ ‡é‡é“¾å¼æ³•åˆ™)
    - [2. å‘é‡é“¾å¼æ³•åˆ™](#2-å‘é‡é“¾å¼æ³•åˆ™)
    - [3. é›…å¯æ¯”çŸ©é˜µ](#3-é›…å¯æ¯”çŸ©é˜µ)
  - [ğŸ’¡ æ¢¯åº¦ä¸‹é™çš„æ•°å­¦åŸç†](#-æ¢¯åº¦ä¸‹é™çš„æ•°å­¦åŸç†)
    - [1. æœ€é€Ÿä¸‹é™æ–¹å‘](#1-æœ€é€Ÿä¸‹é™æ–¹å‘)
    - [2. æ”¶æ•›æ€§åˆ†æ](#2-æ”¶æ•›æ€§åˆ†æ)
    - [3. æ­¥é•¿é€‰æ‹©](#3-æ­¥é•¿é€‰æ‹©)
  - [ğŸ¨ çº¦æŸä¼˜åŒ–](#-çº¦æŸä¼˜åŒ–)
    - [1. æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°æ³•](#1-æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°æ³•)
    - [2. KKTæ¡ä»¶](#2-kktæ¡ä»¶)
  - [ğŸ”§ åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„åº”ç”¨](#-åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„åº”ç”¨)
    - [1. åå‘ä¼ æ’­](#1-åå‘ä¼ æ’­)
    - [2. æŸå¤±å‡½æ•°çš„æ›²ç‡](#2-æŸå¤±å‡½æ•°çš„æ›²ç‡)
    - [3. ä¼˜åŒ–ç®—æ³•](#3-ä¼˜åŒ–ç®—æ³•)
  - [ğŸ’» Pythonå®ç°](#-pythonå®ç°)
  - [ğŸ“š ç»ƒä¹ é¢˜](#-ç»ƒä¹ é¢˜)
    - [ç»ƒä¹ 1ï¼šæ¢¯åº¦è®¡ç®—](#ç»ƒä¹ 1æ¢¯åº¦è®¡ç®—)
    - [ç»ƒä¹ 2ï¼šHessiançŸ©é˜µ](#ç»ƒä¹ 2hessiançŸ©é˜µ)
    - [ç»ƒä¹ 3ï¼šæ¢¯åº¦ä¸‹é™](#ç»ƒä¹ 3æ¢¯åº¦ä¸‹é™)
    - [ç»ƒä¹ 4ï¼šçº¦æŸä¼˜åŒ–](#ç»ƒä¹ 4çº¦æŸä¼˜åŒ–)
  - [ğŸ“ ç›¸å…³è¯¾ç¨‹](#-ç›¸å…³è¯¾ç¨‹)
  - [ğŸ“– å‚è€ƒæ–‡çŒ®](#-å‚è€ƒæ–‡çŒ®)

---

## ğŸ“‹ æ ¸å¿ƒæ€æƒ³

**å¤šå…ƒå¾®ç§¯åˆ†**æ˜¯ç ”ç©¶å¤šå˜é‡å‡½æ•°çš„å¾®åˆ†ä¸ç§¯åˆ†ï¼Œæ˜¯æ·±åº¦å­¦ä¹ ä¼˜åŒ–çš„æ•°å­¦åŸºç¡€ã€‚

**æ ¸å¿ƒæ¦‚å¿µ**ï¼š

```text
å•å˜é‡å¾®ç§¯åˆ†:
    f: â„ â†’ â„
    å¯¼æ•°: f'(x)
    
å¤šå…ƒå¾®ç§¯åˆ†:
    f: â„â¿ â†’ â„
    åå¯¼æ•°: âˆ‚f/âˆ‚xáµ¢
    æ¢¯åº¦: âˆ‡f = [âˆ‚f/âˆ‚xâ‚, ..., âˆ‚f/âˆ‚xâ‚™]áµ€
    
æ·±åº¦å­¦ä¹ :
    æŸå¤±å‡½æ•°: L: â„áµˆ â†’ â„ (d = å‚æ•°æ•°é‡)
    ä¼˜åŒ–: Î¸* = argmin L(Î¸)
    æ¢¯åº¦ä¸‹é™: Î¸ â† Î¸ - Î·âˆ‡L(Î¸)
```

---

## ğŸ¯ åå¯¼æ•°ä¸æ¢¯åº¦

### 1. åå¯¼æ•°

**å®šä¹‰ 1.1 (åå¯¼æ•°)**:

å‡½æ•° $f: \mathbb{R}^n \to \mathbb{R}$ å…³äºç¬¬ $i$ ä¸ªå˜é‡çš„åå¯¼æ•°ï¼š

$$
\frac{\partial f}{\partial x_i} = \lim_{h \to 0} \frac{f(x_1, \ldots, x_i + h, \ldots, x_n) - f(x_1, \ldots, x_i, \ldots, x_n)}{h}
$$

**ç›´è§‰**ï¼šå›ºå®šå…¶ä»–å˜é‡ï¼Œåªå¯¹ $x_i$ æ±‚å¯¼ã€‚

**ç¤ºä¾‹**ï¼š

$$
f(x, y) = x^2 + 3xy + y^2
$$

$$
\frac{\partial f}{\partial x} = 2x + 3y, \quad \frac{\partial f}{\partial y} = 3x + 2y
$$

---

### 2. æ¢¯åº¦å‘é‡

**å®šä¹‰ 2.1 (æ¢¯åº¦)**:

å‡½æ•° $f: \mathbb{R}^n \to \mathbb{R}$ çš„æ¢¯åº¦æ˜¯æ‰€æœ‰åå¯¼æ•°ç»„æˆçš„å‘é‡ï¼š

$$
\nabla f(x) = \begin{bmatrix}
\frac{\partial f}{\partial x_1} \\
\frac{\partial f}{\partial x_2} \\
\vdots \\
\frac{\partial f}{\partial x_n}
\end{bmatrix}
$$

**å‡ ä½•æ„ä¹‰**ï¼š

- æ¢¯åº¦æŒ‡å‘å‡½æ•°**å¢é•¿æœ€å¿«**çš„æ–¹å‘
- æ¢¯åº¦çš„æ¨¡ $\|\nabla f\|$ æ˜¯å¢é•¿çš„é€Ÿç‡

**ç¤ºä¾‹**ï¼š

$$
f(x, y) = x^2 + y^2
$$

$$
\nabla f = \begin{bmatrix} 2x \\ 2y \end{bmatrix}
$$

åœ¨ç‚¹ $(1, 1)$ï¼š$\nabla f(1, 1) = \begin{bmatrix} 2 \\ 2 \end{bmatrix}$

---

### 3. æ–¹å‘å¯¼æ•°

**å®šä¹‰ 3.1 (æ–¹å‘å¯¼æ•°)**:

å‡½æ•° $f$ åœ¨ç‚¹ $x$ æ²¿æ–¹å‘ $v$ ï¼ˆå•ä½å‘é‡ï¼‰çš„æ–¹å‘å¯¼æ•°ï¼š

$$
D_v f(x) = \lim_{t \to 0} \frac{f(x + tv) - f(x)}{t}
$$

**å®šç† 3.1**ï¼š

$$
D_v f(x) = \nabla f(x) \cdot v
$$

**æ¨è®º**ï¼š

- å½“ $v = \frac{\nabla f}{\|\nabla f\|}$ æ—¶ï¼Œ$D_v f$ æœ€å¤§ï¼ˆæœ€é€Ÿä¸Šå‡ï¼‰
- å½“ $v = -\frac{\nabla f}{\|\nabla f\|}$ æ—¶ï¼Œ$D_v f$ æœ€å°ï¼ˆæœ€é€Ÿä¸‹é™ï¼‰

---

## ğŸ“Š å¤šå…ƒå‡½æ•°çš„æ³°å‹’å±•å¼€

### 1. ä¸€é˜¶æ³°å‹’å±•å¼€

**å®šç† 1.1 (ä¸€é˜¶æ³°å‹’å±•å¼€)**:

$$
f(x + \Delta x) \approx f(x) + \nabla f(x)^T \Delta x
$$

**åº”ç”¨**ï¼šçº¿æ€§è¿‘ä¼¼

**ç¤ºä¾‹**ï¼š

$$
f(x, y) = x^2 + y^2
$$

åœ¨ç‚¹ $(1, 1)$ é™„è¿‘ï¼š

$$
f(1 + \Delta x, 1 + \Delta y) \approx 2 + 2\Delta x + 2\Delta y
$$

---

### 2. äºŒé˜¶æ³°å‹’å±•å¼€

**å®šç† 2.1 (äºŒé˜¶æ³°å‹’å±•å¼€)**:

$$
f(x + \Delta x) \approx f(x) + \nabla f(x)^T \Delta x + \frac{1}{2} \Delta x^T H(x) \Delta x
$$

å…¶ä¸­ $H(x)$ æ˜¯HessiançŸ©é˜µã€‚

**åº”ç”¨**ï¼šäºŒæ¬¡è¿‘ä¼¼ï¼Œç‰›é¡¿æ³•

---

### 3. HessiançŸ©é˜µ

**å®šä¹‰ 3.1 (HessiançŸ©é˜µ)**:

$$
H(x) = \begin{bmatrix}
\frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
\frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & \cdots & \frac{\partial^2 f}{\partial x_2 \partial x_n} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial^2 f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_n \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_n^2}
\end{bmatrix}
$$

**æ€§è´¨**ï¼š

- **å¯¹ç§°æ€§**ï¼š$H_{ij} = H_{ji}$ ï¼ˆSchwarzå®šç†ï¼‰
- **æ›²ç‡ä¿¡æ¯**ï¼šæè¿°å‡½æ•°çš„å±€éƒ¨æ›²ç‡

**ç‰¹å¾å€¼ä¸æ›²ç‡**ï¼š

- æ‰€æœ‰ç‰¹å¾å€¼ > 0ï¼šå±€éƒ¨æå°å€¼ï¼ˆå‡¸ï¼‰
- æ‰€æœ‰ç‰¹å¾å€¼ < 0ï¼šå±€éƒ¨æå¤§å€¼ï¼ˆå‡¹ï¼‰
- ç‰¹å¾å€¼æœ‰æ­£æœ‰è´Ÿï¼šéç‚¹

**ç¤ºä¾‹**ï¼š

$$
f(x, y) = x^2 + y^2
$$

$$
H = \begin{bmatrix} 2 & 0 \\ 0 & 2 \end{bmatrix}
$$

ç‰¹å¾å€¼ï¼š$\lambda_1 = \lambda_2 = 2 > 0$ â†’ å‡¸å‡½æ•°

---

## ğŸ”¬ é“¾å¼æ³•åˆ™

### 1. æ ‡é‡é“¾å¼æ³•åˆ™

**å®šç† 1.1 (æ ‡é‡é“¾å¼æ³•åˆ™)**:

è®¾ $y = f(u)$ï¼Œ$u = g(x)$ï¼Œåˆ™ï¼š

$$
\frac{dy}{dx} = \frac{dy}{du} \cdot \frac{du}{dx}
$$

**å¤šå…ƒæƒ…å†µ**ï¼š

è®¾ $z = f(x, y)$ï¼Œ$x = g(t)$ï¼Œ$y = h(t)$ï¼Œåˆ™ï¼š

$$
\frac{dz}{dt} = \frac{\partial z}{\partial x} \frac{dx}{dt} + \frac{\partial z}{\partial y} \frac{dy}{dt}
$$

---

### 2. å‘é‡é“¾å¼æ³•åˆ™

**å®šç† 2.1 (å‘é‡é“¾å¼æ³•åˆ™)**:

è®¾ $y = f(u)$ï¼Œ$u = g(x)$ï¼Œå…¶ä¸­ $f: \mathbb{R}^m \to \mathbb{R}$ï¼Œ$g: \mathbb{R}^n \to \mathbb{R}^m$ï¼Œåˆ™ï¼š

$$
\nabla_x f = J_g^T \nabla_u f
$$

å…¶ä¸­ $J_g$ æ˜¯ $g$ çš„é›…å¯æ¯”çŸ©é˜µã€‚

**åå‘ä¼ æ’­çš„æ•°å­¦åŸºç¡€**ï¼

---

### 3. é›…å¯æ¯”çŸ©é˜µ

**å®šä¹‰ 3.1 (é›…å¯æ¯”çŸ©é˜µ)**:

è®¾ $f: \mathbb{R}^n \to \mathbb{R}^m$ï¼Œ$f(x) = [f_1(x), \ldots, f_m(x)]^T$ï¼Œåˆ™é›…å¯æ¯”çŸ©é˜µï¼š

$$
J_f(x) = \begin{bmatrix}
\frac{\partial f_1}{\partial x_1} & \cdots & \frac{\partial f_1}{\partial x_n} \\
\vdots & \ddots & \vdots \\
\frac{\partial f_m}{\partial x_1} & \cdots & \frac{\partial f_m}{\partial x_n}
\end{bmatrix}_{m \times n}
$$

**ç¤ºä¾‹**ï¼š

$$
f(x, y) = \begin{bmatrix} x^2 + y \\ xy \end{bmatrix}
$$

$$
J_f = \begin{bmatrix}
2x & 1 \\
y & x
\end{bmatrix}
$$

---

## ğŸ’¡ æ¢¯åº¦ä¸‹é™çš„æ•°å­¦åŸç†

### 1. æœ€é€Ÿä¸‹é™æ–¹å‘

**å®šç† 1.1**ï¼š

è´Ÿæ¢¯åº¦æ–¹å‘ $-\nabla f(x)$ æ˜¯å‡½æ•° $f$ åœ¨ç‚¹ $x$ çš„**æœ€é€Ÿä¸‹é™æ–¹å‘**ã€‚

**è¯æ˜**ï¼š

æ–¹å‘å¯¼æ•°ï¼š$D_v f(x) = \nabla f(x) \cdot v$

æœ€å°åŒ– $D_v f$ ç­‰ä»·äºæœ€å°åŒ– $\nabla f \cdot v = \|\nabla f\| \|v\| \cos \theta$

å½“ $\theta = \pi$ æ—¶æœ€å°ï¼Œå³ $v = -\frac{\nabla f}{\|\nabla f\|}$ã€‚

---

### 2. æ”¶æ•›æ€§åˆ†æ

**å®šç† 2.1 (æ¢¯åº¦ä¸‹é™æ”¶æ•›, å‡¸æƒ…å†µ)**:

å‡è®¾ $f$ æ˜¯ $L$-å…‰æ»‘çš„å‡¸å‡½æ•°ï¼Œä½¿ç”¨å›ºå®šæ­¥é•¿ $\eta \leq 1/L$ï¼š

$$
f(x_t) - f^* \leq \frac{\|x_0 - x^*\|^2}{2\eta t}
$$

**æ”¶æ•›ç‡**ï¼š$O(1/t)$

**å¼ºå‡¸æƒ…å†µ**ï¼š

å‡è®¾ $f$ æ˜¯ $\mu$-å¼ºå‡¸çš„ï¼Œåˆ™ï¼š

$$
\|x_t - x^*\|^2 \leq (1 - \mu \eta)^t \|x_0 - x^*\|^2
$$

**æ”¶æ•›ç‡**ï¼š$O(e^{-\mu \eta t})$ ï¼ˆçº¿æ€§æ”¶æ•›ï¼‰

---

### 3. æ­¥é•¿é€‰æ‹©

**å›ºå®šæ­¥é•¿**ï¼š$\eta = \text{const}$

- ç®€å•
- éœ€è¦è°ƒå‚

**çº¿æœç´¢** (Line Search)ï¼š

åœ¨æ¯æ­¥é€‰æ‹©æœ€ä¼˜æ­¥é•¿ï¼š

$$
\eta_t = \arg\min_{\eta > 0} f(x_t - \eta \nabla f(x_t))
$$

**Armijoæ¡ä»¶** (Backtracking Line Search)ï¼š

é€‰æ‹© $\eta$ ä½¿å¾—ï¼š

$$
f(x - \eta \nabla f) \leq f(x) - c \eta \|\nabla f\|^2
$$

å…¶ä¸­ $c \in (0, 1)$ï¼ˆé€šå¸¸0.5ï¼‰ã€‚

---

## ğŸ¨ çº¦æŸä¼˜åŒ–

### 1. æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°æ³•

**é—®é¢˜**ï¼š

$$
\min_{x} f(x) \quad \text{s.t.} \quad g(x) = 0
$$

**æ‹‰æ ¼æœ—æ—¥å‡½æ•°**ï¼š

$$
\mathcal{L}(x, \lambda) = f(x) + \lambda g(x)
$$

**æœ€ä¼˜æ€§æ¡ä»¶**ï¼š

$$
\nabla_x \mathcal{L} = \nabla f(x) + \lambda \nabla g(x) = 0
$$

$$
\nabla_\lambda \mathcal{L} = g(x) = 0
$$

**å‡ ä½•è§£é‡Š**ï¼š

åœ¨æœ€ä¼˜ç‚¹ï¼Œ$\nabla f$ å’Œ $\nabla g$ å¹³è¡Œã€‚

---

### 2. KKTæ¡ä»¶

**é—®é¢˜**ï¼š

$$
\min_{x} f(x) \quad \text{s.t.} \quad g_i(x) \leq 0, \; h_j(x) = 0
$$

**æ‹‰æ ¼æœ—æ—¥å‡½æ•°**ï¼š

$$
\mathcal{L}(x, \mu, \lambda) = f(x) + \sum_i \mu_i g_i(x) + \sum_j \lambda_j h_j(x)
$$

**KKTæ¡ä»¶**ï¼š

1. **å¹³ç¨³æ€§**ï¼š$\nabla_x \mathcal{L} = 0$
2. **åŸå§‹å¯è¡Œæ€§**ï¼š$g_i(x) \leq 0$ï¼Œ$h_j(x) = 0$
3. **å¯¹å¶å¯è¡Œæ€§**ï¼š$\mu_i \geq 0$
4. **äº’è¡¥æ¾å¼›æ€§**ï¼š$\mu_i g_i(x) = 0$

---

## ğŸ”§ åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„åº”ç”¨

### 1. åå‘ä¼ æ’­

**é“¾å¼æ³•åˆ™çš„åº”ç”¨**ï¼š

è®¾ç¥ç»ç½‘ç»œ $f(x; \theta) = f_L \circ f_{L-1} \circ \cdots \circ f_1(x)$

æŸå¤±å‡½æ•°ï¼š$\mathcal{L}(\theta) = \ell(f(x; \theta), y)$

**æ¢¯åº¦è®¡ç®—**ï¼š

$$
\frac{\partial \mathcal{L}}{\partial \theta_l} = \frac{\partial \mathcal{L}}{\partial f_L} \frac{\partial f_L}{\partial f_{L-1}} \cdots \frac{\partial f_{l+1}}{\partial f_l} \frac{\partial f_l}{\partial \theta_l}
$$

**åå‘ä¼ æ’­ç®—æ³•**ï¼š

ä»è¾“å‡ºå±‚åˆ°è¾“å…¥å±‚ï¼Œé€å±‚è®¡ç®—æ¢¯åº¦ã€‚

---

### 2. æŸå¤±å‡½æ•°çš„æ›²ç‡

**HessiançŸ©é˜µçš„ä½œç”¨**ï¼š

- **æ›²ç‡ä¿¡æ¯**ï¼šæè¿°æŸå¤±å‡½æ•°çš„å±€éƒ¨å½¢çŠ¶
- **ä¼˜åŒ–éš¾åº¦**ï¼šé«˜æ›²ç‡æ–¹å‘éš¾ä¼˜åŒ–
- **äºŒé˜¶æ–¹æ³•**ï¼šç‰›é¡¿æ³•åˆ©ç”¨HessianåŠ é€Ÿæ”¶æ•›

**æ¡ä»¶æ•°**ï¼š

$$
\kappa(H) = \frac{\lambda_{\max}}{\lambda_{\min}}
$$

- æ¡ä»¶æ•°å¤§ï¼šä¼˜åŒ–å›°éš¾
- é¢„å¤„ç†ï¼šæ”¹å–„æ¡ä»¶æ•°

---

### 3. ä¼˜åŒ–ç®—æ³•

**ä¸€é˜¶æ–¹æ³•**ï¼š

- æ¢¯åº¦ä¸‹é™
- SGD
- Momentum
- Adam

**äºŒé˜¶æ–¹æ³•**ï¼š

- ç‰›é¡¿æ³•ï¼š$x_{t+1} = x_t - H^{-1} \nabla f$
- L-BFGSï¼šè¿‘ä¼¼Hessian

---

## ğŸ’» Pythonå®ç°

```python
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# 1. æ¢¯åº¦è®¡ç®—
def compute_gradient(f, x, h=1e-5):
    """æ•°å€¼è®¡ç®—æ¢¯åº¦"""
    n = len(x)
    grad = np.zeros(n)
    
    for i in range(n):
        x_plus = x.copy()
        x_plus[i] += h
        x_minus = x.copy()
        x_minus[i] -= h
        
        grad[i] = (f(x_plus) - f(x_minus)) / (2 * h)
    
    return grad


# 2. HessiançŸ©é˜µè®¡ç®—
def compute_hessian(f, x, h=1e-5):
    """æ•°å€¼è®¡ç®—HessiançŸ©é˜µ"""
    n = len(x)
    H = np.zeros((n, n))
    
    for i in range(n):
        for j in range(n):
            x_pp = x.copy()
            x_pp[i] += h
            x_pp[j] += h
            
            x_pm = x.copy()
            x_pm[i] += h
            x_pm[j] -= h
            
            x_mp = x.copy()
            x_mp[i] -= h
            x_mp[j] += h
            
            x_mm = x.copy()
            x_mm[i] -= h
            x_mm[j] -= h
            
            H[i, j] = (f(x_pp) - f(x_pm) - f(x_mp) + f(x_mm)) / (4 * h * h)
    
    return H


# 3. æ¢¯åº¦ä¸‹é™
def gradient_descent(f, grad_f, x0, lr=0.01, max_iter=1000, tol=1e-6):
    """æ¢¯åº¦ä¸‹é™ç®—æ³•"""
    x = x0.copy()
    trajectory = [x.copy()]
    
    for i in range(max_iter):
        grad = grad_f(x)
        
        if np.linalg.norm(grad) < tol:
            print(f"Converged in {i} iterations")
            break
        
        x = x - lr * grad
        trajectory.append(x.copy())
    
    return x, np.array(trajectory)


# 4. å¸¦Armijoçº¿æœç´¢çš„æ¢¯åº¦ä¸‹é™
def gradient_descent_armijo(f, grad_f, x0, max_iter=1000, tol=1e-6, c=0.5, rho=0.9):
    """å¸¦Armijoçº¿æœç´¢çš„æ¢¯åº¦ä¸‹é™"""
    x = x0.copy()
    trajectory = [x.copy()]
    
    for i in range(max_iter):
        grad = grad_f(x)
        
        if np.linalg.norm(grad) < tol:
            print(f"Converged in {i} iterations")
            break
        
        # Armijoçº¿æœç´¢
        lr = 1.0
        while f(x - lr * grad) > f(x) - c * lr * np.dot(grad, grad):
            lr *= rho
        
        x = x - lr * grad
        trajectory.append(x.copy())
    
    return x, np.array(trajectory)


# 5. ç‰›é¡¿æ³•
def newton_method(f, grad_f, hess_f, x0, max_iter=100, tol=1e-6):
    """ç‰›é¡¿æ³•"""
    x = x0.copy()
    trajectory = [x.copy()]
    
    for i in range(max_iter):
        grad = grad_f(x)
        
        if np.linalg.norm(grad) < tol:
            print(f"Converged in {i} iterations")
            break
        
        H = hess_f(x)
        
        # æ±‚è§£ H * d = -grad
        try:
            d = np.linalg.solve(H, -grad)
        except np.linalg.LinAlgError:
            print("Singular Hessian, using gradient descent step")
            d = -grad
        
        x = x + d
        trajectory.append(x.copy())
    
    return x, np.array(trajectory)


# ç¤ºä¾‹ï¼šRosenbrockå‡½æ•°
def rosenbrock(x):
    """Rosenbrockå‡½æ•°: f(x,y) = (1-x)^2 + 100(y-x^2)^2"""
    return (1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2

def rosenbrock_grad(x):
    """Rosenbrockå‡½æ•°çš„æ¢¯åº¦"""
    grad = np.zeros(2)
    grad[0] = -2 * (1 - x[0]) - 400 * x[0] * (x[1] - x[0]**2)
    grad[1] = 200 * (x[1] - x[0]**2)
    return grad

def rosenbrock_hess(x):
    """Rosenbrockå‡½æ•°çš„Hessian"""
    H = np.zeros((2, 2))
    H[0, 0] = 2 - 400 * x[1] + 1200 * x[0]**2
    H[0, 1] = -400 * x[0]
    H[1, 0] = -400 * x[0]
    H[1, 1] = 200
    return H


# å¯è§†åŒ–
def visualize_optimization():
    """å¯è§†åŒ–ä¼˜åŒ–è¿‡ç¨‹"""
    # åˆå§‹ç‚¹
    x0 = np.array([-1.5, 2.0])
    
    # è¿è¡Œä¸åŒç®—æ³•
    x_gd, traj_gd = gradient_descent(rosenbrock, rosenbrock_grad, x0, lr=0.001, max_iter=5000)
    x_armijo, traj_armijo = gradient_descent_armijo(rosenbrock, rosenbrock_grad, x0, max_iter=1000)
    x_newton, traj_newton = newton_method(rosenbrock, rosenbrock_grad, rosenbrock_hess, x0, max_iter=50)
    
    # ç»˜åˆ¶ç­‰é«˜çº¿
    x_range = np.linspace(-2, 2, 100)
    y_range = np.linspace(-1, 3, 100)
    X, Y = np.meshgrid(x_range, y_range)
    Z = np.zeros_like(X)
    
    for i in range(X.shape[0]):
        for j in range(X.shape[1]):
            Z[i, j] = rosenbrock(np.array([X[i, j], Y[i, j]]))
    
    plt.figure(figsize=(15, 5))
    
    # æ¢¯åº¦ä¸‹é™
    plt.subplot(1, 3, 1)
    plt.contour(X, Y, Z, levels=np.logspace(-1, 3, 20), cmap='gray', alpha=0.3)
    plt.plot(traj_gd[:, 0], traj_gd[:, 1], 'r-o', markersize=3, label='GD')
    plt.plot(1, 1, 'g*', markersize=15, label='Optimum')
    plt.title('Gradient Descent')
    plt.xlabel('x')
    plt.ylabel('y')
    plt.legend()
    
    # Armijoçº¿æœç´¢
    plt.subplot(1, 3, 2)
    plt.contour(X, Y, Z, levels=np.logspace(-1, 3, 20), cmap='gray', alpha=0.3)
    plt.plot(traj_armijo[:, 0], traj_armijo[:, 1], 'b-o', markersize=3, label='GD + Armijo')
    plt.plot(1, 1, 'g*', markersize=15, label='Optimum')
    plt.title('GD with Armijo Line Search')
    plt.xlabel('x')
    plt.ylabel('y')
    plt.legend()
    
    # ç‰›é¡¿æ³•
    plt.subplot(1, 3, 3)
    plt.contour(X, Y, Z, levels=np.logspace(-1, 3, 20), cmap='gray', alpha=0.3)
    plt.plot(traj_newton[:, 0], traj_newton[:, 1], 'g-o', markersize=3, label='Newton')
    plt.plot(1, 1, 'g*', markersize=15, label='Optimum')
    plt.title('Newton Method')
    plt.xlabel('x')
    plt.ylabel('y')
    plt.legend()
    
    plt.tight_layout()
    # plt.show()
    
    print(f"GD iterations: {len(traj_gd)}")
    print(f"GD + Armijo iterations: {len(traj_armijo)}")
    print(f"Newton iterations: {len(traj_newton)}")


if __name__ == "__main__":
    print("=== å¤šå…ƒå¾®ç§¯åˆ†ç¤ºä¾‹ ===")
    
    # æµ‹è¯•æ¢¯åº¦è®¡ç®—
    x = np.array([1.0, 2.0])
    grad_numerical = compute_gradient(rosenbrock, x)
    grad_analytical = rosenbrock_grad(x)
    
    print(f"\næ•°å€¼æ¢¯åº¦: {grad_numerical}")
    print(f"è§£ææ¢¯åº¦: {grad_analytical}")
    print(f"è¯¯å·®: {np.linalg.norm(grad_numerical - grad_analytical)}")
    
    # æµ‹è¯•Hessianè®¡ç®—
    hess_numerical = compute_hessian(rosenbrock, x)
    hess_analytical = rosenbrock_hess(x)
    
    print(f"\næ•°å€¼Hessian:\n{hess_numerical}")
    print(f"è§£æHessian:\n{hess_analytical}")
    print(f"è¯¯å·®: {np.linalg.norm(hess_numerical - hess_analytical)}")
    
    # å¯è§†åŒ–ä¼˜åŒ–
    print("\n=== ä¼˜åŒ–ç®—æ³•å¯¹æ¯” ===")
    visualize_optimization()
```

---

## ğŸ“š ç»ƒä¹ é¢˜

### ç»ƒä¹ 1ï¼šæ¢¯åº¦è®¡ç®—

è®¡ç®—ä»¥ä¸‹å‡½æ•°çš„æ¢¯åº¦ï¼š

1. $f(x, y) = x^2 + 2xy + 3y^2$
2. $f(x, y) = e^{x+y}$
3. $f(x, y, z) = x^2 y + y^2 z + z^2 x$

### ç»ƒä¹ 2ï¼šHessiançŸ©é˜µ

è®¡ç®— $f(x, y) = x^3 + y^3 - 3xy$ çš„HessiançŸ©é˜µï¼Œå¹¶åˆ¤æ–­ç‚¹ $(1, 1)$ çš„æ€§è´¨ã€‚

### ç»ƒä¹ 3ï¼šæ¢¯åº¦ä¸‹é™

ä½¿ç”¨æ¢¯åº¦ä¸‹é™æœ€å°åŒ– $f(x, y) = x^2 + 4y^2$ï¼Œåˆå§‹ç‚¹ $(2, 2)$ï¼Œå­¦ä¹ ç‡ $\eta = 0.1$ã€‚

### ç»ƒä¹ 4ï¼šçº¦æŸä¼˜åŒ–

ä½¿ç”¨æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°æ³•æ±‚è§£ï¼š

$$
\min_{x, y} x^2 + y^2 \quad \text{s.t.} \quad x + y = 1
$$

---

## ğŸ“ ç›¸å…³è¯¾ç¨‹

| å¤§å­¦ | è¯¾ç¨‹ |
|------|------|
| **MIT** | 18.02 Multivariable Calculus |
| **Stanford** | Math 51 Linear Algebra & Multivariable Calculus |
| **UC Berkeley** | Math 53 Multivariable Calculus |
| **CMU** | 21-259 Calculus in Three Dimensions |

---

## ğŸ“– å‚è€ƒæ–‡çŒ®

1. **Stewart, J. (2015)**. *Multivariable Calculus*. Cengage Learning.

2. **Nocedal & Wright (2006)**. *Numerical Optimization*. Springer.

3. **Boyd & Vandenberghe (2004)**. *Convex Optimization*. Cambridge University Press.

4. **Goodfellow et al. (2016)**. *Deep Learning*. MIT Press. (Chapter 4: Numerical Computation)

---

*æœ€åæ›´æ–°ï¼š2025å¹´10æœˆ*-
