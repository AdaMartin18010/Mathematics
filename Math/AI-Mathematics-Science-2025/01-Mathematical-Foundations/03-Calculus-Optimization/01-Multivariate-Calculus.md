# å¤šå…ƒå¾®ç§¯åˆ† (Multivariate Calculus)

> **The Mathematical Foundation of Deep Learning Optimization**
>
> æ·±åº¦å­¦ä¹ ä¼˜åŒ–çš„æ•°å­¦åŸºçŸ³

---

## ç›®å½•

- [å¤šå…ƒå¾®ç§¯åˆ† (Multivariate Calculus)](#å¤šå…ƒå¾®ç§¯åˆ†-multivariate-calculus)
  - [ç›®å½•](#ç›®å½•)
  - [ğŸ“‹ æ ¸å¿ƒæ€æƒ³](#-æ ¸å¿ƒæ€æƒ³)
  - [ğŸ¯ åå¯¼æ•°ä¸æ¢¯åº¦](#-åå¯¼æ•°ä¸æ¢¯åº¦)
    - [1. åå¯¼æ•°](#1-åå¯¼æ•°)
    - [2. æ¢¯åº¦å‘é‡](#2-æ¢¯åº¦å‘é‡)
    - [3. æ–¹å‘å¯¼æ•°](#3-æ–¹å‘å¯¼æ•°)
  - [ğŸ“Š å¤šå…ƒå‡½æ•°çš„æ³°å‹’å±•å¼€](#-å¤šå…ƒå‡½æ•°çš„æ³°å‹’å±•å¼€)
    - [1. ä¸€é˜¶æ³°å‹’å±•å¼€](#1-ä¸€é˜¶æ³°å‹’å±•å¼€)
    - [2. äºŒé˜¶æ³°å‹’å±•å¼€](#2-äºŒé˜¶æ³°å‹’å±•å¼€)
    - [3. HessiançŸ©é˜µ](#3-hessiançŸ©é˜µ)
  - [ğŸ”¬ é“¾å¼æ³•åˆ™](#-é“¾å¼æ³•åˆ™)
    - [1. æ ‡é‡é“¾å¼æ³•åˆ™](#1-æ ‡é‡é“¾å¼æ³•åˆ™)
    - [2. å‘é‡é“¾å¼æ³•åˆ™](#2-å‘é‡é“¾å¼æ³•åˆ™)
    - [3. é›…å¯æ¯”çŸ©é˜µ](#3-é›…å¯æ¯”çŸ©é˜µ)
  - [ğŸ’¡ æ¢¯åº¦ä¸‹é™çš„æ•°å­¦åŸç†](#-æ¢¯åº¦ä¸‹é™çš„æ•°å­¦åŸç†)
    - [1. æœ€é€Ÿä¸‹é™æ–¹å‘](#1-æœ€é€Ÿä¸‹é™æ–¹å‘)
    - [2. æ”¶æ•›æ€§åˆ†æ](#2-æ”¶æ•›æ€§åˆ†æ)
    - [3. æ­¥é•¿é€‰æ‹©](#3-æ­¥é•¿é€‰æ‹©)
  - [ğŸ¨ çº¦æŸä¼˜åŒ–](#-çº¦æŸä¼˜åŒ–)
    - [1. æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°æ³•](#1-æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°æ³•)
    - [2. KKTæ¡ä»¶](#2-kktæ¡ä»¶)
  - [ğŸ”§ åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„åº”ç”¨](#-åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„åº”ç”¨)
    - [1. åå‘ä¼ æ’­](#1-åå‘ä¼ æ’­)
    - [2. æŸå¤±å‡½æ•°çš„æ›²ç‡](#2-æŸå¤±å‡½æ•°çš„æ›²ç‡)
    - [3. ä¼˜åŒ–ç®—æ³•](#3-ä¼˜åŒ–ç®—æ³•)
  - [ğŸ’» Pythonå®ç°](#-pythonå®ç°)
  - [ğŸ“š ç»ƒä¹ é¢˜](#-ç»ƒä¹ é¢˜)
    - [ç»ƒä¹ 1ï¼šæ¢¯åº¦è®¡ç®—](#ç»ƒä¹ 1æ¢¯åº¦è®¡ç®—)
    - [ç»ƒä¹ 2ï¼šHessiançŸ©é˜µ](#ç»ƒä¹ 2hessiançŸ©é˜µ)
    - [ç»ƒä¹ 3ï¼šæ¢¯åº¦ä¸‹é™](#ç»ƒä¹ 3æ¢¯åº¦ä¸‹é™)
    - [ç»ƒä¹ 4ï¼šçº¦æŸä¼˜åŒ–](#ç»ƒä¹ 4çº¦æŸä¼˜åŒ–)
  - [ğŸ“ ç›¸å…³è¯¾ç¨‹](#-ç›¸å…³è¯¾ç¨‹)
  - [ğŸ“– å‚è€ƒæ–‡çŒ®](#-å‚è€ƒæ–‡çŒ®)

---

## ğŸ“‹ æ ¸å¿ƒæ€æƒ³

**å¤šå…ƒå¾®ç§¯åˆ†**æ˜¯ç ”ç©¶å¤šå˜é‡å‡½æ•°çš„å¾®åˆ†ä¸ç§¯åˆ†ï¼Œæ˜¯æ·±åº¦å­¦ä¹ ä¼˜åŒ–çš„æ•°å­¦åŸºç¡€ã€‚

**æ ¸å¿ƒæ¦‚å¿µ**ï¼š

```text
å•å˜é‡å¾®ç§¯åˆ†:
    f: â„ â†’ â„
    å¯¼æ•°: f'(x)

å¤šå…ƒå¾®ç§¯åˆ†:
    f: â„â¿ â†’ â„
    åå¯¼æ•°: âˆ‚f/âˆ‚xáµ¢
    æ¢¯åº¦: âˆ‡f = [âˆ‚f/âˆ‚xâ‚, ..., âˆ‚f/âˆ‚xâ‚™]áµ€

æ·±åº¦å­¦ä¹ :
    æŸå¤±å‡½æ•°: L: â„áµˆ â†’ â„ (d = å‚æ•°æ•°é‡)
    ä¼˜åŒ–: Î¸* = argmin L(Î¸)
    æ¢¯åº¦ä¸‹é™: Î¸ â† Î¸ - Î·âˆ‡L(Î¸)
```

---

## ğŸ¯ åå¯¼æ•°ä¸æ¢¯åº¦

### 1. åå¯¼æ•°

**å®šä¹‰ 1.1 (åå¯¼æ•°)**:

å‡½æ•° $f: \mathbb{R}^n \to \mathbb{R}$ å…³äºç¬¬ $i$ ä¸ªå˜é‡çš„åå¯¼æ•°ï¼š

$$
\frac{\partial f}{\partial x_i} = \lim_{h \to 0} \frac{f(x_1, \ldots, x_i + h, \ldots, x_n) - f(x_1, \ldots, x_i, \ldots, x_n)}{h}
$$

**ç›´è§‰**ï¼šå›ºå®šå…¶ä»–å˜é‡ï¼Œåªå¯¹ $x_i$ æ±‚å¯¼ã€‚

**ç¤ºä¾‹**ï¼š

$$
f(x, y) = x^2 + 3xy + y^2
$$

$$
\frac{\partial f}{\partial x} = 2x + 3y, \quad \frac{\partial f}{\partial y} = 3x + 2y
$$

---

### 2. æ¢¯åº¦å‘é‡

**å®šä¹‰ 2.1 (æ¢¯åº¦)**:

å‡½æ•° $f: \mathbb{R}^n \to \mathbb{R}$ çš„æ¢¯åº¦æ˜¯æ‰€æœ‰åå¯¼æ•°ç»„æˆçš„å‘é‡ï¼š

$$
\nabla f(x) = \begin{bmatrix}
\frac{\partial f}{\partial x_1} \\
\frac{\partial f}{\partial x_2} \\
\vdots \\
\frac{\partial f}{\partial x_n}
\end{bmatrix}
$$

**å‡ ä½•æ„ä¹‰**ï¼š

- æ¢¯åº¦æŒ‡å‘å‡½æ•°**å¢é•¿æœ€å¿«**çš„æ–¹å‘
- æ¢¯åº¦çš„æ¨¡ $\|\nabla f\|$ æ˜¯å¢é•¿çš„é€Ÿç‡

**ç¤ºä¾‹**ï¼š

$$
f(x, y) = x^2 + y^2
$$

$$
\nabla f = \begin{bmatrix} 2x \\ 2y \end{bmatrix}
$$

åœ¨ç‚¹ $(1, 1)$ï¼š$\nabla f(1, 1) = \begin{bmatrix} 2 \\ 2 \end{bmatrix}$

---

### 3. æ–¹å‘å¯¼æ•°

**å®šä¹‰ 3.1 (æ–¹å‘å¯¼æ•°)**:

å‡½æ•° $f$ åœ¨ç‚¹ $x$ æ²¿æ–¹å‘ $v$ ï¼ˆå•ä½å‘é‡ï¼‰çš„æ–¹å‘å¯¼æ•°ï¼š

$$
D_v f(x) = \lim_{t \to 0} \frac{f(x + tv) - f(x)}{t}
$$

**å®šç† 3.1**ï¼š

$$
D_v f(x) = \nabla f(x) \cdot v
$$

**æ¨è®º**ï¼š

- å½“ $v = \frac{\nabla f}{\|\nabla f\|}$ æ—¶ï¼Œ$D_v f$ æœ€å¤§ï¼ˆæœ€é€Ÿä¸Šå‡ï¼‰
- å½“ $v = -\frac{\nabla f}{\|\nabla f\|}$ æ—¶ï¼Œ$D_v f$ æœ€å°ï¼ˆæœ€é€Ÿä¸‹é™ï¼‰

---

## ğŸ“Š å¤šå…ƒå‡½æ•°çš„æ³°å‹’å±•å¼€

### 1. ä¸€é˜¶æ³°å‹’å±•å¼€

**å®šç† 1.1 (ä¸€é˜¶æ³°å‹’å±•å¼€)**:

$$
f(x + \Delta x) \approx f(x) + \nabla f(x)^T \Delta x
$$

**åº”ç”¨**ï¼šçº¿æ€§è¿‘ä¼¼

**ç¤ºä¾‹**ï¼š

$$
f(x, y) = x^2 + y^2
$$

åœ¨ç‚¹ $(1, 1)$ é™„è¿‘ï¼š

$$
f(1 + \Delta x, 1 + \Delta y) \approx 2 + 2\Delta x + 2\Delta y
$$

---

### 2. äºŒé˜¶æ³°å‹’å±•å¼€

**å®šç† 2.1 (äºŒé˜¶æ³°å‹’å±•å¼€)**:

$$
f(x + \Delta x) \approx f(x) + \nabla f(x)^T \Delta x + \frac{1}{2} \Delta x^T H(x) \Delta x
$$

å…¶ä¸­ $H(x)$ æ˜¯HessiançŸ©é˜µã€‚

**åº”ç”¨**ï¼šäºŒæ¬¡è¿‘ä¼¼ï¼Œç‰›é¡¿æ³•

---

### 3. HessiançŸ©é˜µ

**å®šä¹‰ 3.1 (HessiançŸ©é˜µ)**:

$$
H(x) = \begin{bmatrix}
\frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
\frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & \cdots & \frac{\partial^2 f}{\partial x_2 \partial x_n} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial^2 f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_n \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_n^2}
\end{bmatrix}
$$

**æ€§è´¨**ï¼š

- **å¯¹ç§°æ€§**ï¼š$H_{ij} = H_{ji}$ ï¼ˆSchwarzå®šç†ï¼‰
- **æ›²ç‡ä¿¡æ¯**ï¼šæè¿°å‡½æ•°çš„å±€éƒ¨æ›²ç‡

**ç‰¹å¾å€¼ä¸æ›²ç‡**ï¼š

- æ‰€æœ‰ç‰¹å¾å€¼ > 0ï¼šå±€éƒ¨æå°å€¼ï¼ˆå‡¸ï¼‰
- æ‰€æœ‰ç‰¹å¾å€¼ < 0ï¼šå±€éƒ¨æå¤§å€¼ï¼ˆå‡¹ï¼‰
- ç‰¹å¾å€¼æœ‰æ­£æœ‰è´Ÿï¼šéç‚¹

**ç¤ºä¾‹**ï¼š

$$
f(x, y) = x^2 + y^2
$$

$$
H = \begin{bmatrix} 2 & 0 \\ 0 & 2 \end{bmatrix}
$$

ç‰¹å¾å€¼ï¼š$\lambda_1 = \lambda_2 = 2 > 0$ â†’ å‡¸å‡½æ•°

---

## ğŸ”¬ é“¾å¼æ³•åˆ™

### 1. æ ‡é‡é“¾å¼æ³•åˆ™

**å®šç† 1.1 (æ ‡é‡é“¾å¼æ³•åˆ™)**:

è®¾ $y = f(u)$ï¼Œ$u = g(x)$ï¼Œåˆ™ï¼š

$$
\frac{dy}{dx} = \frac{dy}{du} \cdot \frac{du}{dx}
$$

**å¤šå…ƒæƒ…å†µ**ï¼š

è®¾ $z = f(x, y)$ï¼Œ$x = g(t)$ï¼Œ$y = h(t)$ï¼Œåˆ™ï¼š

$$
\frac{dz}{dt} = \frac{\partial z}{\partial x} \frac{dx}{dt} + \frac{\partial z}{\partial y} \frac{dy}{dt}
$$

---

### 2. å‘é‡é“¾å¼æ³•åˆ™

**å®šç† 2.1 (å‘é‡é“¾å¼æ³•åˆ™)**:

è®¾ $y = f(u)$ï¼Œ$u = g(x)$ï¼Œå…¶ä¸­ $f: \mathbb{R}^m \to \mathbb{R}$ï¼Œ$g: \mathbb{R}^n \to \mathbb{R}^m$ï¼Œåˆ™ï¼š

$$
\nabla_x f = J_g^T \nabla_u f
$$

å…¶ä¸­ $J_g$ æ˜¯ $g$ çš„é›…å¯æ¯”çŸ©é˜µã€‚

**åå‘ä¼ æ’­çš„æ•°å­¦åŸºç¡€**ï¼

---

### 3. é›…å¯æ¯”çŸ©é˜µ

**å®šä¹‰ 3.1 (é›…å¯æ¯”çŸ©é˜µ)**:

è®¾ $f: \mathbb{R}^n \to \mathbb{R}^m$ï¼Œ$f(x) = [f_1(x), \ldots, f_m(x)]^T$ï¼Œåˆ™é›…å¯æ¯”çŸ©é˜µï¼š

$$
J_f(x) = \begin{bmatrix}
\frac{\partial f_1}{\partial x_1} & \cdots & \frac{\partial f_1}{\partial x_n} \\
\vdots & \ddots & \vdots \\
\frac{\partial f_m}{\partial x_1} & \cdots & \frac{\partial f_m}{\partial x_n}
\end{bmatrix}_{m \times n}
$$

**ç¤ºä¾‹**ï¼š

$$
f(x, y) = \begin{bmatrix} x^2 + y \\ xy \end{bmatrix}
$$

$$
J_f = \begin{bmatrix}
2x & 1 \\
y & x
\end{bmatrix}
$$

---

## ğŸ’¡ æ¢¯åº¦ä¸‹é™çš„æ•°å­¦åŸç†

### 1. æœ€é€Ÿä¸‹é™æ–¹å‘

**å®šç† 1.1**ï¼š

è´Ÿæ¢¯åº¦æ–¹å‘ $-\nabla f(x)$ æ˜¯å‡½æ•° $f$ åœ¨ç‚¹ $x$ çš„**æœ€é€Ÿä¸‹é™æ–¹å‘**ã€‚

**è¯æ˜**ï¼š

æ–¹å‘å¯¼æ•°ï¼š$D_v f(x) = \nabla f(x) \cdot v$

æœ€å°åŒ– $D_v f$ ç­‰ä»·äºæœ€å°åŒ– $\nabla f \cdot v = \|\nabla f\| \|v\| \cos \theta$

å½“ $\theta = \pi$ æ—¶æœ€å°ï¼Œå³ $v = -\frac{\nabla f}{\|\nabla f\|}$ã€‚

---

### 2. æ”¶æ•›æ€§åˆ†æ

**å®šç† 2.1 (æ¢¯åº¦ä¸‹é™æ”¶æ•›, å‡¸æƒ…å†µ)**:

å‡è®¾ $f$ æ˜¯ $L$-å…‰æ»‘çš„å‡¸å‡½æ•°ï¼Œä½¿ç”¨å›ºå®šæ­¥é•¿ $\eta \leq 1/L$ï¼š

$$
f(x_t) - f^* \leq \frac{\|x_0 - x^*\|^2}{2\eta t}
$$

**æ”¶æ•›ç‡**ï¼š$O(1/t)$

**å¼ºå‡¸æƒ…å†µ**ï¼š

å‡è®¾ $f$ æ˜¯ $\mu$-å¼ºå‡¸çš„ï¼Œåˆ™ï¼š

$$
\|x_t - x^*\|^2 \leq (1 - \mu \eta)^t \|x_0 - x^*\|^2
$$

**æ”¶æ•›ç‡**ï¼š$O(e^{-\mu \eta t})$ ï¼ˆçº¿æ€§æ”¶æ•›ï¼‰

---

### 3. æ­¥é•¿é€‰æ‹©

**å›ºå®šæ­¥é•¿**ï¼š$\eta = \text{const}$

- ç®€å•
- éœ€è¦è°ƒå‚

**çº¿æœç´¢** (Line Search)ï¼š

åœ¨æ¯æ­¥é€‰æ‹©æœ€ä¼˜æ­¥é•¿ï¼š

$$
\eta_t = \arg\min_{\eta > 0} f(x_t - \eta \nabla f(x_t))
$$

**Armijoæ¡ä»¶** (Backtracking Line Search)ï¼š

é€‰æ‹© $\eta$ ä½¿å¾—ï¼š

$$
f(x - \eta \nabla f) \leq f(x) - c \eta \|\nabla f\|^2
$$

å…¶ä¸­ $c \in (0, 1)$ï¼ˆé€šå¸¸0.5ï¼‰ã€‚

---

## ğŸ¨ çº¦æŸä¼˜åŒ–

### 1. æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°æ³•

**é—®é¢˜**ï¼š

$$
\min_{x} f(x) \quad \text{s.t.} \quad g(x) = 0
$$

**æ‹‰æ ¼æœ—æ—¥å‡½æ•°**ï¼š

$$
\mathcal{L}(x, \lambda) = f(x) + \lambda g(x)
$$

**æœ€ä¼˜æ€§æ¡ä»¶**ï¼š

$$
\nabla_x \mathcal{L} = \nabla f(x) + \lambda \nabla g(x) = 0
$$

$$
\nabla_\lambda \mathcal{L} = g(x) = 0
$$

**å‡ ä½•è§£é‡Š**ï¼š

åœ¨æœ€ä¼˜ç‚¹ï¼Œ$\nabla f$ å’Œ $\nabla g$ å¹³è¡Œã€‚

---

### 2. KKTæ¡ä»¶

**é—®é¢˜**ï¼š

$$
\min_{x} f(x) \quad \text{s.t.} \quad g_i(x) \leq 0, \; h_j(x) = 0
$$

**æ‹‰æ ¼æœ—æ—¥å‡½æ•°**ï¼š

$$
\mathcal{L}(x, \mu, \lambda) = f(x) + \sum_i \mu_i g_i(x) + \sum_j \lambda_j h_j(x)
$$

**KKTæ¡ä»¶**ï¼š

1. **å¹³ç¨³æ€§**ï¼š$\nabla_x \mathcal{L} = 0$
2. **åŸå§‹å¯è¡Œæ€§**ï¼š$g_i(x) \leq 0$ï¼Œ$h_j(x) = 0$
3. **å¯¹å¶å¯è¡Œæ€§**ï¼š$\mu_i \geq 0$
4. **äº’è¡¥æ¾å¼›æ€§**ï¼š$\mu_i g_i(x) = 0$

---

## ğŸ”§ åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„åº”ç”¨

### 1. åå‘ä¼ æ’­

**é“¾å¼æ³•åˆ™çš„åº”ç”¨**ï¼š

è®¾ç¥ç»ç½‘ç»œ $f(x; \theta) = f_L \circ f_{L-1} \circ \cdots \circ f_1(x)$

æŸå¤±å‡½æ•°ï¼š$\mathcal{L}(\theta) = \ell(f(x; \theta), y)$

**æ¢¯åº¦è®¡ç®—**ï¼š

$$
\frac{\partial \mathcal{L}}{\partial \theta_l} = \frac{\partial \mathcal{L}}{\partial f_L} \frac{\partial f_L}{\partial f_{L-1}} \cdots \frac{\partial f_{l+1}}{\partial f_l} \frac{\partial f_l}{\partial \theta_l}
$$

**åå‘ä¼ æ’­ç®—æ³•**ï¼š

ä»è¾“å‡ºå±‚åˆ°è¾“å…¥å±‚ï¼Œé€å±‚è®¡ç®—æ¢¯åº¦ã€‚

---

### 2. æŸå¤±å‡½æ•°çš„æ›²ç‡

**HessiançŸ©é˜µçš„ä½œç”¨**ï¼š

- **æ›²ç‡ä¿¡æ¯**ï¼šæè¿°æŸå¤±å‡½æ•°çš„å±€éƒ¨å½¢çŠ¶
- **ä¼˜åŒ–éš¾åº¦**ï¼šé«˜æ›²ç‡æ–¹å‘éš¾ä¼˜åŒ–
- **äºŒé˜¶æ–¹æ³•**ï¼šç‰›é¡¿æ³•åˆ©ç”¨HessianåŠ é€Ÿæ”¶æ•›

**æ¡ä»¶æ•°**ï¼š

$$
\kappa(H) = \frac{\lambda_{\max}}{\lambda_{\min}}
$$

- æ¡ä»¶æ•°å¤§ï¼šä¼˜åŒ–å›°éš¾
- é¢„å¤„ç†ï¼šæ”¹å–„æ¡ä»¶æ•°

---

### 3. ä¼˜åŒ–ç®—æ³•

**ä¸€é˜¶æ–¹æ³•**ï¼š

- æ¢¯åº¦ä¸‹é™
- SGD
- Momentum
- Adam

**äºŒé˜¶æ–¹æ³•**ï¼š

- ç‰›é¡¿æ³•ï¼š$x_{t+1} = x_t - H^{-1} \nabla f$
- L-BFGSï¼šè¿‘ä¼¼Hessian

---

### 4. ç¥ç»ç½‘ç»œè®­ç»ƒä¸­çš„æ¢¯åº¦æµ

**æ¢¯åº¦æ¶ˆå¤±ä¸çˆ†ç‚¸é—®é¢˜**ï¼š

åœ¨æ·±å±‚ç½‘ç»œä¸­ï¼Œæ¢¯åº¦é€šè¿‡é“¾å¼æ³•åˆ™ä¼ æ’­ï¼š

$$
\frac{\partial \mathcal{L}}{\partial W_1} = \frac{\partial \mathcal{L}}{\partial f_L} \prod_{i=2}^{L} \frac{\partial f_i}{\partial f_{i-1}} \frac{\partial f_1}{\partial W_1}
$$

**é—®é¢˜åˆ†æ**ï¼š

- **æ¢¯åº¦æ¶ˆå¤±**ï¼šå½“ $\left|\frac{\partial f_i}{\partial f_{i-1}}\right| < 1$ æ—¶ï¼Œæ¢¯åº¦æŒ‡æ•°è¡°å‡
- **æ¢¯åº¦çˆ†ç‚¸**ï¼šå½“ $\left|\frac{\partial f_i}{\partial f_{i-1}}\right| > 1$ æ—¶ï¼Œæ¢¯åº¦æŒ‡æ•°å¢é•¿

**è§£å†³æ–¹æ¡ˆ**ï¼š

1. **æ¢¯åº¦è£å‰ª**ï¼š$\text{grad} \leftarrow \text{grad} \cdot \min(1, \frac{\tau}{\|\text{grad}\|})$
2. **æ®‹å·®è¿æ¥**ï¼šæä¾›æ¢¯åº¦ç›´é€šè·¯å¾„
3. **å½’ä¸€åŒ–**ï¼šBatchNormã€LayerNormç¨³å®šè®­ç»ƒ

**æ•°å€¼ç¤ºä¾‹**ï¼š

```python
def analyze_gradient_flow(network, input_data):
    """åˆ†æç½‘ç»œä¸­çš„æ¢¯åº¦æµ"""
    gradients = []

    # å‰å‘ä¼ æ’­
    activations = [input_data]
    for layer in network:
        activations.append(layer.forward(activations[-1]))

    # åå‘ä¼ æ’­
    grad = np.ones_like(activations[-1])  # è¾“å‡ºå±‚æ¢¯åº¦
    for i in range(len(network) - 1, -1, -1):
        grad = network[i].backward(grad, activations[i])
        gradients.append(np.linalg.norm(grad))

    return gradients[::-1]  # ä»è¾“å…¥åˆ°è¾“å‡º
```

---

### 5. è¶…å‚æ•°ä¼˜åŒ–

**ç½‘æ ¼æœç´¢ä¸éšæœºæœç´¢**ï¼š

ä¼ ç»Ÿæ–¹æ³•ï¼šåœ¨è¶…å‚æ•°ç©ºé—´ $\Theta$ ä¸­æœç´¢æœ€ä¼˜å€¼

$$
\theta^* = \arg\min_{\theta \in \Theta} \mathcal{L}(\theta)
$$

**è´å¶æ–¯ä¼˜åŒ–**ï¼š

ä½¿ç”¨é«˜æ–¯è¿‡ç¨‹å»ºæ¨¡æŸå¤±å‡½æ•°ï¼š

$$
\mathcal{L}(\theta) \sim \mathcal{GP}(\mu(\theta), k(\theta, \theta'))
$$

**æ¢¯åº¦å¼•å¯¼ä¼˜åŒ–**ï¼š

å¯¹äºå¯å¾®è¶…å‚æ•°ï¼ˆå¦‚å­¦ä¹ ç‡ã€æ­£åˆ™åŒ–ç³»æ•°ï¼‰ï¼Œä½¿ç”¨æ¢¯åº¦ä¿¡æ¯ï¼š

$$
\frac{\partial \mathcal{L}}{\partial \eta} = \sum_{t} \frac{\partial \mathcal{L}}{\partial \theta_t} \frac{\partial \theta_t}{\partial \eta}
$$

å…¶ä¸­ $\theta_t = \theta_{t-1} - \eta \nabla_{\theta} \mathcal{L}(\theta_{t-1})$ã€‚

**å®è·µç¤ºä¾‹**ï¼š

```python
def hyperparameter_optimization(model, train_data, val_data,
                                 lr_range=(1e-5, 1e-1),
                                 reg_range=(1e-6, 1e-2)):
    """è¶…å‚æ•°ä¼˜åŒ–"""
    best_loss = float('inf')
    best_params = None

    # éšæœºæœç´¢
    for _ in range(100):
        lr = np.random.uniform(*lr_range)
        reg = np.random.uniform(*reg_range)

        # è®­ç»ƒæ¨¡å‹
        model.set_hyperparameters(lr=lr, reg=reg)
        loss = train_and_evaluate(model, train_data, val_data)

        if loss < best_loss:
            best_loss = loss
            best_params = {'lr': lr, 'reg': reg}

    return best_params, best_loss
```

---

### 6. å¯¹æŠ—è®­ç»ƒä¸é²æ£’æ€§

**å¯¹æŠ—æ ·æœ¬ç”Ÿæˆ**ï¼š

ä½¿ç”¨æ¢¯åº¦ä¿¡æ¯ç”Ÿæˆå¯¹æŠ—æ ·æœ¬ï¼š

$$
x_{\text{adv}} = x + \epsilon \cdot \text{sign}(\nabla_x \mathcal{L}(f(x), y))
$$

å…¶ä¸­ $\epsilon$ æ˜¯æ‰°åŠ¨å¤§å°ã€‚

**PGDæ”»å‡»**ï¼š

è¿­ä»£å¼å¯¹æŠ—æ”»å‡»ï¼š

$$
x^{(t+1)} = \text{Proj}_\mathcal{B}(x^{(t)} + \alpha \cdot \text{sign}(\nabla_x \mathcal{L}(f(x^{(t)}), y)))
$$

å…¶ä¸­ $\mathcal{B} = \{x' : \|x' - x\|_\infty \leq \epsilon\}$ æ˜¯æ‰°åŠ¨çƒã€‚

**å¯¹æŠ—è®­ç»ƒ**ï¼š

åœ¨è®­ç»ƒæ—¶åŒæ—¶ä¼˜åŒ–æ­£å¸¸æ ·æœ¬å’Œå¯¹æŠ—æ ·æœ¬ï¼š

$$
\min_\theta \mathbb{E}_{(x,y)} \left[\mathcal{L}(f(x), y) + \lambda \mathcal{L}(f(x_{\text{adv}}), y)\right]
$$

**Pythonå®ç°**ï¼š

```python
def generate_adversarial_example(model, x, y, epsilon=0.1):
    """ç”Ÿæˆå¯¹æŠ—æ ·æœ¬"""
    x.requires_grad = True

    # å‰å‘ä¼ æ’­
    output = model(x)
    loss = criterion(output, y)

    # åå‘ä¼ æ’­
    loss.backward()

    # ç”Ÿæˆå¯¹æŠ—æ ·æœ¬
    x_adv = x + epsilon * x.grad.sign()
    x_adv = torch.clamp(x_adv, 0, 1)  # ç¡®ä¿åœ¨æœ‰æ•ˆèŒƒå›´å†…

    return x_adv

def adversarial_training(model, train_loader, epochs=10, epsilon=0.1):
    """å¯¹æŠ—è®­ç»ƒ"""
    for epoch in range(epochs):
        for x, y in train_loader:
            # æ­£å¸¸è®­ç»ƒ
            loss_normal = train_step(model, x, y)

            # ç”Ÿæˆå¯¹æŠ—æ ·æœ¬
            x_adv = generate_adversarial_example(model, x, y, epsilon)

            # å¯¹æŠ—è®­ç»ƒ
            loss_adv = train_step(model, x_adv, y)

            # æ€»æŸå¤±
            loss = loss_normal + 0.5 * loss_adv
            loss.backward()
            optimizer.step()
```

---

### 7. å…ƒå­¦ä¹ ä¸­çš„æ¢¯åº¦

**MAML (Model-Agnostic Meta-Learning)**ï¼š

åœ¨å…ƒå­¦ä¹ ä¸­ï¼Œéœ€è¦è®¡ç®—å…³äºåˆå§‹å‚æ•°çš„æ¢¯åº¦ï¼š

$$
\theta^* = \arg\min_\theta \sum_{\mathcal{T}_i} \mathcal{L}_{\mathcal{T}_i}(U^k(\theta))
$$

å…¶ä¸­ $U^k(\theta)$ æ˜¯åœ¨ä»»åŠ¡ $\mathcal{T}_i$ ä¸Šç»è¿‡ $k$ æ­¥æ›´æ–°åçš„å‚æ•°ã€‚

**æ¢¯åº¦è®¡ç®—**ï¼š

$$
\nabla_\theta \mathcal{L}_{\mathcal{T}_i}(U^k(\theta)) = \frac{\partial U^k(\theta)}{\partial \theta} \nabla_{U^k(\theta)} \mathcal{L}_{\mathcal{T}_i}(U^k(\theta))
$$

è¿™éœ€è¦è®¡ç®—é«˜é˜¶æ¢¯åº¦ï¼ˆå…³äº $\theta$ çš„æ¢¯åº¦ï¼Œè€Œ $U^k(\theta)$ æœ¬èº«æ˜¯ $\theta$ çš„å‡½æ•°ï¼‰ã€‚

**å®ç°ç¤ºä¾‹**ï¼š

```python
def maml_step(model, tasks, inner_lr=0.01, inner_steps=5):
    """MAMLä¸€æ­¥æ›´æ–°"""
    meta_grad = 0

    for task in tasks:
        # å†…å±‚æ›´æ–°ï¼ˆåœ¨ä»»åŠ¡ä¸Šå¿«é€Ÿé€‚åº”ï¼‰
        theta_prime = model.parameters()
        for _ in range(inner_steps):
            loss = compute_loss(model, task.train_data)
            theta_prime = [p - inner_lr * g for p, g in
                          zip(theta_prime, torch.autograd.grad(loss, model.parameters()))]

        # å¤–å±‚æ›´æ–°ï¼ˆå…ƒæ¢¯åº¦ï¼‰
        val_loss = compute_loss(model, task.val_data)
        meta_grad += torch.autograd.grad(val_loss, model.parameters(),
                                         create_graph=True)

    # æ›´æ–°åˆå§‹å‚æ•°
    for param, grad in zip(model.parameters(), meta_grad):
        param.data -= outer_lr * grad
```

---

### 8. ç¥ç»æ¶æ„æœç´¢ (NAS)

**å¯å¾®æ¶æ„æœç´¢ (DARTS)**ï¼š

å°†ç¦»æ•£çš„æ¶æ„é€‰æ‹©æ¾å¼›ä¸ºè¿ç»­ä¼˜åŒ–é—®é¢˜ï¼š

$$
\alpha^* = \arg\min_\alpha \mathcal{L}_{\text{val}}(w^*(\alpha), \alpha)
$$

å…¶ä¸­ $w^*(\alpha) = \arg\min_w \mathcal{L}_{\text{train}}(w, \alpha)$ã€‚

**åŒå±‚ä¼˜åŒ–**ï¼š

$$
\begin{align}
\min_\alpha &\quad \mathcal{L}_{\text{val}}(w^*(\alpha), \alpha) \\
\text{s.t.} &\quad w^*(\alpha) = \arg\min_w \mathcal{L}_{\text{train}}(w, \alpha)
\end{align}
$$

**æ¢¯åº¦è®¡ç®—**ï¼š

ä½¿ç”¨éšå‡½æ•°å®šç†ï¼š

$$
\frac{d \mathcal{L}_{\text{val}}}{d \alpha} = \frac{\partial \mathcal{L}_{\text{val}}}{\partial \alpha} -
\frac{\partial \mathcal{L}_{\text{val}}}{\partial w} \left(\frac{\partial^2 \mathcal{L}_{\text{train}}}{\partial w^2}\right)^{-1}
\frac{\partial^2 \mathcal{L}_{\text{train}}}{\partial w \partial \alpha}
$$

è¿™éœ€è¦è®¡ç®—HessiançŸ©é˜µçš„é€†ï¼Œè®¡ç®—æˆæœ¬é«˜ï¼Œé€šå¸¸ä½¿ç”¨è¿‘ä¼¼æ–¹æ³•ã€‚

---

## ğŸ’» Pythonå®ç°

```python
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# 1. æ¢¯åº¦è®¡ç®—
def compute_gradient(f, x, h=1e-5):
    """æ•°å€¼è®¡ç®—æ¢¯åº¦"""
    n = len(x)
    grad = np.zeros(n)

    for i in range(n):
        x_plus = x.copy()
        x_plus[i] += h
        x_minus = x.copy()
        x_minus[i] -= h

        grad[i] = (f(x_plus) - f(x_minus)) / (2 * h)

    return grad


# 2. HessiançŸ©é˜µè®¡ç®—
def compute_hessian(f, x, h=1e-5):
    """æ•°å€¼è®¡ç®—HessiançŸ©é˜µ"""
    n = len(x)
    H = np.zeros((n, n))

    for i in range(n):
        for j in range(n):
            x_pp = x.copy()
            x_pp[i] += h
            x_pp[j] += h

            x_pm = x.copy()
            x_pm[i] += h
            x_pm[j] -= h

            x_mp = x.copy()
            x_mp[i] -= h
            x_mp[j] += h

            x_mm = x.copy()
            x_mm[i] -= h
            x_mm[j] -= h

            H[i, j] = (f(x_pp) - f(x_pm) - f(x_mp) + f(x_mm)) / (4 * h * h)

    return H


# 3. æ¢¯åº¦ä¸‹é™
def gradient_descent(f, grad_f, x0, lr=0.01, max_iter=1000, tol=1e-6):
    """æ¢¯åº¦ä¸‹é™ç®—æ³•"""
    x = x0.copy()
    trajectory = [x.copy()]

    for i in range(max_iter):
        grad = grad_f(x)

        if np.linalg.norm(grad) < tol:
            print(f"Converged in {i} iterations")
            break

        x = x - lr * grad
        trajectory.append(x.copy())

    return x, np.array(trajectory)


# 4. å¸¦Armijoçº¿æœç´¢çš„æ¢¯åº¦ä¸‹é™
def gradient_descent_armijo(f, grad_f, x0, max_iter=1000, tol=1e-6, c=0.5, rho=0.9):
    """å¸¦Armijoçº¿æœç´¢çš„æ¢¯åº¦ä¸‹é™"""
    x = x0.copy()
    trajectory = [x.copy()]

    for i in range(max_iter):
        grad = grad_f(x)

        if np.linalg.norm(grad) < tol:
            print(f"Converged in {i} iterations")
            break

        # Armijoçº¿æœç´¢
        lr = 1.0
        while f(x - lr * grad) > f(x) - c * lr * np.dot(grad, grad):
            lr *= rho

        x = x - lr * grad
        trajectory.append(x.copy())

    return x, np.array(trajectory)


# 5. ç‰›é¡¿æ³•
def newton_method(f, grad_f, hess_f, x0, max_iter=100, tol=1e-6):
    """ç‰›é¡¿æ³•"""
    x = x0.copy()
    trajectory = [x.copy()]

    for i in range(max_iter):
        grad = grad_f(x)

        if np.linalg.norm(grad) < tol:
            print(f"Converged in {i} iterations")
            break

        H = hess_f(x)

        # æ±‚è§£ H * d = -grad
        try:
            d = np.linalg.solve(H, -grad)
        except np.linalg.LinAlgError:
            print("Singular Hessian, using gradient descent step")
            d = -grad

        x = x + d
        trajectory.append(x.copy())

    return x, np.array(trajectory)


# ç¤ºä¾‹ï¼šRosenbrockå‡½æ•°
def rosenbrock(x):
    """Rosenbrockå‡½æ•°: f(x,y) = (1-x)^2 + 100(y-x^2)^2"""
    return (1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2

def rosenbrock_grad(x):
    """Rosenbrockå‡½æ•°çš„æ¢¯åº¦"""
    grad = np.zeros(2)
    grad[0] = -2 * (1 - x[0]) - 400 * x[0] * (x[1] - x[0]**2)
    grad[1] = 200 * (x[1] - x[0]**2)
    return grad

def rosenbrock_hess(x):
    """Rosenbrockå‡½æ•°çš„Hessian"""
    H = np.zeros((2, 2))
    H[0, 0] = 2 - 400 * x[1] + 1200 * x[0]**2
    H[0, 1] = -400 * x[0]
    H[1, 0] = -400 * x[0]
    H[1, 1] = 200
    return H


# å¯è§†åŒ–
def visualize_optimization():
    """å¯è§†åŒ–ä¼˜åŒ–è¿‡ç¨‹"""
    # åˆå§‹ç‚¹
    x0 = np.array([-1.5, 2.0])

    # è¿è¡Œä¸åŒç®—æ³•
    x_gd, traj_gd = gradient_descent(rosenbrock, rosenbrock_grad, x0, lr=0.001, max_iter=5000)
    x_armijo, traj_armijo = gradient_descent_armijo(rosenbrock, rosenbrock_grad, x0, max_iter=1000)
    x_newton, traj_newton = newton_method(rosenbrock, rosenbrock_grad, rosenbrock_hess, x0, max_iter=50)

    # ç»˜åˆ¶ç­‰é«˜çº¿
    x_range = np.linspace(-2, 2, 100)
    y_range = np.linspace(-1, 3, 100)
    X, Y = np.meshgrid(x_range, y_range)
    Z = np.zeros_like(X)

    for i in range(X.shape[0]):
        for j in range(X.shape[1]):
            Z[i, j] = rosenbrock(np.array([X[i, j], Y[i, j]]))

    plt.figure(figsize=(15, 5))

    # æ¢¯åº¦ä¸‹é™
    plt.subplot(1, 3, 1)
    plt.contour(X, Y, Z, levels=np.logspace(-1, 3, 20), cmap='gray', alpha=0.3)
    plt.plot(traj_gd[:, 0], traj_gd[:, 1], 'r-o', markersize=3, label='GD')
    plt.plot(1, 1, 'g*', markersize=15, label='Optimum')
    plt.title('Gradient Descent')
    plt.xlabel('x')
    plt.ylabel('y')
    plt.legend()

    # Armijoçº¿æœç´¢
    plt.subplot(1, 3, 2)
    plt.contour(X, Y, Z, levels=np.logspace(-1, 3, 20), cmap='gray', alpha=0.3)
    plt.plot(traj_armijo[:, 0], traj_armijo[:, 1], 'b-o', markersize=3, label='GD + Armijo')
    plt.plot(1, 1, 'g*', markersize=15, label='Optimum')
    plt.title('GD with Armijo Line Search')
    plt.xlabel('x')
    plt.ylabel('y')
    plt.legend()

    # ç‰›é¡¿æ³•
    plt.subplot(1, 3, 3)
    plt.contour(X, Y, Z, levels=np.logspace(-1, 3, 20), cmap='gray', alpha=0.3)
    plt.plot(traj_newton[:, 0], traj_newton[:, 1], 'g-o', markersize=3, label='Newton')
    plt.plot(1, 1, 'g*', markersize=15, label='Optimum')
    plt.title('Newton Method')
    plt.xlabel('x')
    plt.ylabel('y')
    plt.legend()

    plt.tight_layout()
    # plt.show()

    print(f"GD iterations: {len(traj_gd)}")
    print(f"GD + Armijo iterations: {len(traj_armijo)}")
    print(f"Newton iterations: {len(traj_newton)}")


if __name__ == "__main__":
    print("=== å¤šå…ƒå¾®ç§¯åˆ†ç¤ºä¾‹ ===")

    # æµ‹è¯•æ¢¯åº¦è®¡ç®—
    x = np.array([1.0, 2.0])
    grad_numerical = compute_gradient(rosenbrock, x)
    grad_analytical = rosenbrock_grad(x)

    print(f"\næ•°å€¼æ¢¯åº¦: {grad_numerical}")
    print(f"è§£ææ¢¯åº¦: {grad_analytical}")
    print(f"è¯¯å·®: {np.linalg.norm(grad_numerical - grad_analytical)}")

    # æµ‹è¯•Hessianè®¡ç®—
    hess_numerical = compute_hessian(rosenbrock, x)
    hess_analytical = rosenbrock_hess(x)

    print(f"\næ•°å€¼Hessian:\n{hess_numerical}")
    print(f"è§£æHessian:\n{hess_analytical}")
    print(f"è¯¯å·®: {np.linalg.norm(hess_numerical - hess_analytical)}")

    # å¯è§†åŒ–ä¼˜åŒ–
    print("\n=== ä¼˜åŒ–ç®—æ³•å¯¹æ¯” ===")
    visualize_optimization()
```

---

## ğŸ“š ç»ƒä¹ é¢˜

### ç»ƒä¹ 1ï¼šæ¢¯åº¦è®¡ç®—

è®¡ç®—ä»¥ä¸‹å‡½æ•°çš„æ¢¯åº¦ï¼š

1. $f(x, y) = x^2 + 2xy + 3y^2$
2. $f(x, y) = e^{x+y}$
3. $f(x, y, z) = x^2 y + y^2 z + z^2 x$

### ç»ƒä¹ 2ï¼šHessiançŸ©é˜µ

è®¡ç®— $f(x, y) = x^3 + y^3 - 3xy$ çš„HessiançŸ©é˜µï¼Œå¹¶åˆ¤æ–­ç‚¹ $(1, 1)$ çš„æ€§è´¨ã€‚

### ç»ƒä¹ 3ï¼šæ¢¯åº¦ä¸‹é™

ä½¿ç”¨æ¢¯åº¦ä¸‹é™æœ€å°åŒ– $f(x, y) = x^2 + 4y^2$ï¼Œåˆå§‹ç‚¹ $(2, 2)$ï¼Œå­¦ä¹ ç‡ $\eta = 0.1$ã€‚

### ç»ƒä¹ 4ï¼šçº¦æŸä¼˜åŒ–

ä½¿ç”¨æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°æ³•æ±‚è§£ï¼š

$$
\min_{x, y} x^2 + y^2 \quad \text{s.t.} \quad x + y = 1
$$

---

## ğŸ“ ç›¸å…³è¯¾ç¨‹

| å¤§å­¦ | è¯¾ç¨‹ |
|------|------|
| **MIT** | 18.02 Multivariable Calculus |
| **Stanford** | Math 51 Linear Algebra & Multivariable Calculus |
| **UC Berkeley** | Math 53 Multivariable Calculus |
| **CMU** | 21-259 Calculus in Three Dimensions |

---

## ğŸ“– å‚è€ƒæ–‡çŒ®

1. **Stewart, J. (2015)**. *Multivariable Calculus*. Cengage Learning.

2. **Nocedal & Wright (2006)**. *Numerical Optimization*. Springer.

3. **Boyd & Vandenberghe (2004)**. *Convex Optimization*. Cambridge University Press.

4. **Goodfellow et al. (2016)**. *Deep Learning*. MIT Press. (Chapter 4: Numerical Computation)

---

*æœ€åæ›´æ–°ï¼š2025å¹´10æœˆ*-
