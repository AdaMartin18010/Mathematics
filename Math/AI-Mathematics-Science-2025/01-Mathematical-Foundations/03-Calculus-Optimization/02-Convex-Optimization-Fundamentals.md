# å‡¸ä¼˜åŒ–åŸºç¡€ (Convex Optimization Fundamentals)

> **The Mathematical Foundation of Machine Learning Optimization**
>
> æœºå™¨å­¦ä¹ ä¼˜åŒ–çš„æ•°å­¦åŸºç¡€

---

## ç›®å½•

- [å‡¸ä¼˜åŒ–åŸºç¡€ (Convex Optimization Fundamentals)](#å‡¸ä¼˜åŒ–åŸºç¡€-convex-optimization-fundamentals)
  - [ç›®å½•](#ç›®å½•)
  - [ğŸ“‹ æ ¸å¿ƒæ€æƒ³](#-æ ¸å¿ƒæ€æƒ³)
  - [ğŸ¯ å‡¸é›†ä¸å‡¸å‡½æ•°](#-å‡¸é›†ä¸å‡¸å‡½æ•°)
    - [1. å‡¸é›†å®šä¹‰](#1-å‡¸é›†å®šä¹‰)
    - [2. å‡¸å‡½æ•°å®šä¹‰](#2-å‡¸å‡½æ•°å®šä¹‰)
    - [3. å¼ºå‡¸æ€§](#3-å¼ºå‡¸æ€§)
  - [ğŸ“Š å‡¸ä¼˜åŒ–é—®é¢˜](#-å‡¸ä¼˜åŒ–é—®é¢˜)
    - [1. æ ‡å‡†å½¢å¼](#1-æ ‡å‡†å½¢å¼)
    - [2. å¯¹å¶é—®é¢˜](#2-å¯¹å¶é—®é¢˜)
    - [3. KKTæ¡ä»¶](#3-kktæ¡ä»¶)
  - [ğŸ”¬ ä¼˜åŒ–ç®—æ³•](#-ä¼˜åŒ–ç®—æ³•)
    - [1. æ¢¯åº¦ä¸‹é™æ³•](#1-æ¢¯åº¦ä¸‹é™æ³•)
    - [2. ç‰›é¡¿æ³•](#2-ç‰›é¡¿æ³•)
    - [3. å†…ç‚¹æ³•](#3-å†…ç‚¹æ³•)
  - [ğŸ¤– åœ¨æœºå™¨å­¦ä¹ ä¸­çš„åº”ç”¨](#-åœ¨æœºå™¨å­¦ä¹ ä¸­çš„åº”ç”¨)
    - [1. çº¿æ€§å›å½’](#1-çº¿æ€§å›å½’)
    - [2. æ”¯æŒå‘é‡æœº](#2-æ”¯æŒå‘é‡æœº)
    - [3. Lassoå›å½’](#3-lassoå›å½’)
  - [ğŸ’» Pythonå®ç°](#-pythonå®ç°)
  - [ğŸ“š æ ¸å¿ƒå®šç†æ€»ç»“](#-æ ¸å¿ƒå®šç†æ€»ç»“)
  - [ğŸ“ ç›¸å…³è¯¾ç¨‹](#-ç›¸å…³è¯¾ç¨‹)
  - [ğŸ“– å‚è€ƒæ–‡çŒ®](#-å‚è€ƒæ–‡çŒ®)

---

## ğŸ“‹ æ ¸å¿ƒæ€æƒ³

**å‡¸ä¼˜åŒ–**ç ”ç©¶åœ¨å‡¸é›†ä¸Šæœ€å°åŒ–å‡¸å‡½æ•°çš„é—®é¢˜ï¼Œæ˜¯æœºå™¨å­¦ä¹ ä¼˜åŒ–çš„ç†è®ºåŸºç¡€ã€‚

**æ ¸å¿ƒä¼˜åŠ¿**:

- å±€éƒ¨æœ€ä¼˜ = å…¨å±€æœ€ä¼˜
- é«˜æ•ˆç®—æ³•ï¼ˆå¤šé¡¹å¼æ—¶é—´ï¼‰
- ç†è®ºä¿è¯å®Œå–„

---

## ğŸ¯ å‡¸é›†ä¸å‡¸å‡½æ•°

### 1. å‡¸é›†å®šä¹‰

**å®šä¹‰ 1.1 (å‡¸é›†)**:

é›†åˆ $C \subseteq \mathbb{R}^n$ æ˜¯å‡¸é›†ï¼Œå¦‚æœå¯¹äºä»»æ„ $x, y \in C$ å’Œ $\theta \in [0, 1]$ï¼š

$$\theta x + (1-\theta) y \in C$$

**å‡ ä½•è§£é‡Š**: è¿æ¥é›†åˆä¸­ä»»æ„ä¸¤ç‚¹çš„çº¿æ®µéƒ½åœ¨é›†åˆå†…ã€‚

**å¸¸è§å‡¸é›†**:

- è¶…å¹³é¢: $\{x \mid a^T x = b\}$
- åŠç©ºé—´: $\{x \mid a^T x \leq b\}$
- çƒ: $\{x \mid \|x - c\|_2 \leq r\}$
- å¤šé¢ä½“: $\{x \mid Ax \leq b\}$

---

### 2. å‡¸å‡½æ•°å®šä¹‰

**å®šä¹‰ 1.2 (å‡¸å‡½æ•°)**:

å‡½æ•° $f: C \to \mathbb{R}$ æ˜¯å‡¸å‡½æ•°ï¼Œå¦‚æœå¯¹äºä»»æ„ $x, y \in C$ å’Œ $\theta \in [0, 1]$ï¼š

$$f(\theta x + (1-\theta) y) \leq \theta f(x) + (1-\theta) f(y)$$

**ç­‰ä»·æ¡ä»¶**:

- ä¸€é˜¶æ¡ä»¶: $f(y) \geq f(x) + \nabla f(x)^T (y - x)$
- äºŒé˜¶æ¡ä»¶: $\nabla^2 f(x) \succeq 0$ (HessiançŸ©é˜µåŠæ­£å®š)

**å¸¸è§å‡¸å‡½æ•°**:

- çº¿æ€§å‡½æ•°: $f(x) = a^T x + b$
- äºŒæ¬¡å‡½æ•°: $f(x) = x^T Q x$ (å½“ $Q \succeq 0$)
- è´Ÿå¯¹æ•°: $f(x) = -\log x$ (åœ¨ $x > 0$)
- èŒƒæ•°: $f(x) = \|x\|_p$ (å½“ $p \geq 1$)

---

### 3. å¼ºå‡¸æ€§

**å®šä¹‰ 1.3 (å¼ºå‡¸å‡½æ•°)**:

å‡½æ•° $f: C \to \mathbb{R}$ æ˜¯ $\mu$-å¼ºå‡¸çš„ï¼Œå¦‚æœå­˜åœ¨ $\mu > 0$ï¼Œä½¿å¾—ï¼š

$$f(y) \geq f(x) + \nabla f(x)^T (y - x) + \frac{\mu}{2} \|y - x\|^2$$

**é‡è¦æ€§**: å¼ºå‡¸å‡½æ•°æœ‰å”¯ä¸€çš„å…¨å±€æœ€ä¼˜è§£ï¼Œä¸”ä¼˜åŒ–ç®—æ³•æ”¶æ•›æ›´å¿«ã€‚

---

## ğŸ“Š å‡¸ä¼˜åŒ–é—®é¢˜

### 1. æ ‡å‡†å½¢å¼

**æ ‡å‡†å‡¸ä¼˜åŒ–é—®é¢˜**:

$$
\begin{align}
\min_{x} \quad & f(x) \\
\text{s.t.} \quad & g_i(x) \leq 0, \quad i = 1, \ldots, m \\
& h_j(x) = 0, \quad j = 1, \ldots, p \\
& x \in C
\end{align}
$$

å…¶ä¸­ $f$ å’Œ $g_i$ æ˜¯å‡¸å‡½æ•°ï¼Œ$h_j$ æ˜¯ä»¿å°„å‡½æ•°ï¼Œ$C$ æ˜¯å‡¸é›†ã€‚

---

### 2. å¯¹å¶é—®é¢˜

**æ‹‰æ ¼æœ—æ—¥å‡½æ•°**:

$$L(x, \lambda, \nu) = f(x) + \sum_{i=1}^m \lambda_i g_i(x) + \sum_{j=1}^p \nu_j h_j(x)$$

**å¯¹å¶å‡½æ•°**:

$$g(\lambda, \nu) = \inf_{x \in C} L(x, \lambda, \nu)$$

**å¯¹å¶é—®é¢˜**:

$$\max_{\lambda \geq 0, \nu} g(\lambda, \nu)$$

**å¼±å¯¹å¶**: $d^* \leq p^*$ (å¯¹å¶æœ€ä¼˜å€¼ â‰¤ åŸé—®é¢˜æœ€ä¼˜å€¼)

**å¼ºå¯¹å¶**: åœ¨Slateræ¡ä»¶ä¸‹ï¼Œ$d^* = p^*$

---

### 3. KKTæ¡ä»¶

**KKTæ¡ä»¶** (Karush-Kuhn-Tucker):

å¯¹äºå‡¸ä¼˜åŒ–é—®é¢˜ï¼Œåœ¨Slateræ¡ä»¶ä¸‹ï¼Œ$x^*$ æ˜¯æœ€ä¼˜è§£çš„å……è¦æ¡ä»¶æ˜¯å­˜åœ¨ $\lambda^* \geq 0$ å’Œ $\nu^*$ ä½¿å¾—ï¼š

1. **åŸå§‹å¯è¡Œæ€§**: $g_i(x^*) \leq 0$, $h_j(x^*) = 0$
2. **å¯¹å¶å¯è¡Œæ€§**: $\lambda_i^* \geq 0$
3. **äº’è¡¥æ¾å¼›**: $\lambda_i^* g_i(x^*) = 0$
4. **æ¢¯åº¦æ¡ä»¶**: $\nabla_x L(x^*, \lambda^*, \nu^*) = 0$

---

## ğŸ”¬ ä¼˜åŒ–ç®—æ³•

### 1. æ¢¯åº¦ä¸‹é™æ³•

**ç®—æ³•**:

$$x_{k+1} = x_k - \alpha_k \nabla f(x_k)$$

**æ”¶æ•›æ€§**:

- å¯¹äº $L$-å…‰æ»‘å‡¸å‡½æ•°ï¼Œæ­¥é•¿ $\alpha_k = 1/L$ æ—¶ï¼Œæ”¶æ•›ç‡ $O(1/k)$
- å¯¹äº $\mu$-å¼ºå‡¸å‡½æ•°ï¼Œæ”¶æ•›ç‡ $O((1 - \mu/L)^k)$

---

### 2. ç‰›é¡¿æ³•

**ç®—æ³•**:

$$x_{k+1} = x_k - [\nabla^2 f(x_k)]^{-1} \nabla f(x_k)$$

**ä¼˜åŠ¿**: äºŒæ¬¡æ”¶æ•›é€Ÿåº¦

**åŠ£åŠ¿**: éœ€è¦è®¡ç®—å’Œå­˜å‚¨HessiançŸ©é˜µ

---

### 3. å†…ç‚¹æ³•

**æ€æƒ³**: å°†çº¦æŸä¼˜åŒ–é—®é¢˜è½¬åŒ–ä¸ºæ— çº¦æŸé—®é¢˜

**éšœç¢å‡½æ•°**:

$$B(x) = -\sum_{i=1}^m \log(-g_i(x))$$

**æ— çº¦æŸé—®é¢˜**:

$$\min_x f(x) + t B(x)$$

å…¶ä¸­ $t > 0$ æ˜¯éšœç¢å‚æ•°ã€‚

---

## ğŸ¤– åœ¨æœºå™¨å­¦ä¹ ä¸­çš„åº”ç”¨

### 1. çº¿æ€§å›å½’

**é—®é¢˜**:

$$\min_w \frac{1}{2} \|Xw - y\|^2$$

**æ€§è´¨**: å‡¸ä¼˜åŒ–é—®é¢˜ï¼ˆäºŒæ¬¡å‡½æ•°æ˜¯å‡¸çš„ï¼‰

**è§£**: $w^* = (X^T X)^{-1} X^T y$

---

### 2. æ”¯æŒå‘é‡æœº

**é—®é¢˜**:

$$\min_{w, b} \frac{1}{2} \|w\|^2 + C \sum_{i=1}^n \xi_i$$

$$\text{s.t.} \quad y_i(w^T x_i + b) \geq 1 - \xi_i, \quad \xi_i \geq 0$$

**æ€§è´¨**: å‡¸ä¼˜åŒ–é—®é¢˜ï¼ˆäºŒæ¬¡è§„åˆ’ï¼‰

---

### 3. Lassoå›å½’

**é—®é¢˜**:

$$\min_w \frac{1}{2} \|Xw - y\|^2 + \lambda \|w\|_1$$

**æ€§è´¨**: å‡¸ä¼˜åŒ–é—®é¢˜ï¼ˆL1èŒƒæ•°æ˜¯å‡¸çš„ï¼‰

**ç‰¹ç‚¹**: äº§ç”Ÿç¨€ç–è§£

---

## ğŸ’» Pythonå®ç°

```python
import numpy as np
from scipy.optimize import minimize

# å‡¸ä¼˜åŒ–é—®é¢˜ç¤ºä¾‹ï¼šæœ€å°äºŒä¹˜
def least_squares_objective(w, X, y):
    """æœ€å°äºŒä¹˜ç›®æ ‡å‡½æ•°"""
    return 0.5 * np.sum((X @ w - y)**2)

def least_squares_gradient(w, X, y):
    """æ¢¯åº¦"""
    return X.T @ (X @ w - y)

# ä½¿ç”¨scipyä¼˜åŒ–
X = np.random.randn(100, 10)
y = np.random.randn(100)
w0 = np.zeros(10)

result = minimize(
    least_squares_objective,
    w0,
    args=(X, y),
    method='BFGS',
    jac=least_squares_gradient
)

print(f"æœ€ä¼˜è§£: {result.x}")
print(f"æœ€ä¼˜å€¼: {result.fun}")
```

---

## ğŸ“š æ ¸å¿ƒå®šç†æ€»ç»“

**å®šç† 1 (å‡¸å‡½æ•°çš„ä¸€é˜¶æ¡ä»¶)**:
å‡½æ•° $f$ æ˜¯å‡¸çš„å½“ä¸”ä»…å½“ $f(y) \geq f(x) + \nabla f(x)^T (y - x)$

**å®šç† 2 (å‡¸å‡½æ•°çš„äºŒé˜¶æ¡ä»¶)**:
å¦‚æœ $f$ äºŒé˜¶å¯å¾®ï¼Œåˆ™ $f$ æ˜¯å‡¸çš„å½“ä¸”ä»…å½“ $\nabla^2 f(x) \succeq 0$

**å®šç† 3 (å¼ºå¯¹å¶)**:
åœ¨Slateræ¡ä»¶ä¸‹ï¼Œå‡¸ä¼˜åŒ–é—®é¢˜çš„å¯¹å¶é—´éš™ä¸ºé›¶

**å®šç† 4 (KKTæ¡ä»¶)**:
åœ¨Slateræ¡ä»¶ä¸‹ï¼ŒKKTæ¡ä»¶æ˜¯å‡¸ä¼˜åŒ–é—®é¢˜æœ€ä¼˜è§£çš„å……è¦æ¡ä»¶

---

## ğŸ“ ç›¸å…³è¯¾ç¨‹

- MIT 6.252J - Nonlinear Programming
- Stanford EE364A - Convex Optimization
- CMU 10-725 - Convex Optimization

---

## ğŸ“– å‚è€ƒæ–‡çŒ®

1. Boyd, S., & Vandenberghe, L. (2004). *Convex Optimization*. Cambridge University Press.
2. Nesterov, Y. (2018). *Lectures on Convex Optimization*. Springer.
3. Bertsekas, D. P. (2016). *Convex Optimization Algorithms*. Athena Scientific.

---

**æ›´æ–°é¢‘ç‡**: æ ¹æ®å†…å®¹å®Œå–„æƒ…å†µæ›´æ–°
**æœ€åæ›´æ–°**: 2025-12-20
**ç»´æŠ¤**: Mathematicsé¡¹ç›®å›¢é˜Ÿ
