# Hilbert空间与再生核Hilbert空间 (Hilbert Spaces & RKHS)

> **The Mathematical Foundation of Kernel Methods**
>
> 核方法的数学基础

---

## 目录

- [Hilbert空间与再生核Hilbert空间 (Hilbert Spaces \& RKHS)](#hilbert空间与再生核hilbert空间-hilbert-spaces--rkhs)
  - [目录](#目录)
  - [📋 核心思想](#-核心思想)
  - [🎯 Hilbert空间](#-hilbert空间)
    - [1. 内积空间](#1-内积空间)
    - [2. Hilbert空间定义](#2-hilbert空间定义)
    - [3. 正交性与投影](#3-正交性与投影)
  - [📊 再生核Hilbert空间 (RKHS)](#-再生核hilbert空间-rkhs)
    - [1. RKHS定义](#1-rkhs定义)
    - [2. 再生核](#2-再生核)
    - [3. Moore-Aronszajn定理](#3-moore-aronszajn定理)
  - [🔬 核函数](#-核函数)
    - [1. 核函数定义](#1-核函数定义)
    - [2. 常见核函数](#2-常见核函数)
    - [3. 核的性质](#3-核的性质)
  - [💡 Representer定理](#-representer定理)
    - [1. 定理陈述](#1-定理陈述)
    - [2. 证明思路](#2-证明思路)
    - [3. 应用](#3-应用)
  - [🎨 核技巧 (Kernel Trick)](#-核技巧-kernel-trick)
    - [1. 核技巧思想](#1-核技巧思想)
    - [2. 特征映射](#2-特征映射)
    - [3. 核矩阵](#3-核矩阵)
  - [🔧 在机器学习中的应用](#-在机器学习中的应用)
    - [1. 支持向量机 (SVM)](#1-支持向量机-svm)
    - [2. 核岭回归](#2-核岭回归)
    - [3. 高斯过程](#3-高斯过程)
    - [4. 核PCA](#4-核pca)
  - [💻 Python实现](#-python实现)
  - [📚 练习题](#-练习题)
    - [练习1：内积计算](#练习1内积计算)
    - [练习2：核函数验证](#练习2核函数验证)
    - [练习3：Representer定理](#练习3representer定理)
    - [练习4：核SVM](#练习4核svm)
  - [🎓 相关课程](#-相关课程)
  - [📖 参考文献](#-参考文献)

---

## 📋 核心思想

**Hilbert空间**是完备的内积空间，是泛函分析的核心概念。
**再生核Hilbert空间 (RKHS)** 是一类特殊的Hilbert空间，在机器学习中有广泛应用。

**为什么RKHS重要**:

```text
机器学习中的核方法:
├─ 支持向量机 (SVM)
├─ 核岭回归
├─ 高斯过程
└─ 核PCA

核心优势:
├─ 处理非线性问题
├─ 无需显式特征映射
├─ 理论保证 (Representer定理)
└─ 计算高效 (核技巧)
```

---

## 🎯 Hilbert空间

### 1. 内积空间

**定义 1.1 (内积空间)**:

向量空间 $V$ 配备内积 $\langle \cdot, \cdot \rangle: V \times V \to \mathbb{R}$ (或 $\mathbb{C}$)，满足：

1. **正定性**: $\langle x, x \rangle \geq 0$，且 $\langle x, x \rangle = 0 \Leftrightarrow x = 0$
2. **线性性**: $\langle ax + by, z \rangle = a\langle x, z \rangle + b\langle y, z \rangle$
3. **对称性**: $\langle x, y \rangle = \langle y, x \rangle$ (实数情况)

**范数**:

$$
\|x\| = \sqrt{\langle x, x \rangle}
$$

**示例**:

- $\mathbb{R}^n$ 配备标准内积：$\langle x, y \rangle = \sum_{i=1}^n x_i y_i$
- $L^2[a, b]$ 配备：$\langle f, g \rangle = \int_a^b f(x)g(x) \, dx$

---

### 2. Hilbert空间定义

**定义 2.1 (Hilbert空间)**:

**完备的内积空间**称为Hilbert空间。即，所有Cauchy序列都收敛。

**示例**:

- $\mathbb{R}^n$ 是有限维Hilbert空间
- $L^2(\mathbb{R})$ 是无限维Hilbert空间
- $\ell^2 = \{(x_1, x_2, \ldots) : \sum_{i=1}^\infty x_i^2 < \infty\}$ 是序列空间

**性质**:

- 有限维内积空间都是Hilbert空间
- Hilbert空间是Banach空间（完备的赋范空间）

---

### 3. 正交性与投影

**定义 3.1 (正交)**:

$x \perp y$ 如果 $\langle x, y \rangle = 0$。

**投影定理**:

设 $M$ 是Hilbert空间 $\mathcal{H}$ 的闭子空间，则对于任意 $x \in \mathcal{H}$，存在唯一的 $y \in M$ 使得：

$$
\|x - y\| = \inf_{z \in M} \|x - z\|
$$

$y$ 称为 $x$ 在 $M$ 上的**正交投影**。

**正交分解**:

$$
\mathcal{H} = M \oplus M^\perp
$$

其中 $M^\perp = \{x \in \mathcal{H} : \langle x, y \rangle = 0, \forall y \in M\}$。

---

## 📊 再生核Hilbert空间 (RKHS)

### 1. RKHS定义

**定义 1.1 (RKHS)**:

设 $\mathcal{H}$ 是定义在集合 $\mathcal{X}$ 上的函数空间。如果对于所有 $x \in \mathcal{X}$，**点评估泛函** $\delta_x: \mathcal{H} \to \mathbb{R}$ 定义为：

$$
\delta_x(f) = f(x)
$$

是**连续的**（有界的），则 $\mathcal{H}$ 称为**再生核Hilbert空间**。

**直觉**：在RKHS中，函数值可以通过内积"再生"。

---

### 2. 再生核

**定理 2.1 (Riesz表示定理)**:

对于RKHS $\mathcal{H}$，对于每个 $x \in \mathcal{X}$，存在唯一的 $k_x \in \mathcal{H}$ 使得：

$$
f(x) = \langle f, k_x \rangle_{\mathcal{H}}, \quad \forall f \in \mathcal{H}
$$

**定义 2.2 (再生核)**:

函数 $k: \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ 定义为：

$$
k(x, y) = \langle k_x, k_y \rangle_{\mathcal{H}}
$$

称为 $\mathcal{H}$ 的**再生核**。

**再生性质**:

1. $f(x) = \langle f, k(\cdot, x) \rangle_{\mathcal{H}}$
2. $k(x, y) = \langle k(\cdot, x), k(\cdot, y) \rangle_{\mathcal{H}}$

---

### 3. Moore-Aronszajn定理

**定理 3.1 (Moore-Aronszajn定理)**:

设 $k: \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ 是**正定核**（即对于任意 $n$ 和 $x_1, \ldots, x_n \in \mathcal{X}$，矩阵 $K_{ij} = k(x_i, x_j)$ 是半正定的），则存在唯一的RKHS $\mathcal{H}_k$，其再生核为 $k$。

**意义**：正定核与RKHS一一对应。

---

## 🔬 核函数

### 1. 核函数定义

**定义 1.1 (核函数)**:

函数 $k: \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ 是**核函数**，如果存在特征映射 $\phi: \mathcal{X} \to \mathcal{H}$ 使得：

$$
k(x, y) = \langle \phi(x), \phi(y) \rangle_{\mathcal{H}}
$$

**等价条件** (Mercer定理):

$k$ 是核函数当且仅当 $k$ 是**对称的**且**正定的**。

---

### 2. 常见核函数

**线性核**:

$$
k(x, y) = x^T y
$$

**多项式核**:

$$
k(x, y) = (x^T y + c)^d
$$

**高斯核 (RBF核)**:

$$
k(x, y) = \exp\left(-\frac{\|x - y\|^2}{2\sigma^2}\right)
$$

**Laplacian核**:

$$
k(x, y) = \exp\left(-\frac{\|x - y\|}{\sigma}\right)
$$

**Sigmoid核**:

$$
k(x, y) = \tanh(\alpha x^T y + c)
$$

---

### 3. 核的性质

**性质 3.1 (核的封闭性)**:

1. **加法**: 如果 $k_1, k_2$ 是核，则 $k_1 + k_2$ 是核
2. **数乘**: 如果 $k$ 是核，$c > 0$，则 $ck$ 是核
3. **乘法**: 如果 $k_1, k_2$ 是核，则 $k_1 \cdot k_2$ 是核
4. **复合**: 如果 $k$ 是核，$f$ 是正函数，则 $f(k)$ 可能是核

**示例**:

高斯核可以表示为：

$$
k(x, y) = \exp\left(-\frac{\|x\|^2 + \|y\|^2}{2\sigma^2}\right) \exp\left(\frac{x^T y}{\sigma^2}\right)
$$

---

## 💡 Representer定理

### 1. 定理陈述

**定理 1.1 (Representer定理)**:

设 $\mathcal{H}_k$ 是RKHS，给定训练数据 $\{(x_i, y_i)\}_{i=1}^n$，考虑优化问题：

$$
\min_{f \in \mathcal{H}_k} \sum_{i=1}^n L(y_i, f(x_i)) + \lambda \|f\|_{\mathcal{H}_k}^2
$$

其中 $L$ 是损失函数，$\lambda > 0$ 是正则化参数。

则最优解 $f^*$ 可以表示为：

$$
f^*(x) = \sum_{i=1}^n \alpha_i k(x, x_i)
$$

**意义**：最优解位于由训练数据张成的有限维子空间中。

---

### 2. 证明思路

**证明**:

将 $f$ 分解为：

$$
f = f_{\parallel} + f_{\perp}
$$

其中 $f_{\parallel} \in \text{span}\{k(\cdot, x_i)\}_{i=1}^n$，$f_{\perp} \perp f_{\parallel}$。

由再生性质：

$$
f(x_i) = \langle f, k(\cdot, x_i) \rangle = \langle f_{\parallel}, k(\cdot, x_i) \rangle
$$

因此损失项只依赖于 $f_{\parallel}$，而 $\|f\|^2 = \|f_{\parallel}\|^2 + \|f_{\perp}\|^2$。

所以 $f_{\perp} = 0$ 时目标函数最小。

---

### 3. 应用

**核岭回归**:

$$
\min_{f} \sum_{i=1}^n (y_i - f(x_i))^2 + \lambda \|f\|^2
$$

解为：

$$
f^*(x) = \sum_{i=1}^n \alpha_i k(x, x_i)
$$

其中 $\alpha = (K + \lambda I)^{-1} y$，$K_{ij} = k(x_i, x_j)$。

---

## 🎨 核技巧 (Kernel Trick)

### 1. 核技巧思想

**核心思想**：

不需要显式计算特征映射 $\phi(x)$，只需要计算核函数 $k(x, y) = \langle \phi(x), \phi(y) \rangle$。

**优势**:

- 避免高维（甚至无限维）特征空间的显式计算
- 计算复杂度只依赖于数据量，而非特征维度

---

### 2. 特征映射

**示例** (多项式核):

对于 $x, y \in \mathbb{R}^2$，考虑 $k(x, y) = (x^T y)^2$。

显式特征映射：

$$
\phi(x) = (x_1^2, \sqrt{2}x_1 x_2, x_2^2)
$$

则：

$$
\langle \phi(x), \phi(y) \rangle = x_1^2 y_1^2 + 2x_1 x_2 y_1 y_2 + x_2^2 y_2^2 = (x^T y)^2
$$

**高斯核**：对应无限维特征空间！

---

### 3. 核矩阵

**定义 3.1 (Gram矩阵/核矩阵)**:

给定数据 $\{x_1, \ldots, x_n\}$，核矩阵 $K \in \mathbb{R}^{n \times n}$ 定义为：

$$
K_{ij} = k(x_i, x_j)
$$

**性质**:

- $K$ 是对称的
- $K$ 是半正定的（如果 $k$ 是正定核）

---

## 🔧 在机器学习中的应用

### 1. 支持向量机 (SVM)

**原始问题** (线性SVM):

$$
\min_{w, b} \frac{1}{2} \|w\|^2 \quad \text{s.t.} \quad y_i(w^T x_i + b) \geq 1
$$

**对偶问题**:

$$
\max_{\alpha} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i,j} \alpha_i \alpha_j y_i y_j x_i^T x_j
$$

**核化** (核SVM):

$$
\max_{\alpha} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i,j} \alpha_i \alpha_j y_i y_j k(x_i, x_j)
$$

**决策函数**:

$$
f(x) = \text{sign}\left(\sum_{i=1}^n \alpha_i y_i k(x, x_i) + b\right)
$$

---

### 2. 核岭回归

**问题**:

$$
\min_{f \in \mathcal{H}_k} \sum_{i=1}^n (y_i - f(x_i))^2 + \lambda \|f\|_{\mathcal{H}_k}^2
$$

**解**:

$$
f^*(x) = \sum_{i=1}^n \alpha_i k(x, x_i)
$$

其中：

$$
\alpha = (K + \lambda I)^{-1} y
$$

---

### 3. 高斯过程

**高斯过程**是函数的分布，完全由均值函数和协方差函数（核函数）确定：

$$
f \sim \mathcal{GP}(m(x), k(x, x'))
$$

**预测**:

给定训练数据 $\{(x_i, y_i)\}_{i=1}^n$，在新点 $x_*$ 的预测分布：

$$
p(f_* | x_*, X, y) = \mathcal{N}(\mu_*, \sigma_*^2)
$$

其中：

$$
\mu_* = k_*^T (K + \sigma_n^2 I)^{-1} y
$$

$$
\sigma_*^2 = k(x_*, x_*) - k_*^T (K + \sigma_n^2 I)^{-1} k_*
$$

---

### 4. 核PCA

**主成分分析 (PCA)** 在特征空间中：

1. 计算核矩阵 $K$
2. 中心化：$\tilde{K} = K - \mathbf{1}_n K - K \mathbf{1}_n + \mathbf{1}_n K \mathbf{1}_n$
3. 特征值分解：$\tilde{K} = V \Lambda V^T$
4. 主成分：$\alpha_k = V[:, k]$

**投影**:

$$
z_k(x) = \sum_{i=1}^n \alpha_{ki} k(x, x_i)
$$

---

### 5. 神经切线核 (Neural Tangent Kernel, NTK)

**核心思想**：

在无限宽神经网络中，训练过程等价于核方法，对应的核函数称为神经切线核。

**定义**：

对于神经网络 $f(x; \theta)$，NTK定义为：

$$
k_{\text{NTK}}(x, x') = \left\langle \frac{\partial f(x; \theta)}{\partial \theta}, \frac{\partial f(x'; \theta)}{\partial \theta} \right\rangle
$$

**关键性质**：

1. **训练动力学**：在梯度下降下，网络输出演化由NTK控制：
   $$
   \frac{d f(x; \theta_t)}{dt} = -\eta \sum_{i=1}^n k_{\text{NTK}}(x, x_i) (f(x_i; \theta_t) - y_i)
   $$

2. **泛化保证**：NTK理论提供了神经网络的泛化界

3. **架构依赖**：不同架构（MLP、CNN、Transformer）对应不同的NTK

**实际应用**：

- **快速训练**：使用NTK近似可以加速训练
- **架构设计**：通过分析NTK设计更好的架构
- **理论分析**：理解深度学习的优化和泛化

**Python实现示例**：

```python
import torch
import torch.nn as nn

def compute_ntk(model, x1, x2):
    """
    计算神经切线核

    参数:
        model: 神经网络模型
        x1, x2: 输入样本
    """
    model.eval()

    # 计算梯度
    def get_grad(x):
        model.zero_grad()
        output = model(x)
        grad = torch.autograd.grad(
            outputs=output.sum(),
            inputs=model.parameters(),
            create_graph=True,
            retain_graph=True
        )
        return torch.cat([g.flatten() for g in grad])

    grad1 = get_grad(x1)
    grad2 = get_grad(x2)

    # NTK = 内积
    ntk = torch.dot(grad1, grad2)
    return ntk.item()

# 示例：MLP的NTK
class SimpleMLP(nn.Module):
    def __init__(self, width=1000):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(2, width),
            nn.ReLU(),
            nn.Linear(width, width),
            nn.ReLU(),
            nn.Linear(width, 1)
        )

    def forward(self, x):
        return self.layers(x)

model = SimpleMLP(width=1000)
x1 = torch.randn(1, 2)
x2 = torch.randn(1, 2)

ntk_value = compute_ntk(model, x1, x2)
print(f"NTK value: {ntk_value}")
```

---

### 6. 核方法在深度学习中的应用

**核方法与深度学习的联系**：

1. **无限宽网络**：无限宽神经网络等价于核方法（NTK理论）
2. **特征学习**：深度网络学习特征映射，类似于核方法中的特征空间
3. **表示学习**：深度网络学习的数据表示可以视为核函数的隐式特征映射

**核方法 vs 深度学习**：

| 特性 | 核方法 | 深度学习 |
| ---- |--------| ---- |
| **特征** | 显式核函数 | 隐式学习 |
| **可解释性** | 高（核函数明确） | 低（黑盒） |
| **数据需求** | 小样本 | 大数据 |
| **计算** | $O(n^2)$ 核矩阵 | $O(n)$ 前向传播 |
| **理论保证** | 强（RKHS理论） | 弱（经验成功） |

**混合方法**：

- **核初始化**：使用核方法初始化深度网络
- **核正则化**：在损失函数中加入核正则项
- **深度核学习**：学习深度核函数

---

### 7. 多任务学习中的核方法

**多任务核学习**：

在多任务学习中，不同任务共享核函数：

$$
k_{\text{MTL}}((x, t), (x', t')) = k_t(x, x') \cdot \delta(t, t') + k_0(x, x')
$$

其中：
- $k_t$ 是任务特定的核
- $k_0$ 是共享核
- $\delta(t, t')$ 是任务指示函数

**应用场景**：

- **迁移学习**：源域和目标域共享核
- **多输出回归**：多个相关输出任务
- **领域适应**：不同领域间的知识共享

---

### 8. 核方法在强化学习中的应用

**核化值函数**：

在强化学习中，值函数可以表示为：

$$
V(s) = \sum_{i=1}^n \alpha_i k(s, s_i)
$$

其中 $k(s, s')$ 是状态空间的核函数。

**核化策略梯度**：

策略梯度方法中的优势函数可以核化：

$$
A(s, a) = \sum_{i=1}^n \alpha_i k((s, a), (s_i, a_i))
$$

**优势**：

- **函数逼近**：无需显式特征工程
- **样本效率**：核方法在小样本下表现好
- **理论保证**：RKHS理论提供收敛保证

**Python实现示例**：

```python
import numpy as np
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF

class KernelizedValueFunction:
    """核化值函数"""
    def __init__(self, kernel=RBF(length_scale=1.0)):
        self.gp = GaussianProcessRegressor(kernel=kernel)
        self.states = []
        self.values = []

    def update(self, states, values):
        """更新值函数估计"""
        self.states = np.array(states)
        self.values = np.array(values)
        self.gp.fit(self.states, self.values)

    def predict(self, state):
        """预测状态值"""
        return self.gp.predict([state])[0]

    def predict_with_uncertainty(self, state):
        """预测值及不确定性"""
        mean, std = self.gp.predict([state], return_std=True)
        return mean[0], std[0]

# 使用示例
vf = KernelizedValueFunction()
states = np.random.randn(100, 4)  # 100个状态，每个4维
values = np.random.randn(100)     # 对应的值

vf.update(states, values)
new_state = np.random.randn(4)
value, uncertainty = vf.predict_with_uncertainty(new_state)
print(f"Value: {value:.3f}, Uncertainty: {uncertainty:.3f}")
```

---

### 9. 核方法在时间序列中的应用

**核化自回归模型**：

时间序列预测可以表示为：

$$
y_{t+1} = \sum_{i=1}^T \alpha_i k(x_t, x_i)
$$

其中 $x_t = [y_t, y_{t-1}, \ldots, y_{t-p}]$ 是历史窗口。

**高斯过程时间序列**：

使用高斯过程建模时间序列：

$$
y(t) \sim \mathcal{GP}(m(t), k(t, t'))
$$

**核函数选择**：

- **RBF核**：平滑时间序列
- **周期核**：周期性时间序列
- **Matern核**：非平滑时间序列
- **组合核**：复杂模式

**应用**：

- **股票预测**：金融时间序列
- **天气预测**：气象数据
- **需求预测**：供应链管理

---

## 💻 Python实现

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_circles, make_moons
from sklearn.svm import SVC
from sklearn.metrics.pairwise import rbf_kernel, polynomial_kernel

# 1. 核函数实现
def linear_kernel(X, Y):
    """线性核"""
    return X @ Y.T

def polynomial_kernel_custom(X, Y, degree=3, coef0=1):
    """多项式核"""
    return (X @ Y.T + coef0) ** degree

def rbf_kernel_custom(X, Y, gamma=1.0):
    """高斯核 (RBF)"""
    # ||x - y||^2 = ||x||^2 + ||y||^2 - 2x^Ty
    X_norm = np.sum(X**2, axis=1).reshape(-1, 1)
    Y_norm = np.sum(Y**2, axis=1).reshape(1, -1)
    distances_sq = X_norm + Y_norm - 2 * X @ Y.T
    return np.exp(-gamma * distances_sq)


# 2. 核岭回归
class KernelRidgeRegression:
    """核岭回归"""

    def __init__(self, kernel='rbf', gamma=1.0, lambda_=1.0):
        self.kernel = kernel
        self.gamma = gamma
        self.lambda_ = lambda_

    def _compute_kernel(self, X, Y):
        """计算核矩阵"""
        if self.kernel == 'linear':
            return linear_kernel(X, Y)
        elif self.kernel == 'rbf':
            return rbf_kernel_custom(X, Y, self.gamma)
        elif self.kernel == 'polynomial':
            return polynomial_kernel_custom(X, Y)

    def fit(self, X, y):
        """训练"""
        self.X_train = X
        K = self._compute_kernel(X, X)
        n = len(y)
        self.alpha = np.linalg.solve(K + self.lambda_ * np.eye(n), y)

    def predict(self, X):
        """预测"""
        K = self._compute_kernel(X, self.X_train)
        return K @ self.alpha


# 3. 核SVM可视化
def visualize_kernel_svm():
    """可视化核SVM"""
    # 生成非线性可分数据
    X, y = make_circles(n_samples=200, noise=0.1, factor=0.3, random_state=42)

    # 训练不同核的SVM
    kernels = ['linear', 'poly', 'rbf']
    titles = ['Linear Kernel', 'Polynomial Kernel (degree=3)', 'RBF Kernel']

    fig, axes = plt.subplots(1, 3, figsize=(15, 5))

    for ax, kernel, title in zip(axes, kernels, titles):
        # 训练SVM
        if kernel == 'poly':
            clf = SVC(kernel=kernel, degree=3, gamma='auto')
        else:
            clf = SVC(kernel=kernel, gamma='auto')
        clf.fit(X, y)

        # 绘制决策边界
        xx, yy = np.meshgrid(np.linspace(X[:, 0].min()-0.5, X[:, 0].max()+0.5, 100),
                             np.linspace(X[:, 1].min()-0.5, X[:, 1].max()+0.5, 100))
        Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
        Z = Z.reshape(xx.shape)

        # 绘制
        ax.contourf(xx, yy, Z, levels=20, cmap='RdBu', alpha=0.6)
        ax.scatter(X[:, 0], X[:, 1], c=y, cmap='RdBu', edgecolors='k')
        ax.contour(xx, yy, Z, levels=[0], linewidths=2, colors='k')
        ax.set_title(title)
        ax.set_xlabel('x1')
        ax.set_ylabel('x2')

    plt.tight_layout()
    # plt.show()


# 4. 核岭回归示例
def kernel_ridge_regression_demo():
    """核岭回归示例"""
    np.random.seed(42)

    # 生成非线性数据
    X_train = np.linspace(0, 10, 50).reshape(-1, 1)
    y_train = np.sin(X_train).ravel() + 0.2 * np.random.randn(50)

    X_test = np.linspace(0, 10, 200).reshape(-1, 1)
    y_true = np.sin(X_test).ravel()

    # 训练不同核的模型
    models = {
        'Linear': KernelRidgeRegression(kernel='linear', lambda_=0.1),
        'RBF': KernelRidgeRegression(kernel='rbf', gamma=0.5, lambda_=0.1),
        'Polynomial': KernelRidgeRegression(kernel='polynomial', lambda_=0.1)
    }

    plt.figure(figsize=(15, 5))

    for idx, (name, model) in enumerate(models.items(), 1):
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

        plt.subplot(1, 3, idx)
        plt.scatter(X_train, y_train, color='red', label='Training data')
        plt.plot(X_test, y_true, 'g-', label='True function', linewidth=2)
        plt.plot(X_test, y_pred, 'b--', label='Prediction', linewidth=2)
        plt.title(f'Kernel Ridge Regression ({name} Kernel)')
        plt.xlabel('x')
        plt.ylabel('y')
        plt.legend()
        plt.grid(True, alpha=0.3)

    plt.tight_layout()
    # plt.show()


# 5. 核矩阵可视化
def visualize_kernel_matrix():
    """可视化核矩阵"""
    np.random.seed(42)

    # 生成数据
    X = np.random.randn(50, 2)

    # 计算不同核的核矩阵
    K_linear = linear_kernel(X, X)
    K_rbf = rbf_kernel_custom(X, X, gamma=1.0)
    K_poly = polynomial_kernel_custom(X, X, degree=3)

    fig, axes = plt.subplots(1, 3, figsize=(15, 5))

    # 线性核
    im1 = axes[0].imshow(K_linear, cmap='viridis')
    axes[0].set_title('Linear Kernel Matrix')
    plt.colorbar(im1, ax=axes[0])

    # RBF核
    im2 = axes[1].imshow(K_rbf, cmap='viridis')
    axes[1].set_title('RBF Kernel Matrix')
    plt.colorbar(im2, ax=axes[1])

    # 多项式核
    im3 = axes[2].imshow(K_poly, cmap='viridis')
    axes[2].set_title('Polynomial Kernel Matrix')
    plt.colorbar(im3, ax=axes[2])

    plt.tight_layout()
    # plt.show()


if __name__ == "__main__":
    print("=== Hilbert空间与RKHS示例 ===\n")

    print("1. 核SVM可视化")
    visualize_kernel_svm()

    print("\n2. 核岭回归")
    kernel_ridge_regression_demo()

    print("\n3. 核矩阵可视化")
    visualize_kernel_matrix()
```

---

## 📚 练习题

### 练习1：内积计算

在 $L^2[0, 1]$ 中，计算 $\langle f, g \rangle$，其中 $f(x) = x$，$g(x) = x^2$。

### 练习2：核函数验证

验证高斯核 $k(x, y) = \exp(-\|x - y\|^2 / 2\sigma^2)$ 是正定核。

### 练习3：Representer定理

证明在核岭回归中，最优解可以表示为 $f^*(x) = \sum_{i=1}^n \alpha_i k(x, x_i)$。

### 练习4：核SVM

实现核SVM，并在非线性可分数据上测试。

---

## 🎓 相关课程

| 大学 | 课程 |
| ---- |------|
| **MIT** | 18.102 - Introduction to Functional Analysis |
| **Stanford** | STATS315A - Modern Applied Statistics: Learning |
| **UC Berkeley** | STAT210B - Theoretical Statistics |
| **CMU** | 10-701 - Machine Learning |

---

## 📖 参考文献

1. **Berlinet & Thomas-Agnan (2004)**. *Reproducing Kernel Hilbert Spaces in Probability and Statistics*. Springer.

2. **Schölkopf & Smola (2002)**. *Learning with Kernels*. MIT Press.

3. **Steinwart & Christmann (2008)**. *Support Vector Machines*. Springer.

4. **Rasmussen & Williams (2006)**. *Gaussian Processes for Machine Learning*. MIT Press.

5. **Shawe-Taylor & Cristianini (2004)**. *Kernel Methods for Pattern Analysis*. Cambridge University Press.

---

*最后更新：2025年10月*-
