# Hilbertç©ºé—´ä¸å†ç”Ÿæ ¸Hilbertç©ºé—´ (Hilbert Spaces & RKHS)

> **The Mathematical Foundation of Kernel Methods**
>
> æ ¸æ–¹æ³•çš„æ•°å­¦åŸºç¡€

---

## ç›®å½•

- [Hilbertç©ºé—´ä¸å†ç”Ÿæ ¸Hilbertç©ºé—´ (Hilbert Spaces \& RKHS)](#hilbertç©ºé—´ä¸å†ç”Ÿæ ¸hilbertç©ºé—´-hilbert-spaces--rkhs)
  - [ç›®å½•](#ç›®å½•)
  - [ğŸ“‹ æ ¸å¿ƒæ€æƒ³](#-æ ¸å¿ƒæ€æƒ³)
  - [ğŸ¯ Hilbertç©ºé—´](#-hilbertç©ºé—´)
    - [1. å†…ç§¯ç©ºé—´](#1-å†…ç§¯ç©ºé—´)
    - [2. Hilbertç©ºé—´å®šä¹‰](#2-hilbertç©ºé—´å®šä¹‰)
    - [3. æ­£äº¤æ€§ä¸æŠ•å½±](#3-æ­£äº¤æ€§ä¸æŠ•å½±)
  - [ğŸ“Š å†ç”Ÿæ ¸Hilbertç©ºé—´ (RKHS)](#-å†ç”Ÿæ ¸hilbertç©ºé—´-rkhs)
    - [1. RKHSå®šä¹‰](#1-rkhså®šä¹‰)
    - [2. å†ç”Ÿæ ¸](#2-å†ç”Ÿæ ¸)
    - [3. Moore-Aronszajnå®šç†](#3-moore-aronszajnå®šç†)
  - [ğŸ”¬ æ ¸å‡½æ•°](#-æ ¸å‡½æ•°)
    - [1. æ ¸å‡½æ•°å®šä¹‰](#1-æ ¸å‡½æ•°å®šä¹‰)
    - [2. å¸¸è§æ ¸å‡½æ•°](#2-å¸¸è§æ ¸å‡½æ•°)
    - [3. æ ¸çš„æ€§è´¨](#3-æ ¸çš„æ€§è´¨)
  - [ğŸ’¡ Representerå®šç†](#-representerå®šç†)
    - [1. å®šç†é™ˆè¿°](#1-å®šç†é™ˆè¿°)
    - [2. è¯æ˜æ€è·¯](#2-è¯æ˜æ€è·¯)
    - [3. åº”ç”¨](#3-åº”ç”¨)
  - [ğŸ¨ æ ¸æŠ€å·§ (Kernel Trick)](#-æ ¸æŠ€å·§-kernel-trick)
    - [1. æ ¸æŠ€å·§æ€æƒ³](#1-æ ¸æŠ€å·§æ€æƒ³)
    - [2. ç‰¹å¾æ˜ å°„](#2-ç‰¹å¾æ˜ å°„)
    - [3. æ ¸çŸ©é˜µ](#3-æ ¸çŸ©é˜µ)
  - [ğŸ”§ åœ¨æœºå™¨å­¦ä¹ ä¸­çš„åº”ç”¨](#-åœ¨æœºå™¨å­¦ä¹ ä¸­çš„åº”ç”¨)
    - [1. æ”¯æŒå‘é‡æœº (SVM)](#1-æ”¯æŒå‘é‡æœº-svm)
    - [2. æ ¸å²­å›å½’](#2-æ ¸å²­å›å½’)
    - [3. é«˜æ–¯è¿‡ç¨‹](#3-é«˜æ–¯è¿‡ç¨‹)
    - [4. æ ¸PCA](#4-æ ¸pca)
  - [ğŸ’» Pythonå®ç°](#-pythonå®ç°)
  - [ğŸ“š ç»ƒä¹ é¢˜](#-ç»ƒä¹ é¢˜)
    - [ç»ƒä¹ 1ï¼šå†…ç§¯è®¡ç®—](#ç»ƒä¹ 1å†…ç§¯è®¡ç®—)
    - [ç»ƒä¹ 2ï¼šæ ¸å‡½æ•°éªŒè¯](#ç»ƒä¹ 2æ ¸å‡½æ•°éªŒè¯)
    - [ç»ƒä¹ 3ï¼šRepresenterå®šç†](#ç»ƒä¹ 3representerå®šç†)
    - [ç»ƒä¹ 4ï¼šæ ¸SVM](#ç»ƒä¹ 4æ ¸svm)
  - [ğŸ“ ç›¸å…³è¯¾ç¨‹](#-ç›¸å…³è¯¾ç¨‹)
  - [ğŸ“– å‚è€ƒæ–‡çŒ®](#-å‚è€ƒæ–‡çŒ®)

---

## ğŸ“‹ æ ¸å¿ƒæ€æƒ³

**Hilbertç©ºé—´**æ˜¯å®Œå¤‡çš„å†…ç§¯ç©ºé—´ï¼Œæ˜¯æ³›å‡½åˆ†æçš„æ ¸å¿ƒæ¦‚å¿µã€‚
**å†ç”Ÿæ ¸Hilbertç©ºé—´ (RKHS)** æ˜¯ä¸€ç±»ç‰¹æ®Šçš„Hilbertç©ºé—´ï¼Œåœ¨æœºå™¨å­¦ä¹ ä¸­æœ‰å¹¿æ³›åº”ç”¨ã€‚

**ä¸ºä»€ä¹ˆRKHSé‡è¦**:

```text
æœºå™¨å­¦ä¹ ä¸­çš„æ ¸æ–¹æ³•:
â”œâ”€ æ”¯æŒå‘é‡æœº (SVM)
â”œâ”€ æ ¸å²­å›å½’
â”œâ”€ é«˜æ–¯è¿‡ç¨‹
â””â”€ æ ¸PCA

æ ¸å¿ƒä¼˜åŠ¿:
â”œâ”€ å¤„ç†éçº¿æ€§é—®é¢˜
â”œâ”€ æ— éœ€æ˜¾å¼ç‰¹å¾æ˜ å°„
â”œâ”€ ç†è®ºä¿è¯ (Representerå®šç†)
â””â”€ è®¡ç®—é«˜æ•ˆ (æ ¸æŠ€å·§)
```

---

## ğŸ¯ Hilbertç©ºé—´

### 1. å†…ç§¯ç©ºé—´

**å®šä¹‰ 1.1 (å†…ç§¯ç©ºé—´)**:

å‘é‡ç©ºé—´ $V$ é…å¤‡å†…ç§¯ $\langle \cdot, \cdot \rangle: V \times V \to \mathbb{R}$ (æˆ– $\mathbb{C}$)ï¼Œæ»¡è¶³ï¼š

1. **æ­£å®šæ€§**: $\langle x, x \rangle \geq 0$ï¼Œä¸” $\langle x, x \rangle = 0 \Leftrightarrow x = 0$
2. **çº¿æ€§æ€§**: $\langle ax + by, z \rangle = a\langle x, z \rangle + b\langle y, z \rangle$
3. **å¯¹ç§°æ€§**: $\langle x, y \rangle = \langle y, x \rangle$ (å®æ•°æƒ…å†µ)

**èŒƒæ•°**:

$$
\|x\| = \sqrt{\langle x, x \rangle}
$$

**ç¤ºä¾‹**:

- $\mathbb{R}^n$ é…å¤‡æ ‡å‡†å†…ç§¯ï¼š$\langle x, y \rangle = \sum_{i=1}^n x_i y_i$
- $L^2[a, b]$ é…å¤‡ï¼š$\langle f, g \rangle = \int_a^b f(x)g(x) \, dx$

---

### 2. Hilbertç©ºé—´å®šä¹‰

**å®šä¹‰ 2.1 (Hilbertç©ºé—´)**:

**å®Œå¤‡çš„å†…ç§¯ç©ºé—´**ç§°ä¸ºHilbertç©ºé—´ã€‚å³ï¼Œæ‰€æœ‰Cauchyåºåˆ—éƒ½æ”¶æ•›ã€‚

**ç¤ºä¾‹**:

- $\mathbb{R}^n$ æ˜¯æœ‰é™ç»´Hilbertç©ºé—´
- $L^2(\mathbb{R})$ æ˜¯æ— é™ç»´Hilbertç©ºé—´
- $\ell^2 = \{(x_1, x_2, \ldots) : \sum_{i=1}^\infty x_i^2 < \infty\}$ æ˜¯åºåˆ—ç©ºé—´

**æ€§è´¨**:

- æœ‰é™ç»´å†…ç§¯ç©ºé—´éƒ½æ˜¯Hilbertç©ºé—´
- Hilbertç©ºé—´æ˜¯Banachç©ºé—´ï¼ˆå®Œå¤‡çš„èµ‹èŒƒç©ºé—´ï¼‰

---

### 3. æ­£äº¤æ€§ä¸æŠ•å½±

**å®šä¹‰ 3.1 (æ­£äº¤)**:

$x \perp y$ å¦‚æœ $\langle x, y \rangle = 0$ã€‚

**æŠ•å½±å®šç†**:

è®¾ $M$ æ˜¯Hilbertç©ºé—´ $\mathcal{H}$ çš„é—­å­ç©ºé—´ï¼Œåˆ™å¯¹äºä»»æ„ $x \in \mathcal{H}$ï¼Œå­˜åœ¨å”¯ä¸€çš„ $y \in M$ ä½¿å¾—ï¼š

$$
\|x - y\| = \inf_{z \in M} \|x - z\|
$$

$y$ ç§°ä¸º $x$ åœ¨ $M$ ä¸Šçš„**æ­£äº¤æŠ•å½±**ã€‚

**æ­£äº¤åˆ†è§£**:

$$
\mathcal{H} = M \oplus M^\perp
$$

å…¶ä¸­ $M^\perp = \{x \in \mathcal{H} : \langle x, y \rangle = 0, \forall y \in M\}$ã€‚

---

## ğŸ“Š å†ç”Ÿæ ¸Hilbertç©ºé—´ (RKHS)

### 1. RKHSå®šä¹‰

**å®šä¹‰ 1.1 (RKHS)**:

è®¾ $\mathcal{H}$ æ˜¯å®šä¹‰åœ¨é›†åˆ $\mathcal{X}$ ä¸Šçš„å‡½æ•°ç©ºé—´ã€‚å¦‚æœå¯¹äºæ‰€æœ‰ $x \in \mathcal{X}$ï¼Œ**ç‚¹è¯„ä¼°æ³›å‡½** $\delta_x: \mathcal{H} \to \mathbb{R}$ å®šä¹‰ä¸ºï¼š

$$
\delta_x(f) = f(x)
$$

æ˜¯**è¿ç»­çš„**ï¼ˆæœ‰ç•Œçš„ï¼‰ï¼Œåˆ™ $\mathcal{H}$ ç§°ä¸º**å†ç”Ÿæ ¸Hilbertç©ºé—´**ã€‚

**ç›´è§‰**ï¼šåœ¨RKHSä¸­ï¼Œå‡½æ•°å€¼å¯ä»¥é€šè¿‡å†…ç§¯"å†ç”Ÿ"ã€‚

---

### 2. å†ç”Ÿæ ¸

**å®šç† 2.1 (Rieszè¡¨ç¤ºå®šç†)**:

å¯¹äºRKHS $\mathcal{H}$ï¼Œå¯¹äºæ¯ä¸ª $x \in \mathcal{X}$ï¼Œå­˜åœ¨å”¯ä¸€çš„ $k_x \in \mathcal{H}$ ä½¿å¾—ï¼š

$$
f(x) = \langle f, k_x \rangle_{\mathcal{H}}, \quad \forall f \in \mathcal{H}
$$

**å®šä¹‰ 2.2 (å†ç”Ÿæ ¸)**:

å‡½æ•° $k: \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ å®šä¹‰ä¸ºï¼š

$$
k(x, y) = \langle k_x, k_y \rangle_{\mathcal{H}}
$$

ç§°ä¸º $\mathcal{H}$ çš„**å†ç”Ÿæ ¸**ã€‚

**å†ç”Ÿæ€§è´¨**:

1. $f(x) = \langle f, k(\cdot, x) \rangle_{\mathcal{H}}$
2. $k(x, y) = \langle k(\cdot, x), k(\cdot, y) \rangle_{\mathcal{H}}$

---

### 3. Moore-Aronszajnå®šç†

**å®šç† 3.1 (Moore-Aronszajnå®šç†)**:

è®¾ $k: \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ æ˜¯**æ­£å®šæ ¸**ï¼ˆå³å¯¹äºä»»æ„ $n$ å’Œ $x_1, \ldots, x_n \in \mathcal{X}$ï¼ŒçŸ©é˜µ $K_{ij} = k(x_i, x_j)$ æ˜¯åŠæ­£å®šçš„ï¼‰ï¼Œåˆ™å­˜åœ¨å”¯ä¸€çš„RKHS $\mathcal{H}_k$ï¼Œå…¶å†ç”Ÿæ ¸ä¸º $k$ã€‚

**æ„ä¹‰**ï¼šæ­£å®šæ ¸ä¸RKHSä¸€ä¸€å¯¹åº”ã€‚

---

## ğŸ”¬ æ ¸å‡½æ•°

### 1. æ ¸å‡½æ•°å®šä¹‰

**å®šä¹‰ 1.1 (æ ¸å‡½æ•°)**:

å‡½æ•° $k: \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ æ˜¯**æ ¸å‡½æ•°**ï¼Œå¦‚æœå­˜åœ¨ç‰¹å¾æ˜ å°„ $\phi: \mathcal{X} \to \mathcal{H}$ ä½¿å¾—ï¼š

$$
k(x, y) = \langle \phi(x), \phi(y) \rangle_{\mathcal{H}}
$$

**ç­‰ä»·æ¡ä»¶** (Mercerå®šç†):

$k$ æ˜¯æ ¸å‡½æ•°å½“ä¸”ä»…å½“ $k$ æ˜¯**å¯¹ç§°çš„**ä¸”**æ­£å®šçš„**ã€‚

---

### 2. å¸¸è§æ ¸å‡½æ•°

**çº¿æ€§æ ¸**:

$$
k(x, y) = x^T y
$$

**å¤šé¡¹å¼æ ¸**:

$$
k(x, y) = (x^T y + c)^d
$$

**é«˜æ–¯æ ¸ (RBFæ ¸)**:

$$
k(x, y) = \exp\left(-\frac{\|x - y\|^2}{2\sigma^2}\right)
$$

**Laplacianæ ¸**:

$$
k(x, y) = \exp\left(-\frac{\|x - y\|}{\sigma}\right)
$$

**Sigmoidæ ¸**:

$$
k(x, y) = \tanh(\alpha x^T y + c)
$$

---

### 3. æ ¸çš„æ€§è´¨

**æ€§è´¨ 3.1 (æ ¸çš„å°é—­æ€§)**:

1. **åŠ æ³•**: å¦‚æœ $k_1, k_2$ æ˜¯æ ¸ï¼Œåˆ™ $k_1 + k_2$ æ˜¯æ ¸
2. **æ•°ä¹˜**: å¦‚æœ $k$ æ˜¯æ ¸ï¼Œ$c > 0$ï¼Œåˆ™ $ck$ æ˜¯æ ¸
3. **ä¹˜æ³•**: å¦‚æœ $k_1, k_2$ æ˜¯æ ¸ï¼Œåˆ™ $k_1 \cdot k_2$ æ˜¯æ ¸
4. **å¤åˆ**: å¦‚æœ $k$ æ˜¯æ ¸ï¼Œ$f$ æ˜¯æ­£å‡½æ•°ï¼Œåˆ™ $f(k)$ å¯èƒ½æ˜¯æ ¸

**ç¤ºä¾‹**:

é«˜æ–¯æ ¸å¯ä»¥è¡¨ç¤ºä¸ºï¼š

$$
k(x, y) = \exp\left(-\frac{\|x\|^2 + \|y\|^2}{2\sigma^2}\right) \exp\left(\frac{x^T y}{\sigma^2}\right)
$$

---

## ğŸ’¡ Representerå®šç†

### 1. å®šç†é™ˆè¿°

**å®šç† 1.1 (Representerå®šç†)**:

è®¾ $\mathcal{H}_k$ æ˜¯RKHSï¼Œç»™å®šè®­ç»ƒæ•°æ® $\{(x_i, y_i)\}_{i=1}^n$ï¼Œè€ƒè™‘ä¼˜åŒ–é—®é¢˜ï¼š

$$
\min_{f \in \mathcal{H}_k} \sum_{i=1}^n L(y_i, f(x_i)) + \lambda \|f\|_{\mathcal{H}_k}^2
$$

å…¶ä¸­ $L$ æ˜¯æŸå¤±å‡½æ•°ï¼Œ$\lambda > 0$ æ˜¯æ­£åˆ™åŒ–å‚æ•°ã€‚

åˆ™æœ€ä¼˜è§£ $f^*$ å¯ä»¥è¡¨ç¤ºä¸ºï¼š

$$
f^*(x) = \sum_{i=1}^n \alpha_i k(x, x_i)
$$

**æ„ä¹‰**ï¼šæœ€ä¼˜è§£ä½äºç”±è®­ç»ƒæ•°æ®å¼ æˆçš„æœ‰é™ç»´å­ç©ºé—´ä¸­ã€‚

---

### 2. è¯æ˜æ€è·¯

**è¯æ˜**:

å°† $f$ åˆ†è§£ä¸ºï¼š

$$
f = f_{\parallel} + f_{\perp}
$$

å…¶ä¸­ $f_{\parallel} \in \text{span}\{k(\cdot, x_i)\}_{i=1}^n$ï¼Œ$f_{\perp} \perp f_{\parallel}$ã€‚

ç”±å†ç”Ÿæ€§è´¨ï¼š

$$
f(x_i) = \langle f, k(\cdot, x_i) \rangle = \langle f_{\parallel}, k(\cdot, x_i) \rangle
$$

å› æ­¤æŸå¤±é¡¹åªä¾èµ–äº $f_{\parallel}$ï¼Œè€Œ $\|f\|^2 = \|f_{\parallel}\|^2 + \|f_{\perp}\|^2$ã€‚

æ‰€ä»¥ $f_{\perp} = 0$ æ—¶ç›®æ ‡å‡½æ•°æœ€å°ã€‚

---

### 3. åº”ç”¨

**æ ¸å²­å›å½’**:

$$
\min_{f} \sum_{i=1}^n (y_i - f(x_i))^2 + \lambda \|f\|^2
$$

è§£ä¸ºï¼š

$$
f^*(x) = \sum_{i=1}^n \alpha_i k(x, x_i)
$$

å…¶ä¸­ $\alpha = (K + \lambda I)^{-1} y$ï¼Œ$K_{ij} = k(x_i, x_j)$ã€‚

---

## ğŸ¨ æ ¸æŠ€å·§ (Kernel Trick)

### 1. æ ¸æŠ€å·§æ€æƒ³

**æ ¸å¿ƒæ€æƒ³**ï¼š

ä¸éœ€è¦æ˜¾å¼è®¡ç®—ç‰¹å¾æ˜ å°„ $\phi(x)$ï¼Œåªéœ€è¦è®¡ç®—æ ¸å‡½æ•° $k(x, y) = \langle \phi(x), \phi(y) \rangle$ã€‚

**ä¼˜åŠ¿**:

- é¿å…é«˜ç»´ï¼ˆç”šè‡³æ— é™ç»´ï¼‰ç‰¹å¾ç©ºé—´çš„æ˜¾å¼è®¡ç®—
- è®¡ç®—å¤æ‚åº¦åªä¾èµ–äºæ•°æ®é‡ï¼Œè€Œéç‰¹å¾ç»´åº¦

---

### 2. ç‰¹å¾æ˜ å°„

**ç¤ºä¾‹** (å¤šé¡¹å¼æ ¸):

å¯¹äº $x, y \in \mathbb{R}^2$ï¼Œè€ƒè™‘ $k(x, y) = (x^T y)^2$ã€‚

æ˜¾å¼ç‰¹å¾æ˜ å°„ï¼š

$$
\phi(x) = (x_1^2, \sqrt{2}x_1 x_2, x_2^2)
$$

åˆ™ï¼š

$$
\langle \phi(x), \phi(y) \rangle = x_1^2 y_1^2 + 2x_1 x_2 y_1 y_2 + x_2^2 y_2^2 = (x^T y)^2
$$

**é«˜æ–¯æ ¸**ï¼šå¯¹åº”æ— é™ç»´ç‰¹å¾ç©ºé—´ï¼

---

### 3. æ ¸çŸ©é˜µ

**å®šä¹‰ 3.1 (GramçŸ©é˜µ/æ ¸çŸ©é˜µ)**:

ç»™å®šæ•°æ® $\{x_1, \ldots, x_n\}$ï¼Œæ ¸çŸ©é˜µ $K \in \mathbb{R}^{n \times n}$ å®šä¹‰ä¸ºï¼š

$$
K_{ij} = k(x_i, x_j)
$$

**æ€§è´¨**:

- $K$ æ˜¯å¯¹ç§°çš„
- $K$ æ˜¯åŠæ­£å®šçš„ï¼ˆå¦‚æœ $k$ æ˜¯æ­£å®šæ ¸ï¼‰

---

## ğŸ”§ åœ¨æœºå™¨å­¦ä¹ ä¸­çš„åº”ç”¨

### 1. æ”¯æŒå‘é‡æœº (SVM)

**åŸå§‹é—®é¢˜** (çº¿æ€§SVM):

$$
\min_{w, b} \frac{1}{2} \|w\|^2 \quad \text{s.t.} \quad y_i(w^T x_i + b) \geq 1
$$

**å¯¹å¶é—®é¢˜**:

$$
\max_{\alpha} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i,j} \alpha_i \alpha_j y_i y_j x_i^T x_j
$$

**æ ¸åŒ–** (æ ¸SVM):

$$
\max_{\alpha} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i,j} \alpha_i \alpha_j y_i y_j k(x_i, x_j)
$$

**å†³ç­–å‡½æ•°**:

$$
f(x) = \text{sign}\left(\sum_{i=1}^n \alpha_i y_i k(x, x_i) + b\right)
$$

---

### 2. æ ¸å²­å›å½’

**é—®é¢˜**:

$$
\min_{f \in \mathcal{H}_k} \sum_{i=1}^n (y_i - f(x_i))^2 + \lambda \|f\|_{\mathcal{H}_k}^2
$$

**è§£**:

$$
f^*(x) = \sum_{i=1}^n \alpha_i k(x, x_i)
$$

å…¶ä¸­ï¼š

$$
\alpha = (K + \lambda I)^{-1} y
$$

---

### 3. é«˜æ–¯è¿‡ç¨‹

**é«˜æ–¯è¿‡ç¨‹**æ˜¯å‡½æ•°çš„åˆ†å¸ƒï¼Œå®Œå…¨ç”±å‡å€¼å‡½æ•°å’Œåæ–¹å·®å‡½æ•°ï¼ˆæ ¸å‡½æ•°ï¼‰ç¡®å®šï¼š

$$
f \sim \mathcal{GP}(m(x), k(x, x'))
$$

**é¢„æµ‹**:

ç»™å®šè®­ç»ƒæ•°æ® $\{(x_i, y_i)\}_{i=1}^n$ï¼Œåœ¨æ–°ç‚¹ $x_*$ çš„é¢„æµ‹åˆ†å¸ƒï¼š

$$
p(f_* | x_*, X, y) = \mathcal{N}(\mu_*, \sigma_*^2)
$$

å…¶ä¸­ï¼š

$$
\mu_* = k_*^T (K + \sigma_n^2 I)^{-1} y
$$

$$
\sigma_*^2 = k(x_*, x_*) - k_*^T (K + \sigma_n^2 I)^{-1} k_*
$$

---

### 4. æ ¸PCA

**ä¸»æˆåˆ†åˆ†æ (PCA)** åœ¨ç‰¹å¾ç©ºé—´ä¸­ï¼š

1. è®¡ç®—æ ¸çŸ©é˜µ $K$
2. ä¸­å¿ƒåŒ–ï¼š$\tilde{K} = K - \mathbf{1}_n K - K \mathbf{1}_n + \mathbf{1}_n K \mathbf{1}_n$
3. ç‰¹å¾å€¼åˆ†è§£ï¼š$\tilde{K} = V \Lambda V^T$
4. ä¸»æˆåˆ†ï¼š$\alpha_k = V[:, k]$

**æŠ•å½±**:

$$
z_k(x) = \sum_{i=1}^n \alpha_{ki} k(x, x_i)
$$

---

### 5. ç¥ç»åˆ‡çº¿æ ¸ (Neural Tangent Kernel, NTK)

**æ ¸å¿ƒæ€æƒ³**ï¼š

åœ¨æ— é™å®½ç¥ç»ç½‘ç»œä¸­ï¼Œè®­ç»ƒè¿‡ç¨‹ç­‰ä»·äºæ ¸æ–¹æ³•ï¼Œå¯¹åº”çš„æ ¸å‡½æ•°ç§°ä¸ºç¥ç»åˆ‡çº¿æ ¸ã€‚

**å®šä¹‰**ï¼š

å¯¹äºç¥ç»ç½‘ç»œ $f(x; \theta)$ï¼ŒNTKå®šä¹‰ä¸ºï¼š

$$
k_{\text{NTK}}(x, x') = \left\langle \frac{\partial f(x; \theta)}{\partial \theta}, \frac{\partial f(x'; \theta)}{\partial \theta} \right\rangle
$$

**å…³é”®æ€§è´¨**ï¼š

1. **è®­ç»ƒåŠ¨åŠ›å­¦**ï¼šåœ¨æ¢¯åº¦ä¸‹é™ä¸‹ï¼Œç½‘ç»œè¾“å‡ºæ¼”åŒ–ç”±NTKæ§åˆ¶ï¼š
   $$
   \frac{d f(x; \theta_t)}{dt} = -\eta \sum_{i=1}^n k_{\text{NTK}}(x, x_i) (f(x_i; \theta_t) - y_i)
   $$

2. **æ³›åŒ–ä¿è¯**ï¼šNTKç†è®ºæä¾›äº†ç¥ç»ç½‘ç»œçš„æ³›åŒ–ç•Œ

3. **æ¶æ„ä¾èµ–**ï¼šä¸åŒæ¶æ„ï¼ˆMLPã€CNNã€Transformerï¼‰å¯¹åº”ä¸åŒçš„NTK

**å®é™…åº”ç”¨**ï¼š

- **å¿«é€Ÿè®­ç»ƒ**ï¼šä½¿ç”¨NTKè¿‘ä¼¼å¯ä»¥åŠ é€Ÿè®­ç»ƒ
- **æ¶æ„è®¾è®¡**ï¼šé€šè¿‡åˆ†æNTKè®¾è®¡æ›´å¥½çš„æ¶æ„
- **ç†è®ºåˆ†æ**ï¼šç†è§£æ·±åº¦å­¦ä¹ çš„ä¼˜åŒ–å’Œæ³›åŒ–

**Pythonå®ç°ç¤ºä¾‹**ï¼š

```python
import torch
import torch.nn as nn

def compute_ntk(model, x1, x2):
    """
    è®¡ç®—ç¥ç»åˆ‡çº¿æ ¸

    å‚æ•°:
        model: ç¥ç»ç½‘ç»œæ¨¡å‹
        x1, x2: è¾“å…¥æ ·æœ¬
    """
    model.eval()

    # è®¡ç®—æ¢¯åº¦
    def get_grad(x):
        model.zero_grad()
        output = model(x)
        grad = torch.autograd.grad(
            outputs=output.sum(),
            inputs=model.parameters(),
            create_graph=True,
            retain_graph=True
        )
        return torch.cat([g.flatten() for g in grad])

    grad1 = get_grad(x1)
    grad2 = get_grad(x2)

    # NTK = å†…ç§¯
    ntk = torch.dot(grad1, grad2)
    return ntk.item()

# ç¤ºä¾‹ï¼šMLPçš„NTK
class SimpleMLP(nn.Module):
    def __init__(self, width=1000):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(2, width),
            nn.ReLU(),
            nn.Linear(width, width),
            nn.ReLU(),
            nn.Linear(width, 1)
        )

    def forward(self, x):
        return self.layers(x)

model = SimpleMLP(width=1000)
x1 = torch.randn(1, 2)
x2 = torch.randn(1, 2)

ntk_value = compute_ntk(model, x1, x2)
print(f"NTK value: {ntk_value}")
```

---

### 6. æ ¸æ–¹æ³•åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„åº”ç”¨

**æ ¸æ–¹æ³•ä¸æ·±åº¦å­¦ä¹ çš„è”ç³»**ï¼š

1. **æ— é™å®½ç½‘ç»œ**ï¼šæ— é™å®½ç¥ç»ç½‘ç»œç­‰ä»·äºæ ¸æ–¹æ³•ï¼ˆNTKç†è®ºï¼‰
2. **ç‰¹å¾å­¦ä¹ **ï¼šæ·±åº¦ç½‘ç»œå­¦ä¹ ç‰¹å¾æ˜ å°„ï¼Œç±»ä¼¼äºæ ¸æ–¹æ³•ä¸­çš„ç‰¹å¾ç©ºé—´
3. **è¡¨ç¤ºå­¦ä¹ **ï¼šæ·±åº¦ç½‘ç»œå­¦ä¹ çš„æ•°æ®è¡¨ç¤ºå¯ä»¥è§†ä¸ºæ ¸å‡½æ•°çš„éšå¼ç‰¹å¾æ˜ å°„

**æ ¸æ–¹æ³• vs æ·±åº¦å­¦ä¹ **ï¼š

| ç‰¹æ€§ | æ ¸æ–¹æ³• | æ·±åº¦å­¦ä¹  |
|------|--------|----------|
| **ç‰¹å¾** | æ˜¾å¼æ ¸å‡½æ•° | éšå¼å­¦ä¹  |
| **å¯è§£é‡Šæ€§** | é«˜ï¼ˆæ ¸å‡½æ•°æ˜ç¡®ï¼‰ | ä½ï¼ˆé»‘ç›’ï¼‰ |
| **æ•°æ®éœ€æ±‚** | å°æ ·æœ¬ | å¤§æ•°æ® |
| **è®¡ç®—** | $O(n^2)$ æ ¸çŸ©é˜µ | $O(n)$ å‰å‘ä¼ æ’­ |
| **ç†è®ºä¿è¯** | å¼ºï¼ˆRKHSç†è®ºï¼‰ | å¼±ï¼ˆç»éªŒæˆåŠŸï¼‰ |

**æ··åˆæ–¹æ³•**ï¼š

- **æ ¸åˆå§‹åŒ–**ï¼šä½¿ç”¨æ ¸æ–¹æ³•åˆå§‹åŒ–æ·±åº¦ç½‘ç»œ
- **æ ¸æ­£åˆ™åŒ–**ï¼šåœ¨æŸå¤±å‡½æ•°ä¸­åŠ å…¥æ ¸æ­£åˆ™é¡¹
- **æ·±åº¦æ ¸å­¦ä¹ **ï¼šå­¦ä¹ æ·±åº¦æ ¸å‡½æ•°

---

### 7. å¤šä»»åŠ¡å­¦ä¹ ä¸­çš„æ ¸æ–¹æ³•

**å¤šä»»åŠ¡æ ¸å­¦ä¹ **ï¼š

åœ¨å¤šä»»åŠ¡å­¦ä¹ ä¸­ï¼Œä¸åŒä»»åŠ¡å…±äº«æ ¸å‡½æ•°ï¼š

$$
k_{\text{MTL}}((x, t), (x', t')) = k_t(x, x') \cdot \delta(t, t') + k_0(x, x')
$$

å…¶ä¸­ï¼š
- $k_t$ æ˜¯ä»»åŠ¡ç‰¹å®šçš„æ ¸
- $k_0$ æ˜¯å…±äº«æ ¸
- $\delta(t, t')$ æ˜¯ä»»åŠ¡æŒ‡ç¤ºå‡½æ•°

**åº”ç”¨åœºæ™¯**ï¼š

- **è¿ç§»å­¦ä¹ **ï¼šæºåŸŸå’Œç›®æ ‡åŸŸå…±äº«æ ¸
- **å¤šè¾“å‡ºå›å½’**ï¼šå¤šä¸ªç›¸å…³è¾“å‡ºä»»åŠ¡
- **é¢†åŸŸé€‚åº”**ï¼šä¸åŒé¢†åŸŸé—´çš„çŸ¥è¯†å…±äº«

---

### 8. æ ¸æ–¹æ³•åœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„åº”ç”¨

**æ ¸åŒ–å€¼å‡½æ•°**ï¼š

åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œå€¼å‡½æ•°å¯ä»¥è¡¨ç¤ºä¸ºï¼š

$$
V(s) = \sum_{i=1}^n \alpha_i k(s, s_i)
$$

å…¶ä¸­ $k(s, s')$ æ˜¯çŠ¶æ€ç©ºé—´çš„æ ¸å‡½æ•°ã€‚

**æ ¸åŒ–ç­–ç•¥æ¢¯åº¦**ï¼š

ç­–ç•¥æ¢¯åº¦æ–¹æ³•ä¸­çš„ä¼˜åŠ¿å‡½æ•°å¯ä»¥æ ¸åŒ–ï¼š

$$
A(s, a) = \sum_{i=1}^n \alpha_i k((s, a), (s_i, a_i))
$$

**ä¼˜åŠ¿**ï¼š

- **å‡½æ•°é€¼è¿‘**ï¼šæ— éœ€æ˜¾å¼ç‰¹å¾å·¥ç¨‹
- **æ ·æœ¬æ•ˆç‡**ï¼šæ ¸æ–¹æ³•åœ¨å°æ ·æœ¬ä¸‹è¡¨ç°å¥½
- **ç†è®ºä¿è¯**ï¼šRKHSç†è®ºæä¾›æ”¶æ•›ä¿è¯

**Pythonå®ç°ç¤ºä¾‹**ï¼š

```python
import numpy as np
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF

class KernelizedValueFunction:
    """æ ¸åŒ–å€¼å‡½æ•°"""
    def __init__(self, kernel=RBF(length_scale=1.0)):
        self.gp = GaussianProcessRegressor(kernel=kernel)
        self.states = []
        self.values = []

    def update(self, states, values):
        """æ›´æ–°å€¼å‡½æ•°ä¼°è®¡"""
        self.states = np.array(states)
        self.values = np.array(values)
        self.gp.fit(self.states, self.values)

    def predict(self, state):
        """é¢„æµ‹çŠ¶æ€å€¼"""
        return self.gp.predict([state])[0]

    def predict_with_uncertainty(self, state):
        """é¢„æµ‹å€¼åŠä¸ç¡®å®šæ€§"""
        mean, std = self.gp.predict([state], return_std=True)
        return mean[0], std[0]

# ä½¿ç”¨ç¤ºä¾‹
vf = KernelizedValueFunction()
states = np.random.randn(100, 4)  # 100ä¸ªçŠ¶æ€ï¼Œæ¯ä¸ª4ç»´
values = np.random.randn(100)     # å¯¹åº”çš„å€¼

vf.update(states, values)
new_state = np.random.randn(4)
value, uncertainty = vf.predict_with_uncertainty(new_state)
print(f"Value: {value:.3f}, Uncertainty: {uncertainty:.3f}")
```

---

### 9. æ ¸æ–¹æ³•åœ¨æ—¶é—´åºåˆ—ä¸­çš„åº”ç”¨

**æ ¸åŒ–è‡ªå›å½’æ¨¡å‹**ï¼š

æ—¶é—´åºåˆ—é¢„æµ‹å¯ä»¥è¡¨ç¤ºä¸ºï¼š

$$
y_{t+1} = \sum_{i=1}^T \alpha_i k(x_t, x_i)
$$

å…¶ä¸­ $x_t = [y_t, y_{t-1}, \ldots, y_{t-p}]$ æ˜¯å†å²çª—å£ã€‚

**é«˜æ–¯è¿‡ç¨‹æ—¶é—´åºåˆ—**ï¼š

ä½¿ç”¨é«˜æ–¯è¿‡ç¨‹å»ºæ¨¡æ—¶é—´åºåˆ—ï¼š

$$
y(t) \sim \mathcal{GP}(m(t), k(t, t'))
$$

**æ ¸å‡½æ•°é€‰æ‹©**ï¼š

- **RBFæ ¸**ï¼šå¹³æ»‘æ—¶é—´åºåˆ—
- **å‘¨æœŸæ ¸**ï¼šå‘¨æœŸæ€§æ—¶é—´åºåˆ—
- **Maternæ ¸**ï¼šéå¹³æ»‘æ—¶é—´åºåˆ—
- **ç»„åˆæ ¸**ï¼šå¤æ‚æ¨¡å¼

**åº”ç”¨**ï¼š

- **è‚¡ç¥¨é¢„æµ‹**ï¼šé‡‘èæ—¶é—´åºåˆ—
- **å¤©æ°”é¢„æµ‹**ï¼šæ°”è±¡æ•°æ®
- **éœ€æ±‚é¢„æµ‹**ï¼šä¾›åº”é“¾ç®¡ç†

---

## ğŸ’» Pythonå®ç°

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_circles, make_moons
from sklearn.svm import SVC
from sklearn.metrics.pairwise import rbf_kernel, polynomial_kernel

# 1. æ ¸å‡½æ•°å®ç°
def linear_kernel(X, Y):
    """çº¿æ€§æ ¸"""
    return X @ Y.T

def polynomial_kernel_custom(X, Y, degree=3, coef0=1):
    """å¤šé¡¹å¼æ ¸"""
    return (X @ Y.T + coef0) ** degree

def rbf_kernel_custom(X, Y, gamma=1.0):
    """é«˜æ–¯æ ¸ (RBF)"""
    # ||x - y||^2 = ||x||^2 + ||y||^2 - 2x^Ty
    X_norm = np.sum(X**2, axis=1).reshape(-1, 1)
    Y_norm = np.sum(Y**2, axis=1).reshape(1, -1)
    distances_sq = X_norm + Y_norm - 2 * X @ Y.T
    return np.exp(-gamma * distances_sq)


# 2. æ ¸å²­å›å½’
class KernelRidgeRegression:
    """æ ¸å²­å›å½’"""

    def __init__(self, kernel='rbf', gamma=1.0, lambda_=1.0):
        self.kernel = kernel
        self.gamma = gamma
        self.lambda_ = lambda_

    def _compute_kernel(self, X, Y):
        """è®¡ç®—æ ¸çŸ©é˜µ"""
        if self.kernel == 'linear':
            return linear_kernel(X, Y)
        elif self.kernel == 'rbf':
            return rbf_kernel_custom(X, Y, self.gamma)
        elif self.kernel == 'polynomial':
            return polynomial_kernel_custom(X, Y)

    def fit(self, X, y):
        """è®­ç»ƒ"""
        self.X_train = X
        K = self._compute_kernel(X, X)
        n = len(y)
        self.alpha = np.linalg.solve(K + self.lambda_ * np.eye(n), y)

    def predict(self, X):
        """é¢„æµ‹"""
        K = self._compute_kernel(X, self.X_train)
        return K @ self.alpha


# 3. æ ¸SVMå¯è§†åŒ–
def visualize_kernel_svm():
    """å¯è§†åŒ–æ ¸SVM"""
    # ç”Ÿæˆéçº¿æ€§å¯åˆ†æ•°æ®
    X, y = make_circles(n_samples=200, noise=0.1, factor=0.3, random_state=42)

    # è®­ç»ƒä¸åŒæ ¸çš„SVM
    kernels = ['linear', 'poly', 'rbf']
    titles = ['Linear Kernel', 'Polynomial Kernel (degree=3)', 'RBF Kernel']

    fig, axes = plt.subplots(1, 3, figsize=(15, 5))

    for ax, kernel, title in zip(axes, kernels, titles):
        # è®­ç»ƒSVM
        if kernel == 'poly':
            clf = SVC(kernel=kernel, degree=3, gamma='auto')
        else:
            clf = SVC(kernel=kernel, gamma='auto')
        clf.fit(X, y)

        # ç»˜åˆ¶å†³ç­–è¾¹ç•Œ
        xx, yy = np.meshgrid(np.linspace(X[:, 0].min()-0.5, X[:, 0].max()+0.5, 100),
                             np.linspace(X[:, 1].min()-0.5, X[:, 1].max()+0.5, 100))
        Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
        Z = Z.reshape(xx.shape)

        # ç»˜åˆ¶
        ax.contourf(xx, yy, Z, levels=20, cmap='RdBu', alpha=0.6)
        ax.scatter(X[:, 0], X[:, 1], c=y, cmap='RdBu', edgecolors='k')
        ax.contour(xx, yy, Z, levels=[0], linewidths=2, colors='k')
        ax.set_title(title)
        ax.set_xlabel('x1')
        ax.set_ylabel('x2')

    plt.tight_layout()
    # plt.show()


# 4. æ ¸å²­å›å½’ç¤ºä¾‹
def kernel_ridge_regression_demo():
    """æ ¸å²­å›å½’ç¤ºä¾‹"""
    np.random.seed(42)

    # ç”Ÿæˆéçº¿æ€§æ•°æ®
    X_train = np.linspace(0, 10, 50).reshape(-1, 1)
    y_train = np.sin(X_train).ravel() + 0.2 * np.random.randn(50)

    X_test = np.linspace(0, 10, 200).reshape(-1, 1)
    y_true = np.sin(X_test).ravel()

    # è®­ç»ƒä¸åŒæ ¸çš„æ¨¡å‹
    models = {
        'Linear': KernelRidgeRegression(kernel='linear', lambda_=0.1),
        'RBF': KernelRidgeRegression(kernel='rbf', gamma=0.5, lambda_=0.1),
        'Polynomial': KernelRidgeRegression(kernel='polynomial', lambda_=0.1)
    }

    plt.figure(figsize=(15, 5))

    for idx, (name, model) in enumerate(models.items(), 1):
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

        plt.subplot(1, 3, idx)
        plt.scatter(X_train, y_train, color='red', label='Training data')
        plt.plot(X_test, y_true, 'g-', label='True function', linewidth=2)
        plt.plot(X_test, y_pred, 'b--', label='Prediction', linewidth=2)
        plt.title(f'Kernel Ridge Regression ({name} Kernel)')
        plt.xlabel('x')
        plt.ylabel('y')
        plt.legend()
        plt.grid(True, alpha=0.3)

    plt.tight_layout()
    # plt.show()


# 5. æ ¸çŸ©é˜µå¯è§†åŒ–
def visualize_kernel_matrix():
    """å¯è§†åŒ–æ ¸çŸ©é˜µ"""
    np.random.seed(42)

    # ç”Ÿæˆæ•°æ®
    X = np.random.randn(50, 2)

    # è®¡ç®—ä¸åŒæ ¸çš„æ ¸çŸ©é˜µ
    K_linear = linear_kernel(X, X)
    K_rbf = rbf_kernel_custom(X, X, gamma=1.0)
    K_poly = polynomial_kernel_custom(X, X, degree=3)

    fig, axes = plt.subplots(1, 3, figsize=(15, 5))

    # çº¿æ€§æ ¸
    im1 = axes[0].imshow(K_linear, cmap='viridis')
    axes[0].set_title('Linear Kernel Matrix')
    plt.colorbar(im1, ax=axes[0])

    # RBFæ ¸
    im2 = axes[1].imshow(K_rbf, cmap='viridis')
    axes[1].set_title('RBF Kernel Matrix')
    plt.colorbar(im2, ax=axes[1])

    # å¤šé¡¹å¼æ ¸
    im3 = axes[2].imshow(K_poly, cmap='viridis')
    axes[2].set_title('Polynomial Kernel Matrix')
    plt.colorbar(im3, ax=axes[2])

    plt.tight_layout()
    # plt.show()


if __name__ == "__main__":
    print("=== Hilbertç©ºé—´ä¸RKHSç¤ºä¾‹ ===\n")

    print("1. æ ¸SVMå¯è§†åŒ–")
    visualize_kernel_svm()

    print("\n2. æ ¸å²­å›å½’")
    kernel_ridge_regression_demo()

    print("\n3. æ ¸çŸ©é˜µå¯è§†åŒ–")
    visualize_kernel_matrix()
```

---

## ğŸ“š ç»ƒä¹ é¢˜

### ç»ƒä¹ 1ï¼šå†…ç§¯è®¡ç®—

åœ¨ $L^2[0, 1]$ ä¸­ï¼Œè®¡ç®— $\langle f, g \rangle$ï¼Œå…¶ä¸­ $f(x) = x$ï¼Œ$g(x) = x^2$ã€‚

### ç»ƒä¹ 2ï¼šæ ¸å‡½æ•°éªŒè¯

éªŒè¯é«˜æ–¯æ ¸ $k(x, y) = \exp(-\|x - y\|^2 / 2\sigma^2)$ æ˜¯æ­£å®šæ ¸ã€‚

### ç»ƒä¹ 3ï¼šRepresenterå®šç†

è¯æ˜åœ¨æ ¸å²­å›å½’ä¸­ï¼Œæœ€ä¼˜è§£å¯ä»¥è¡¨ç¤ºä¸º $f^*(x) = \sum_{i=1}^n \alpha_i k(x, x_i)$ã€‚

### ç»ƒä¹ 4ï¼šæ ¸SVM

å®ç°æ ¸SVMï¼Œå¹¶åœ¨éçº¿æ€§å¯åˆ†æ•°æ®ä¸Šæµ‹è¯•ã€‚

---

## ğŸ“ ç›¸å…³è¯¾ç¨‹

| å¤§å­¦ | è¯¾ç¨‹ |
|------|------|
| **MIT** | 18.102 - Introduction to Functional Analysis |
| **Stanford** | STATS315A - Modern Applied Statistics: Learning |
| **UC Berkeley** | STAT210B - Theoretical Statistics |
| **CMU** | 10-701 - Machine Learning |

---

## ğŸ“– å‚è€ƒæ–‡çŒ®

1. **Berlinet & Thomas-Agnan (2004)**. *Reproducing Kernel Hilbert Spaces in Probability and Statistics*. Springer.

2. **SchÃ¶lkopf & Smola (2002)**. *Learning with Kernels*. MIT Press.

3. **Steinwart & Christmann (2008)**. *Support Vector Machines*. Springer.

4. **Rasmussen & Williams (2006)**. *Gaussian Processes for Machine Learning*. MIT Press.

5. **Shawe-Taylor & Cristianini (2004)**. *Kernel Methods for Pattern Analysis*. Cambridge University Press.

---

*æœ€åæ›´æ–°ï¼š2025å¹´10æœˆ*-
