# é€šç”¨é€¼è¿‘å®šç†

> **Universal Approximation Theorem**
>
> ç¥ç»ç½‘ç»œä¸ºä»€ä¹ˆæœ‰æ•ˆï¼Ÿç†è®ºåŸºç¡€ä¸æ·±åº¦çš„ä½œç”¨

---

## ç›®å½•

- [é€šç”¨é€¼è¿‘å®šç†](#é€šç”¨é€¼è¿‘å®šç†)
  - [ç›®å½•](#ç›®å½•)
  - [ğŸ“‹ æ ¸å¿ƒé—®é¢˜](#-æ ¸å¿ƒé—®é¢˜)
  - [ğŸ¯ ç»å…¸é€šç”¨é€¼è¿‘å®šç†](#-ç»å…¸é€šç”¨é€¼è¿‘å®šç†)
    - [1. Cybenkoå®šç† (1989)](#1-cybenkoå®šç†-1989)
    - [2. Hornikå®šç† (1991)](#2-hornikå®šç†-1991)
    - [3. æ„é€ æ€§è¯æ˜æ€è·¯](#3-æ„é€ æ€§è¯æ˜æ€è·¯)
  - [ğŸ—ï¸ æ·±åº¦çš„ä½œç”¨](#ï¸-æ·±åº¦çš„ä½œç”¨)
    - [1. å®½åº¦ vs æ·±åº¦](#1-å®½åº¦-vs-æ·±åº¦)
    - [2. è¡¨ç¤ºæ•ˆç‡](#2-è¡¨ç¤ºæ•ˆç‡)
    - [3. æ·±åº¦åˆ†ç¦»å®šç†](#3-æ·±åº¦åˆ†ç¦»å®šç†)
  - [ğŸ“Š ç°ä»£æ‰©å±•](#-ç°ä»£æ‰©å±•)
    - [1. ReLUç½‘ç»œ](#1-reluç½‘ç»œ)
    - [2. å·ç§¯ç¥ç»ç½‘ç»œ](#2-å·ç§¯ç¥ç»ç½‘ç»œ)
    - [3. Transformerä¸æ³¨æ„åŠ›æœºåˆ¶](#3-transformerä¸æ³¨æ„åŠ›æœºåˆ¶)
  - [ğŸ”¬ é€¼è¿‘é€Ÿç‡ç†è®º](#-é€¼è¿‘é€Ÿç‡ç†è®º)
    - [1. å‚æ•°æ•°é‡ä¸é€¼è¿‘è¯¯å·®](#1-å‚æ•°æ•°é‡ä¸é€¼è¿‘è¯¯å·®)
    - [2. ç»´æ•°ç¾éš¾](#2-ç»´æ•°ç¾éš¾)
    - [3. ç»„åˆæ€§ä¸å½’çº³åç½®](#3-ç»„åˆæ€§ä¸å½’çº³åç½®)
  - [ğŸ¤– å®é™…æ„ä¹‰ä¸å±€é™](#-å®é™…æ„ä¹‰ä¸å±€é™)
  - [ğŸ’» Pythonå®ç°](#-pythonå®ç°)
    - [1. å¯è§†åŒ–å•éšå±‚ç½‘ç»œé€¼è¿‘](#1-å¯è§†åŒ–å•éšå±‚ç½‘ç»œé€¼è¿‘)
    - [2. æ·±åº¦ç½‘ç»œçš„è¡¨ç¤ºä¼˜åŠ¿](#2-æ·±åº¦ç½‘ç»œçš„è¡¨ç¤ºä¼˜åŠ¿)
  - [ğŸ”¬ å½¢å¼åŒ–è¯æ˜ (Lean 4)](#-å½¢å¼åŒ–è¯æ˜-lean-4)
  - [ğŸ“š æ ¸å¿ƒå®šç†æ€»ç»“](#-æ ¸å¿ƒå®šç†æ€»ç»“)
  - [ğŸ“ ç›¸å…³è¯¾ç¨‹](#-ç›¸å…³è¯¾ç¨‹)
  - [ğŸ“– å‚è€ƒæ–‡çŒ®](#-å‚è€ƒæ–‡çŒ®)
  - [ğŸ”— ç›¸å…³æ–‡æ¡£](#-ç›¸å…³æ–‡æ¡£)
  - [âœï¸ ç»ƒä¹ ](#ï¸-ç»ƒä¹ )

---

## ğŸ“‹ æ ¸å¿ƒé—®é¢˜

**é€šç”¨é€¼è¿‘å®šç†**å›ç­”äº†æ·±åº¦å­¦ä¹ æœ€åŸºæœ¬çš„é—®é¢˜ï¼š

> **ç¥ç»ç½‘ç»œèƒ½è¡¨ç¤ºä»€ä¹ˆæ ·çš„å‡½æ•°ï¼Ÿ**

**æ ¸å¿ƒç»“è®º**ï¼š

- âœ… **å•éšå±‚ç¥ç»ç½‘ç»œ**å¯ä»¥é€¼è¿‘ä»»æ„è¿ç»­å‡½æ•°ï¼ˆåœ¨ç´§é›†ä¸Šï¼‰
- âš ï¸ ä½†éœ€è¦çš„**ç¥ç»å…ƒæ•°é‡**å¯èƒ½éšç»´åº¦æŒ‡æ•°å¢é•¿
- ğŸš€ **æ·±åº¦ç½‘ç»œ**èƒ½ä»¥æŒ‡æ•°çº§æ›´å°‘çš„å‚æ•°å®ç°ç›¸åŒé€¼è¿‘ç²¾åº¦

---

## ğŸ¯ ç»å…¸é€šç”¨é€¼è¿‘å®šç†

### 1. Cybenkoå®šç† (1989)

**å®šç† 1.1 (Cybenko, 1989)**:

è®¾ $\sigma : \mathbb{R} \to \mathbb{R}$ ä¸ºéå¸¸æ•°ã€æœ‰ç•Œã€å•è°ƒé€’å¢çš„è¿ç»­å‡½æ•°ï¼ˆå¦‚sigmoidï¼‰ã€‚åˆ™å¯¹äºä»»æ„ç´§é›† $K \subseteq \mathbb{R}^d$ï¼Œä»»æ„è¿ç»­å‡½æ•° $f : K \to \mathbb{R}$ï¼Œä»»æ„ $\epsilon > 0$ï¼Œå­˜åœ¨å•éšå±‚ç¥ç»ç½‘ç»œï¼š

$$
F(x) = \sum_{i=1}^{N} c_i \sigma(w_i^\top x + b_i)
$$

ä½¿å¾—ï¼š

$$
\sup_{x \in K} |F(x) - f(x)| < \epsilon
$$

---

**è¯æ˜æ€è·¯ï¼ˆä½¿ç”¨Hahn-Banachå®šç†ï¼‰**ï¼š

1. **åè¯æ³•**ï¼šå‡è®¾å•éšå±‚ç½‘ç»œæ„æˆçš„å‡½æ•°æ— $\mathcal{G}$ åœ¨ $C(K)$ ä¸­ä¸ç¨ å¯†

2. **æ³›å‡½åˆ†æ**ï¼šç”±Hahn-Banachå®šç†ï¼Œå­˜åœ¨éé›¶æœ‰ç•Œçº¿æ€§æ³›å‡½ $\mu \in C(K)^*$ ä½¿å¾—å¯¹æ‰€æœ‰ $g \in \mathcal{G}$ï¼Œ$\mu(g) = 0$

3. **Rieszè¡¨ç¤ºå®šç†**ï¼š$\mu$ å¯è¡¨ç¤ºä¸ºæœ‰ç•ŒBorelæµ‹åº¦

4. **Fourieråˆ†æ**ï¼šè¯æ˜å¯¹æ‰€æœ‰ $w, b$ï¼Œ$\int \sigma(w^\top x + b) d\mu(x) = 0$

5. **å¯¼å‡ºçŸ›ç›¾**ï¼šè¿™æ„å‘³ç€ $\mu$ å¿…é¡»æ˜¯é›¶æµ‹åº¦

---

### 2. Hornikå®šç† (1991)

**å®šç† 2.1 (Hornik, 1991)**:

Cybenkoçš„ç»“æœå¯æ¨å¹¿åˆ°**ä»»æ„éå¤šé¡¹å¼æ¿€æ´»å‡½æ•°**ã€‚è®¾ $\sigma$ ä¸ºéå¤šé¡¹å¼çš„è¿ç»­å‡½æ•°ï¼Œåˆ™å•éšå±‚ç½‘ç»œï¼š

$$
F(x) = \sum_{i=1}^{N} c_i \sigma(w_i^\top x + b_i)
$$

åœ¨ $C(K)$ ä¸­ç¨ å¯†ï¼ˆé…å¤‡ $\sup$ èŒƒæ•°ï¼‰ã€‚

**å…³é”®ç‚¹**ï¼š

- ä¸éœ€è¦ $\sigma$ æœ‰ç•Œæˆ–å•è°ƒ
- ReLU ($\sigma(z) = \max(0, z)$) æ»¡è¶³æ¡ä»¶

---

### 3. æ„é€ æ€§è¯æ˜æ€è·¯

**ç›´è§‰ï¼šåˆ©ç”¨ç¥ç»å…ƒå®ç°Bumpå‡½æ•°**:

**æ­¥éª¤**ï¼š

1. **æ„é€ Bumpå‡½æ•°**ï¼š
   ä½¿ç”¨ä¸¤ä¸ªsigmoidå¯æ„é€ å±€éƒ¨åŒ–çš„bumpï¼š
   $$
   \text{bump}(x; a, b) = \sigma(k(x-a)) - \sigma(k(x-b))
   $$
   å½“ $k \to \infty$ æ—¶ï¼Œè¶‹è¿‘äºæŒ‡ç¤ºå‡½æ•° $\mathbb{1}_{[a,b]}(x)$

2. **å¤šç»´æ¨å¹¿**ï¼š
   $$
   \text{bump}_d(x; \mathbf{a}, \mathbf{b}) = \prod_{i=1}^{d} \text{bump}(x_i; a_i, b_i)
   $$

3. **Riemannæ±‚å’Œ**ï¼š
   ç”¨bumpå‡½æ•°åœ¨ç½‘æ ¼ä¸Šå¯¹ $f$ è¿›è¡ŒRiemannæ±‚å’Œï¼š
   $$
   F(x) = \sum_{\text{grid}} f(x_{\text{grid}}) \cdot \text{bump}_d(x; \text{grid cell})
   $$

4. **è¯¯å·®åˆ†æ**ï¼š
   ç”± $f$ çš„è¿ç»­æ€§å’Œç½‘æ ¼ç»†åŒ–ï¼Œè¯¯å·® $\to 0$

**é—®é¢˜**ï¼šéœ€è¦çš„ç¥ç»å…ƒæ•°é‡ $N = O(\epsilon^{-d})$ï¼ˆç»´æ•°ç¾éš¾ï¼ï¼‰

---

## ğŸ—ï¸ æ·±åº¦çš„ä½œç”¨

### 1. å®½åº¦ vs æ·±åº¦

**å…³é”®é—®é¢˜**ï¼šä¸ºä»€ä¹ˆæ·±åº¦å­¦ä¹ ä¸­ä½¿ç”¨**æ·±åº¦**ç½‘ç»œï¼Œè€Œä¸æ˜¯**å®½åº¦**å¾ˆå¤§çš„æµ…å±‚ç½‘ç»œï¼Ÿ

**ç†è®ºç­”æ¡ˆ**ï¼šæ·±åº¦æä¾›**ç»„åˆæ€§**å’Œ**å±‚æ¬¡åŒ–è¡¨ç¤º**ã€‚

---

### 2. è¡¨ç¤ºæ•ˆç‡

**å®šä¹‰ 2.1 (è¡¨ç¤ºæ•ˆç‡)**:

å¯¹äºå‡½æ•°æ— $\mathcal{F}$ï¼Œè‹¥æ·±åº¦ä¸º $L$ çš„ç½‘ç»œåªéœ€ $N_L$ ä¸ªå‚æ•°å³å¯å®ç° $\epsilon$ ç²¾åº¦ï¼Œè€Œæ·±åº¦ä¸º $L' < L$ çš„ç½‘ç»œéœ€è¦ $N_{L'}$ ä¸ªå‚æ•°ï¼Œåˆ™æ·±åº¦å¸¦æ¥çš„**è¡¨ç¤ºæ•ˆç‡å¢ç›Š**ä¸ºï¼š

$$
\text{Gain}(L, L') = \frac{N_{L'}}{N_L}
$$

---

**ç¤ºä¾‹ 2.2 (ç»„åˆé€»è¾‘)**:

è€ƒè™‘ $d$ ä¸ªå¸ƒå°”å˜é‡çš„å¥‡å¶å‡½æ•° $f(x_1, \ldots, x_d) = x_1 \oplus x_2 \oplus \cdots \oplus x_d$ã€‚

- **æ·±åº¦ç½‘ç»œ** (æ ‘çŠ¶ç»“æ„)ï¼š$O(d)$ ä¸ªç¥ç»å…ƒï¼Œæ·±åº¦ $O(\log d)$
  
- **å•éšå±‚ç½‘ç»œ**ï¼šéœ€è¦ $O(2^d)$ ä¸ªç¥ç»å…ƒï¼ˆå¿…é¡»æšä¸¾æ‰€æœ‰å¥‡æ•°ä¸ª1çš„æƒ…å†µï¼‰

**è¡¨ç¤ºæ•ˆç‡å¢ç›Š**ï¼šæŒ‡æ•°çº§ï¼

---

### 3. æ·±åº¦åˆ†ç¦»å®šç†

**å®šç† 3.1 (Telgarsky, 2016)**:

å­˜åœ¨ä¸€æ—ä¸‰å±‚ReLUç½‘ç»œèƒ½è¡¨ç¤ºçš„å‡½æ•°ï¼Œä»»ä½•ä¸¤å±‚ReLUç½‘ç»œè¦å®ç°ç›¸åŒé€¼è¿‘ç²¾åº¦ï¼Œå®½åº¦å¿…é¡»æŒ‡æ•°çº§å¢é•¿ã€‚

**å…·ä½“æ„é€ **ï¼š

è€ƒè™‘"ä¸‰è§’æ³¢"å‡½æ•° $f : [0,1] \to [0,1]$ï¼Œå®šä¹‰ä¸ºï¼š
$$
f_k(x) = \text{triangle}_k(x)
$$
å…¶ä¸­ $\text{triangle}_k$ æ˜¯ $k$ å±‚æŠ˜å çš„ä¸‰è§’æ³¢ã€‚

- **æ·±åº¦ $k$ ç½‘ç»œ**ï¼š$O(k)$ ä¸ªç¥ç»å…ƒ
- **æ·±åº¦ 2 ç½‘ç»œ**ï¼šéœ€è¦ $\Omega(2^k)$ ä¸ªç¥ç»å…ƒ

---

**ç›´è§‰ç†è§£ï¼šç»„åˆæ€§**:

æ·±åº¦ç½‘ç»œé€šè¿‡**å±‚å±‚ç»„åˆ**ç®€å•ç‰¹å¾æ„å»ºå¤æ‚ç‰¹å¾ï¼š

```text
è¾“å…¥
  â†“
è¾¹ç¼˜æ£€æµ‹ (Layer 1)
  â†“
å±€éƒ¨æ¨¡å¼ (Layer 2)
  â†“
å¯¹è±¡éƒ¨ä»¶ (Layer 3)
  â†“
å®Œæ•´å¯¹è±¡ (Layer 4)
  â†“
åœºæ™¯ç†è§£ (Output)
```

æ¯ä¸€å±‚åªéœ€å¤„ç†å‰ä¸€å±‚çš„"é«˜çº§"è¡¨ç¤ºï¼Œé¿å…ç›´æ¥å¤„ç†åŸå§‹è¾“å…¥çš„å¤æ‚æ€§ã€‚

---

## ğŸ“Š ç°ä»£æ‰©å±•

### 1. ReLUç½‘ç»œ

**å®šç† 1.1 (Leshno et al., 1993)**:

ReLUç½‘ç»œ $\sigma(z) = \max(0, z)$ åŒæ ·å…·æœ‰é€šç”¨é€¼è¿‘æ€§è´¨ã€‚

**ä¼˜åŠ¿**ï¼š

- éé¥±å’Œï¼šæ¢¯åº¦ä¸æ¶ˆå¤±
- ç¨€ç–æ¿€æ´»ï¼šæé«˜è¡¨ç¤ºæ•ˆç‡
- åˆ†æ®µçº¿æ€§ï¼šé€¼è¿‘åˆ†ææ›´ç®€å•

---

**ReLUé€¼è¿‘çš„åˆ†æ**:

ReLUç½‘ç»œå¯ä»¥å®ç°**åˆ†æ®µçº¿æ€§å‡½æ•°**ã€‚

- $L$ å±‚ ReLU ç½‘ç»œï¼Œæ¯å±‚ $n$ ä¸ªç¥ç»å…ƒ
- å¯ä»¥è¡¨ç¤ºæœ€å¤š $O(n^L)$ ä¸ªçº¿æ€§åŒºåŸŸ
- åœ¨æ¯ä¸ªåŒºåŸŸå†…ï¼Œå‡½æ•°æ˜¯çº¿æ€§çš„

**é€¼è¿‘å…‰æ»‘å‡½æ•°**ï¼š
ç”¨è¶³å¤Ÿå¤šçš„çº¿æ€§ç‰‡æ®µé€¼è¿‘æ›²çº¿ï¼Œç±»ä¼¼å¤šè¾¹å½¢é€¼è¿‘åœ†ã€‚

---

### 2. å·ç§¯ç¥ç»ç½‘ç»œ

**å®šç† 2.1 (Zhou, 2020)**:

å·ç§¯ç¥ç»ç½‘ç»œ (CNN) å¯¹äº**å±€éƒ¨å¹³æ»‘**ã€å…·æœ‰**å¹³ç§»ä¸å˜æ€§**çš„å‡½æ•°æ—ï¼Œè¡¨ç¤ºæ•ˆç‡è¿œè¶…å…¨è¿æ¥ç½‘ç»œã€‚

**å…³é”®**ï¼š

- å‚æ•°å…±äº« â†’ æ ·æœ¬å¤æ‚åº¦é™ä½
- å±€éƒ¨è¿æ¥ â†’ åˆ©ç”¨ç©ºé—´ç»“æ„
- æ± åŒ– â†’ å±‚æ¬¡åŒ–æŠ½è±¡

---

### 3. Transformerä¸æ³¨æ„åŠ›æœºåˆ¶

**å®šç† 3.1 (Yun et al., 2020)**:

Transformeræ˜¯**å›¾çµå®Œå¤‡**çš„ï¼Œå¯ä»¥æ¨¡æ‹Ÿä»»æ„ç®—æ³•ï¼ˆç»™å®šè¶³å¤Ÿçš„å±‚æ•°å’Œå®½åº¦ï¼‰ã€‚

**å…³é”®æœºåˆ¶**ï¼š

- è‡ªæ³¨æ„åŠ› â†’ åŠ¨æ€åŠ æƒèšåˆ
- æ®‹å·®è¿æ¥ â†’ ä¿¡æ¯æµé€šç•…
- ä½ç½®ç¼–ç  â†’ åºåˆ—ä¿¡æ¯

**é€¼è¿‘æ€§è´¨**ï¼š
Transformerå¯ä»¥é€¼è¿‘ä»»æ„**åºåˆ—åˆ°åºåˆ—**çš„å‡½æ•°ï¼ˆåœ¨åˆé€‚çš„å‡½æ•°ç©ºé—´ä¸­ï¼‰ã€‚

---

## ğŸ”¬ é€¼è¿‘é€Ÿç‡ç†è®º

### 1. å‚æ•°æ•°é‡ä¸é€¼è¿‘è¯¯å·®

**å®šç† 1.1 (Barron, 1993)**:

å¯¹äºFourieré¢‘è°±è¡°å‡çš„å‡½æ•° $f : \mathbb{R}^d \to \mathbb{R}$ï¼ˆæ»¡è¶³ $\int |\omega| |\hat{f}(\omega)| d\omega < \infty$ï¼‰ï¼Œå•éšå±‚ç¥ç»ç½‘ç»œåªéœ€ $N$ ä¸ªç¥ç»å…ƒå³å¯è¾¾åˆ°ï¼š

$$
\mathbb{E}_{x \sim \mu}\left[(F_N(x) - f(x))^2\right] \leq O\left(\frac{C_f^2}{N}\right)
$$

å…¶ä¸­ $C_f = \int |\omega| |\hat{f}(\omega)| d\omega$ æ˜¯**Barronå¸¸æ•°**ã€‚

**é‡è¦æ€§**ï¼š

- è¯¯å·®è¡°å‡ç‡ $O(1/N)$ **ä¸ç»´åº¦ $d$ æ— å…³**ï¼
- æ‰“ç ´äº†ç»´æ•°ç¾éš¾ï¼ˆå¯¹äºç‰¹å®šå‡½æ•°ç±»ï¼‰

---

### 2. ç»´æ•°ç¾éš¾

**å®šç† 2.1 (DeVore et al., 1989)**:

å¯¹äºä¸€èˆ¬çš„ $s$ é˜¶å…‰æ»‘å‡½æ•°ï¼ˆ$s$ é˜¶å¯¼æ•°æœ‰ç•Œï¼‰ï¼Œè¦è¾¾åˆ° $\epsilon$ ç²¾åº¦ï¼Œéœ€è¦çš„å‚æ•°æ•°é‡ä¸ºï¼š

$$
N = \Omega\left(\epsilon^{-d/s}\right)
$$

**ç¤ºä¾‹**ï¼š

- $d = 100$ï¼Œ$s = 2$ï¼Œ$\epsilon = 0.01$
- $N \sim 10^{100}$ ï¼ˆä¸å¯è¡Œï¼ï¼‰

**æ·±åº¦å­¦ä¹ çš„å®é™…æˆåŠŸ**ï¼š

- çœŸå®ä¸–ç•Œçš„æ•°æ®ä¸æ˜¯ä¸€èˆ¬çš„ $d$ ç»´å‡½æ•°
- å…·æœ‰**ä½ç»´æµå½¢ç»“æ„**ã€**ç¨€ç–æ€§**ã€**ç»„åˆæ€§**
- æ·±åº¦ç½‘ç»œçš„å½’çº³åç½®æ­£å¥½åŒ¹é…è¿™äº›ç»“æ„

---

### 3. ç»„åˆæ€§ä¸å½’çº³åç½®

**å®šç† 3.1 (Poggio et al., 2017)**:

å¯¹äºå…·æœ‰**ç»„åˆç»“æ„**çš„å‡½æ•°ï¼ˆå¯ä»¥è¡¨ç¤ºä¸ºç®€å•å‡½æ•°çš„å±‚æ¬¡åŒ–ç»„åˆï¼‰ï¼Œæ·±åº¦ç½‘ç»œçš„å‚æ•°æ•°é‡å¯ä»¥æ˜¯ï¼š

$$
N = O(d \cdot L)
$$

è€Œæµ…å±‚ç½‘ç»œéœ€è¦ï¼š

$$
N = O(d^L)
$$

**ç¤ºä¾‹**ï¼š

- $d = 10$ï¼Œ$L = 5$
- æ·±åº¦ç½‘ç»œï¼š$N = 50$
- æµ…å±‚ç½‘ç»œï¼š$N = 100000$

---

## ğŸ¤– å®é™…æ„ä¹‰ä¸å±€é™

**é€šç”¨é€¼è¿‘å®šç†å‘Šè¯‰æˆ‘ä»¬ä»€ä¹ˆï¼Ÿ**

âœ… **ç†è®ºä¸Šçš„å¯èƒ½æ€§**ï¼š

- ç¥ç»ç½‘ç»œ**å¯ä»¥**è¡¨ç¤ºä»»æ„å‡½æ•°
- æ·±åº¦æä¾›è¡¨ç¤ºæ•ˆç‡

âŒ **æ²¡æœ‰å‘Šè¯‰æˆ‘ä»¬ä»€ä¹ˆï¼Ÿ**ï¼š

- å¦‚ä½•**æ‰¾åˆ°**å¥½çš„å‚æ•°ï¼ˆè®­ç»ƒç®—æ³•ï¼‰
- éœ€è¦å¤šå°‘**æ•°æ®**ï¼ˆæ³›åŒ–ï¼‰
- **å“ªäº›**å‡½æ•°å®¹æ˜“å­¦ä¹ 

---

**å±€é™ä¸å¼€æ”¾é—®é¢˜**ï¼š

1. **å­˜åœ¨æ€§ vs å¯å­¦ä¹ æ€§**ï¼š
   - é€šç”¨é€¼è¿‘ â‰  èƒ½ä»æ•°æ®ä¸­å­¦åˆ°
   - éœ€è¦ç»“åˆä¼˜åŒ–ç†è®ºå’Œæ³›åŒ–ç†è®º

2. **è¡¨ç¤º vs æ³›åŒ–**ï¼š
   - è¿‡å‚æ•°åŒ–ç½‘ç»œå¯ä»¥è®°ä½æ‰€æœ‰è®­ç»ƒæ•°æ®
   - ä½†ä»èƒ½æ³›åŒ–ï¼ˆåŒä¸‹é™ç°è±¡ã€éšå¼æ­£åˆ™åŒ–ï¼‰

3. **æ·±åº¦çš„å¿…è¦æ€§**ï¼š
   - ä½•æ—¶æ·±åº¦æ˜¯**å¿…éœ€**çš„ï¼Ÿ
   - ä½•æ—¶å®½åº¦å°±è¶³å¤Ÿï¼Ÿ

---

## ğŸ’» Pythonå®ç°

### 1. å¯è§†åŒ–å•éšå±‚ç½‘ç»œé€¼è¿‘

```python
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim

# 1. ç›®æ ‡å‡½æ•°
def target_function(x):
    """å¤æ‚çš„ç›®æ ‡å‡½æ•°"""
    return np.sin(2 * np.pi * x) + 0.5 * np.sin(8 * np.pi * x)

# 2. å•éšå±‚ç¥ç»ç½‘ç»œ
class SingleLayerNet(nn.Module):
    def __init__(self, hidden_size):
        super().__init__()
        self.hidden = nn.Linear(1, hidden_size)
        self.output = nn.Linear(hidden_size, 1)
        self.activation = nn.Tanh()  # Sigmoidçš„æ›¿ä»£
    
    def forward(self, x):
        h = self.activation(self.hidden(x))
        return self.output(h)

# 3. è®­ç»ƒç½‘ç»œ
def train_network(hidden_size, epochs=5000):
    # æ•°æ®
    x_train = np.linspace(0, 1, 100).reshape(-1, 1)
    y_train = target_function(x_train)
    
    x_tensor = torch.FloatTensor(x_train)
    y_tensor = torch.FloatTensor(y_train)
    
    # æ¨¡å‹
    model = SingleLayerNet(hidden_size)
    optimizer = optim.Adam(model.parameters(), lr=0.01)
    criterion = nn.MSELoss()
    
    # è®­ç»ƒ
    for epoch in range(epochs):
        optimizer.zero_grad()
        y_pred = model(x_tensor)
        loss = criterion(y_pred, y_tensor)
        loss.backward()
        optimizer.step()
        
        if (epoch + 1) % 1000 == 0:
            print(f"Epoch {epoch+1}, Loss: {loss.item():.6f}")
    
    return model

# 4. å¯è§†åŒ–ä¸åŒç¥ç»å…ƒæ•°é‡çš„é€¼è¿‘æ•ˆæœ
def visualize_approximation():
    x_plot = np.linspace(0, 1, 200).reshape(-1, 1)
    y_true = target_function(x_plot)
    
    hidden_sizes = [5, 10, 20, 50]
    
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    axes = axes.flatten()
    
    for idx, h_size in enumerate(hidden_sizes):
        print(f"\nTraining with {h_size} hidden neurons...")
        model = train_network(h_size, epochs=3000)
        
        # é¢„æµ‹
        with torch.no_grad():
            x_tensor = torch.FloatTensor(x_plot)
            y_pred = model(x_tensor).numpy()
        
        # ç»˜å›¾
        ax = axes[idx]
        ax.plot(x_plot, y_true, 'b-', linewidth=2, label='True function')
        ax.plot(x_plot, y_pred, 'r--', linewidth=2, label='NN approximation')
        ax.set_title(f'Hidden neurons: {h_size}', fontsize=12)
        ax.set_xlabel('x')
        ax.set_ylabel('y')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # è®¡ç®—è¯¯å·®
        mse = np.mean((y_true - y_pred)**2)
        ax.text(0.05, 0.95, f'MSE: {mse:.4f}', 
                transform=ax.transAxes, fontsize=10,
                verticalalignment='top', bbox=dict(boxstyle='round', 
                facecolor='wheat', alpha=0.5))
    
    plt.tight_layout()
    plt.savefig('universal_approximation.png', dpi=150)
    plt.show()

# visualize_approximation()
```

---

### 2. æ·±åº¦ç½‘ç»œçš„è¡¨ç¤ºä¼˜åŠ¿

```python
import torch
import torch.nn as nn
import matplotlib.pyplot as plt

# æ„é€ éœ€è¦ç»„åˆæ€§çš„å‡½æ•°: åµŒå¥—ç»å¯¹å€¼
def compositional_function(x):
    """f(x) = |x - 0.5| - |x - 0.3|"""
    return np.abs(x - 0.5) - np.abs(x - 0.3)

# æµ…å±‚ç½‘ç»œ
class ShallowNet(nn.Module):
    def __init__(self, width):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(1, width),
            nn.ReLU(),
            nn.Linear(width, 1)
        )
    
    def forward(self, x):
        return self.net(x)

# æ·±åº¦ç½‘ç»œ
class DeepNet(nn.Module):
    def __init__(self, width, depth):
        super().__init__()
        layers = [nn.Linear(1, width), nn.ReLU()]
        for _ in range(depth - 2):
            layers.extend([nn.Linear(width, width), nn.ReLU()])
        layers.append(nn.Linear(width, 1))
        self.net = nn.Sequential(*layers)
    
    def forward(self, x):
        return self.net(x)

# è®­ç»ƒå¹¶æ¯”è¾ƒ
def compare_depth_vs_width():
    x_train = np.linspace(0, 1, 200).reshape(-1, 1)
    y_train = compositional_function(x_train)
    
    x_tensor = torch.FloatTensor(x_train)
    y_tensor = torch.FloatTensor(y_train)
    
    # 1. æµ…å±‚å®½ç½‘ç»œ
    shallow = ShallowNet(width=50)
    
    # 2. æ·±å±‚çª„ç½‘ç»œ
    deep = DeepNet(width=10, depth=4)
    
    print(f"Shallow net parameters: {sum(p.numel() for p in shallow.parameters())}")
    print(f"Deep net parameters: {sum(p.numel() for p in deep.parameters())}")
    
    # è®­ç»ƒ
    models = {'Shallow (width=50)': shallow, 'Deep (width=10, depth=4)': deep}
    results = {}
    
    for name, model in models.items():
        print(f"\nTraining {name}...")
        optimizer = optim.Adam(model.parameters(), lr=0.01)
        criterion = nn.MSELoss()
        
        for epoch in range(2000):
            optimizer.zero_grad()
            y_pred = model(x_tensor)
            loss = criterion(y_pred, y_tensor)
            loss.backward()
            optimizer.step()
        
        with torch.no_grad():
            y_pred = model(x_tensor).numpy()
        
        results[name] = y_pred
    
    # å¯è§†åŒ–
    plt.figure(figsize=(12, 6))
    plt.plot(x_train, y_train, 'k-', linewidth=2, label='True function')
    for name, y_pred in results.items():
        plt.plot(x_train, y_pred, '--', linewidth=2, label=name)
    
    plt.xlabel('x', fontsize=12)
    plt.ylabel('y', fontsize=12)
    plt.title('Depth vs Width: Approximating Compositional Functions', fontsize=14)
    plt.legend(fontsize=11)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig('depth_vs_width.png', dpi=150)
    plt.show()

# compare_depth_vs_width()
```

---

## ğŸ”¬ å½¢å¼åŒ–è¯æ˜ (Lean 4)

```lean
import Mathlib.Analysis.NormedSpace.Basic
import Mathlib.Topology.Instances.Real
import Mathlib.MeasureTheory.Integral.Bochner

-- é€šç”¨é€¼è¿‘å®šç†çš„å½¢å¼åŒ–æ¡†æ¶

-- æ¿€æ´»å‡½æ•°
structure ActivationFunction where
  Ïƒ : â„ â†’ â„
  continuous : Continuous Ïƒ
  nonpolynomial : Â¬ âˆƒ (p : Polynomial â„), âˆ€ x, Ïƒ x = p.eval x

-- å•éšå±‚ç¥ç»ç½‘ç»œ
structure SingleLayerNetwork (d : â„•) (n : â„•) where
  weights : Fin n â†’ â„^d
  biases : Fin n â†’ â„
  output_weights : Fin n â†’ â„
  activation : ActivationFunction

-- ç½‘ç»œçš„å‰å‘ä¼ æ’­
def SingleLayerNetwork.forward {d n : â„•} 
  (net : SingleLayerNetwork d n) (x : â„^d) : â„ :=
  âˆ‘ i, net.output_weights i * net.activation.Ïƒ (âŸ¨net.weights i, xâŸ© + net.biases i)

-- é€šç”¨é€¼è¿‘å®šç†é™ˆè¿°
theorem universal_approximation_theorem
  {d : â„•} (K : Set (â„^d)) (hK : IsCompact K)
  (f : C(K, â„)) (Îµ : â„) (hÎµ : 0 < Îµ)
  (Ïƒ : ActivationFunction) :
  âˆƒ (n : â„•) (net : SingleLayerNetwork d n),
    âˆ€ x âˆˆ K, |net.forward x - f x| < Îµ := by
  sorry

-- æ·±åº¦ç½‘ç»œ
structure DeepNetwork (d : â„•) (architecture : List â„•) where
  layers : âˆ€ (i : Fin architecture.length),
    Matrix (architecture.get i) (architecture.get (i+1)) â„
  activation : ActivationFunction

-- æ·±åº¦åˆ†ç¦»å®šç†
theorem depth_separation_theorem :
  âˆƒ (f : â„ â†’ â„),
    -- æ·±åº¦ç½‘ç»œå¯ç”¨ O(k) å‚æ•°è¡¨ç¤º
    (âˆƒ (net_deep : DeepNetwork 1 [10, 10, 10]),  -- 3å±‚
      âˆ€ x, |net_deep.forward x - f x| < 0.01) âˆ§
    -- æµ…å±‚ç½‘ç»œéœ€è¦æŒ‡æ•°å¤šå‚æ•°
    (âˆ€ (net_shallow : SingleLayerNetwork 1 n),
      n < 2^10 â†’
      âˆƒ x, |net_shallow.forward x - f x| â‰¥ 0.01) := by
  sorry
```

---

## ğŸ“š æ ¸å¿ƒå®šç†æ€»ç»“

| å®šç† | ç»“è®º | æ„ä¹‰ |
|------|------|------|
| **Cybenko (1989)** | å•éšå±‚sigmoidç½‘ç»œé€šç”¨é€¼è¿‘ | ç¥ç»ç½‘ç»œçš„ç†è®ºåŸºç¡€ |
| **Hornik (1991)** | æ¨å¹¿åˆ°ä»»æ„éå¤šé¡¹å¼æ¿€æ´» | ReLUä¹Ÿæœ‰æ•ˆ |
| **Barron (1993)** | Fourierå…‰æ»‘å‡½æ•° $O(1/N)$ é€¼è¿‘ç‡ | æ‰“ç ´ç»´æ•°ç¾éš¾ï¼ˆç‰¹å®šå‡½æ•°ç±»ï¼‰ |
| **Telgarsky (2016)** | æ·±åº¦åˆ†ç¦»å®šç† | æ·±åº¦çš„æŒ‡æ•°ä¼˜åŠ¿ |
| **Yun et al. (2020)** | Transformerå›¾çµå®Œå¤‡ | åºåˆ—å»ºæ¨¡çš„ç†è®ºåŸºç¡€ |

---

## ğŸ“ ç›¸å…³è¯¾ç¨‹

| å¤§å­¦ | è¯¾ç¨‹ | è¦†ç›–å†…å®¹ |
|------|------|----------|
| **MIT** | 6.883 Computational Learning Theory | é€šç”¨é€¼è¿‘ã€VCç»´ã€æ·±åº¦ä½œç”¨ |
| **Stanford** | CS229 Machine Learning | ç¥ç»ç½‘ç»œç†è®ºåŸºç¡€ |
| **Stanford** | CS236 Deep Generative Models | æ·±åº¦ç½‘ç»œè¡¨ç¤ºèƒ½åŠ› |
| **CMU** | 10-715 Advanced ML Theory | é€¼è¿‘ç†è®ºã€ä¼˜åŒ–ç†è®º |
| **NYU** | DS-GA 1008 Deep Learning | Yann LeCunè®²æˆï¼Œç†è®ºä¸å®è·µ |

---

## ğŸ“– å‚è€ƒæ–‡çŒ®

1. **Cybenko, G. (1989)**. "Approximation by Superpositions of a Sigmoidal Function". *Mathematics of Control, Signals and Systems*.

2. **Hornik, K. (1991)**. "Approximation Capabilities of Multilayer Feedforward Networks". *Neural Networks*.

3. **Barron, A. R. (1993)**. "Universal Approximation Bounds for Superpositions of a Sigmoidal Function". *IEEE Transactions on Information Theory*.

4. **Telgarsky, M. (2016)**. "Benefits of Depth in Neural Networks". *COLT*.

5. **Poggio, T. et al. (2017)**. "Why and When Can Deep Networks Avoid the Curse of Dimensionality?" *PNAS*.

6. **Yun, C. et al. (2020)**. "Are Transformers Universal Approximators of Sequence-to-Sequence Functions?" *ICLR*.

---

## ğŸ”— ç›¸å…³æ–‡æ¡£

- [åå‘ä¼ æ’­ç®—æ³•](02-Backpropagation.md)
- [ç¥ç»æ­£åˆ‡æ ¸ç†è®º](03-Neural-Tangent-Kernel.md)
- [VCç»´ä¸Rademacherå¤æ‚åº¦](../01-Statistical-Learning/02-VC-Dimension-Rademacher-Complexity.md)
- [Transformeræ•°å­¦åŸç†](../../04-Frontiers/01-LLM-Theory/01-Transformer-Mathematics.md)

---

## âœï¸ ç»ƒä¹ 

**ç»ƒä¹  1 (åŸºç¡€)**ï¼šè¯æ˜ $\mathbb{R}$ ä¸Šçš„ä»»æ„é˜¶è·ƒå‡½æ•°å¯ä»¥ç”¨æœ‰é™ä¸ªsigmoidå‡½æ•°çš„çº¿æ€§ç»„åˆé€¼è¿‘ã€‚

**ç»ƒä¹  2 (ä¸­ç­‰)**ï¼šå®ç°ä¸€ä¸ªå•éšå±‚ç¥ç»ç½‘ç»œï¼Œé€¼è¿‘ $f(x) = x^3 - 3x^2 + 2x$ åœ¨ $[0, 2]$ ä¸Šã€‚å¯è§†åŒ–ä¸åŒéšå±‚å®½åº¦çš„æ•ˆæœã€‚

**ç»ƒä¹  3 (ä¸­ç­‰)**ï¼šæ„é€ ä¸€ä¸ªæ·±åº¦ReLUç½‘ç»œï¼Œç”¨æœ€å°‘çš„å‚æ•°ç²¾ç¡®è¡¨ç¤º $f(x) = |x - 0.5|$ã€‚

**ç»ƒä¹  4 (å›°éš¾)**ï¼šè¯æ˜å¥‡å¶å‡½æ•° $\bigoplus_{i=1}^d x_i$ éœ€è¦ $\Omega(2^d)$ ä¸ªå•éšå±‚ç¥ç»å…ƒã€‚

**ç»ƒä¹  5 (ç ”ç©¶)**ï¼šé˜…è¯»Poggioç­‰äººå…³äºç»´æ•°ç¾éš¾çš„è®ºæ–‡ï¼Œç†è§£"ç»„åˆæ€§å‡è®¾"ã€‚

**ç»ƒä¹  6 (å®è·µ)**ï¼šåœ¨MNISTæ•°æ®é›†ä¸Šæ¯”è¾ƒä¸åŒæ·±åº¦å’Œå®½åº¦çš„ç½‘ç»œï¼Œè®°å½•å‚æ•°æ•°é‡ã€è®­ç»ƒæ—¶é—´å’Œå‡†ç¡®ç‡ã€‚

---

*æœ€åæ›´æ–°ï¼š2025å¹´10æœˆ*-
