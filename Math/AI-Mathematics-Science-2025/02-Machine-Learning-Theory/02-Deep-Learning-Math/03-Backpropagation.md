# åå‘ä¼ æ’­ç®—æ³•

> **Backpropagation Algorithm**
>
> æ·±åº¦å­¦ä¹ çš„æ ¸å¿ƒï¼šé«˜æ•ˆè®¡ç®—æ¢¯åº¦

---

## ç›®å½•

- [åå‘ä¼ æ’­ç®—æ³•](#åå‘ä¼ æ’­ç®—æ³•)
  - [ç›®å½•](#ç›®å½•)
  - [ğŸ“‹ æ ¸å¿ƒæ€æƒ³](#-æ ¸å¿ƒæ€æƒ³)
  - [ğŸ¯ ç®—æ³•æ¨å¯¼](#-ç®—æ³•æ¨å¯¼)
    - [1. å‰å‘ä¼ æ’­](#1-å‰å‘ä¼ æ’­)
    - [2. åå‘ä¼ æ’­](#2-åå‘ä¼ æ’­)
    - [3. è®¡ç®—å¤æ‚åº¦](#3-è®¡ç®—å¤æ‚åº¦)
  - [ğŸ“Š çŸ©é˜µå½¢å¼](#-çŸ©é˜µå½¢å¼)
  - [ğŸ”§ è‡ªåŠ¨å¾®åˆ†](#-è‡ªåŠ¨å¾®åˆ†)
  - [ğŸ’» Pythonå®ç°](#-pythonå®ç°)
  - [ğŸ“š æ ¸å¿ƒè¦ç‚¹](#-æ ¸å¿ƒè¦ç‚¹)
  - [ğŸ“ ç›¸å…³è¯¾ç¨‹](#-ç›¸å…³è¯¾ç¨‹)
  - [ğŸ“– å‚è€ƒæ–‡çŒ®](#-å‚è€ƒæ–‡çŒ®)

---

## ğŸ“‹ æ ¸å¿ƒæ€æƒ³

**åå‘ä¼ æ’­ (Backpropagation)** æ˜¯é«˜æ•ˆè®¡ç®—ç¥ç»ç½‘ç»œæ¢¯åº¦çš„ç®—æ³•ã€‚

**æ ¸å¿ƒ**ï¼š

- åˆ©ç”¨**é“¾å¼æ³•åˆ™**é€’å½’è®¡ç®—æ¢¯åº¦
- æ—¶é—´å¤æ‚åº¦ä¸å‰å‘ä¼ æ’­ç›¸åŒ
- ä½¿æ·±åº¦å­¦ä¹ æˆä¸ºå¯èƒ½

---

## ğŸ¯ ç®—æ³•æ¨å¯¼

### 1. å‰å‘ä¼ æ’­

è€ƒè™‘ $L$ å±‚å…¨è¿æ¥ç½‘ç»œï¼š

$$
\begin{align}
z^{(\ell)} &= W^{(\ell)} a^{(\ell-1)} + b^{(\ell)} \\
a^{(\ell)} &= \sigma(z^{(\ell)})
\end{align}
$$

å…¶ä¸­ $a^{(0)} = x$ æ˜¯è¾“å…¥ã€‚

---

### 2. åå‘ä¼ æ’­

**ç›®æ ‡**ï¼šè®¡ç®— $\frac{\partial L}{\partial W^{(\ell)}}$ å’Œ $\frac{\partial L}{\partial b^{(\ell)}}$ã€‚

**å®šä¹‰è¯¯å·®é¡¹**ï¼š

$$
\delta^{(\ell)} = \frac{\partial L}{\partial z^{(\ell)}}
$$

**é€’å½’å…¬å¼**ï¼š

$$
\delta^{(\ell)} = (W^{(\ell+1)})^\top \delta^{(\ell+1)} \odot \sigma'(z^{(\ell)})
$$

**æ¢¯åº¦**ï¼š

$$
\frac{\partial L}{\partial W^{(\ell)}} = \delta^{(\ell)} (a^{(\ell-1)})^\top
$$

$$
\frac{\partial L}{\partial b^{(\ell)}} = \delta^{(\ell)}
$$

---

### 3. è®¡ç®—å¤æ‚åº¦

- **å‰å‘ä¼ æ’­**ï¼š$O(W)$ï¼ˆ$W$ æ˜¯å‚æ•°æ•°é‡ï¼‰
- **åå‘ä¼ æ’­**ï¼š$O(W)$

**å…³é”®**ï¼šåªéœ€å‰å‘ä¼ æ’­çš„2å€æ—¶é—´ï¼

---

## ğŸ“Š çŸ©é˜µå½¢å¼

**æ‰¹é‡å¤„ç†**ï¼š

è¾“å…¥æ‰¹é‡ $X \in \mathbb{R}^{n \times d}$ï¼ˆ$n$ ä¸ªæ ·æœ¬ï¼‰ï¼š

$$
Z^{(\ell)} = A^{(\ell-1)} (W^{(\ell)})^\top + \mathbf{1} (b^{(\ell)})^\top
$$

$$
\Delta^{(\ell)} = \Delta^{(\ell+1)} W^{(\ell+1)} \odot \sigma'(Z^{(\ell)})
$$

$$
\frac{\partial L}{\partial W^{(\ell)}} = (\Delta^{(\ell)})^\top A^{(\ell-1)}
$$

---

## ğŸ”§ è‡ªåŠ¨å¾®åˆ†

**ç°ä»£æ¡†æ¶** (PyTorch, TensorFlow) ä½¿ç”¨**è‡ªåŠ¨å¾®åˆ†**ï¼š

- **å‰å‘æ¨¡å¼**ï¼šè®¡ç®—æ–¹å‘å¯¼æ•°
- **åå‘æ¨¡å¼**ï¼šåå‘ä¼ æ’­çš„æ³›åŒ–

**è®¡ç®—å›¾**ï¼š

```text
x â†’ fâ‚ â†’ yâ‚ â†’ fâ‚‚ â†’ yâ‚‚ â†’ ... â†’ L
```

**åå‘éå†**ï¼šä» $L$ åˆ° $x$ è®¡ç®—æ‰€æœ‰æ¢¯åº¦ã€‚

---

## ğŸ’» Pythonå®ç°

```python
import numpy as np

class NeuralNetwork:
    def __init__(self, layer_sizes):
        self.L = len(layer_sizes) - 1
        self.W = []
        self.b = []
        
        for i in range(self.L):
            W = np.random.randn(layer_sizes[i+1], layer_sizes[i]) * 0.01
            b = np.zeros((layer_sizes[i+1], 1))
            self.W.append(W)
            self.b.append(b)
    
    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))
    
    def sigmoid_derivative(self, z):
        s = self.sigmoid(z)
        return s * (1 - s)
    
    def forward(self, X):
        """å‰å‘ä¼ æ’­"""
        self.Z = []
        self.A = [X]
        
        for l in range(self.L):
            Z = self.W[l] @ self.A[l] + self.b[l]
            A = self.sigmoid(Z)
            self.Z.append(Z)
            self.A.append(A)
        
        return self.A[-1]
    
    def backward(self, X, Y):
        """åå‘ä¼ æ’­"""
        m = X.shape[1]
        
        # è¾“å‡ºå±‚è¯¯å·®
        dZ = self.A[-1] - Y
        
        # å­˜å‚¨æ¢¯åº¦
        dW = []
        db = []
        
        # åå‘éå†
        for l in reversed(range(self.L)):
            dW_l = (1/m) * dZ @ self.A[l].T
            db_l = (1/m) * np.sum(dZ, axis=1, keepdims=True)
            
            dW.insert(0, dW_l)
            db.insert(0, db_l)
            
            if l > 0:
                dZ = (self.W[l].T @ dZ) * self.sigmoid_derivative(self.Z[l-1])
        
        return dW, db
    
    def train(self, X, Y, epochs=1000, lr=0.01):
        """è®­ç»ƒ"""
        for epoch in range(epochs):
            # å‰å‘
            Y_pred = self.forward(X)
            
            # åå‘
            dW, db = self.backward(X, Y)
            
            # æ›´æ–°
            for l in range(self.L):
                self.W[l] -= lr * dW[l]
                self.b[l] -= lr * db[l]
            
            if epoch % 100 == 0:
                loss = np.mean((Y_pred - Y)**2)
                print(f"Epoch {epoch}, Loss: {loss:.6f}")

# ç¤ºä¾‹
X = np.random.randn(2, 100)
Y = (X[0] + X[1] > 0).astype(float).reshape(1, -1)

nn = NeuralNetwork([2, 4, 1])
nn.train(X, Y, epochs=1000, lr=0.5)
```

---

## ğŸ“š æ ¸å¿ƒè¦ç‚¹

| æ¦‚å¿µ | è¯´æ˜ |
|------|------|
| **é“¾å¼æ³•åˆ™** | $\frac{\partial L}{\partial w} = \frac{\partial L}{\partial z} \frac{\partial z}{\partial w}$ |
| **è¯¯å·®é¡¹** | $\delta^{(\ell)} = \frac{\partial L}{\partial z^{(\ell)}}$ |
| **é€’å½’** | ä»è¾“å‡ºå±‚åˆ°è¾“å…¥å±‚ä¼ æ’­è¯¯å·® |
| **æ•ˆç‡** | $O(W)$ æ—¶é—´å¤æ‚åº¦ |

---

## ğŸ“ ç›¸å…³è¯¾ç¨‹

| å¤§å­¦ | è¯¾ç¨‹ |
|------|------|
| **Stanford** | CS231n CNN for Visual Recognition |
| **MIT** | 6.036 Introduction to Machine Learning |
| **DeepLearning.AI** | Deep Learning Specialization |

---

## ğŸ“– å‚è€ƒæ–‡çŒ®

1. **Rumelhart et al. (1986)**. "Learning Representations by Back-propagating Errors". *Nature*.

2. **Goodfellow et al. (2016)**. *Deep Learning*. MIT Press.

---

*æœ€åæ›´æ–°ï¼š2025å¹´10æœˆ*-
