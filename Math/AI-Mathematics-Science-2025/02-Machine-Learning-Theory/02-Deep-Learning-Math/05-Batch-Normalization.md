# æ‰¹å½’ä¸€åŒ– (Batch Normalization) ç†è®º

> **Batch Normalization: Theory and Mathematics**
>
> æ·±åº¦å­¦ä¹ è®­ç»ƒåŠ é€Ÿçš„å…³é”®æŠ€æœ¯

---

## ç›®å½•

- [æ‰¹å½’ä¸€åŒ– (Batch Normalization) ç†è®º](#æ‰¹å½’ä¸€åŒ–-batch-normalization-ç†è®º)
  - [ç›®å½•](#ç›®å½•)
  - [ğŸ“‹ æ ¸å¿ƒæ€æƒ³](#-æ ¸å¿ƒæ€æƒ³)
  - [ğŸ¯ Internal Covariate Shift](#-internal-covariate-shift)
    - [1. é—®é¢˜å®šä¹‰](#1-é—®é¢˜å®šä¹‰)
    - [2. å½±å“åˆ†æ](#2-å½±å“åˆ†æ)
  - [ğŸ“Š æ‰¹å½’ä¸€åŒ–ç®—æ³•](#-æ‰¹å½’ä¸€åŒ–ç®—æ³•)
    - [1. è®­ç»ƒæ—¶çš„BN](#1-è®­ç»ƒæ—¶çš„bn)
    - [2. æ¨ç†æ—¶çš„BN](#2-æ¨ç†æ—¶çš„bn)
    - [3. å¯å­¦ä¹ å‚æ•°](#3-å¯å­¦ä¹ å‚æ•°)
  - [ğŸ”¬ æ•°å­¦åˆ†æ](#-æ•°å­¦åˆ†æ)
    - [1. å½’ä¸€åŒ–çš„ä½œç”¨](#1-å½’ä¸€åŒ–çš„ä½œç”¨)
    - [2. æ¢¯åº¦æµæ”¹å–„](#2-æ¢¯åº¦æµæ”¹å–„)
    - [3. æ­£åˆ™åŒ–æ•ˆæœ](#3-æ­£åˆ™åŒ–æ•ˆæœ)
  - [ğŸ’» åå‘ä¼ æ’­æ¨å¯¼](#-åå‘ä¼ æ’­æ¨å¯¼)
    - [1. å‰å‘ä¼ æ’­](#1-å‰å‘ä¼ æ’­)
    - [2. åå‘ä¼ æ’­](#2-åå‘ä¼ æ’­)
  - [ğŸ¨ Pythonå®ç°](#-pythonå®ç°)
  - [ğŸ“š ç†è®ºæ·±åŒ–](#-ç†è®ºæ·±åŒ–)
    - [1. BNçš„çœŸæ­£ä½œç”¨](#1-bnçš„çœŸæ­£ä½œç”¨)
    - [2. ä¼˜åŒ–æ™¯è§‚å¹³æ»‘åŒ–](#2-ä¼˜åŒ–æ™¯è§‚å¹³æ»‘åŒ–)
    - [3. ä¸å…¶ä»–å½’ä¸€åŒ–çš„å…³ç³»](#3-ä¸å…¶ä»–å½’ä¸€åŒ–çš„å…³ç³»)
  - [ğŸ”§ å½’ä¸€åŒ–å˜ä½“](#-å½’ä¸€åŒ–å˜ä½“)
    - [1. Layer Normalization](#1-layer-normalization)
    - [2. Instance Normalization](#2-instance-normalization)
    - [3. Group Normalization](#3-group-normalization)
  - [ğŸ“ ç›¸å…³è¯¾ç¨‹](#-ç›¸å…³è¯¾ç¨‹)
  - [ğŸ“– å‚è€ƒæ–‡çŒ®](#-å‚è€ƒæ–‡çŒ®)

---

## ğŸ“‹ æ ¸å¿ƒæ€æƒ³

**Batch Normalization (BN)** é€šè¿‡å½’ä¸€åŒ–å±‚çš„è¾“å…¥æ¥åŠ é€Ÿè®­ç»ƒã€‚

**æ ¸å¿ƒæ“ä½œ**ï¼š

```text
è¾“å…¥ x â†’ å½’ä¸€åŒ– â†’ (x - Î¼) / Ïƒ â†’ ç¼©æ”¾å¹³ç§» â†’ Î³xÌ‚ + Î² â†’ è¾“å‡º
```

**ä¸»è¦æ•ˆæœ**ï¼š

- åŠ é€Ÿè®­ç»ƒï¼ˆå¯ç”¨æ›´å¤§å­¦ä¹ ç‡ï¼‰
- ç¼“è§£æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸
- æ­£åˆ™åŒ–æ•ˆæœï¼ˆå‡å°‘å¯¹Dropoutçš„ä¾èµ–ï¼‰
- å…è®¸æ›´æ·±çš„ç½‘ç»œ

---

## ğŸ¯ Internal Covariate Shift

### 1. é—®é¢˜å®šä¹‰

**å®šä¹‰ 1.1 (Internal Covariate Shift, ICS)**:

è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œç½‘ç»œå†…éƒ¨å±‚çš„è¾“å…¥åˆ†å¸ƒä¸æ–­å˜åŒ–ã€‚

**åŸå› **ï¼š

- å‰å±‚å‚æ•°æ›´æ–°
- å¯¼è‡´åå±‚è¾“å…¥åˆ†å¸ƒæ”¹å˜
- åå±‚éœ€è¦ä¸æ–­é€‚åº”æ–°åˆ†å¸ƒ

**ç±»æ¯”**ï¼š

```text
ç¬¬lå±‚å­¦ä¹ : f(x) where x ~ D_old
å‚æ•°æ›´æ–°å: x ~ D_new (åˆ†å¸ƒæ”¹å˜!)
ç¬¬lå±‚éœ€è¦é‡æ–°é€‚åº”
```

---

### 2. å½±å“åˆ†æ

**é—®é¢˜1ï¼šè®­ç»ƒç¼“æ…¢**:

- æ¯å±‚éƒ½åœ¨è¿½é€ç§»åŠ¨çš„ç›®æ ‡
- éœ€è¦å°å­¦ä¹ ç‡ä»¥ä¿æŒç¨³å®š

**é—®é¢˜2ï¼šæ¢¯åº¦é—®é¢˜**:

- è¾“å…¥åˆ†å¸ƒå˜åŒ–å¯¼è‡´æ¢¯åº¦ä¸ç¨³å®š
- å¯èƒ½è¿›å…¥é¥±å’ŒåŒºï¼ˆå¦‚sigmoidï¼‰

**é—®é¢˜3ï¼šåˆå§‹åŒ–æ•æ„Ÿ**:

- åˆ†å¸ƒå˜åŒ–æ”¾å¤§åˆå§‹åŒ–çš„å½±å“
- éœ€è¦ç²¾å¿ƒè®¾è®¡åˆå§‹åŒ–

---

## ğŸ“Š æ‰¹å½’ä¸€åŒ–ç®—æ³•

### 1. è®­ç»ƒæ—¶çš„BN

**ç®—æ³• 1.1 (Batch Normalization - Training)**:

**è¾“å…¥**ï¼š

- å°æ‰¹é‡ $\mathcal{B} = \{x_1, \ldots, x_m\}$
- å¯å­¦ä¹ å‚æ•° $\gamma, \beta$

**æ­¥éª¤**ï¼š

1. **è®¡ç®—å‡å€¼**ï¼š
   $$
   \mu_{\mathcal{B}} = \frac{1}{m} \sum_{i=1}^{m} x_i
   $$

2. **è®¡ç®—æ–¹å·®**ï¼š
   $$
   \sigma_{\mathcal{B}}^2 = \frac{1}{m} \sum_{i=1}^{m} (x_i - \mu_{\mathcal{B}})^2
   $$

3. **å½’ä¸€åŒ–**ï¼š
   $$
   \hat{x}_i = \frac{x_i - \mu_{\mathcal{B}}}{\sqrt{\sigma_{\mathcal{B}}^2 + \epsilon}}
   $$

4. **ç¼©æ”¾å’Œå¹³ç§»**ï¼š
   $$
   y_i = \gamma \hat{x}_i + \beta = \text{BN}_{\gamma, \beta}(x_i)
   $$

**æ³¨æ„**ï¼š$\epsilon$ æ˜¯å°å¸¸æ•°ï¼ˆå¦‚ $10^{-5}$ï¼‰ï¼Œé˜²æ­¢é™¤é›¶ã€‚

---

### 2. æ¨ç†æ—¶çš„BN

**é—®é¢˜**ï¼šæ¨ç†æ—¶é€šå¸¸æ˜¯å•æ ·æœ¬ï¼Œæ— æ³•è®¡ç®—æ‰¹ç»Ÿè®¡é‡ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼šä½¿ç”¨è®­ç»ƒæ—¶çš„ç§»åŠ¨å¹³å‡ã€‚

**ç®—æ³• 1.2 (Batch Normalization - Inference)**:

è®­ç»ƒæ—¶ç»´æŠ¤ï¼š

$$
\begin{align}
\mu_{\text{running}} &\leftarrow (1 - \alpha) \mu_{\text{running}} + \alpha \mu_{\mathcal{B}} \\
\sigma_{\text{running}}^2 &\leftarrow (1 - \alpha) \sigma_{\text{running}}^2 + \alpha \sigma_{\mathcal{B}}^2
\end{align}
$$

æ¨ç†æ—¶ä½¿ç”¨ï¼š

$$
y = \gamma \frac{x - \mu_{\text{running}}}{\sqrt{\sigma_{\text{running}}^2 + \epsilon}} + \beta
$$

**æ³¨æ„**ï¼š$\alpha$ æ˜¯åŠ¨é‡ï¼ˆé€šå¸¸0.1ï¼‰ã€‚

---

### 3. å¯å­¦ä¹ å‚æ•°

**ä¸ºä»€ä¹ˆéœ€è¦ $\gamma, \beta$ï¼Ÿ**

**åŸå› **ï¼šå½’ä¸€åŒ–å¯èƒ½é™åˆ¶è¡¨ç¤ºèƒ½åŠ›ã€‚

**ä¾‹å­**ï¼šsigmoidæ¿€æ´»

- å½’ä¸€åŒ–åè¾“å…¥æ¥è¿‘0
- sigmoidåœ¨0é™„è¿‘è¿‘ä¼¼çº¿æ€§
- å¤±å»éçº¿æ€§èƒ½åŠ›

**è§£å†³æ–¹æ¡ˆ**ï¼š

- $\gamma, \beta$ å…è®¸ç½‘ç»œæ¢å¤åŸå§‹åˆ†å¸ƒ
- å¦‚æœ $\gamma = \sqrt{\sigma^2 + \epsilon}, \beta = \mu$ï¼Œåˆ™ $y = x$
- ç½‘ç»œå¯ä»¥å­¦ä¹ æ˜¯å¦éœ€è¦å½’ä¸€åŒ–

---

## ğŸ”¬ æ•°å­¦åˆ†æ

### 1. å½’ä¸€åŒ–çš„ä½œç”¨

**å®šç† 1.1 (å½’ä¸€åŒ–æ•ˆæœ)**:

BNåçš„æ¿€æ´»æ»¡è¶³ï¼š

$$
\mathbb{E}[\hat{x}] = 0, \quad \text{Var}[\hat{x}] = 1
$$

**è¯æ˜**ï¼š

$$
\mathbb{E}[\hat{x}] = \mathbb{E}\left[\frac{x - \mu}{\sigma}\right] = \frac{\mathbb{E}[x] - \mu}{\sigma} = 0
$$

$$
\text{Var}[\hat{x}] = \text{Var}\left[\frac{x - \mu}{\sigma}\right] = \frac{\text{Var}[x]}{\sigma^2} = 1
$$

**æ„ä¹‰**ï¼š

- æ¿€æ´»åˆ†å¸ƒç¨³å®š
- é¿å…è¿›å…¥é¥±å’ŒåŒº
- æ¢¯åº¦æ›´ç¨³å®š

---

### 2. æ¢¯åº¦æµæ”¹å–„

**å®šç† 2.1 (æ¢¯åº¦å°ºåº¦)**:

BNå‡å°äº†æ¢¯åº¦å¯¹å‚æ•°å°ºåº¦çš„ä¾èµ–ã€‚

**ç›´è§‰**ï¼š

è€ƒè™‘ $y = Wx$ï¼Œå¦‚æœ $W$ å¢å¤§ï¼š

- æ— BNï¼š$y$ å¢å¤§ï¼Œæ¢¯åº¦å¯èƒ½çˆ†ç‚¸
- æœ‰BNï¼š$y$ è¢«å½’ä¸€åŒ–ï¼Œæ¢¯åº¦ç¨³å®š

**æ•°å­¦**ï¼š

$$
\frac{\partial \text{BN}(Wx)}{\partial W} \propto \frac{1}{\|Wx\|}
$$

**æ„ä¹‰**ï¼šæ¢¯åº¦è‡ªåŠ¨è°ƒæ•´ï¼Œå…è®¸æ›´å¤§å­¦ä¹ ç‡ã€‚

---

### 3. æ­£åˆ™åŒ–æ•ˆæœ

**è§‚å¯Ÿ**ï¼šBNå¼•å…¥å™ªå£°ï¼ˆæ‰¹ç»Ÿè®¡é‡çš„éšæœºæ€§ï¼‰ã€‚

**åˆ†æ**ï¼š

- æ¯ä¸ªæ ·æœ¬çš„å½’ä¸€åŒ–ä¾èµ–äºæ‰¹å†…å…¶ä»–æ ·æœ¬
- å¼•å…¥éšæœºæ€§ â†’ æ­£åˆ™åŒ–
- ç±»ä¼¼Dropoutçš„æ•ˆæœ

**å®éªŒ**ï¼š

- ä½¿ç”¨BNåï¼ŒDropoutå¯ä»¥å‡å°‘æˆ–å»é™¤
- BNæœ¬èº«æœ‰é˜²æ­¢è¿‡æ‹Ÿåˆçš„ä½œç”¨

---

## ğŸ’» åå‘ä¼ æ’­æ¨å¯¼

### 1. å‰å‘ä¼ æ’­

**ç¬¦å·**ï¼š

- $x_i$ï¼šè¾“å…¥
- $\hat{x}_i$ï¼šå½’ä¸€åŒ–å
- $y_i$ï¼šè¾“å‡º

**è®¡ç®—å›¾**ï¼š

$$
x_i \to \mu, \sigma^2 \to \hat{x}_i \to y_i
$$

---

### 2. åå‘ä¼ æ’­

**ç›®æ ‡**ï¼šè®¡ç®— $\frac{\partial \mathcal{L}}{\partial x_i}, \frac{\partial \mathcal{L}}{\partial \gamma}, \frac{\partial \mathcal{L}}{\partial \beta}$

**å·²çŸ¥**ï¼š$\frac{\partial \mathcal{L}}{\partial y_i}$

**æ­¥éª¤1**ï¼š$\gamma, \beta$ çš„æ¢¯åº¦

$$
\begin{align}
\frac{\partial \mathcal{L}}{\partial \gamma} &= \sum_{i=1}^{m} \frac{\partial \mathcal{L}}{\partial y_i} \hat{x}_i \\
\frac{\partial \mathcal{L}}{\partial \beta} &= \sum_{i=1}^{m} \frac{\partial \mathcal{L}}{\partial y_i}
\end{align}
$$

**æ­¥éª¤2**ï¼š$\hat{x}_i$ çš„æ¢¯åº¦

$$
\frac{\partial \mathcal{L}}{\partial \hat{x}_i} = \frac{\partial \mathcal{L}}{\partial y_i} \gamma
$$

**æ­¥éª¤3**ï¼š$\sigma^2$ çš„æ¢¯åº¦

$$
\frac{\partial \mathcal{L}}{\partial \sigma^2} = \sum_{i=1}^{m} \frac{\partial \mathcal{L}}{\partial \hat{x}_i} (x_i - \mu) \cdot \left(-\frac{1}{2}\right) (\sigma^2 + \epsilon)^{-3/2}
$$

**æ­¥éª¤4**ï¼š$\mu$ çš„æ¢¯åº¦

$$
\frac{\partial \mathcal{L}}{\partial \mu} = \sum_{i=1}^{m} \frac{\partial \mathcal{L}}{\partial \hat{x}_i} \cdot \frac{-1}{\sqrt{\sigma^2 + \epsilon}} + \frac{\partial \mathcal{L}}{\partial \sigma^2} \cdot \frac{-2}{m} \sum_{i=1}^{m} (x_i - \mu)
$$

**æ­¥éª¤5**ï¼š$x_i$ çš„æ¢¯åº¦

$$
\frac{\partial \mathcal{L}}{\partial x_i} = \frac{\partial \mathcal{L}}{\partial \hat{x}_i} \cdot \frac{1}{\sqrt{\sigma^2 + \epsilon}} + \frac{\partial \mathcal{L}}{\partial \sigma^2} \cdot \frac{2(x_i - \mu)}{m} + \frac{\partial \mathcal{L}}{\partial \mu} \cdot \frac{1}{m}
$$

---

## ğŸ¨ Pythonå®ç°

```python
import torch
import torch.nn as nn
import numpy as np

class BatchNorm1d(nn.Module):
    """ä»é›¶å®ç°1D Batch Normalization"""
    def __init__(self, num_features, eps=1e-5, momentum=0.1):
        super().__init__()
        self.num_features = num_features
        self.eps = eps
        self.momentum = momentum

        # å¯å­¦ä¹ å‚æ•°
        self.gamma = nn.Parameter(torch.ones(num_features))
        self.beta = nn.Parameter(torch.zeros(num_features))

        # ç§»åŠ¨å¹³å‡ï¼ˆä¸å‚ä¸æ¢¯åº¦ï¼‰
        self.register_buffer('running_mean', torch.zeros(num_features))
        self.register_buffer('running_var', torch.ones(num_features))

    def forward(self, x):
        """
        Args:
            x: (batch_size, num_features) æˆ– (batch_size, num_features, *)
        """
        if self.training:
            # è®­ç»ƒæ¨¡å¼ï¼šä½¿ç”¨æ‰¹ç»Ÿè®¡é‡
            if x.dim() == 2:
                # (N, C)
                batch_mean = x.mean(dim=0)
                batch_var = x.var(dim=0, unbiased=False)
            else:
                # (N, C, *): å¯¹Nå’Œç©ºé—´ç»´åº¦æ±‚å‡å€¼
                batch_mean = x.mean(dim=[0] + list(range(2, x.dim())))
                batch_var = x.var(dim=[0] + list(range(2, x.dim())), unbiased=False)

            # æ›´æ–°ç§»åŠ¨å¹³å‡
            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean
            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var

            # å½’ä¸€åŒ–
            x_norm = (x - batch_mean) / torch.sqrt(batch_var + self.eps)
        else:
            # æ¨ç†æ¨¡å¼ï¼šä½¿ç”¨ç§»åŠ¨å¹³å‡
            x_norm = (x - self.running_mean) / torch.sqrt(self.running_var + self.eps)

        # ç¼©æ”¾å’Œå¹³ç§»
        out = self.gamma * x_norm + self.beta
        return out


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºBNå±‚
    bn = BatchNorm1d(num_features=10)

    # è®­ç»ƒæ¨¡å¼
    bn.train()
    x_train = torch.randn(32, 10)
    y_train = bn(x_train)

    print(f"Training mode:")
    print(f"Input mean: {x_train.mean(dim=0)[:3]}")
    print(f"Output mean: {y_train.mean(dim=0)[:3]}")
    print(f"Output std: {y_train.std(dim=0)[:3]}")

    # æ¨ç†æ¨¡å¼
    bn.eval()
    x_test = torch.randn(1, 10)
    y_test = bn(x_test)

    print(f"\nInference mode:")
    print(f"Running mean: {bn.running_mean[:3]}")
    print(f"Running var: {bn.running_var[:3]}")


# å®Œæ•´CNNç¤ºä¾‹
class ConvNetWithBN(nn.Module):
    """å¸¦BNçš„å·ç§¯ç½‘ç»œ"""
    def __init__(self, num_classes=10):
        super().__init__()

        self.features = nn.Sequential(
            # Conv1
            nn.Conv2d(3, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),

            # Conv2
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),

            # Conv3
            nn.Conv2d(128, 256, kernel_size=3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
        )

        self.classifier = nn.Sequential(
            nn.Linear(256 * 4 * 4, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(512, num_classes)
        )

    def forward(self, x):
        x = self.features(x)
        x = torch.flatten(x, 1)
        x = self.classifier(x)
        return x


# BNçš„ä½ç½®å®éªŒ
def bn_position_experiment():
    """å®éªŒï¼šBNåœ¨æ¿€æ´»å‰vsæ¿€æ´»å"""

    # BNåœ¨æ¿€æ´»å (åŸå§‹è®ºæ–‡)
    model1 = nn.Sequential(
        nn.Linear(10, 20),
        nn.ReLU(),
        nn.BatchNorm1d(20),
        nn.Linear(20, 10)
    )

    # BNåœ¨æ¿€æ´»å‰ (Pre-Activation)
    model2 = nn.Sequential(
        nn.Linear(10, 20),
        nn.BatchNorm1d(20),
        nn.ReLU(),
        nn.Linear(10, 10)
    )

    x = torch.randn(32, 10)

    model1.train()
    model2.train()

    y1 = model1(x)
    y2 = model2(x)

    print(f"BN after activation: {y1.mean():.4f}, {y1.std():.4f}")
    print(f"BN before activation: {y2.mean():.4f}, {y2.std():.4f}")

# bn_position_experiment()
```

---

## ğŸ“š ç†è®ºæ·±åŒ–

### 1. BNçš„çœŸæ­£ä½œç”¨

**äº‰è®®**ï¼šBNæ˜¯å¦çœŸçš„å‡å°‘ICSï¼Ÿ

**Santurkar et al. (2018)** ç ”ç©¶å‘ç°ï¼š

- BNçš„ä¸»è¦ä½œç”¨ä¸æ˜¯å‡å°‘ICS
- è€Œæ˜¯**å¹³æ»‘ä¼˜åŒ–æ™¯è§‚**

**å®éªŒ**ï¼š

- æ·»åŠ å™ªå£°æ•…æ„å¢åŠ ICS
- æœ‰BNçš„ç½‘ç»œä»ç„¶è®­ç»ƒè‰¯å¥½
- è¯´æ˜ICSä¸æ˜¯ä¸»è¦å› ç´ 

---

### 2. ä¼˜åŒ–æ™¯è§‚å¹³æ»‘åŒ–

**å®šç† 2.1 (Loss Landscape Smoothing)**:

BNä½¿æŸå¤±å‡½æ•°çš„Lipschitzå¸¸æ•°æ›´å°ã€‚

**æ•°å­¦**ï¼š

$$
\|\nabla \mathcal{L}(x_1) - \nabla \mathcal{L}(x_2)\| \leq L \|x_1 - x_2\|
$$

BNå‡å°äº† $L$ï¼ˆLipschitzå¸¸æ•°ï¼‰ã€‚

**æ„ä¹‰**ï¼š

- æ¢¯åº¦å˜åŒ–æ›´å¹³æ»‘
- å¯ä»¥ä½¿ç”¨æ›´å¤§å­¦ä¹ ç‡
- è®­ç»ƒæ›´ç¨³å®š

---

### 3. ä¸å…¶ä»–å½’ä¸€åŒ–çš„å…³ç³»

**å½’ä¸€åŒ–å®¶æ—**ï¼š

| æ–¹æ³• | å½’ä¸€åŒ–ç»´åº¦ | é€‚ç”¨åœºæ™¯ |
|------|-----------|----------|
| **Batch Norm** | (N, H, W) | å¤§æ‰¹é‡è®­ç»ƒ |
| **Layer Norm** | (C, H, W) | RNN, Transformer |
| **Instance Norm** | (H, W) | é£æ ¼è¿ç§» |
| **Group Norm** | (C/G, H, W) | å°æ‰¹é‡è®­ç»ƒ |

---

## ğŸ”§ å½’ä¸€åŒ–å˜ä½“

### 1. Layer Normalization

**æ ¸å¿ƒæ€æƒ³**ï¼šå¯¹æ¯ä¸ªæ ·æœ¬çš„æ‰€æœ‰ç‰¹å¾å½’ä¸€åŒ–ã€‚

**å…¬å¼**ï¼š

$$
\mu_i = \frac{1}{H} \sum_{j=1}^{H} x_{ij}, \quad \sigma_i^2 = \frac{1}{H} \sum_{j=1}^{H} (x_{ij} - \mu_i)^2
$$

$$
\hat{x}_{ij} = \frac{x_{ij} - \mu_i}{\sqrt{\sigma_i^2 + \epsilon}}
$$

**ä¼˜åŠ¿**ï¼š

- ä¸ä¾èµ–æ‰¹å¤§å°
- é€‚ç”¨äºRNNï¼ˆåºåˆ—é•¿åº¦ä¸åŒï¼‰
- Transformerçš„æ ‡å‡†é€‰æ‹©

---

### 2. Instance Normalization

**æ ¸å¿ƒæ€æƒ³**ï¼šå¯¹æ¯ä¸ªæ ·æœ¬çš„æ¯ä¸ªé€šé“ç‹¬ç«‹å½’ä¸€åŒ–ã€‚

**å…¬å¼**ï¼š

$$
\mu_{ic} = \frac{1}{HW} \sum_{h, w} x_{ichw}
$$

$$
\hat{x}_{ichw} = \frac{x_{ichw} - \mu_{ic}}{\sqrt{\sigma_{ic}^2 + \epsilon}}
$$

**åº”ç”¨**ï¼š

- é£æ ¼è¿ç§»
- å›¾åƒç”Ÿæˆ
- ä¸å¸Œæœ›æ‰¹å†…æ ·æœ¬ç›¸äº’å½±å“

---

### 3. Group Normalization

**æ ¸å¿ƒæ€æƒ³**ï¼šå°†é€šé“åˆ†ç»„ï¼Œç»„å†…å½’ä¸€åŒ–ã€‚

**å…¬å¼**ï¼š

- å°†Cä¸ªé€šé“åˆ†æˆGç»„
- æ¯ç»„å†…å½’ä¸€åŒ–

**ä¼˜åŠ¿**ï¼š

- ä¸ä¾èµ–æ‰¹å¤§å°
- å°æ‰¹é‡è®­ç»ƒæ—¶ä¼˜äºBN
- é€‚ç”¨äºç›®æ ‡æ£€æµ‹ã€åˆ†å‰²

---

## ğŸ”§ å®é™…åº”ç”¨æ¡ˆä¾‹

### 1. å›¾åƒåˆ†ç±»

**ImageNetè®­ç»ƒåŠ é€Ÿ**:

Batch Normalizationä½¿ImageNetè®­ç»ƒåŠ é€Ÿ10å€ä»¥ä¸Šã€‚

**æ•ˆæœ**:
- å¯ä»¥ä½¿ç”¨10å€å¤§çš„å­¦ä¹ ç‡
- è®­ç»ƒæ—¶é—´ä»æ•°å‘¨ç¼©çŸ­åˆ°æ•°å¤©
- è¾¾åˆ°ç›¸åŒæˆ–æ›´å¥½çš„å‡†ç¡®ç‡

**å®è·µç¤ºä¾‹**:

```python
import torch
import torch.nn as nn

class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, 3, padding=1)
        self.bn = nn.BatchNorm2d(out_channels)  # BNå±‚
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.conv(x)
        x = self.bn(x)  # å½’ä¸€åŒ–
        x = self.relu(x)
        return x
```

---

### 2. ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ (GAN)

**ç¨³å®šGANè®­ç»ƒ**:

Batch Normalizationåœ¨ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨ä¸­çš„åº”ç”¨ã€‚

**é…ç½®**:
- ç”Ÿæˆå™¨: æ¯å±‚åä½¿ç”¨BN
- åˆ¤åˆ«å™¨: ä¸ä½¿ç”¨BNï¼ˆæˆ–åªåœ¨éƒ¨åˆ†å±‚ä½¿ç”¨ï¼‰

**ä¼˜åŠ¿**:
- ç¨³å®šè®­ç»ƒ
- é˜²æ­¢æ¨¡å¼å´©å¡Œ
- ç”Ÿæˆæ›´é«˜è´¨é‡çš„å›¾åƒ

**æ³¨æ„**: åˆ¤åˆ«å™¨æœ€åä¸€å±‚ä¸ä½¿ç”¨BNï¼Œé¿å…ç ´ååˆ¤åˆ«èƒ½åŠ›ã€‚

---

### 3. ç›®æ ‡æ£€æµ‹

**Faster R-CNN / YOLO**:

Batch Normalizationåœ¨ç›®æ ‡æ£€æµ‹ä¸­çš„åº”ç”¨ã€‚

**ä¼˜åŠ¿**:
- åŠ é€Ÿè®­ç»ƒ
- æé«˜æ£€æµ‹ç²¾åº¦
- å¤„ç†å¤šå°ºåº¦ç›®æ ‡

**å®è·µç¤ºä¾‹**:

```python
class DetectionBackbone(nn.Module):
    def __init__(self):
        super().__init__()
        # æ¯ä¸ªå·ç§¯å±‚åä½¿ç”¨BN
        self.conv1 = nn.Sequential(
            nn.Conv2d(3, 64, 7, stride=2, padding=3),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(3, stride=2, padding=1)
        )
        # ... æ›´å¤šå±‚
```

---

### 4. è¯­ä¹‰åˆ†å‰²

**DeepLab / U-Net**:

Batch Normalizationåœ¨è¯­ä¹‰åˆ†å‰²ä¸­çš„åº”ç”¨ã€‚

**ä¼˜åŠ¿**:
- å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒ
- ç¨³å®šè®­ç»ƒ
- æé«˜åˆ†å‰²ç²¾åº¦

**æ³¨æ„**: æ¨ç†æ—¶ä½¿ç”¨è¿è¡Œå‡å€¼å’Œæ–¹å·®ã€‚

---

### 5. è‡ªç„¶è¯­è¨€å¤„ç†

**Transformerä¸­çš„Layer Norm**:

è™½ç„¶Transformerä½¿ç”¨Layer Normalizationè€ŒéBatch Normalizationï¼Œä½†åŸç†ç›¸ä¼¼ã€‚

**å¯¹æ¯”**:
- **Batch Norm**: å¯¹batchç»´åº¦å½’ä¸€åŒ–
- **Layer Norm**: å¯¹ç‰¹å¾ç»´åº¦å½’ä¸€åŒ–

**åº”ç”¨**:
- BERTã€GPTç­‰Transformeræ¨¡å‹
- ç¨³å®šè®­ç»ƒ
- åŠ é€Ÿæ”¶æ•›

---

### 6. å¼ºåŒ–å­¦ä¹ 

**ç¨³å®šç­–ç•¥è®­ç»ƒ**:

Batch Normalizationåœ¨ç­–ç•¥ç½‘ç»œä¸­çš„åº”ç”¨ã€‚

**ä¼˜åŠ¿**:
- ç¨³å®šè®­ç»ƒ
- å¤„ç†ä¸åŒå°ºåº¦çš„çŠ¶æ€
- åŠ é€Ÿæ”¶æ•›

**å®è·µç¤ºä¾‹**:

```python
class PolicyNetwork(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim, 256),
            nn.BatchNorm1d(256),  # BNå±‚
            nn.ReLU(),
            nn.Linear(256, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Linear(256, action_dim)
        )
```

---

### 7. å°æ‰¹é‡è®­ç»ƒ

**Group Normalizationæ›¿ä»£**:

å½“æ‰¹é‡å¤§å°å¾ˆå°æ—¶ï¼Œä½¿ç”¨Group Normalizationã€‚

**åœºæ™¯**:
- æ‰¹é‡å¤§å° = 1 æˆ– 2
- å†…å­˜å—é™
- åœ¨çº¿å­¦ä¹ 

**ä¼˜åŠ¿**:
- ä¸ä¾èµ–æ‰¹é‡å¤§å°
- æ€§èƒ½ç¨³å®š
- é€‚ç”¨äºå°æ‰¹é‡

---

### 8. è¿ç§»å­¦ä¹ 

**Fine-tuningä¸­çš„BN**:

Batch Normalizationåœ¨è¿ç§»å­¦ä¹ ä¸­çš„åº”ç”¨ã€‚

**ç­–ç•¥**:
- å†»ç»“BNçš„ç»Ÿè®¡é‡ï¼ˆä½¿ç”¨é¢„è®­ç»ƒå‡å€¼å’Œæ–¹å·®ï¼‰
- æˆ–æ›´æ–°BNç»Ÿè®¡é‡ï¼ˆé€‚åº”æ–°æ•°æ®åˆ†å¸ƒï¼‰

**å®è·µç¤ºä¾‹**:

```python
# å†»ç»“BNç»Ÿè®¡é‡
for module in model.modules():
    if isinstance(module, nn.BatchNorm2d):
        module.eval()  # ä½¿ç”¨è¿è¡Œç»Ÿè®¡é‡ï¼Œä¸æ›´æ–°
        module.requires_grad = False
```

---

### 9. åŒ»å­¦å½±åƒ

**å¤„ç†æ•°æ®åˆ†å¸ƒå·®å¼‚**:

Batch Normalizationå¤„ç†ä¸åŒæ‰«æä»ªã€ä¸åŒåŒ»é™¢çš„æ•°æ®ã€‚

**ä¼˜åŠ¿**:
- å½’ä¸€åŒ–ä¸åŒæ¥æºçš„æ•°æ®
- æé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›
- ç¨³å®šè®­ç»ƒ

**æ³¨æ„**: éœ€è¦ä»”ç»†å¤„ç†æ¨ç†æ—¶çš„ç»Ÿè®¡é‡ã€‚

---

### 10. å®æ—¶æ¨ç†

**æ¨ç†ä¼˜åŒ–**:

Batch Normalizationåœ¨æ¨ç†æ—¶çš„ä¼˜åŒ–ã€‚

**æ–¹æ³•**:
- èåˆBNåˆ°å·ç§¯å±‚
- å‡å°‘è®¡ç®—é‡
- åŠ é€Ÿæ¨ç†

**å…¬å¼**:
$$
y = \gamma \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta = \frac{\gamma}{\sqrt{\sigma^2 + \epsilon}} x + \left(\beta - \frac{\gamma \mu}{\sqrt{\sigma^2 + \epsilon}}\right)
$$

å¯ä»¥åˆå¹¶åˆ°å·ç§¯æƒé‡ä¸­ã€‚

**å®è·µç¤ºä¾‹**:

```python
# èåˆBNåˆ°å·ç§¯
def fuse_bn_conv(conv, bn):
    # è®¡ç®—èåˆåçš„æƒé‡å’Œåç½®
    gamma = bn.weight
    beta = bn.bias
    mean = bn.running_mean
    var = bn.running_var
    eps = bn.eps

    # èåˆ
    scale = gamma / torch.sqrt(var + eps)
    fused_weight = conv.weight * scale.view(-1, 1, 1, 1)
    fused_bias = beta - mean * scale

    return fused_weight, fused_bias
```

---

## ğŸ“ ç›¸å…³è¯¾ç¨‹

| å¤§å­¦ | è¯¾ç¨‹ |
|------|------|
| **Stanford** | CS231n Convolutional Neural Networks |
| **MIT** | 6.S191 Introduction to Deep Learning |
| **UC Berkeley** | CS182 Deep Learning |
| **CMU** | 11-785 Introduction to Deep Learning |

---

## ğŸ“– å‚è€ƒæ–‡çŒ®

1. **Ioffe & Szegedy (2015)**. "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift". *ICML*.

2. **Santurkar et al. (2018)**. "How Does Batch Normalization Help Optimization?". *NeurIPS*.

3. **Ba et al. (2016)**. "Layer Normalization". *arXiv*.

4. **Ulyanov et al. (2016)**. "Instance Normalization: The Missing Ingredient for Fast Stylization". *arXiv*.

5. **Wu & He (2018)**. "Group Normalization". *ECCV*.

---

*æœ€åæ›´æ–°ï¼š2025å¹´12æœˆ20æ—¥*-
