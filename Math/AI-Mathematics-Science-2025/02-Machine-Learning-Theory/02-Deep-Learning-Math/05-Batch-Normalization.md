# 批归一化 (Batch Normalization) 理论

> **Batch Normalization: Theory and Mathematics**
>
> 深度学习训练加速的关键技术

---

## 目录

- [批归一化 (Batch Normalization) 理论](#批归一化-batch-normalization-理论)
  - [目录](#目录)
  - [📋 核心思想](#-核心思想)
  - [🎯 Internal Covariate Shift](#-internal-covariate-shift)
    - [1. 问题定义](#1-问题定义)
    - [2. 影响分析](#2-影响分析)
  - [📊 批归一化算法](#-批归一化算法)
    - [1. 训练时的BN](#1-训练时的bn)
    - [2. 推理时的BN](#2-推理时的bn)
    - [3. 可学习参数](#3-可学习参数)
  - [🔬 数学分析](#-数学分析)
    - [1. 归一化的作用](#1-归一化的作用)
    - [2. 梯度流改善](#2-梯度流改善)
    - [3. 正则化效果](#3-正则化效果)
  - [💻 反向传播推导](#-反向传播推导)
    - [1. 前向传播](#1-前向传播)
    - [2. 反向传播](#2-反向传播)
  - [🎨 Python实现](#-python实现)
  - [📚 理论深化](#-理论深化)
    - [1. BN的真正作用](#1-bn的真正作用)
    - [2. 优化景观平滑化](#2-优化景观平滑化)
    - [3. 与其他归一化的关系](#3-与其他归一化的关系)
  - [🔧 归一化变体](#-归一化变体)
    - [1. Layer Normalization](#1-layer-normalization)
    - [2. Instance Normalization](#2-instance-normalization)
    - [3. Group Normalization](#3-group-normalization)
  - [🎓 相关课程](#-相关课程)
  - [📖 参考文献](#-参考文献)

---

## 📋 核心思想

**Batch Normalization (BN)** 通过归一化层的输入来加速训练。

**核心操作**：

```text
输入 x → 归一化 → (x - μ) / σ → 缩放平移 → γx̂ + β → 输出
```

**主要效果**：

- 加速训练（可用更大学习率）
- 缓解梯度消失/爆炸
- 正则化效果（减少对Dropout的依赖）
- 允许更深的网络

---

## 🎯 Internal Covariate Shift

### 1. 问题定义

**定义 1.1 (Internal Covariate Shift, ICS)**:

训练过程中，网络内部层的输入分布不断变化。

**原因**：

- 前层参数更新
- 导致后层输入分布改变
- 后层需要不断适应新分布

**类比**：

```text
第l层学习: f(x) where x ~ D_old
参数更新后: x ~ D_new (分布改变!)
第l层需要重新适应
```

---

### 2. 影响分析

**问题1：训练缓慢**:

- 每层都在追逐移动的目标
- 需要小学习率以保持稳定

**问题2：梯度问题**:

- 输入分布变化导致梯度不稳定
- 可能进入饱和区（如sigmoid）

**问题3：初始化敏感**:

- 分布变化放大初始化的影响
- 需要精心设计初始化

---

## 📊 批归一化算法

### 1. 训练时的BN

**算法 1.1 (Batch Normalization - Training)**:

**输入**：

- 小批量 $\mathcal{B} = \{x_1, \ldots, x_m\}$
- 可学习参数 $\gamma, \beta$

**步骤**：

1. **计算均值**：
   $$
   \mu_{\mathcal{B}} = \frac{1}{m} \sum_{i=1}^{m} x_i
   $$

2. **计算方差**：
   $$
   \sigma_{\mathcal{B}}^2 = \frac{1}{m} \sum_{i=1}^{m} (x_i - \mu_{\mathcal{B}})^2
   $$

3. **归一化**：
   $$
   \hat{x}_i = \frac{x_i - \mu_{\mathcal{B}}}{\sqrt{\sigma_{\mathcal{B}}^2 + \epsilon}}
   $$

4. **缩放和平移**：
   $$
   y_i = \gamma \hat{x}_i + \beta = \text{BN}_{\gamma, \beta}(x_i)
   $$

**注意**：$\epsilon$ 是小常数（如 $10^{-5}$），防止除零。

---

### 2. 推理时的BN

**问题**：推理时通常是单样本，无法计算批统计量。

**解决方案**：使用训练时的移动平均。

**算法 1.2 (Batch Normalization - Inference)**:

训练时维护：

$$
\begin{align}
\mu_{\text{running}} &\leftarrow (1 - \alpha) \mu_{\text{running}} + \alpha \mu_{\mathcal{B}} \\
\sigma_{\text{running}}^2 &\leftarrow (1 - \alpha) \sigma_{\text{running}}^2 + \alpha \sigma_{\mathcal{B}}^2
\end{align}
$$

推理时使用：

$$
y = \gamma \frac{x - \mu_{\text{running}}}{\sqrt{\sigma_{\text{running}}^2 + \epsilon}} + \beta
$$

**注意**：$\alpha$ 是动量（通常0.1）。

---

### 3. 可学习参数

**为什么需要 $\gamma, \beta$？**

**原因**：归一化可能限制表示能力。

**例子**：sigmoid激活

- 归一化后输入接近0
- sigmoid在0附近近似线性
- 失去非线性能力

**解决方案**：

- $\gamma, \beta$ 允许网络恢复原始分布
- 如果 $\gamma = \sqrt{\sigma^2 + \epsilon}, \beta = \mu$，则 $y = x$
- 网络可以学习是否需要归一化

---

## 🔬 数学分析

### 1. 归一化的作用

**定理 1.1 (归一化效果)**:

BN后的激活满足：

$$
\mathbb{E}[\hat{x}] = 0, \quad \text{Var}[\hat{x}] = 1
$$

**证明**：

$$
\mathbb{E}[\hat{x}] = \mathbb{E}\left[\frac{x - \mu}{\sigma}\right] = \frac{\mathbb{E}[x] - \mu}{\sigma} = 0
$$

$$
\text{Var}[\hat{x}] = \text{Var}\left[\frac{x - \mu}{\sigma}\right] = \frac{\text{Var}[x]}{\sigma^2} = 1
$$

**意义**：

- 激活分布稳定
- 避免进入饱和区
- 梯度更稳定

---

### 2. 梯度流改善

**定理 2.1 (梯度尺度)**:

BN减小了梯度对参数尺度的依赖。

**直觉**：

考虑 $y = Wx$，如果 $W$ 增大：

- 无BN：$y$ 增大，梯度可能爆炸
- 有BN：$y$ 被归一化，梯度稳定

**数学**：

$$
\frac{\partial \text{BN}(Wx)}{\partial W} \propto \frac{1}{\|Wx\|}
$$

**意义**：梯度自动调整，允许更大学习率。

---

### 3. 正则化效果

**观察**：BN引入噪声（批统计量的随机性）。

**分析**：

- 每个样本的归一化依赖于批内其他样本
- 引入随机性 → 正则化
- 类似Dropout的效果

**实验**：

- 使用BN后，Dropout可以减少或去除
- BN本身有防止过拟合的作用

---

## 💻 反向传播推导

### 1. 前向传播

**符号**：

- $x_i$：输入
- $\hat{x}_i$：归一化后
- $y_i$：输出

**计算图**：

$$
x_i \to \mu, \sigma^2 \to \hat{x}_i \to y_i
$$

---

### 2. 反向传播

**目标**：计算 $\frac{\partial \mathcal{L}}{\partial x_i}, \frac{\partial \mathcal{L}}{\partial \gamma}, \frac{\partial \mathcal{L}}{\partial \beta}$

**已知**：$\frac{\partial \mathcal{L}}{\partial y_i}$

**步骤1**：$\gamma, \beta$ 的梯度

$$
\begin{align}
\frac{\partial \mathcal{L}}{\partial \gamma} &= \sum_{i=1}^{m} \frac{\partial \mathcal{L}}{\partial y_i} \hat{x}_i \\
\frac{\partial \mathcal{L}}{\partial \beta} &= \sum_{i=1}^{m} \frac{\partial \mathcal{L}}{\partial y_i}
\end{align}
$$

**步骤2**：$\hat{x}_i$ 的梯度

$$
\frac{\partial \mathcal{L}}{\partial \hat{x}_i} = \frac{\partial \mathcal{L}}{\partial y_i} \gamma
$$

**步骤3**：$\sigma^2$ 的梯度

$$
\frac{\partial \mathcal{L}}{\partial \sigma^2} = \sum_{i=1}^{m} \frac{\partial \mathcal{L}}{\partial \hat{x}_i} (x_i - \mu) \cdot \left(-\frac{1}{2}\right) (\sigma^2 + \epsilon)^{-3/2}
$$

**步骤4**：$\mu$ 的梯度

$$
\frac{\partial \mathcal{L}}{\partial \mu} = \sum_{i=1}^{m} \frac{\partial \mathcal{L}}{\partial \hat{x}_i} \cdot \frac{-1}{\sqrt{\sigma^2 + \epsilon}} + \frac{\partial \mathcal{L}}{\partial \sigma^2} \cdot \frac{-2}{m} \sum_{i=1}^{m} (x_i - \mu)
$$

**步骤5**：$x_i$ 的梯度

$$
\frac{\partial \mathcal{L}}{\partial x_i} = \frac{\partial \mathcal{L}}{\partial \hat{x}_i} \cdot \frac{1}{\sqrt{\sigma^2 + \epsilon}} + \frac{\partial \mathcal{L}}{\partial \sigma^2} \cdot \frac{2(x_i - \mu)}{m} + \frac{\partial \mathcal{L}}{\partial \mu} \cdot \frac{1}{m}
$$

---

## 🎨 Python实现

```python
import torch
import torch.nn as nn
import numpy as np

class BatchNorm1d(nn.Module):
    """从零实现1D Batch Normalization"""
    def __init__(self, num_features, eps=1e-5, momentum=0.1):
        super().__init__()
        self.num_features = num_features
        self.eps = eps
        self.momentum = momentum
        
        # 可学习参数
        self.gamma = nn.Parameter(torch.ones(num_features))
        self.beta = nn.Parameter(torch.zeros(num_features))
        
        # 移动平均（不参与梯度）
        self.register_buffer('running_mean', torch.zeros(num_features))
        self.register_buffer('running_var', torch.ones(num_features))
    
    def forward(self, x):
        """
        Args:
            x: (batch_size, num_features) 或 (batch_size, num_features, *)
        """
        if self.training:
            # 训练模式：使用批统计量
            if x.dim() == 2:
                # (N, C)
                batch_mean = x.mean(dim=0)
                batch_var = x.var(dim=0, unbiased=False)
            else:
                # (N, C, *): 对N和空间维度求均值
                batch_mean = x.mean(dim=[0] + list(range(2, x.dim())))
                batch_var = x.var(dim=[0] + list(range(2, x.dim())), unbiased=False)
            
            # 更新移动平均
            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean
            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var
            
            # 归一化
            x_norm = (x - batch_mean) / torch.sqrt(batch_var + self.eps)
        else:
            # 推理模式：使用移动平均
            x_norm = (x - self.running_mean) / torch.sqrt(self.running_var + self.eps)
        
        # 缩放和平移
        out = self.gamma * x_norm + self.beta
        return out


# 使用示例
if __name__ == "__main__":
    # 创建BN层
    bn = BatchNorm1d(num_features=10)
    
    # 训练模式
    bn.train()
    x_train = torch.randn(32, 10)
    y_train = bn(x_train)
    
    print(f"Training mode:")
    print(f"Input mean: {x_train.mean(dim=0)[:3]}")
    print(f"Output mean: {y_train.mean(dim=0)[:3]}")
    print(f"Output std: {y_train.std(dim=0)[:3]}")
    
    # 推理模式
    bn.eval()
    x_test = torch.randn(1, 10)
    y_test = bn(x_test)
    
    print(f"\nInference mode:")
    print(f"Running mean: {bn.running_mean[:3]}")
    print(f"Running var: {bn.running_var[:3]}")


# 完整CNN示例
class ConvNetWithBN(nn.Module):
    """带BN的卷积网络"""
    def __init__(self, num_classes=10):
        super().__init__()
        
        self.features = nn.Sequential(
            # Conv1
            nn.Conv2d(3, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            
            # Conv2
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            
            # Conv3
            nn.Conv2d(128, 256, kernel_size=3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
        )
        
        self.classifier = nn.Sequential(
            nn.Linear(256 * 4 * 4, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(512, num_classes)
        )
    
    def forward(self, x):
        x = self.features(x)
        x = torch.flatten(x, 1)
        x = self.classifier(x)
        return x


# BN的位置实验
def bn_position_experiment():
    """实验：BN在激活前vs激活后"""
    
    # BN在激活后 (原始论文)
    model1 = nn.Sequential(
        nn.Linear(10, 20),
        nn.ReLU(),
        nn.BatchNorm1d(20),
        nn.Linear(20, 10)
    )
    
    # BN在激活前 (Pre-Activation)
    model2 = nn.Sequential(
        nn.Linear(10, 20),
        nn.BatchNorm1d(20),
        nn.ReLU(),
        nn.Linear(10, 10)
    )
    
    x = torch.randn(32, 10)
    
    model1.train()
    model2.train()
    
    y1 = model1(x)
    y2 = model2(x)
    
    print(f"BN after activation: {y1.mean():.4f}, {y1.std():.4f}")
    print(f"BN before activation: {y2.mean():.4f}, {y2.std():.4f}")

# bn_position_experiment()
```

---

## 📚 理论深化

### 1. BN的真正作用

**争议**：BN是否真的减少ICS？

**Santurkar et al. (2018)** 研究发现：

- BN的主要作用不是减少ICS
- 而是**平滑优化景观**

**实验**：

- 添加噪声故意增加ICS
- 有BN的网络仍然训练良好
- 说明ICS不是主要因素

---

### 2. 优化景观平滑化

**定理 2.1 (Loss Landscape Smoothing)**:

BN使损失函数的Lipschitz常数更小。

**数学**：

$$
\|\nabla \mathcal{L}(x_1) - \nabla \mathcal{L}(x_2)\| \leq L \|x_1 - x_2\|
$$

BN减小了 $L$（Lipschitz常数）。

**意义**：

- 梯度变化更平滑
- 可以使用更大学习率
- 训练更稳定

---

### 3. 与其他归一化的关系

**归一化家族**：

| 方法 | 归一化维度 | 适用场景 |
|------|-----------|----------|
| **Batch Norm** | (N, H, W) | 大批量训练 |
| **Layer Norm** | (C, H, W) | RNN, Transformer |
| **Instance Norm** | (H, W) | 风格迁移 |
| **Group Norm** | (C/G, H, W) | 小批量训练 |

---

## 🔧 归一化变体

### 1. Layer Normalization

**核心思想**：对每个样本的所有特征归一化。

**公式**：

$$
\mu_i = \frac{1}{H} \sum_{j=1}^{H} x_{ij}, \quad \sigma_i^2 = \frac{1}{H} \sum_{j=1}^{H} (x_{ij} - \mu_i)^2
$$

$$
\hat{x}_{ij} = \frac{x_{ij} - \mu_i}{\sqrt{\sigma_i^2 + \epsilon}}
$$

**优势**：

- 不依赖批大小
- 适用于RNN（序列长度不同）
- Transformer的标准选择

---

### 2. Instance Normalization

**核心思想**：对每个样本的每个通道独立归一化。

**公式**：

$$
\mu_{ic} = \frac{1}{HW} \sum_{h, w} x_{ichw}
$$

$$
\hat{x}_{ichw} = \frac{x_{ichw} - \mu_{ic}}{\sqrt{\sigma_{ic}^2 + \epsilon}}
$$

**应用**：

- 风格迁移
- 图像生成
- 不希望批内样本相互影响

---

### 3. Group Normalization

**核心思想**：将通道分组，组内归一化。

**公式**：

- 将C个通道分成G组
- 每组内归一化

**优势**：

- 不依赖批大小
- 小批量训练时优于BN
- 适用于目标检测、分割

---

## 🎓 相关课程

| 大学 | 课程 |
|------|------|
| **Stanford** | CS231n Convolutional Neural Networks |
| **MIT** | 6.S191 Introduction to Deep Learning |
| **UC Berkeley** | CS182 Deep Learning |
| **CMU** | 11-785 Introduction to Deep Learning |

---

## 📖 参考文献

1. **Ioffe & Szegedy (2015)**. "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift". *ICML*.

2. **Santurkar et al. (2018)**. "How Does Batch Normalization Help Optimization?". *NeurIPS*.

3. **Ba et al. (2016)**. "Layer Normalization". *arXiv*.

4. **Ulyanov et al. (2016)**. "Instance Normalization: The Missing Ingredient for Fast Stylization". *arXiv*.

5. **Wu & He (2018)**. "Group Normalization". *ECCV*.

---

*最后更新：2025年10月*-
