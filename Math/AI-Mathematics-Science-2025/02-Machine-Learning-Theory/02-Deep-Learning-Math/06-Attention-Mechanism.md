# æ³¨æ„åŠ›æœºåˆ¶ (Attention Mechanism) æ•°å­¦åŸç†

> **Attention Mechanism: Mathematics and Theory**
>
> Transformerä¸ç°ä»£LLMçš„æ ¸å¿ƒæŠ€æœ¯

---

## ç›®å½•

- [æ³¨æ„åŠ›æœºåˆ¶ (Attention Mechanism) æ•°å­¦åŸç†](#æ³¨æ„åŠ›æœºåˆ¶-attention-mechanism-æ•°å­¦åŸç†)
  - [ç›®å½•](#ç›®å½•)
  - [ğŸ“‹ æ ¸å¿ƒæ€æƒ³](#-æ ¸å¿ƒæ€æƒ³)
  - [ğŸ¯ é—®é¢˜åŠ¨æœº](#-é—®é¢˜åŠ¨æœº)
    - [1. åºåˆ—å»ºæ¨¡çš„æŒ‘æˆ˜](#1-åºåˆ—å»ºæ¨¡çš„æŒ‘æˆ˜)
    - [2. æ³¨æ„åŠ›çš„ç›´è§‰](#2-æ³¨æ„åŠ›çš„ç›´è§‰)
  - [ğŸ“Š Scaled Dot-Product Attention](#-scaled-dot-product-attention)
    - [1. æ•°å­¦å®šä¹‰](#1-æ•°å­¦å®šä¹‰)
    - [2. ç¼©æ”¾å› å­çš„ä½œç”¨](#2-ç¼©æ”¾å› å­çš„ä½œç”¨)
    - [3. Softmaxçš„ä½œç”¨](#3-softmaxçš„ä½œç”¨)
  - [ğŸ”¬ Multi-Head Attention](#-multi-head-attention)
    - [1. æ ¸å¿ƒæ€æƒ³](#1-æ ¸å¿ƒæ€æƒ³)
    - [2. æ•°å­¦å½¢å¼åŒ–](#2-æ•°å­¦å½¢å¼åŒ–)
    - [3. ä¸ºä»€ä¹ˆå¤šå¤´æœ‰æ•ˆ](#3-ä¸ºä»€ä¹ˆå¤šå¤´æœ‰æ•ˆ)
  - [ğŸ’» Self-Attention vs Cross-Attention](#-self-attention-vs-cross-attention)
    - [1. Self-Attention](#1-self-attention)
    - [2. Cross-Attention](#2-cross-attention)
    - [3. åº”ç”¨åœºæ™¯](#3-åº”ç”¨åœºæ™¯)
  - [ğŸ¨ Pythonå®ç°](#-pythonå®ç°)
  - [ğŸ“š ç†è®ºåˆ†æ](#-ç†è®ºåˆ†æ)
    - [1. æ³¨æ„åŠ›çš„è¡¨ç¤ºèƒ½åŠ›](#1-æ³¨æ„åŠ›çš„è¡¨ç¤ºèƒ½åŠ›)
    - [2. è®¡ç®—å¤æ‚åº¦](#2-è®¡ç®—å¤æ‚åº¦)
    - [3. é•¿åºåˆ—é—®é¢˜](#3-é•¿åºåˆ—é—®é¢˜)
  - [ğŸ”§ æ³¨æ„åŠ›å˜ä½“](#-æ³¨æ„åŠ›å˜ä½“)
    - [1. Sparse Attention](#1-sparse-attention)
    - [2. Linear Attention](#2-linear-attention)
    - [3. Flash Attention](#3-flash-attention)
  - [ğŸ”§ å®é™…åº”ç”¨æ¡ˆä¾‹](#-å®é™…åº”ç”¨æ¡ˆä¾‹)
    - [1. è‡ªç„¶è¯­è¨€å¤„ç†](#1-è‡ªç„¶è¯­è¨€å¤„ç†)
    - [2. è®¡ç®—æœºè§†è§‰](#2-è®¡ç®—æœºè§†è§‰)
    - [3. å¤šæ¨¡æ€å­¦ä¹ ](#3-å¤šæ¨¡æ€å­¦ä¹ )
    - [4. è¯­éŸ³è¯†åˆ«](#4-è¯­éŸ³è¯†åˆ«)
    - [5. æ¨èç³»ç»Ÿ](#5-æ¨èç³»ç»Ÿ)
    - [6. æ—¶é—´åºåˆ—é¢„æµ‹](#6-æ—¶é—´åºåˆ—é¢„æµ‹)
    - [7. ä»£ç ç”Ÿæˆä¸ç†è§£](#7-ä»£ç ç”Ÿæˆä¸ç†è§£)
    - [8. å›¾ç¥ç»ç½‘ç»œ](#8-å›¾ç¥ç»ç½‘ç»œ)
    - [9. å¼ºåŒ–å­¦ä¹ ](#9-å¼ºåŒ–å­¦ä¹ )
    - [10. åŒ»å­¦å½±åƒåˆ†æ](#10-åŒ»å­¦å½±åƒåˆ†æ)
  - [ğŸ“ ç›¸å…³è¯¾ç¨‹](#-ç›¸å…³è¯¾ç¨‹)
  - [ğŸ“– å‚è€ƒæ–‡çŒ®](#-å‚è€ƒæ–‡çŒ®)

---

## ğŸ“‹ æ ¸å¿ƒæ€æƒ³

**æ³¨æ„åŠ›æœºåˆ¶**å…è®¸æ¨¡å‹**åŠ¨æ€å…³æ³¨**è¾“å…¥çš„ä¸åŒéƒ¨åˆ†ã€‚

**æ ¸å¿ƒæµç¨‹**ï¼š

```text
Query (æŸ¥è¯¢) + Key (é”®) + Value (å€¼)
        â†“
  è®¡ç®—ç›¸ä¼¼åº¦ (QÂ·K^T)
        â†“
   å½’ä¸€åŒ– (Softmax)
        â†“
  åŠ æƒæ±‚å’Œ (AttentionÂ·V)
        â†“
      è¾“å‡º
```

**å…³é”®ä¼˜åŠ¿**ï¼š

- æ•è·é•¿è·ç¦»ä¾èµ–
- å¹¶è¡Œè®¡ç®—ï¼ˆç›¸æ¯”RNNï¼‰
- å¯è§£é‡Šæ€§ï¼ˆæ³¨æ„åŠ›æƒé‡ï¼‰

---

## ğŸ¯ é—®é¢˜åŠ¨æœº

### 1. åºåˆ—å»ºæ¨¡çš„æŒ‘æˆ˜

**RNNçš„é—®é¢˜**ï¼š

- **é¡ºåºä¾èµ–**ï¼šå¿…é¡»é€æ­¥å¤„ç†ï¼Œæ— æ³•å¹¶è¡Œ
- **é•¿è·ç¦»ä¾èµ–**ï¼šæ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸
- **å›ºå®šå®¹é‡**ï¼šéšçŠ¶æ€ç»´åº¦é™åˆ¶ä¿¡æ¯é‡

**ç¤ºä¾‹**ï¼š

```text
"The cat, which was very hungry, ate the fish."
```

è¦ç†è§£"ate"ï¼Œéœ€è¦å…³æ³¨"cat"ï¼ˆä¸»è¯­ï¼‰ï¼Œä½†ä¸­é—´éš”äº†å¾ˆå¤šè¯ã€‚

---

### 2. æ³¨æ„åŠ›çš„ç›´è§‰

**äººç±»é˜…è¯»**ï¼š

- ä¸æ˜¯å‡åŒ€å…³æ³¨æ‰€æœ‰è¯
- æ ¹æ®ä»»åŠ¡åŠ¨æ€è°ƒæ•´æ³¨æ„åŠ›
- å¯ä»¥"è·³è·ƒ"å…³æ³¨ç›¸å…³ä¿¡æ¯

**æ³¨æ„åŠ›æœºåˆ¶**ï¼š

- ä¸ºæ¯ä¸ªè¾“å‡ºä½ç½®è®¡ç®—å¯¹æ‰€æœ‰è¾“å…¥çš„æ³¨æ„åŠ›æƒé‡
- æƒé‡åæ˜ ç›¸å…³æ€§
- è¾“å‡ºæ˜¯è¾“å…¥çš„åŠ æƒå’Œ

---

## ğŸ“Š Scaled Dot-Product Attention

### 1. æ•°å­¦å®šä¹‰

**å®šä¹‰ 1.1 (Scaled Dot-Product Attention)**:

ç»™å®šæŸ¥è¯¢ $Q \in \mathbb{R}^{n \times d_k}$ï¼Œé”® $K \in \mathbb{R}^{m \times d_k}$ï¼Œå€¼ $V \in \mathbb{R}^{m \times d_v}$ï¼š

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V
$$

**ç¬¦å·è¯´æ˜**ï¼š

- $n$ï¼šæŸ¥è¯¢åºåˆ—é•¿åº¦
- $m$ï¼šé”®/å€¼åºåˆ—é•¿åº¦
- $d_k$ï¼šé”®/æŸ¥è¯¢ç»´åº¦
- $d_v$ï¼šå€¼ç»´åº¦

**æ­¥éª¤åˆ†è§£**ï¼š

1. **è®¡ç®—ç›¸ä¼¼åº¦**ï¼š$S = QK^T \in \mathbb{R}^{n \times m}$
2. **ç¼©æ”¾**ï¼š$S' = S / \sqrt{d_k}$
3. **å½’ä¸€åŒ–**ï¼š$A = \text{softmax}(S') \in \mathbb{R}^{n \times m}$
4. **åŠ æƒæ±‚å’Œ**ï¼š$\text{Output} = AV \in \mathbb{R}^{n \times d_v}$

---

### 2. ç¼©æ”¾å› å­çš„ä½œç”¨

**ä¸ºä»€ä¹ˆé™¤ä»¥ $\sqrt{d_k}$ï¼Ÿ**

**å®šç† 2.1 (ç¼©æ”¾å¿…è¦æ€§, Vaswani et al. 2017)**:

å‡è®¾ $Q, K$ çš„å…ƒç´ ç‹¬ç«‹åŒåˆ†å¸ƒï¼Œå‡å€¼0ï¼Œæ–¹å·®1ï¼Œåˆ™ï¼š

$$
\mathbb{E}[QK^T] = 0, \quad \text{Var}[QK^T] = d_k
$$

**é—®é¢˜**ï¼šå½“ $d_k$ å¾ˆå¤§æ—¶ï¼Œç‚¹ç§¯æ–¹å·®å¾ˆå¤§ã€‚

**åæœ**ï¼š

- Softmaxè¾“å…¥è¿›å…¥é¥±å’ŒåŒº
- æ¢¯åº¦æ¥è¿‘0
- è®­ç»ƒå›°éš¾

**è§£å†³æ–¹æ¡ˆ**ï¼šé™¤ä»¥ $\sqrt{d_k}$ï¼Œä½¿æ–¹å·®å½’ä¸€åŒ–ä¸º1ã€‚

$$
\text{Var}\left[\frac{QK^T}{\sqrt{d_k}}\right] = 1
$$

---

### 3. Softmaxçš„ä½œç”¨

**Softmaxå®šä¹‰**ï¼š

$$
\text{softmax}(z_i) = \frac{\exp(z_i)}{\sum_{j=1}^{m} \exp(z_j)}
$$

**æ€§è´¨**ï¼š

1. **å½’ä¸€åŒ–**ï¼š$\sum_{j=1}^{m} A_{ij} = 1$ï¼ˆæ¯è¡Œå’Œä¸º1ï¼‰
2. **éè´Ÿ**ï¼š$A_{ij} \geq 0$
3. **å¯å¾®**ï¼šæ¢¯åº¦å­˜åœ¨

**æ„ä¹‰**ï¼š

- å°†ç›¸ä¼¼åº¦è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ
- é«˜ç›¸ä¼¼åº¦ä½ç½®è·å¾—æ›´å¤§æƒé‡
- ä½ç›¸ä¼¼åº¦ä½ç½®æƒé‡æ¥è¿‘0

---

## ğŸ”¬ Multi-Head Attention

### 1. æ ¸å¿ƒæ€æƒ³

**å•å¤´æ³¨æ„åŠ›çš„å±€é™**ï¼š

- åªèƒ½å­¦ä¹ ä¸€ç§ç›¸ä¼¼åº¦åº¦é‡
- éš¾ä»¥åŒæ—¶æ•è·å¤šç§å…³ç³»

**å¤šå¤´æ³¨æ„åŠ›**ï¼š

- å¹¶è¡Œè¿è¡Œå¤šä¸ªæ³¨æ„åŠ›"å¤´"
- æ¯ä¸ªå¤´å­¦ä¹ ä¸åŒçš„è¡¨ç¤ºå­ç©ºé—´
- æ‹¼æ¥æ‰€æœ‰å¤´çš„è¾“å‡º

---

### 2. æ•°å­¦å½¢å¼åŒ–

**å®šä¹‰ 2.1 (Multi-Head Attention)**:

$$
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h) W^O
$$

å…¶ä¸­æ¯ä¸ªå¤´ï¼š

$$
\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
$$

**å‚æ•°**ï¼š

- $W_i^Q \in \mathbb{R}^{d_{\text{model}} \times d_k}$ï¼šæŸ¥è¯¢æŠ•å½±
- $W_i^K \in \mathbb{R}^{d_{\text{model}} \times d_k}$ï¼šé”®æŠ•å½±
- $W_i^V \in \mathbb{R}^{d_{\text{model}} \times d_v}$ï¼šå€¼æŠ•å½±
- $W^O \in \mathbb{R}^{hd_v \times d_{\text{model}}}$ï¼šè¾“å‡ºæŠ•å½±

**å…¸å‹è®¾ç½®**ï¼š

- $h = 8$ï¼ˆå¤´æ•°ï¼‰
- $d_k = d_v = d_{\text{model}} / h = 64$ï¼ˆTransformerä¸­ $d_{\text{model}} = 512$ï¼‰

---

### 3. ä¸ºä»€ä¹ˆå¤šå¤´æœ‰æ•ˆ

**å®šç† 3.1 (è¡¨ç¤ºèƒ½åŠ›)**:

å¤šå¤´æ³¨æ„åŠ›å¯ä»¥åŒæ—¶å…³æ³¨ä¸åŒçš„è¡¨ç¤ºå­ç©ºé—´ã€‚

**ç›´è§‰**ï¼š

- **Head 1**ï¼šå¯èƒ½å…³æ³¨å¥æ³•å…³ç³»ï¼ˆä¸»è°“å®¾ï¼‰
- **Head 2**ï¼šå¯èƒ½å…³æ³¨è¯­ä¹‰å…³ç³»ï¼ˆåŒä¹‰è¯ï¼‰
- **Head 3**ï¼šå¯èƒ½å…³æ³¨ä½ç½®å…³ç³»ï¼ˆç›¸é‚»è¯ï¼‰

**å®éªŒè¯æ®**ï¼š

- ä¸åŒå¤´å­¦ä¹ åˆ°ä¸åŒçš„æ³¨æ„åŠ›æ¨¡å¼
- æŸäº›å¤´ä¸“æ³¨äºå±€éƒ¨ï¼ŒæŸäº›å¤´ä¸“æ³¨äºå…¨å±€
- å¤šå¤´ä¼˜äºå•å¤´ï¼ˆå®éªŒéªŒè¯ï¼‰

---

## ğŸ’» Self-Attention vs Cross-Attention

### 1. Self-Attention

**å®šä¹‰**ï¼š$Q, K, V$ æ¥è‡ªåŒä¸€åºåˆ—ã€‚

$$
\text{SelfAttention}(X) = \text{Attention}(XW^Q, XW^K, XW^V)
$$

**åº”ç”¨**ï¼š

- Transformerç¼–ç å™¨
- BERT
- GPT

**ä½œç”¨**ï¼š

- æ¯ä¸ªä½ç½®å…³æ³¨åºåˆ—ä¸­çš„æ‰€æœ‰ä½ç½®
- æ•è·ä¸Šä¸‹æ–‡ä¿¡æ¯
- å­¦ä¹ è¯ä¹‹é—´çš„å…³ç³»

---

### 2. Cross-Attention

**å®šä¹‰**ï¼š$Q$ æ¥è‡ªä¸€ä¸ªåºåˆ—ï¼Œ$K, V$ æ¥è‡ªå¦ä¸€ä¸ªåºåˆ—ã€‚

$$
\text{CrossAttention}(X, Y) = \text{Attention}(XW^Q, YW^K, YW^V)
$$

**åº”ç”¨**ï¼š

- Transformerè§£ç å™¨
- æœºå™¨ç¿»è¯‘
- å›¾åƒæè¿°ç”Ÿæˆ

**ä½œç”¨**ï¼š

- æŸ¥è¯¢åºåˆ—å…³æ³¨æºåºåˆ—
- å¯¹é½ä¸åŒæ¨¡æ€
- ä¿¡æ¯èåˆ

---

### 3. åº”ç”¨åœºæ™¯

| ç±»å‹ | Qæ¥æº | K/Væ¥æº | åº”ç”¨ |
| ---- | ---- | ---- | ---- |
| **Self-Attention** | åŒä¸€åºåˆ— | åŒä¸€åºåˆ— | BERT, GPT |
| **Cross-Attention** | ç›®æ ‡åºåˆ— | æºåºåˆ— | ç¿»è¯‘, VQA |
| **Masked Self-Attention** | åŒä¸€åºåˆ— | åŒä¸€åºåˆ—ï¼ˆæ©ç ï¼‰ | GPTè§£ç  |

---

## ğŸ¨ Pythonå®ç°

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class ScaledDotProductAttention(nn.Module):
    """Scaled Dot-Product Attention"""
    def __init__(self, dropout=0.1):
        super().__init__()
        self.dropout = nn.Dropout(dropout)

    def forward(self, Q, K, V, mask=None):
        """
        Args:
            Q: (batch, n_heads, seq_len_q, d_k)
            K: (batch, n_heads, seq_len_k, d_k)
            V: (batch, n_heads, seq_len_v, d_v)
            mask: (batch, 1, seq_len_q, seq_len_k) or None

        Returns:
            output: (batch, n_heads, seq_len_q, d_v)
            attention_weights: (batch, n_heads, seq_len_q, seq_len_k)
        """
        d_k = Q.size(-1)

        # 1. è®¡ç®—ç›¸ä¼¼åº¦: (batch, n_heads, seq_len_q, seq_len_k)
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)

        # 2. åº”ç”¨æ©ç  (å¯é€‰)
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)

        # 3. Softmaxå½’ä¸€åŒ–
        attention_weights = F.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)

        # 4. åŠ æƒæ±‚å’Œ
        output = torch.matmul(attention_weights, V)

        return output, attention_weights


class MultiHeadAttention(nn.Module):
    """Multi-Head Attention"""
    def __init__(self, d_model=512, n_heads=8, dropout=0.1):
        super().__init__()
        assert d_model % n_heads == 0, "d_model must be divisible by n_heads"

        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        self.d_v = d_model // n_heads

        # çº¿æ€§æŠ•å½±å±‚
        self.W_Q = nn.Linear(d_model, d_model)
        self.W_K = nn.Linear(d_model, d_model)
        self.W_V = nn.Linear(d_model, d_model)
        self.W_O = nn.Linear(d_model, d_model)

        self.attention = ScaledDotProductAttention(dropout)
        self.dropout = nn.Dropout(dropout)
        self.layer_norm = nn.LayerNorm(d_model)

    def forward(self, Q, K, V, mask=None):
        """
        Args:
            Q: (batch, seq_len_q, d_model)
            K: (batch, seq_len_k, d_model)
            V: (batch, seq_len_v, d_model)
            mask: (batch, seq_len_q, seq_len_k) or None

        Returns:
            output: (batch, seq_len_q, d_model)
            attention_weights: (batch, n_heads, seq_len_q, seq_len_k)
        """
        batch_size = Q.size(0)
        residual = Q

        # 1. çº¿æ€§æŠ•å½±å¹¶åˆ†å‰²æˆå¤šå¤´
        # (batch, seq_len, d_model) -> (batch, seq_len, n_heads, d_k) -> (batch, n_heads, seq_len, d_k)
        Q = self.W_Q(Q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        K = self.W_K(K).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        V = self.W_V(V).view(batch_size, -1, self.n_heads, self.d_v).transpose(1, 2)

        # 2. è°ƒæ•´maskç»´åº¦
        if mask is not None:
            mask = mask.unsqueeze(1)  # (batch, 1, seq_len_q, seq_len_k)

        # 3. åº”ç”¨æ³¨æ„åŠ›
        output, attention_weights = self.attention(Q, K, V, mask)

        # 4. æ‹¼æ¥å¤šå¤´
        # (batch, n_heads, seq_len_q, d_v) -> (batch, seq_len_q, n_heads, d_v) -> (batch, seq_len_q, d_model)
        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)

        # 5. è¾“å‡ºæŠ•å½±
        output = self.W_O(output)
        output = self.dropout(output)

        # 6. æ®‹å·®è¿æ¥ + Layer Norm
        output = self.layer_norm(output + residual)

        return output, attention_weights


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # å‚æ•°
    batch_size = 2
    seq_len = 10
    d_model = 512
    n_heads = 8

    # åˆ›å»ºæ¨¡å‹
    mha = MultiHeadAttention(d_model=d_model, n_heads=n_heads)

    # è¾“å…¥
    x = torch.randn(batch_size, seq_len, d_model)

    # Self-Attention
    output, attn_weights = mha(x, x, x)

    print(f"Input shape: {x.shape}")
    print(f"Output shape: {output.shape}")
    print(f"Attention weights shape: {attn_weights.shape}")

    # å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡
    import matplotlib.pyplot as plt

    # å–ç¬¬ä¸€ä¸ªæ ·æœ¬çš„ç¬¬ä¸€ä¸ªå¤´
    attn = attn_weights[0, 0].detach().numpy()

    plt.figure(figsize=(8, 6))
    plt.imshow(attn, cmap='viridis')
    plt.colorbar()
    plt.xlabel('Key Position')
    plt.ylabel('Query Position')
    plt.title('Attention Weights (Head 1)')
    plt.tight_layout()
    # plt.show()


# Cross-Attentionç¤ºä¾‹
class CrossAttentionLayer(nn.Module):
    """Cross-Attentionå±‚ (ç”¨äºTransformerè§£ç å™¨)"""
    def __init__(self, d_model=512, n_heads=8, dropout=0.1):
        super().__init__()
        self.cross_attn = MultiHeadAttention(d_model, n_heads, dropout)

    def forward(self, decoder_input, encoder_output, mask=None):
        """
        Args:
            decoder_input: (batch, seq_len_dec, d_model) - Query
            encoder_output: (batch, seq_len_enc, d_model) - Key & Value
            mask: (batch, seq_len_dec, seq_len_enc) or None

        Returns:
            output: (batch, seq_len_dec, d_model)
        """
        output, attn_weights = self.cross_attn(
            Q=decoder_input,
            K=encoder_output,
            V=encoder_output,
            mask=mask
        )
        return output, attn_weights


# Masked Self-Attention (ç”¨äºGPT)
def create_causal_mask(seq_len):
    """åˆ›å»ºå› æœæ©ç  (ä¸‹ä¸‰è§’çŸ©é˜µ)"""
    mask = torch.tril(torch.ones(seq_len, seq_len))
    return mask  # (seq_len, seq_len)


# ç¤ºä¾‹ï¼šGPTé£æ ¼çš„Masked Self-Attention
if __name__ == "__main__":
    seq_len = 5
    d_model = 512

    mha = MultiHeadAttention(d_model=d_model, n_heads=8)
    x = torch.randn(1, seq_len, d_model)

    # åˆ›å»ºå› æœæ©ç 
    causal_mask = create_causal_mask(seq_len).unsqueeze(0)  # (1, seq_len, seq_len)

    # Masked Self-Attention
    output, attn_weights = mha(x, x, x, mask=causal_mask)

    print(f"\nCausal Mask:\n{causal_mask[0]}")
    print(f"\nAttention weights (with mask):\n{attn_weights[0, 0].detach()}")
```

---

## ğŸ“š ç†è®ºåˆ†æ

### 1. æ³¨æ„åŠ›çš„è¡¨ç¤ºèƒ½åŠ›

**å®šç† 1.1 (Universal Approximation)**:

Transformerå¯ä»¥è¿‘ä¼¼ä»»æ„åºåˆ—åˆ°åºåˆ—çš„å‡½æ•°ï¼ˆåœ¨é€‚å½“æ¡ä»¶ä¸‹ï¼‰ã€‚

**è¯æ˜è¦ç‚¹**ï¼š

- æ³¨æ„åŠ›å¯ä»¥å®ç°ä»»æ„çš„åŠ æƒå¹³å‡
- å¤šå±‚Transformerå¯ä»¥ç»„åˆå¤æ‚æ“ä½œ
- FFNæä¾›éçº¿æ€§å˜æ¢

**å®è·µæ„ä¹‰**ï¼š

- ç†è®ºä¸Šå¯ä»¥å­¦ä¹ ä»»æ„åºåˆ—æ¨¡å¼
- å®é™…å—é™äºæ•°æ®å’Œä¼˜åŒ–

---

### 2. è®¡ç®—å¤æ‚åº¦

**Self-Attentionå¤æ‚åº¦**ï¼š

| æ“ä½œ | å¤æ‚åº¦ |
| ---- | ---- |
| **è®¡ç®— $QK^T$** | $O(n^2 d)$ |
| **Softmax** | $O(n^2)$ |
| **åŠ æƒæ±‚å’Œ** | $O(n^2 d)$ |
| **æ€»è®¡** | $O(n^2 d)$ |

å…¶ä¸­ $n$ æ˜¯åºåˆ—é•¿åº¦ï¼Œ$d$ æ˜¯ç»´åº¦ã€‚

**å¯¹æ¯”RNN**ï¼š

- RNNï¼š$O(nd^2)$ï¼ˆé¡ºåºè®¡ç®—ï¼‰
- Attentionï¼š$O(n^2d)$ï¼ˆå¹¶è¡Œè®¡ç®—ï¼‰

**é—®é¢˜**ï¼šåºåˆ—é•¿åº¦ $n$ å¾ˆå¤§æ—¶ï¼Œ$n^2$ é¡¹æˆä¸ºç“¶é¢ˆã€‚

---

### 3. é•¿åºåˆ—é—®é¢˜

**æŒ‘æˆ˜**ï¼š

- å†…å­˜ï¼šå­˜å‚¨ $n \times n$ æ³¨æ„åŠ›çŸ©é˜µ
- è®¡ç®—ï¼š$O(n^2)$ å¤æ‚åº¦

**ç¤ºä¾‹**ï¼š

- $n = 1024$ï¼š1Mæ³¨æ„åŠ›æƒé‡
- $n = 4096$ï¼š16Mæ³¨æ„åŠ›æƒé‡
- $n = 100k$ï¼š10Bæ³¨æ„åŠ›æƒé‡ï¼ˆä¸å¯è¡Œï¼ï¼‰

**è§£å†³æ–¹æ¡ˆ**ï¼šè§ä¸‹èŠ‚"æ³¨æ„åŠ›å˜ä½“"

---

## ğŸ”§ æ³¨æ„åŠ›å˜ä½“

### 1. Sparse Attention

**æ ¸å¿ƒæ€æƒ³**ï¼šåªè®¡ç®—éƒ¨åˆ†æ³¨æ„åŠ›æƒé‡ã€‚

**Longformer (Beltagy et al., 2020)**ï¼š

- **å±€éƒ¨æ³¨æ„åŠ›**ï¼šæ¯ä¸ªtokenåªå…³æ³¨çª—å£å†…çš„token
- **å…¨å±€æ³¨æ„åŠ›**ï¼šç‰¹æ®Štokenå…³æ³¨æ‰€æœ‰token
- **å¤æ‚åº¦**ï¼š$O(n \cdot w)$ï¼Œå…¶ä¸­ $w$ æ˜¯çª—å£å¤§å°

**BigBird (Zaheer et al., 2020)**ï¼š

- **éšæœºæ³¨æ„åŠ›** + **çª—å£æ³¨æ„åŠ›** + **å…¨å±€æ³¨æ„åŠ›**
- **ç†è®ºä¿è¯**ï¼šä»æ˜¯é€šç”¨è¿‘ä¼¼å™¨

---

### 2. Linear Attention

**æ ¸å¿ƒæ€æƒ³**ï¼šé€šè¿‡æ ¸æŠ€å·§é™ä½å¤æ‚åº¦ã€‚

**Linformer (Wang et al., 2020)**ï¼š

- å°† $K, V$ æŠ•å½±åˆ°ä½ç»´
- å¤æ‚åº¦ï¼š$O(nk)$ï¼Œå…¶ä¸­ $k \ll n$

**Performer (Choromanski et al., 2021)**ï¼š

- ç”¨éšæœºç‰¹å¾è¿‘ä¼¼Softmax
- å¤æ‚åº¦ï¼š$O(nd)$ï¼ˆçº¿æ€§ï¼ï¼‰

**å…¬å¼**ï¼š

$$
\text{Attention}(Q, K, V) \approx \phi(Q) (\phi(K)^T V)
$$

å…¶ä¸­ $\phi$ æ˜¯ç‰¹å¾æ˜ å°„ã€‚

---

### 3. Flash Attention

**æ ¸å¿ƒæ€æƒ³**ï¼šä¼˜åŒ–å†…å­˜è®¿é—®æ¨¡å¼ã€‚

**Flash Attention (Dao et al., 2022)**ï¼š

- ä¸æ˜¾å¼å­˜å‚¨ $n \times n$ æ³¨æ„åŠ›çŸ©é˜µ
- åˆ†å—è®¡ç®—ï¼Œå‡å°‘HBMè®¿é—®
- **åŠ é€Ÿ**ï¼š2-4å€
- **å†…å­˜**ï¼šçº¿æ€§è€ŒéäºŒæ¬¡

**æ„ä¹‰**ï¼š

- å…è®¸æ›´é•¿åºåˆ—
- æ›´é«˜æ•ˆè®­ç»ƒ
- æˆä¸ºæ–°æ ‡å‡†

---

## ğŸ”§ å®é™…åº”ç”¨æ¡ˆä¾‹

### 1. è‡ªç„¶è¯­è¨€å¤„ç†

**Transformeræ¨¡å‹**:

æ³¨æ„åŠ›æœºåˆ¶æ˜¯Transformerçš„æ ¸å¿ƒï¼Œå¹¿æ³›åº”ç”¨äºNLPä»»åŠ¡ã€‚

**åº”ç”¨åœºæ™¯**:

- **æœºå™¨ç¿»è¯‘**: BERTã€GPTã€T5ç­‰
- **æ–‡æœ¬æ‘˜è¦**: æå–å…³é”®ä¿¡æ¯
- **é—®ç­”ç³»ç»Ÿ**: ç†è§£é—®é¢˜å’Œæ–‡æ¡£
- **æƒ…æ„Ÿåˆ†æ**: è¯†åˆ«æ–‡æœ¬æƒ…æ„Ÿ

**å®è·µç¤ºä¾‹**:

```python
import torch
import torch.nn as nn
from transformers import BertModel

# BERTä½¿ç”¨å¤šå¤´è‡ªæ³¨æ„åŠ›
model = BertModel.from_pretrained('bert-base-uncased')

# è¾“å…¥æ–‡æœ¬
text = "The cat sat on the mat"
inputs = tokenizer(text, return_tensors='pt')

# è·å–æ³¨æ„åŠ›æƒé‡
outputs = model(**inputs, output_attentions=True)
attention_weights = outputs.attentions  # 12å±‚ï¼Œæ¯å±‚12ä¸ªå¤´

# å¯è§†åŒ–æ³¨æ„åŠ›ï¼ˆæŸ¥çœ‹"cat"å…³æ³¨å“ªäº›è¯ï¼‰
visualize_attention(attention_weights, tokens)
```

---

### 2. è®¡ç®—æœºè§†è§‰

**Vision Transformer (ViT)**:

å°†å›¾åƒåˆ†å‰²æˆpatchesï¼Œä½¿ç”¨è‡ªæ³¨æ„åŠ›å¤„ç†ã€‚

**æ¶æ„**:

1. å›¾åƒ â†’ patches (16Ã—16)
2. Patches â†’ çº¿æ€§æŠ•å½± â†’ åµŒå…¥
3. ä½ç½®ç¼–ç 
4. Transformerç¼–ç å™¨ï¼ˆè‡ªæ³¨æ„åŠ›ï¼‰
5. åˆ†ç±»å¤´

**ä¼˜åŠ¿**:

- æ•è·å…¨å±€ä¾èµ–
- æ— éœ€å·ç§¯å½’çº³åç½®
- å¯æ‰©å±•æ€§å¼º

**åº”ç”¨**:

- å›¾åƒåˆ†ç±»
- ç›®æ ‡æ£€æµ‹
- å›¾åƒåˆ†å‰²

---

### 3. å¤šæ¨¡æ€å­¦ä¹ 

**CLIP (Contrastive Language-Image Pre-training)**:

ä½¿ç”¨äº¤å‰æ³¨æ„åŠ›å¯¹é½æ–‡æœ¬å’Œå›¾åƒã€‚

**æ¶æ„**:

- æ–‡æœ¬ç¼–ç å™¨ï¼ˆTransformerï¼‰
- å›¾åƒç¼–ç å™¨ï¼ˆViTæˆ–CNNï¼‰
- å¯¹æ¯”å­¦ä¹ ç›®æ ‡

**åº”ç”¨**:

- å›¾åƒ-æ–‡æœ¬æ£€ç´¢
- é›¶æ ·æœ¬åˆ†ç±»
- å›¾åƒç”Ÿæˆï¼ˆDALL-Eï¼‰

**å®è·µç¤ºä¾‹**:

```python
import clip

# åŠ è½½é¢„è®­ç»ƒCLIPæ¨¡å‹
model, preprocess = clip.load("ViT-B/32")

# ç¼–ç æ–‡æœ¬å’Œå›¾åƒ
text = clip.tokenize(["a photo of a cat", "a photo of a dog"])
image = preprocess(Image.open("cat.jpg")).unsqueeze(0)

# è·å–ç‰¹å¾
text_features = model.encode_text(text)
image_features = model.encode_image(image)

# è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶å¯¹é½ï¼‰
similarity = (image_features @ text_features.T).softmax(dim=-1)
```

---

### 4. è¯­éŸ³è¯†åˆ«

**Speech Transformer**:

ä½¿ç”¨è‡ªæ³¨æ„åŠ›å¤„ç†è¯­éŸ³åºåˆ—ã€‚

**ä¼˜åŠ¿**:

- å¹¶è¡Œå¤„ç†ï¼ˆç›¸æ¯”RNNï¼‰
- æ•è·é•¿è·ç¦»ä¾èµ–
- æ›´å¥½çš„æ€§èƒ½

**åº”ç”¨**:

- è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰
- è¯­éŸ³ç¿»è¯‘
- è¯­éŸ³åˆæˆ

---

### 5. æ¨èç³»ç»Ÿ

**Transformer-basedæ¨è**:

ä½¿ç”¨è‡ªæ³¨æ„åŠ›å»ºæ¨¡ç”¨æˆ·-ç‰©å“äº¤äº’åºåˆ—ã€‚

**æ¶æ„**:

- ç”¨æˆ·è¡Œä¸ºåºåˆ— â†’ åµŒå…¥
- Transformerç¼–ç å™¨ï¼ˆè‡ªæ³¨æ„åŠ›ï¼‰
- é¢„æµ‹ä¸‹ä¸€ä¸ªç‰©å“

**ä¼˜åŠ¿**:

- æ•è·åºåˆ—æ¨¡å¼
- å¤„ç†é•¿åºåˆ—
- å¯è§£é‡Šæ€§ï¼ˆæ³¨æ„åŠ›æƒé‡ï¼‰

**å®è·µç¤ºä¾‹**:

```python
class TransformerRecommender(nn.Module):
    def __init__(self, n_items, d_model=512, n_heads=8):
        super().__init__()
        self.item_embedding = nn.Embedding(n_items, d_model)
        self.pos_embedding = nn.Embedding(1000, d_model)  # æœ€å¤§åºåˆ—é•¿åº¦
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model, n_heads),
            num_layers=6
        )
        self.output = nn.Linear(d_model, n_items)

    def forward(self, item_seq):
        # åµŒå…¥
        x = self.item_embedding(item_seq)
        pos = torch.arange(item_seq.size(1))
        x = x + self.pos_embedding(pos)

        # Transformerç¼–ç 
        x = self.transformer(x)

        # é¢„æµ‹ä¸‹ä¸€ä¸ªç‰©å“
        logits = self.output(x[:, -1])  # ä½¿ç”¨æœ€åä¸€ä¸ªä½ç½®
        return logits
```

---

### 6. æ—¶é—´åºåˆ—é¢„æµ‹

**Time Series Transformer**:

ä½¿ç”¨è‡ªæ³¨æ„åŠ›é¢„æµ‹æ—¶é—´åºåˆ—ã€‚

**åº”ç”¨**:

- è‚¡ç¥¨ä»·æ ¼é¢„æµ‹
- å¤©æ°”é¢„æŠ¥
- èƒ½æºéœ€æ±‚é¢„æµ‹

**ä¼˜åŠ¿**:

- æ•è·é•¿æœŸä¾èµ–
- å¤„ç†ä¸è§„åˆ™æ—¶é—´é—´éš”
- å¤šå˜é‡æ—¶é—´åºåˆ—

**å®è·µç¤ºä¾‹**:

```python
class TimeSeriesTransformer(nn.Module):
    def __init__(self, input_dim, d_model=512, n_heads=8):
        super().__init__()
        self.input_proj = nn.Linear(input_dim, d_model)
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model, n_heads),
            num_layers=6
        )
        self.output = nn.Linear(d_model, input_dim)

    def forward(self, x):
        # x: [batch, seq_len, features]
        x = self.input_proj(x)
        x = self.transformer(x)
        predictions = self.output(x)
        return predictions
```

---

### 7. ä»£ç ç”Ÿæˆä¸ç†è§£

**CodeBERT / CodeT5**:

ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶ç†è§£ä»£ç ã€‚

**åº”ç”¨**:

- ä»£ç è¡¥å…¨
- ä»£ç æœç´¢
- ä»£ç æ‘˜è¦
- Bugæ£€æµ‹

**ç‰¹ç‚¹**:

- å¤„ç†ä»£ç çš„å±‚æ¬¡ç»“æ„
- ç†è§£ä»£ç è¯­ä¹‰
- è·¨è¯­è¨€ä»£ç ç†è§£

---

### 8. å›¾ç¥ç»ç½‘ç»œ

**Graph Attention Network (GAT)**:

å°†æ³¨æ„åŠ›æœºåˆ¶åº”ç”¨äºå›¾ç»“æ„ã€‚

**æ ¸å¿ƒæ€æƒ³**:

- èŠ‚ç‚¹ä½œä¸ºQuery/Key/Value
- æ³¨æ„åŠ›æƒé‡åŸºäºè¾¹
- èšåˆé‚»å±…ä¿¡æ¯

**å…¬å¼**:
$$
h_i' = \sigma\left(\sum_{j \in \mathcal{N}(i)} \alpha_{ij} W h_j\right)
$$

å…¶ä¸­ $\alpha_{ij}$ æ˜¯æ³¨æ„åŠ›æƒé‡ã€‚

**åº”ç”¨**:

- ç¤¾äº¤ç½‘ç»œåˆ†æ
- åˆ†å­æ€§è´¨é¢„æµ‹
- æ¨èç³»ç»Ÿï¼ˆç”¨æˆ·-ç‰©å“å›¾ï¼‰

---

### 9. å¼ºåŒ–å­¦ä¹ 

**Attention in RL**:

ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶æ”¹è¿›å¼ºåŒ–å­¦ä¹ ã€‚

**åº”ç”¨**:

- **è§†è§‰å¯¼èˆª**: å…³æ³¨å…³é”®è§†è§‰ç‰¹å¾
- **å¤šæ™ºèƒ½ä½“**: å…³æ³¨å…¶ä»–æ™ºèƒ½ä½“
- **å±‚æ¬¡RL**: å…³æ³¨ä¸åŒæ—¶é—´å°ºåº¦

**å®è·µç¤ºä¾‹**:

```python
class AttentionPolicy(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        self.attention = nn.MultiheadAttention(state_dim, num_heads=8)
        self.policy_head = nn.Linear(state_dim, action_dim)

    def forward(self, state_history):
        # state_history: [seq_len, state_dim]
        # ä½¿ç”¨è‡ªæ³¨æ„åŠ›å…³æ³¨é‡è¦å†å²çŠ¶æ€
        attended_state, _ = self.attention(
            state_history, state_history, state_history
        )
        # ä½¿ç”¨æœ€åä¸€ä¸ªçŠ¶æ€
        action_logits = self.policy_head(attended_state[-1])
        return action_logits
```

---

### 10. åŒ»å­¦å½±åƒåˆ†æ

**Medical Image Analysis with Attention**:

ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶åˆ†æåŒ»å­¦å½±åƒã€‚

**åº”ç”¨**:

- ç—…ç¶æ£€æµ‹
- å›¾åƒåˆ†å‰²
- ç–¾ç—…è¯Šæ–­

**ä¼˜åŠ¿**:

- å¯è§£é‡Šæ€§ï¼ˆæ³¨æ„åŠ›çƒ­å›¾ï¼‰
- å…³æ³¨å…³é”®åŒºåŸŸ
- å¤šæ¨¡æ€èåˆï¼ˆå›¾åƒ+æ–‡æœ¬æŠ¥å‘Šï¼‰

---

## ğŸ“ ç›¸å…³è¯¾ç¨‹

| å¤§å­¦ | è¯¾ç¨‹ |
| ---- | ---- |
| **Stanford** | CS224N Natural Language Processing |
| **Stanford** | CS25 Transformers United |
| **MIT** | 6.S191 Introduction to Deep Learning |
| **CMU** | 11-747 Neural Networks for NLP |

---

## ğŸ“– å‚è€ƒæ–‡çŒ®

1. **Vaswani et al. (2017)**. "Attention Is All You Need". *NeurIPS*.

2. **Bahdanau et al. (2015)**. "Neural Machine Translation by Jointly Learning to Align and Translate". *ICLR*.

3. **Beltagy et al. (2020)**. "Longformer: The Long-Document Transformer". *arXiv*.

4. **Zaheer et al. (2020)**. "Big Bird: Transformers for Longer Sequences". *NeurIPS*.

5. **Choromanski et al. (2021)**. "Rethinking Attention with Performers". *ICLR*.

6. **Dao et al. (2022)**. "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness". *NeurIPS*.

---

*æœ€åæ›´æ–°ï¼š2025å¹´12æœˆ20æ—¥*-
