# Dropoutç†è®º (Dropout Theory)

> **Dropout: Mathematics of Regularization**
>
> æ·±åº¦å­¦ä¹ æ­£åˆ™åŒ–çš„ç»å…¸æŠ€æœ¯

---

## ç›®å½•

- [Dropoutç†è®º (Dropout Theory)](#dropoutç†è®º-dropout-theory)
  - [ç›®å½•](#ç›®å½•)
  - [ğŸ“‹ æ ¸å¿ƒæ€æƒ³](#-æ ¸å¿ƒæ€æƒ³)
  - [ğŸ¯ è¿‡æ‹Ÿåˆé—®é¢˜](#-è¿‡æ‹Ÿåˆé—®é¢˜)
    - [1. é—®é¢˜å®šä¹‰](#1-é—®é¢˜å®šä¹‰)
    - [2. ä¼ ç»Ÿæ­£åˆ™åŒ–æ–¹æ³•](#2-ä¼ ç»Ÿæ­£åˆ™åŒ–æ–¹æ³•)
  - [ğŸ“Š Dropoutç®—æ³•](#-dropoutç®—æ³•)
    - [1. è®­ç»ƒæ—¶çš„Dropout](#1-è®­ç»ƒæ—¶çš„dropout)
    - [2. æ¨ç†æ—¶çš„Dropout](#2-æ¨ç†æ—¶çš„dropout)
    - [3. Inverted Dropout](#3-inverted-dropout)
  - [ğŸ”¬ æ•°å­¦åˆ†æ](#-æ•°å­¦åˆ†æ)
    - [1. é›†æˆå­¦ä¹ è§†è§’](#1-é›†æˆå­¦ä¹ è§†è§’)
    - [2. è´å¶æ–¯è§†è§’](#2-è´å¶æ–¯è§†è§’)
    - [3. ä¿¡æ¯è®ºè§†è§’](#3-ä¿¡æ¯è®ºè§†è§’)
  - [ğŸ’» åå‘ä¼ æ’­](#-åå‘ä¼ æ’­)
    - [1. å‰å‘ä¼ æ’­](#1-å‰å‘ä¼ æ’­)
    - [2. åå‘ä¼ æ’­](#2-åå‘ä¼ æ’­)
  - [ğŸ¨ Pythonå®ç°](#-pythonå®ç°)
  - [ğŸ“š ç†è®ºæ·±åŒ–](#-ç†è®ºæ·±åŒ–)
    - [1. ä¸ºä»€ä¹ˆDropoutæœ‰æ•ˆ](#1-ä¸ºä»€ä¹ˆdropoutæœ‰æ•ˆ)
    - [2. Dropoutä½œä¸ºæ­£åˆ™åŒ–](#2-dropoutä½œä¸ºæ­£åˆ™åŒ–)
    - [3. æœ€ä¼˜Dropoutç‡](#3-æœ€ä¼˜dropoutç‡)
  - [ğŸ”§ Dropoutå˜ä½“](#-dropoutå˜ä½“)
    - [1. DropConnect](#1-dropconnect)
    - [2. Spatial Dropout](#2-spatial-dropout)
    - [3. Variational Dropout](#3-variational-dropout)
  - [ğŸ“ ç›¸å…³è¯¾ç¨‹](#-ç›¸å…³è¯¾ç¨‹)
  - [ğŸ“– å‚è€ƒæ–‡çŒ®](#-å‚è€ƒæ–‡çŒ®)

---

## ğŸ“‹ æ ¸å¿ƒæ€æƒ³

**Dropout**é€šè¿‡**éšæœºä¸¢å¼ƒç¥ç»å…ƒ**æ¥é˜²æ­¢è¿‡æ‹Ÿåˆã€‚

**æ ¸å¿ƒæ“ä½œ**ï¼š

```text
è®­ç»ƒæ—¶:
  è¾“å…¥ â†’ éšæœºä¸¢å¼ƒéƒ¨åˆ†ç¥ç»å…ƒ â†’ ç¼©æ”¾ â†’ è¾“å‡º

æ¨ç†æ—¶:
  è¾“å…¥ â†’ ä½¿ç”¨æ‰€æœ‰ç¥ç»å…ƒ â†’ è¾“å‡º
```

**å…³é”®å‚æ•°**ï¼š

- **Dropoutç‡ $p$**ï¼šä¸¢å¼ƒæ¦‚ç‡ï¼ˆé€šå¸¸0.5ï¼‰
- **ä¿ç•™ç‡ $1-p$**ï¼šä¿ç•™æ¦‚ç‡

**ä¸»è¦æ•ˆæœ**ï¼š

- é˜²æ­¢è¿‡æ‹Ÿåˆ
- æé«˜æ³›åŒ–èƒ½åŠ›
- å‡å°‘ç¥ç»å…ƒå…±é€‚åº”

---

## ğŸ¯ è¿‡æ‹Ÿåˆé—®é¢˜

### 1. é—®é¢˜å®šä¹‰

**å®šä¹‰ 1.1 (è¿‡æ‹Ÿåˆ)**:

æ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šè¡¨ç°å¾ˆå¥½ï¼Œä½†åœ¨æµ‹è¯•é›†ä¸Šè¡¨ç°å·®ã€‚

**åŸå› **ï¼š

- æ¨¡å‹å®¹é‡è¿‡å¤§
- è®­ç»ƒæ•°æ®ä¸è¶³
- è®­ç»ƒæ—¶é—´è¿‡é•¿

**è¡¨ç°**ï¼š

```text
è®­ç»ƒè¯¯å·® â†“â†“â†“ (å¾ˆä½)
æµ‹è¯•è¯¯å·® â†‘â†‘â†‘ (å¾ˆé«˜)
```

---

### 2. ä¼ ç»Ÿæ­£åˆ™åŒ–æ–¹æ³•

**L2æ­£åˆ™åŒ– (æƒé‡è¡°å‡)**ï¼š

$$
\mathcal{L}_{\text{reg}} = \mathcal{L} + \lambda \sum_i w_i^2
$$

**L1æ­£åˆ™åŒ– (ç¨€ç–æ€§)**ï¼š

$$
\mathcal{L}_{\text{reg}} = \mathcal{L} + \lambda \sum_i |w_i|
$$

**æ—©åœ (Early Stopping)**ï¼š

- ç›‘æ§éªŒè¯è¯¯å·®
- åœæ­¢è®­ç»ƒå½“éªŒè¯è¯¯å·®å¼€å§‹ä¸Šå‡

**æ•°æ®å¢å¼º**ï¼š

- å¢åŠ è®­ç»ƒæ ·æœ¬å¤šæ ·æ€§
- äººå·¥æ‰©å……æ•°æ®é›†

---

## ğŸ“Š Dropoutç®—æ³•

### 1. è®­ç»ƒæ—¶çš„Dropout

**ç®—æ³• 1.1 (Dropout - Training)**:

**è¾“å…¥**ï¼š

- å±‚æ¿€æ´» $\mathbf{h} \in \mathbb{R}^d$
- Dropoutç‡ $p$

**æ­¥éª¤**ï¼š

1. **ç”Ÿæˆæ©ç **ï¼š
   $$
   \mathbf{m} \sim \text{Bernoulli}(1 - p), \quad \mathbf{m} \in \{0, 1\}^d
   $$

2. **åº”ç”¨æ©ç **ï¼š
   $$
   \tilde{\mathbf{h}} = \mathbf{h} \odot \mathbf{m}
   $$

3. **è¾“å‡º**ï¼š$\tilde{\mathbf{h}}$

**æ•ˆæœ**ï¼šæ¯ä¸ªç¥ç»å…ƒä»¥æ¦‚ç‡ $p$ è¢«ä¸¢å¼ƒï¼ˆç½®ä¸º0ï¼‰ã€‚

---

### 2. æ¨ç†æ—¶çš„Dropout

**é—®é¢˜**ï¼šè®­ç»ƒæ—¶éšæœºä¸¢å¼ƒï¼Œæ¨ç†æ—¶å¦‚ä½•å¤„ç†ï¼Ÿ

**æ–¹æ¡ˆ1ï¼šè’™ç‰¹å¡æ´›Dropout**:

- æ¨ç†æ—¶ä¹Ÿåº”ç”¨Dropout
- å¤šæ¬¡å‰å‘ä¼ æ’­ï¼Œå–å¹³å‡

**æ–¹æ¡ˆ2ï¼šæœŸæœ›è¿‘ä¼¼ï¼ˆæ ‡å‡†æ–¹æ³•ï¼‰**:

- æ¨ç†æ—¶ä¸åº”ç”¨Dropout
- æƒé‡ä¹˜ä»¥ä¿ç•™ç‡ $(1-p)$

**æ•°å­¦**ï¼š

è®­ç»ƒæ—¶æœŸæœ›è¾“å‡ºï¼š

$$
\mathbb{E}[\tilde{\mathbf{h}}] = (1 - p) \mathbf{h}
$$

æ¨ç†æ—¶ä½¿ç”¨ï¼š

$$
\mathbf{h}_{\text{test}} = (1 - p) \mathbf{h}
$$

---

### 3. Inverted Dropout

**é—®é¢˜**ï¼šæ¨ç†æ—¶éœ€è¦ç¼©æ”¾ï¼Œå¢åŠ è®¡ç®—ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼šè®­ç»ƒæ—¶ç¼©æ”¾ï¼

**ç®—æ³• 1.2 (Inverted Dropout)**:

è®­ç»ƒæ—¶ï¼š

$$
\tilde{\mathbf{h}} = \frac{\mathbf{h} \odot \mathbf{m}}{1 - p}
$$

æ¨ç†æ—¶ï¼š

$$
\mathbf{h}_{\text{test}} = \mathbf{h} \quad \text{(æ— éœ€ç¼©æ”¾)}
$$

**ä¼˜åŠ¿**ï¼š

- æ¨ç†æ›´é«˜æ•ˆ
- ä»£ç æ›´ç®€æ´
- ç°ä»£æ¡†æ¶çš„æ ‡å‡†å®ç°

---

## ğŸ”¬ æ•°å­¦åˆ†æ

### 1. é›†æˆå­¦ä¹ è§†è§’

**å®šç† 1.1 (Dropout as Ensemble, Srivastava et al. 2014)**:

Dropoutè®­ç»ƒç­‰ä»·äºè®­ç»ƒæŒ‡æ•°çº§æ•°é‡çš„å­ç½‘ç»œé›†æˆã€‚

**è¯æ˜æ€è·¯**ï¼š

- $d$ ä¸ªç¥ç»å…ƒï¼Œæ¯ä¸ªå¯ä¸¢å¼ƒæˆ–ä¿ç•™
- å…± $2^d$ ç§å¯èƒ½çš„å­ç½‘ç»œ
- æ¯æ¬¡è¿­ä»£éšæœºé‡‡æ ·ä¸€ä¸ªå­ç½‘ç»œè®­ç»ƒ

**æ¨ç†æ—¶**ï¼š

- ç†è®ºä¸Šåº”å¯¹æ‰€æœ‰ $2^d$ ä¸ªå­ç½‘ç»œå–å¹³å‡
- å®è·µä¸­ç”¨æƒé‡ç¼©æ”¾è¿‘ä¼¼

**ç›´è§‰**ï¼š

```text
å®Œæ•´ç½‘ç»œ (æ¨ç†)
    â†“
è¿‘ä¼¼ 2^d ä¸ªå­ç½‘ç»œçš„å¹³å‡
    â†“
æ¯ä¸ªå­ç½‘ç»œåœ¨ä¸åŒè®­ç»ƒæ‰¹æ¬¡ä¸­è®­ç»ƒ
    â†“
é›†æˆæ•ˆæœ â†’ æ›´å¥½æ³›åŒ–
```

---

### 2. è´å¶æ–¯è§†è§’

**å®šç† 2.1 (Dropout as Bayesian Approximation, Gal & Ghahramani 2016)**:

Dropoutå¯ä»¥çœ‹ä½œå˜åˆ†è´å¶æ–¯æ¨æ–­çš„è¿‘ä¼¼ã€‚

**æ•°å­¦**ï¼š

- æƒé‡çš„åéªŒåˆ†å¸ƒï¼š$p(W | D)$
- Dropoutè¿‘ä¼¼ï¼š$q(W) = \prod_i \text{Bernoulli}(w_i; 1-p)$

**MC Dropout**ï¼š

æ¨ç†æ—¶å¤šæ¬¡å‰å‘ä¼ æ’­ï¼ˆä¿ç•™Dropoutï¼‰ï¼š

$$
p(y | x, D) \approx \frac{1}{T} \sum_{t=1}^{T} p(y | x, W_t), \quad W_t \sim q(W)
$$

**æ„ä¹‰**ï¼š

- æä¾›ä¸ç¡®å®šæ€§ä¼°è®¡
- ç”¨äºä¸»åŠ¨å­¦ä¹ 
- ç”¨äºå®‰å…¨å…³é”®åº”ç”¨

---

### 3. ä¿¡æ¯è®ºè§†è§’

**å®šç† 3.1 (Information Bottleneck)**:

Dropouté™åˆ¶äº†ä¿¡æ¯æµï¼Œè¿«ä½¿ç½‘ç»œå­¦ä¹ æ›´é²æ£’çš„ç‰¹å¾ã€‚

**ç›´è§‰**ï¼š

- éšæœºä¸¢å¼ƒç¥ç»å…ƒ â†’ ä¿¡æ¯æŸå¤±
- ç½‘ç»œå¿…é¡»å­¦ä¹ å†—ä½™è¡¨ç¤º
- å•ä¸ªç¥ç»å…ƒå¤±æ•ˆä¸å½±å“æ•´ä½“

**ç±»æ¯”**ï¼š

- ç±»ä¼¼ä¿¡é“å™ªå£°
- è¿«ä½¿ç¼–ç æ›´é²æ£’
- é˜²æ­¢è¿‡åº¦ä¾èµ–å•ä¸ªç‰¹å¾

---

## ğŸ’» åå‘ä¼ æ’­

### 1. å‰å‘ä¼ æ’­

**æ ‡å‡†å±‚**ï¼š

$$
\mathbf{z} = W\mathbf{x} + \mathbf{b}
$$

$$
\mathbf{h} = \sigma(\mathbf{z})
$$

**Dropoutå±‚**ï¼š

$$
\mathbf{m} \sim \text{Bernoulli}(1 - p)
$$

$$
\tilde{\mathbf{h}} = \frac{\mathbf{h} \odot \mathbf{m}}{1 - p}
$$

---

### 2. åå‘ä¼ æ’­

**æŸå¤±**ï¼š$\mathcal{L}$

**å·²çŸ¥**ï¼š$\frac{\partial \mathcal{L}}{\partial \tilde{\mathbf{h}}}$

**ç›®æ ‡**ï¼šè®¡ç®— $\frac{\partial \mathcal{L}}{\partial \mathbf{h}}$

**æ¨å¯¼**ï¼š

$$
\frac{\partial \mathcal{L}}{\partial \mathbf{h}} = \frac{\partial \mathcal{L}}{\partial \tilde{\mathbf{h}}} \odot \frac{\partial \tilde{\mathbf{h}}}{\partial \mathbf{h}}
$$

$$
\frac{\partial \tilde{\mathbf{h}}}{\partial \mathbf{h}} = \frac{\mathbf{m}}{1 - p}
$$

$$
\frac{\partial \mathcal{L}}{\partial \mathbf{h}} = \frac{\partial \mathcal{L}}{\partial \tilde{\mathbf{h}}} \odot \frac{\mathbf{m}}{1 - p}
$$

**å…³é”®**ï¼šæ¢¯åº¦ä¹Ÿè¢«æ©ç ï¼

---

## ğŸ¨ Pythonå®ç°

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

class Dropout(nn.Module):
    """ä»é›¶å®ç°Dropout (Inverted Dropout)"""
    def __init__(self, p=0.5):
        super().__init__()
        self.p = p
    
    def forward(self, x):
        if not self.training:
            # æ¨ç†æ¨¡å¼ï¼šä¸åº”ç”¨Dropout
            return x
        
        if self.p == 0:
            return x
        
        # è®­ç»ƒæ¨¡å¼ï¼šåº”ç”¨Inverted Dropout
        keep_prob = 1 - self.p
        
        # ç”Ÿæˆæ©ç 
        mask = (torch.rand_like(x) < keep_prob).float()
        
        # åº”ç”¨æ©ç å¹¶ç¼©æ”¾
        return x * mask / keep_prob


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºDropoutå±‚
    dropout = Dropout(p=0.5)
    
    # è®­ç»ƒæ¨¡å¼
    dropout.train()
    x_train = torch.randn(10, 20)
    y_train = dropout(x_train)
    
    print("Training mode:")
    print(f"Input mean: {x_train.mean():.4f}, std: {x_train.std():.4f}")
    print(f"Output mean: {y_train.mean():.4f}, std: {y_train.std():.4f}")
    print(f"Zeros in output: {(y_train == 0).sum().item()} / {y_train.numel()}")
    
    # æ¨ç†æ¨¡å¼
    dropout.eval()
    x_test = torch.randn(10, 20)
    y_test = dropout(x_test)
    
    print("\nInference mode:")
    print(f"Input mean: {x_test.mean():.4f}, std: {x_test.std():.4f}")
    print(f"Output mean: {y_test.mean():.4f}, std: {y_test.std():.4f}")
    print(f"Zeros in output: {(y_test == 0).sum().item()} / {y_test.numel()}")


# å®Œæ•´ç¥ç»ç½‘ç»œç¤ºä¾‹
class MLPWithDropout(nn.Module):
    """å¸¦Dropoutçš„å¤šå±‚æ„ŸçŸ¥æœº"""
    def __init__(self, input_dim=784, hidden_dims=[512, 256], output_dim=10, dropout_p=0.5):
        super().__init__()
        
        layers = []
        prev_dim = input_dim
        
        for hidden_dim in hidden_dims:
            layers.append(nn.Linear(prev_dim, hidden_dim))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(dropout_p))
            prev_dim = hidden_dim
        
        layers.append(nn.Linear(prev_dim, output_dim))
        
        self.network = nn.Sequential(*layers)
    
    def forward(self, x):
        return self.network(x)


# MC Dropout for Uncertainty Estimation
class MCDropout(nn.Module):
    """Monte Carlo Dropout for Uncertainty Estimation"""
    def __init__(self, model, n_samples=100):
        super().__init__()
        self.model = model
        self.n_samples = n_samples
    
    def forward(self, x):
        """
        è¿”å›é¢„æµ‹å‡å€¼å’Œæ ‡å‡†å·®
        """
        self.model.train()  # ä¿æŒDropoutæ¿€æ´»
        
        predictions = []
        with torch.no_grad():
            for _ in range(self.n_samples):
                pred = self.model(x)
                predictions.append(pred)
        
        predictions = torch.stack(predictions)  # (n_samples, batch, classes)
        
        mean = predictions.mean(dim=0)
        std = predictions.std(dim=0)
        
        return mean, std


# ç¤ºä¾‹ï¼šä¸ç¡®å®šæ€§ä¼°è®¡
if __name__ == "__main__":
    # åˆ›å»ºæ¨¡å‹
    model = MLPWithDropout(input_dim=10, hidden_dims=[20, 20], output_dim=2, dropout_p=0.5)
    mc_dropout = MCDropout(model, n_samples=100)
    
    # è¾“å…¥
    x = torch.randn(5, 10)
    
    # MC Dropouté¢„æµ‹
    mean, std = mc_dropout(x)
    
    print("\nMC Dropout Uncertainty Estimation:")
    print(f"Prediction mean:\n{mean}")
    print(f"Prediction std (uncertainty):\n{std}")
    
    # é«˜ä¸ç¡®å®šæ€§ â†’ æ¨¡å‹ä¸ç¡®å®š
    # ä½ä¸ç¡®å®šæ€§ â†’ æ¨¡å‹ç¡®å®š


# Dropout vs No Dropout å¯¹æ¯”å®éªŒ
def compare_dropout_effect():
    """å¯¹æ¯”æœ‰æ— Dropoutçš„è¿‡æ‹Ÿåˆæƒ…å†µ"""
    import matplotlib.pyplot as plt
    
    # æ¨¡æ‹Ÿæ•°æ®
    np.random.seed(42)
    X_train = np.random.randn(100, 10)
    y_train = (X_train[:, 0] + X_train[:, 1] > 0).astype(np.float32)
    
    X_test = np.random.randn(100, 10)
    y_test = (X_test[:, 0] + X_test[:, 1] > 0).astype(np.float32)
    
    # è½¬æ¢ä¸ºTensor
    X_train_t = torch.FloatTensor(X_train)
    y_train_t = torch.FloatTensor(y_train).unsqueeze(1)
    X_test_t = torch.FloatTensor(X_test)
    y_test_t = torch.FloatTensor(y_test).unsqueeze(1)
    
    # è®­ç»ƒä¸¤ä¸ªæ¨¡å‹
    model_no_dropout = MLPWithDropout(input_dim=10, hidden_dims=[100, 100], output_dim=1, dropout_p=0.0)
    model_with_dropout = MLPWithDropout(input_dim=10, hidden_dims=[100, 100], output_dim=1, dropout_p=0.5)
    
    def train_model(model, epochs=200):
        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
        criterion = nn.BCEWithLogitsLoss()
        
        train_losses = []
        test_losses = []
        
        for epoch in range(epochs):
            # è®­ç»ƒ
            model.train()
            optimizer.zero_grad()
            pred = model(X_train_t)
            loss = criterion(pred, y_train_t)
            loss.backward()
            optimizer.step()
            train_losses.append(loss.item())
            
            # æµ‹è¯•
            model.eval()
            with torch.no_grad():
                pred_test = model(X_test_t)
                test_loss = criterion(pred_test, y_test_t)
                test_losses.append(test_loss.item())
        
        return train_losses, test_losses
    
    print("\nTraining models...")
    train_no_drop, test_no_drop = train_model(model_no_dropout)
    train_with_drop, test_with_drop = train_model(model_with_dropout)
    
    # å¯è§†åŒ–
    plt.figure(figsize=(12, 5))
    
    plt.subplot(1, 2, 1)
    plt.plot(train_no_drop, label='Train (No Dropout)')
    plt.plot(test_no_drop, label='Test (No Dropout)')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Without Dropout')
    plt.legend()
    plt.grid(True)
    
    plt.subplot(1, 2, 2)
    plt.plot(train_with_drop, label='Train (With Dropout)')
    plt.plot(test_with_drop, label='Test (With Dropout)')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('With Dropout (p=0.5)')
    plt.legend()
    plt.grid(True)
    
    plt.tight_layout()
    # plt.show()
    
    print(f"\nFinal Test Loss (No Dropout): {test_no_drop[-1]:.4f}")
    print(f"Final Test Loss (With Dropout): {test_with_drop[-1]:.4f}")

# compare_dropout_effect()
```

---

## ğŸ“š ç†è®ºæ·±åŒ–

### 1. ä¸ºä»€ä¹ˆDropoutæœ‰æ•ˆ

**åŸå› 1ï¼šå‡å°‘å…±é€‚åº” (Co-adaptation)**:

- ç¥ç»å…ƒä¸èƒ½è¿‡åº¦ä¾èµ–ç‰¹å®šçš„å…¶ä»–ç¥ç»å…ƒ
- æ¯ä¸ªç¥ç»å…ƒå¿…é¡»å­¦ä¹ æ›´é²æ£’çš„ç‰¹å¾
- ç±»ä¼¼"å›¢é˜Ÿåˆä½œ"è€Œé"ä¸ªäººè‹±é›„"

**åŸå› 2ï¼šé›†æˆæ•ˆæœ**:

- è®­ç»ƒå¤šä¸ªå­ç½‘ç»œ
- æ¨ç†æ—¶è¿‘ä¼¼é›†æˆ
- é›†æˆé€šå¸¸ä¼˜äºå•æ¨¡å‹

**åŸå› 3ï¼šæ·»åŠ å™ªå£°**:

- æ­£åˆ™åŒ–æ•ˆæœ
- ç±»ä¼¼æ•°æ®å¢å¼º
- æé«˜é²æ£’æ€§

---

### 2. Dropoutä½œä¸ºæ­£åˆ™åŒ–

**å®šç† 2.1 (Dropout as L2 Regularization)**:

åœ¨æŸäº›æ¡ä»¶ä¸‹ï¼ŒDropoutç­‰ä»·äºL2æ­£åˆ™åŒ–ã€‚

**æ¡ä»¶**ï¼š

- çº¿æ€§æ¨¡å‹
- å°Dropoutç‡

**è¿‘ä¼¼**ï¼š

$$
\mathbb{E}[\mathcal{L}_{\text{dropout}}] \approx \mathcal{L} + \lambda \|W\|^2
$$

**æ„ä¹‰**ï¼š

- Dropoutæ˜¯è‡ªé€‚åº”çš„æ­£åˆ™åŒ–
- ä¸åŒå±‚å¯ä»¥æœ‰ä¸åŒçš„Dropoutç‡
- æ¯”å›ºå®šçš„L2æ›´çµæ´»

---

### 3. æœ€ä¼˜Dropoutç‡

**ç»éªŒæ³•åˆ™**ï¼š

- **å…¨è¿æ¥å±‚**ï¼š$p = 0.5$
- **è¾“å…¥å±‚**ï¼š$p = 0.2$
- **å·ç§¯å±‚**ï¼š$p = 0.1 \sim 0.2$ï¼ˆæˆ–ä¸ç”¨ï¼‰

**ç†è®ºåˆ†æ**ï¼š

- $p = 0.5$ æœ€å¤§åŒ–å­ç½‘ç»œå¤šæ ·æ€§
- ä½†å…·ä½“ä»»åŠ¡å¯èƒ½ä¸åŒ

**å®è·µå»ºè®®**ï¼š

- é€šè¿‡éªŒè¯é›†è°ƒä¼˜
- è¿‡æ‹Ÿåˆä¸¥é‡ â†’ å¢å¤§ $p$
- æ¬ æ‹Ÿåˆ â†’ å‡å° $p$

---

## ğŸ”§ Dropoutå˜ä½“

### 1. DropConnect

**æ ¸å¿ƒæ€æƒ³**ï¼šä¸¢å¼ƒè¿æ¥è€Œéç¥ç»å…ƒã€‚

**æ•°å­¦**ï¼š

$$
\mathbf{h} = \sigma((W \odot M) \mathbf{x})
$$

å…¶ä¸­ $M$ æ˜¯æƒé‡æ©ç ã€‚

**å¯¹æ¯”Dropout**ï¼š

- Dropoutï¼šä¸¢å¼ƒæ¿€æ´»
- DropConnectï¼šä¸¢å¼ƒæƒé‡

**æ•ˆæœ**ï¼š

- æ›´ç»†ç²’åº¦çš„æ­£åˆ™åŒ–
- é€šå¸¸ç•¥ä¼˜äºDropout

---

### 2. Spatial Dropout

**æ ¸å¿ƒæ€æƒ³**ï¼šå¯¹æ•´ä¸ªç‰¹å¾å›¾ä¸¢å¼ƒï¼ˆç”¨äºCNNï¼‰ã€‚

**é—®é¢˜**ï¼š

- æ ‡å‡†Dropoutå¯¹ç›¸é‚»åƒç´ ç‹¬ç«‹ä¸¢å¼ƒ
- CNNä¸­ç›¸é‚»åƒç´ é«˜åº¦ç›¸å…³
- ä¿¡æ¯ä»å¯é€šè¿‡ç›¸é‚»åƒç´ ä¼ é€’

**è§£å†³æ–¹æ¡ˆ**ï¼š

- ä¸¢å¼ƒæ•´ä¸ªé€šé“
- ä¿æŒç©ºé—´ç›¸å…³æ€§

**æ•°å­¦**ï¼š

$$
\tilde{\mathbf{h}}_{:, :, c} = \begin{cases}
\mathbf{h}_{:, :, c} / (1-p) & \text{if } m_c = 1 \\
0 & \text{if } m_c = 0
\end{cases}
$$

---

### 3. Variational Dropout

**æ ¸å¿ƒæ€æƒ³**ï¼šå¯¹åŒä¸€æ ·æœ¬çš„æ‰€æœ‰æ—¶é—´æ­¥ä½¿ç”¨ç›¸åŒæ©ç ï¼ˆç”¨äºRNNï¼‰ã€‚

**é—®é¢˜**ï¼š

- æ ‡å‡†Dropoutåœ¨æ¯ä¸ªæ—¶é—´æ­¥ç”Ÿæˆæ–°æ©ç 
- ç ´åæ—¶é—´ä¾èµ–æ€§

**è§£å†³æ–¹æ¡ˆ**ï¼š

- æ•´ä¸ªåºåˆ—ä½¿ç”¨ç›¸åŒæ©ç 
- ä¿æŒæ—¶é—´ä¸€è‡´æ€§

**åº”ç”¨**ï¼š

- RNN
- LSTM
- GRU

---

## ğŸ“ ç›¸å…³è¯¾ç¨‹

| å¤§å­¦ | è¯¾ç¨‹ |
|------|------|
| **Stanford** | CS231n Convolutional Neural Networks |
| **MIT** | 6.S191 Introduction to Deep Learning |
| **UC Berkeley** | CS182 Deep Learning |
| **CMU** | 11-785 Introduction to Deep Learning |

---

## ğŸ“– å‚è€ƒæ–‡çŒ®

1. **Srivastava et al. (2014)**. "Dropout: A Simple Way to Prevent Neural Networks from Overfitting". *JMLR*.

2. **Gal & Ghahramani (2016)**. "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning". *ICML*.

3. **Wan et al. (2013)**. "Regularization of Neural Networks using DropConnect". *ICML*.

4. **Tompson et al. (2015)**. "Efficient Object Localization Using Convolutional Networks". *CVPR*.

5. **Gal & Ghahramani (2016)**. "A Theoretically Grounded Application of Dropout in Recurrent Neural Networks". *NeurIPS*.

---

*æœ€åæ›´æ–°ï¼š2025å¹´10æœˆ*-
