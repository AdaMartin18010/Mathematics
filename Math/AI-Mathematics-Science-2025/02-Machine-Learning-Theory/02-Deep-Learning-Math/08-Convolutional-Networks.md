# å·ç§¯ç¥ç»ç½‘ç»œ (CNN) æ•°å­¦åŸç†

> **Convolutional Neural Networks: Mathematics and Theory**
>
> è®¡ç®—æœºè§†è§‰çš„æ•°å­¦åŸºç¡€

---

## ç›®å½•

- [å·ç§¯ç¥ç»ç½‘ç»œ (CNN) æ•°å­¦åŸç†](#å·ç§¯ç¥ç»ç½‘ç»œ-cnn-æ•°å­¦åŸç†)
  - [ç›®å½•](#ç›®å½•)
  - [ğŸ“‹ æ ¸å¿ƒæ€æƒ³](#-æ ¸å¿ƒæ€æƒ³)
  - [ğŸ¯ åŠ¨æœºä¸ä¼˜åŠ¿](#-åŠ¨æœºä¸ä¼˜åŠ¿)
    - [1. å…¨è¿æ¥å±‚çš„é—®é¢˜](#1-å…¨è¿æ¥å±‚çš„é—®é¢˜)
    - [2. CNNçš„ä¼˜åŠ¿](#2-cnnçš„ä¼˜åŠ¿)
  - [ğŸ“Š å·ç§¯è¿ç®—](#-å·ç§¯è¿ç®—)
    - [1. æ•°å­¦å®šä¹‰](#1-æ•°å­¦å®šä¹‰)
    - [2. ç¦»æ•£å·ç§¯](#2-ç¦»æ•£å·ç§¯)
    - [3. äº’ç›¸å…³ vs å·ç§¯](#3-äº’ç›¸å…³-vs-å·ç§¯)
  - [ğŸ”¬ å·ç§¯å±‚çš„æ•°å­¦](#-å·ç§¯å±‚çš„æ•°å­¦)
    - [1. å‰å‘ä¼ æ’­](#1-å‰å‘ä¼ æ’­)
    - [2. å‚æ•°å…±äº«](#2-å‚æ•°å…±äº«)
    - [3. å±€éƒ¨è¿æ¥](#3-å±€éƒ¨è¿æ¥)
  - [ğŸ’» æ„Ÿå—é‡åˆ†æ](#-æ„Ÿå—é‡åˆ†æ)
    - [1. æ„Ÿå—é‡å®šä¹‰](#1-æ„Ÿå—é‡å®šä¹‰)
    - [2. æ„Ÿå—é‡è®¡ç®—](#2-æ„Ÿå—é‡è®¡ç®—)
    - [3. æœ‰æ•ˆæ„Ÿå—é‡](#3-æœ‰æ•ˆæ„Ÿå—é‡)
  - [ğŸ¨ æ± åŒ–å±‚](#-æ± åŒ–å±‚)
    - [1. æœ€å¤§æ± åŒ–](#1-æœ€å¤§æ± åŒ–)
    - [2. å¹³å‡æ± åŒ–](#2-å¹³å‡æ± åŒ–)
    - [3. æ± åŒ–çš„ä½œç”¨](#3-æ± åŒ–çš„ä½œç”¨)
  - [ğŸ“ è¾“å‡ºå°ºå¯¸è®¡ç®—](#-è¾“å‡ºå°ºå¯¸è®¡ç®—)
    - [1. å·ç§¯è¾“å‡ºå°ºå¯¸](#1-å·ç§¯è¾“å‡ºå°ºå¯¸)
    - [2. æ± åŒ–è¾“å‡ºå°ºå¯¸](#2-æ± åŒ–è¾“å‡ºå°ºå¯¸)
    - [3. å¡«å……ç­–ç•¥](#3-å¡«å……ç­–ç•¥)
  - [ğŸ”§ åå‘ä¼ æ’­](#-åå‘ä¼ æ’­)
    - [1. å·ç§¯å±‚æ¢¯åº¦](#1-å·ç§¯å±‚æ¢¯åº¦)
    - [2. æ± åŒ–å±‚æ¢¯åº¦](#2-æ± åŒ–å±‚æ¢¯åº¦)
  - [ğŸ’¡ Pythonå®ç°](#-pythonå®ç°)
  - [ğŸ“š ç»å…¸CNNæ¶æ„](#-ç»å…¸cnnæ¶æ„)
    - [1. LeNet-5](#1-lenet-5)
    - [2. AlexNet](#2-alexnet)
    - [3. VGG](#3-vgg)
  - [ğŸ“ ç›¸å…³è¯¾ç¨‹](#-ç›¸å…³è¯¾ç¨‹)
  - [ğŸ“– å‚è€ƒæ–‡çŒ®](#-å‚è€ƒæ–‡çŒ®)

---

## ğŸ“‹ æ ¸å¿ƒæ€æƒ³

**å·ç§¯ç¥ç»ç½‘ç»œ (CNN)** é€šè¿‡**å·ç§¯æ“ä½œ**æå–å›¾åƒçš„å±€éƒ¨ç‰¹å¾ã€‚

**æ ¸å¿ƒåŸç†**ï¼š

```text
è¾“å…¥å›¾åƒ
    â†“
å·ç§¯å±‚ (ç‰¹å¾æå–)
    â†“
æ¿€æ´»å‡½æ•° (éçº¿æ€§)
    â†“
æ± åŒ–å±‚ (é™ç»´)
    â†“
é‡å¤å¤šå±‚
    â†“
å…¨è¿æ¥å±‚ (åˆ†ç±»)
    â†“
è¾“å‡º
```

**å…³é”®æ¦‚å¿µ**ï¼š

- **å±€éƒ¨è¿æ¥**ï¼šæ¯ä¸ªç¥ç»å…ƒåªè¿æ¥å±€éƒ¨åŒºåŸŸ
- **å‚æ•°å…±äº«**ï¼šåŒä¸€å·ç§¯æ ¸åœ¨æ•´ä¸ªå›¾åƒä¸Šå…±äº«
- **å¹³ç§»ä¸å˜æ€§**ï¼šå¯¹è¾“å…¥çš„å¹³ç§»å…·æœ‰é²æ£’æ€§

---

## ğŸ¯ åŠ¨æœºä¸ä¼˜åŠ¿

### 1. å…¨è¿æ¥å±‚çš„é—®é¢˜

**ç¤ºä¾‹**ï¼šå¤„ç† $224 \times 224 \times 3$ çš„RGBå›¾åƒ

**å…¨è¿æ¥å±‚**ï¼š

- è¾“å…¥ç»´åº¦ï¼š$224 \times 224 \times 3 = 150,528$
- ç¬¬ä¸€éšè—å±‚1000ä¸ªç¥ç»å…ƒ
- å‚æ•°æ•°é‡ï¼š$150,528 \times 1000 = 150M$ å‚æ•°ï¼

**é—®é¢˜**ï¼š

- å‚æ•°è¿‡å¤šï¼Œå®¹æ˜“è¿‡æ‹Ÿåˆ
- è®¡ç®—é‡å·¨å¤§
- å¿½ç•¥ç©ºé—´ç»“æ„

---

### 2. CNNçš„ä¼˜åŠ¿

**å‚æ•°å…±äº«**ï¼š

- åŒä¸€å·ç§¯æ ¸åœ¨æ•´ä¸ªå›¾åƒä¸Šæ»‘åŠ¨
- å¤§å¹…å‡å°‘å‚æ•°æ•°é‡

**å±€éƒ¨è¿æ¥**ï¼š

- æ¯ä¸ªç¥ç»å…ƒåªå…³æ³¨å±€éƒ¨åŒºåŸŸ
- ç¬¦åˆè§†è§‰æ„ŸçŸ¥åŸç†

**å¹³ç§»ä¸å˜æ€§**ï¼š

- ç‰¹å¾æ£€æµ‹å™¨åœ¨å›¾åƒä»»ä½•ä½ç½®éƒ½æœ‰æ•ˆ
- å¯¹ç‰©ä½“ä½ç½®å˜åŒ–é²æ£’

**å±‚æ¬¡åŒ–ç‰¹å¾**ï¼š

- ä½å±‚ï¼šè¾¹ç¼˜ã€çº¹ç†
- ä¸­å±‚ï¼šéƒ¨ä»¶ã€å½¢çŠ¶
- é«˜å±‚ï¼šç‰©ä½“ã€åœºæ™¯

---

## ğŸ“Š å·ç§¯è¿ç®—

### 1. æ•°å­¦å®šä¹‰

**è¿ç»­å·ç§¯**ï¼š

$$
(f * g)(t) = \int_{-\infty}^{\infty} f(\tau) g(t - \tau) d\tau
$$

**ç‰©ç†æ„ä¹‰**ï¼š

- $f$ï¼šè¾“å…¥ä¿¡å·
- $g$ï¼šå·ç§¯æ ¸ï¼ˆæ»¤æ³¢å™¨ï¼‰
- $f * g$ï¼šæ»¤æ³¢åçš„ä¿¡å·

---

### 2. ç¦»æ•£å·ç§¯

**2Dç¦»æ•£å·ç§¯**ï¼š

$$
(I * K)(i, j) = \sum_{m} \sum_{n} I(i - m, j - n) K(m, n)
$$

å…¶ä¸­ï¼š

- $I$ï¼šè¾“å…¥å›¾åƒ
- $K$ï¼šå·ç§¯æ ¸
- $(i, j)$ï¼šè¾“å‡ºä½ç½®

**ç¤ºä¾‹**ï¼š$3 \times 3$ å·ç§¯æ ¸

$$
\begin{bmatrix}
I(i-1, j-1) & I(i-1, j) & I(i-1, j+1) \\
I(i, j-1) & I(i, j) & I(i, j+1) \\
I(i+1, j-1) & I(i+1, j) & I(i+1, j+1)
\end{bmatrix}
\odot
\begin{bmatrix}
K(1, 1) & K(1, 2) & K(1, 3) \\
K(2, 1) & K(2, 2) & K(2, 3) \\
K(3, 1) & K(3, 2) & K(3, 3)
\end{bmatrix}
$$

---

### 3. äº’ç›¸å…³ vs å·ç§¯

**äº’ç›¸å…³ (Cross-Correlation)**ï¼š

$$
(I \star K)(i, j) = \sum_{m} \sum_{n} I(i + m, j + n) K(m, n)
$$

**å·ç§¯ (Convolution)**ï¼š

$$
(I * K)(i, j) = \sum_{m} \sum_{n} I(i - m, j - n) K(m, n)
$$

**å…³ç³»**ï¼š

- äº’ç›¸å…³ï¼šä¸ç¿»è½¬å·ç§¯æ ¸
- å·ç§¯ï¼šç¿»è½¬å·ç§¯æ ¸

**æ·±åº¦å­¦ä¹ ä¸­**ï¼š

- é€šå¸¸ä½¿ç”¨äº’ç›¸å…³
- ä½†ä¹ æƒ¯ç§°ä¸º"å·ç§¯"
- å› ä¸ºå·ç§¯æ ¸æ˜¯å­¦ä¹ çš„ï¼Œç¿»è½¬ä¸å¦æ— å…³ç´§è¦

---

## ğŸ”¬ å·ç§¯å±‚çš„æ•°å­¦

### 1. å‰å‘ä¼ æ’­

**è¾“å…¥**ï¼š

- $X \in \mathbb{R}^{H \times W \times C_{in}}$ï¼ˆé«˜åº¦Ã—å®½åº¦Ã—è¾“å…¥é€šé“æ•°ï¼‰

**å·ç§¯æ ¸**ï¼š

- $K \in \mathbb{R}^{k_h \times k_w \times C_{in} \times C_{out}}$

**è¾“å‡º**ï¼š

- $Y \in \mathbb{R}^{H' \times W' \times C_{out}}$

**è®¡ç®—**ï¼š

$$
Y_{i,j,c} = \sum_{m=0}^{k_h-1} \sum_{n=0}^{k_w-1} \sum_{c'=0}^{C_{in}-1} X_{i+m, j+n, c'} \cdot K_{m, n, c', c} + b_c
$$

å…¶ä¸­ $b_c$ æ˜¯åç½®ã€‚

---

### 2. å‚æ•°å…±äº«

**å…¨è¿æ¥å±‚**ï¼š

- æ¯ä¸ªè¾“å‡ºç¥ç»å…ƒæœ‰ç‹¬ç«‹çš„æƒé‡
- å‚æ•°æ•°é‡ï¼š$H \times W \times C_{in} \times C_{out}$

**å·ç§¯å±‚**ï¼š

- åŒä¸€å·ç§¯æ ¸åœ¨æ•´ä¸ªå›¾åƒä¸Šå…±äº«
- å‚æ•°æ•°é‡ï¼š$k_h \times k_w \times C_{in} \times C_{out}$

**ç¤ºä¾‹**ï¼š

- è¾“å…¥ï¼š$224 \times 224 \times 3$
- å·ç§¯æ ¸ï¼š$3 \times 3$ï¼Œ64ä¸ª
- å‚æ•°ï¼š$3 \times 3 \times 3 \times 64 = 1,728$ï¼ˆvs å…¨è¿æ¥çš„150Mï¼ï¼‰

---

### 3. å±€éƒ¨è¿æ¥

**å…¨è¿æ¥å±‚**ï¼š

- æ¯ä¸ªè¾“å‡ºç¥ç»å…ƒè¿æ¥æ‰€æœ‰è¾“å…¥

**å·ç§¯å±‚**ï¼š

- æ¯ä¸ªè¾“å‡ºç¥ç»å…ƒåªè¿æ¥å±€éƒ¨æ„Ÿå—é‡
- æ„Ÿå—é‡å¤§å°ï¼š$k_h \times k_w$

**ä¼˜åŠ¿**ï¼š

- å‡å°‘è®¡ç®—é‡
- ç¬¦åˆè§†è§‰å¤„ç†åŸç†ï¼ˆå±€éƒ¨ç‰¹å¾ï¼‰
- æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›

---

## ğŸ’» æ„Ÿå—é‡åˆ†æ

### 1. æ„Ÿå—é‡å®šä¹‰

**å®šä¹‰ 1.1 (æ„Ÿå—é‡, Receptive Field)**:

æŸå±‚çš„ä¸€ä¸ªç¥ç»å…ƒåœ¨è¾“å…¥å›¾åƒä¸Šèƒ½"çœ‹åˆ°"çš„åŒºåŸŸå¤§å°ã€‚

**ç›´è§‰**ï¼š

```text
è¾“å…¥å±‚ â†’ ç¬¬1å±‚ â†’ ç¬¬2å±‚ â†’ ç¬¬3å±‚
  â–        â–        â–        â– 
(1Ã—1)   (3Ã—3)   (5Ã—5)   (7Ã—7)
```

---

### 2. æ„Ÿå—é‡è®¡ç®—

**å•å±‚å·ç§¯**ï¼š

- å·ç§¯æ ¸å¤§å°ï¼š$k$
- æ„Ÿå—é‡ï¼š$r = k$

**ä¸¤å±‚å·ç§¯**ï¼š

- ç¬¬1å±‚ï¼š$k_1 \times k_1$ï¼Œæ„Ÿå—é‡ $r_1 = k_1$
- ç¬¬2å±‚ï¼š$k_2 \times k_2$ï¼Œæ„Ÿå—é‡ $r_2 = r_1 + (k_2 - 1)$

**é€’æ¨å…¬å¼**ï¼š

$$
r_l = r_{l-1} + (k_l - 1) \times \prod_{i=1}^{l-1} s_i
$$

å…¶ä¸­ $s_i$ æ˜¯ç¬¬ $i$ å±‚çš„æ­¥é•¿ã€‚

**ç¤ºä¾‹**ï¼š

- 3å±‚ $3 \times 3$ å·ç§¯ï¼Œæ­¥é•¿1
- æ„Ÿå—é‡ï¼š$r = 1 + 2 + 2 + 2 = 7$

---

### 3. æœ‰æ•ˆæ„Ÿå—é‡

**é—®é¢˜**ï¼šç†è®ºæ„Ÿå—é‡ â‰  æœ‰æ•ˆæ„Ÿå—é‡

**æœ‰æ•ˆæ„Ÿå—é‡ (Effective Receptive Field)**ï¼š

- æ„Ÿå—é‡ä¸­å¿ƒçš„åƒç´ è´¡çŒ®æœ€å¤§
- è¾¹ç¼˜åƒç´ è´¡çŒ®å¾ˆå°
- å‘ˆé«˜æ–¯åˆ†å¸ƒ

**å®éªŒå‘ç°** (Luo et al., 2016)ï¼š

- æœ‰æ•ˆæ„Ÿå—é‡è¿œå°äºç†è®ºæ„Ÿå—é‡
- ä¸­å¿ƒåŒºåŸŸå ä¸»å¯¼
- éœ€è¦æ›´æ·±çš„ç½‘ç»œæ‰èƒ½è·å¾—å¤§æ„Ÿå—é‡

---

## ğŸ¨ æ± åŒ–å±‚

### 1. æœ€å¤§æ± åŒ–

**å®šä¹‰ 1.1 (Max Pooling)**:

$$
Y_{i,j} = \max_{m=0}^{k-1} \max_{n=0}^{k-1} X_{i \cdot s + m, j \cdot s + n}
$$

å…¶ä¸­ $k$ æ˜¯æ± åŒ–çª—å£å¤§å°ï¼Œ$s$ æ˜¯æ­¥é•¿ã€‚

**ç¤ºä¾‹**ï¼š$2 \times 2$ æœ€å¤§æ± åŒ–

$$
\begin{bmatrix}
1 & 3 & 2 & 4 \\
5 & 6 & 7 & 8 \\
9 & 10 & 11 & 12 \\
13 & 14 & 15 & 16
\end{bmatrix}
\to
\begin{bmatrix}
6 & 8 \\
14 & 16
\end{bmatrix}
$$

---

### 2. å¹³å‡æ± åŒ–

**å®šä¹‰ 2.1 (Average Pooling)**:

$$
Y_{i,j} = \frac{1}{k^2} \sum_{m=0}^{k-1} \sum_{n=0}^{k-1} X_{i \cdot s + m, j \cdot s + n}
$$

**å¯¹æ¯”**ï¼š

- **æœ€å¤§æ± åŒ–**ï¼šä¿ç•™æœ€å¼ºç‰¹å¾ï¼Œå¸¸ç”¨äºå·ç§¯å±‚å
- **å¹³å‡æ± åŒ–**ï¼šå¹³æ»‘ç‰¹å¾ï¼Œå¸¸ç”¨äºå…¨å±€æ± åŒ–

---

### 3. æ± åŒ–çš„ä½œç”¨

**é™ç»´**ï¼š

- å‡å°‘ç‰¹å¾å›¾å°ºå¯¸
- å‡å°‘è®¡ç®—é‡

**å¹³ç§»ä¸å˜æ€§**ï¼š

- å°çš„ä½ç½®å˜åŒ–ä¸å½±å“è¾“å‡º
- æé«˜é²æ£’æ€§

**å¢å¤§æ„Ÿå—é‡**ï¼š

- é—´æ¥å¢åŠ åç»­å±‚çš„æ„Ÿå—é‡

**é˜²æ­¢è¿‡æ‹Ÿåˆ**ï¼š

- å‡å°‘å‚æ•°æ•°é‡
- æ­£åˆ™åŒ–æ•ˆæœ

---

## ğŸ“ è¾“å‡ºå°ºå¯¸è®¡ç®—

### 1. å·ç§¯è¾“å‡ºå°ºå¯¸

**å…¬å¼**ï¼š

$$
H_{out} = \left\lfloor \frac{H_{in} + 2p - k_h}{s} \right\rfloor + 1
$$

$$
W_{out} = \left\lfloor \frac{W_{in} + 2p - k_w}{s} \right\rfloor + 1
$$

å…¶ä¸­ï¼š

- $H_{in}, W_{in}$ï¼šè¾“å…¥é«˜åº¦å’Œå®½åº¦
- $k_h, k_w$ï¼šå·ç§¯æ ¸é«˜åº¦å’Œå®½åº¦
- $p$ï¼šå¡«å…… (padding)
- $s$ï¼šæ­¥é•¿ (stride)

**ç¤ºä¾‹**ï¼š

- è¾“å…¥ï¼š$32 \times 32$
- å·ç§¯æ ¸ï¼š$5 \times 5$
- æ­¥é•¿ï¼š1
- å¡«å……ï¼š0
- è¾“å‡ºï¼š$\lfloor (32 - 5) / 1 \rfloor + 1 = 28$

---

### 2. æ± åŒ–è¾“å‡ºå°ºå¯¸

**å…¬å¼**ï¼š

$$
H_{out} = \left\lfloor \frac{H_{in} - k}{s} \right\rfloor + 1
$$

**ç¤ºä¾‹**ï¼š

- è¾“å…¥ï¼š$28 \times 28$
- æ± åŒ–çª—å£ï¼š$2 \times 2$
- æ­¥é•¿ï¼š2
- è¾“å‡ºï¼š$\lfloor (28 - 2) / 2 \rfloor + 1 = 14$

---

### 3. å¡«å……ç­–ç•¥

**Valid Padding** ($p = 0$)ï¼š

- ä¸å¡«å……
- è¾“å‡ºå°ºå¯¸å‡å°

**Same Padding**ï¼š

- å¡«å……ä½¿è¾“å‡ºå°ºå¯¸ = è¾“å…¥å°ºå¯¸ï¼ˆæ­¥é•¿=1æ—¶ï¼‰
- $p = \lfloor k / 2 \rfloor$

**Full Padding**ï¼š

- å¡«å……ä½¿æ¯ä¸ªè¾“å…¥åƒç´ éƒ½è¢«å·ç§¯æ ¸å®Œæ•´è¦†ç›–
- $p = k - 1$

---

## ğŸ”§ åå‘ä¼ æ’­

### 1. å·ç§¯å±‚æ¢¯åº¦

**å‰å‘**ï¼š$Y = X * K + b$

**å·²çŸ¥**ï¼š$\frac{\partial \mathcal{L}}{\partial Y}$

**ç›®æ ‡**ï¼šè®¡ç®— $\frac{\partial \mathcal{L}}{\partial X}, \frac{\partial \mathcal{L}}{\partial K}, \frac{\partial \mathcal{L}}{\partial b}$

**æ¢¯åº¦è®¡ç®—**ï¼š

1. **å¯¹åç½®**ï¼š
   $$
   \frac{\partial \mathcal{L}}{\partial b_c} = \sum_{i,j} \frac{\partial \mathcal{L}}{\partial Y_{i,j,c}}
   $$

2. **å¯¹å·ç§¯æ ¸**ï¼š
   $$
   \frac{\partial \mathcal{L}}{\partial K_{m,n,c',c}} = \sum_{i,j} \frac{\partial \mathcal{L}}{\partial Y_{i,j,c}} \cdot X_{i+m, j+n, c'}
   $$

3. **å¯¹è¾“å…¥**ï¼š
   $$
   \frac{\partial \mathcal{L}}{\partial X_{i,j,c'}} = \sum_{m,n,c} \frac{\partial \mathcal{L}}{\partial Y_{i-m, j-n, c}} \cdot K_{m,n,c',c}
   $$

**å…³é”®**ï¼šè¾“å…¥æ¢¯åº¦æ˜¯å·ç§¯æ ¸çš„"è½¬ç½®å·ç§¯"ã€‚

---

### 2. æ± åŒ–å±‚æ¢¯åº¦

**æœ€å¤§æ± åŒ–**ï¼š

- æ¢¯åº¦åªä¼ é€’ç»™æœ€å¤§å€¼ä½ç½®
- å…¶ä»–ä½ç½®æ¢¯åº¦ä¸º0

$$
\frac{\partial \mathcal{L}}{\partial X_{i,j}} = \begin{cases}
\frac{\partial \mathcal{L}}{\partial Y_{k,l}} & \text{if } X_{i,j} = \max(\text{pool window}) \\
0 & \text{otherwise}
\end{cases}
$$

**å¹³å‡æ± åŒ–**ï¼š

- æ¢¯åº¦å‡åŒ€åˆ†é…ç»™çª—å£å†…æ‰€æœ‰ä½ç½®

$$
\frac{\partial \mathcal{L}}{\partial X_{i,j}} = \frac{1}{k^2} \frac{\partial \mathcal{L}}{\partial Y_{k,l}}
$$

---

## ğŸ’¡ Pythonå®ç°

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

# 1. ä»é›¶å®ç°2Då·ç§¯
def conv2d_manual(input, kernel, stride=1, padding=0):
    """
    æ‰‹åŠ¨å®ç°2Då·ç§¯
    
    Args:
        input: (batch, in_channels, H, W)
        kernel: (out_channels, in_channels, kH, kW)
        stride: int
        padding: int
    
    Returns:
        output: (batch, out_channels, H_out, W_out)
    """
    batch, in_channels, H, W = input.shape
    out_channels, _, kH, kW = kernel.shape
    
    # æ·»åŠ å¡«å……
    if padding > 0:
        input = F.pad(input, (padding, padding, padding, padding))
        H, W = H + 2 * padding, W + 2 * padding
    
    # è®¡ç®—è¾“å‡ºå°ºå¯¸
    H_out = (H - kH) // stride + 1
    W_out = (W - kW) // stride + 1
    
    # åˆå§‹åŒ–è¾“å‡º
    output = torch.zeros(batch, out_channels, H_out, W_out)
    
    # å·ç§¯æ“ä½œ
    for b in range(batch):
        for oc in range(out_channels):
            for i in range(H_out):
                for j in range(W_out):
                    h_start = i * stride
                    w_start = j * stride
                    
                    # æå–æ„Ÿå—é‡
                    receptive_field = input[b, :, h_start:h_start+kH, w_start:w_start+kW]
                    
                    # å·ç§¯
                    output[b, oc, i, j] = torch.sum(receptive_field * kernel[oc])
    
    return output


# 2. ä½¿ç”¨PyTorchçš„å·ç§¯å±‚
class SimpleCNN(nn.Module):
    """ç®€å•çš„CNNç¤ºä¾‹"""
    def __init__(self, num_classes=10):
        super().__init__()
        
        # å·ç§¯å±‚1: 3 -> 32
        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(32)
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)
        
        # å·ç§¯å±‚2: 32 -> 64
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(64)
        self.pool2 = nn.MaxPool2d(2, 2)
        
        # å·ç§¯å±‚3: 64 -> 128
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.bn3 = nn.BatchNorm2d(128)
        self.pool3 = nn.MaxPool2d(2, 2)
        
        # å…¨è¿æ¥å±‚
        self.fc1 = nn.Linear(128 * 4 * 4, 256)
        self.dropout = nn.Dropout(0.5)
        self.fc2 = nn.Linear(256, num_classes)
    
    def forward(self, x):
        # Conv1: 32x32x3 -> 32x32x32 -> 16x16x32
        x = F.relu(self.bn1(self.conv1(x)))
        x = self.pool1(x)
        
        # Conv2: 16x16x32 -> 16x16x64 -> 8x8x64
        x = F.relu(self.bn2(self.conv2(x)))
        x = self.pool2(x)
        
        # Conv3: 8x8x64 -> 8x8x128 -> 4x4x128
        x = F.relu(self.bn3(self.conv3(x)))
        x = self.pool3(x)
        
        # Flatten
        x = x.view(x.size(0), -1)  # (batch, 128*4*4)
        
        # FC
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        
        return x


# 3. æ„Ÿå—é‡è®¡ç®—
def compute_receptive_field(layers_config):
    """
    è®¡ç®—æ„Ÿå—é‡
    
    Args:
        layers_config: list of (kernel_size, stride)
    
    Returns:
        receptive_field: int
    """
    rf = 1
    stride_product = 1
    
    for k, s in layers_config:
        rf = rf + (k - 1) * stride_product
        stride_product *= s
    
    return rf


# ç¤ºä¾‹ä½¿ç”¨
if __name__ == "__main__":
    # æµ‹è¯•æ‰‹åŠ¨å·ç§¯
    print("=== æµ‹è¯•æ‰‹åŠ¨å·ç§¯ ===")
    input_tensor = torch.randn(1, 3, 5, 5)
    kernel = torch.randn(16, 3, 3, 3)
    
    output_manual = conv2d_manual(input_tensor, kernel, stride=1, padding=1)
    print(f"Input shape: {input_tensor.shape}")
    print(f"Kernel shape: {kernel.shape}")
    print(f"Output shape: {output_manual.shape}")
    
    # å¯¹æ¯”PyTorchå®ç°
    conv_layer = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)
    conv_layer.weight.data = kernel
    output_pytorch = conv_layer(input_tensor)
    
    print(f"PyTorch output shape: {output_pytorch.shape}")
    print(f"Difference: {torch.max(torch.abs(output_manual - output_pytorch)).item():.6f}")
    
    # æµ‹è¯•SimpleCNN
    print("\n=== æµ‹è¯•SimpleCNN ===")
    model = SimpleCNN(num_classes=10)
    x = torch.randn(2, 3, 32, 32)  # CIFAR-10 size
    y = model(x)
    print(f"Input: {x.shape}")
    print(f"Output: {y.shape}")
    print(f"Total parameters: {sum(p.numel() for p in model.parameters()):,}")
    
    # è®¡ç®—æ„Ÿå—é‡
    print("\n=== è®¡ç®—æ„Ÿå—é‡ ===")
    layers = [
        (3, 1),  # conv1
        (2, 2),  # pool1
        (3, 1),  # conv2
        (2, 2),  # pool2
        (3, 1),  # conv3
        (2, 2),  # pool3
    ]
    rf = compute_receptive_field(layers)
    print(f"Receptive field: {rf}x{rf}")


# 4. å¯è§†åŒ–å·ç§¯æ ¸
def visualize_filters(model, layer_name='conv1'):
    """å¯è§†åŒ–å·ç§¯æ ¸"""
    import matplotlib.pyplot as plt
    
    # è·å–ç¬¬ä¸€å±‚å·ç§¯æ ¸
    conv_layer = getattr(model, layer_name)
    filters = conv_layer.weight.data.cpu().numpy()
    
    # filters shape: (out_channels, in_channels, kH, kW)
    num_filters = min(filters.shape[0], 32)  # æœ€å¤šæ˜¾ç¤º32ä¸ª
    
    fig, axes = plt.subplots(4, 8, figsize=(12, 6))
    for i, ax in enumerate(axes.flat):
        if i < num_filters:
            # å–ç¬¬ä¸€ä¸ªè¾“å…¥é€šé“
            filter_img = filters[i, 0, :, :]
            ax.imshow(filter_img, cmap='gray')
            ax.set_title(f'Filter {i}')
        ax.axis('off')
    
    plt.suptitle(f'{layer_name} Filters')
    plt.tight_layout()
    # plt.show()


# 5. ç‰¹å¾å›¾å¯è§†åŒ–
def visualize_feature_maps(model, input_image, layer_name='conv1'):
    """å¯è§†åŒ–ç‰¹å¾å›¾"""
    import matplotlib.pyplot as plt
    
    # å‰å‘ä¼ æ’­åˆ°æŒ‡å®šå±‚
    x = input_image
    for name, module in model.named_children():
        x = module(x)
        if name == layer_name:
            break
    
    # x shape: (1, channels, H, W)
    feature_maps = x.squeeze(0).detach().cpu().numpy()
    num_maps = min(feature_maps.shape[0], 32)
    
    fig, axes = plt.subplots(4, 8, figsize=(12, 6))
    for i, ax in enumerate(axes.flat):
        if i < num_maps:
            ax.imshow(feature_maps[i], cmap='viridis')
            ax.set_title(f'Map {i}')
        ax.axis('off')
    
    plt.suptitle(f'{layer_name} Feature Maps')
    plt.tight_layout()
    # plt.show()
```

---

## ğŸ“š ç»å…¸CNNæ¶æ„

### 1. LeNet-5

**æ¶æ„** (LeCun et al., 1998)ï¼š

```text
Input (32x32x1)
    â†“
Conv1: 6 filters, 5x5 â†’ (28x28x6)
    â†“
AvgPool: 2x2 â†’ (14x14x6)
    â†“
Conv2: 16 filters, 5x5 â†’ (10x10x16)
    â†“
AvgPool: 2x2 â†’ (5x5x16)
    â†“
FC1: 120
    â†“
FC2: 84
    â†“
Output: 10
```

**ç‰¹ç‚¹**ï¼š

- æœ€æ—©çš„CNNä¹‹ä¸€
- ç”¨äºæ‰‹å†™æ•°å­—è¯†åˆ«
- çº¦60Kå‚æ•°

---

### 2. AlexNet

**æ¶æ„** (Krizhevsky et al., 2012)ï¼š

```text
Input (227x227x3)
    â†“
Conv1: 96 filters, 11x11, stride 4
    â†“
MaxPool: 3x3, stride 2
    â†“
Conv2: 256 filters, 5x5
    â†“
MaxPool: 3x3, stride 2
    â†“
Conv3: 384 filters, 3x3
Conv4: 384 filters, 3x3
Conv5: 256 filters, 3x3
    â†“
MaxPool: 3x3, stride 2
    â†“
FC1: 4096
FC2: 4096
FC3: 1000
```

**åˆ›æ–°**ï¼š

- ReLUæ¿€æ´»
- Dropout
- æ•°æ®å¢å¼º
- GPUè®­ç»ƒ

---

### 3. VGG

**æ¶æ„** (Simonyan & Zisserman, 2014)ï¼š

```text
å¤šå±‚ 3x3 å·ç§¯ + 2x2 MaxPool
    â†“
VGG-16: 13ä¸ªå·ç§¯å±‚ + 3ä¸ªå…¨è¿æ¥å±‚
VGG-19: 16ä¸ªå·ç§¯å±‚ + 3ä¸ªå…¨è¿æ¥å±‚
```

**ç‰¹ç‚¹**ï¼š

- å…¨éƒ¨ä½¿ç”¨ $3 \times 3$ å·ç§¯
- æ›´æ·±çš„ç½‘ç»œ
- çº¦138Må‚æ•°

**æ´å¯Ÿ**ï¼š

- ä¸¤ä¸ª $3 \times 3$ å·ç§¯ = ä¸€ä¸ª $5 \times 5$ æ„Ÿå—é‡
- ä½†å‚æ•°æ›´å°‘ï¼š$2 \times (3^2) < 5^2$

---

## ğŸ“ ç›¸å…³è¯¾ç¨‹

| å¤§å­¦ | è¯¾ç¨‹ |
|------|------|
| **Stanford** | CS231n Convolutional Neural Networks |
| **MIT** | 6.S191 Introduction to Deep Learning |
| **UC Berkeley** | CS182 Deep Learning |
| **CMU** | 11-785 Introduction to Deep Learning |

---

## ğŸ“– å‚è€ƒæ–‡çŒ®

1. **LeCun et al. (1998)**. "Gradient-Based Learning Applied to Document Recognition". *Proceedings of the IEEE*.

2. **Krizhevsky et al. (2012)**. "ImageNet Classification with Deep Convolutional Neural Networks". *NeurIPS*.

3. **Simonyan & Zisserman (2014)**. "Very Deep Convolutional Networks for Large-Scale Image Recognition". *ICLR*.

4. **He et al. (2016)**. "Deep Residual Learning for Image Recognition". *CVPR*.

5. **Luo et al. (2016)**. "Understanding the Effective Receptive Field in Deep Convolutional Neural Networks". *NeurIPS*.

---

*æœ€åæ›´æ–°ï¼š2025å¹´10æœˆ*-
