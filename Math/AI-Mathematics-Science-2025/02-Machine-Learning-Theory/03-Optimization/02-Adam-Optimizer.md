# Adamä¼˜åŒ–å™¨

> **Adaptive Moment Estimation (Adam)**
>
> æ·±åº¦å­¦ä¹ ä¸­æœ€æµè¡Œçš„ä¼˜åŒ–ç®—æ³•

---

## ç›®å½•

- [Adamä¼˜åŒ–å™¨](#adamä¼˜åŒ–å™¨)
  - [ç›®å½•](#ç›®å½•)
  - [ğŸ“‹ æ ¸å¿ƒæ€æƒ³](#-æ ¸å¿ƒæ€æƒ³)
  - [ğŸ¯ ç®—æ³•æ¨å¯¼](#-ç®—æ³•æ¨å¯¼)
    - [1. åŠ¨é‡æ–¹æ³•](#1-åŠ¨é‡æ–¹æ³•)
    - [2. RMSProp](#2-rmsprop)
    - [3. Adamç®—æ³•](#3-adamç®—æ³•)
  - [ğŸ“Š ç†è®ºåˆ†æ](#-ç†è®ºåˆ†æ)
    - [1. æ”¶æ•›æ€§](#1-æ”¶æ•›æ€§)
    - [2. åå·®ä¿®æ­£](#2-åå·®ä¿®æ­£)
  - [ğŸ”§ å˜ä½“ä¸æ”¹è¿›](#-å˜ä½“ä¸æ”¹è¿›)
    - [1. AdamW](#1-adamw)
    - [2. AMSGrad](#2-amsgrad)
  - [ğŸ’» Pythonå®ç°](#-pythonå®ç°)
  - [ğŸ“š æ ¸å¿ƒè¦ç‚¹](#-æ ¸å¿ƒè¦ç‚¹)
  - [ğŸ“ ç›¸å…³è¯¾ç¨‹](#-ç›¸å…³è¯¾ç¨‹)
  - [ğŸ“– å‚è€ƒæ–‡çŒ®](#-å‚è€ƒæ–‡çŒ®)

---

## ğŸ“‹ æ ¸å¿ƒæ€æƒ³

**Adam** ç»“åˆäº†**åŠ¨é‡**å’Œ**è‡ªé€‚åº”å­¦ä¹ ç‡**çš„ä¼˜ç‚¹ã€‚

**æ ¸å¿ƒç‰¹æ€§**ï¼š

- ä¸ºæ¯ä¸ªå‚æ•°è‡ªé€‚åº”è°ƒæ•´å­¦ä¹ ç‡
- åˆ©ç”¨ä¸€é˜¶å’ŒäºŒé˜¶çŸ©ä¼°è®¡
- åå·®ä¿®æ­£

**ä¼˜åŠ¿**ï¼š

- å¿«é€Ÿæ”¶æ•›
- å¯¹è¶…å‚æ•°ä¸æ•æ„Ÿ
- é€‚ç”¨äºå¤§è§„æ¨¡é—®é¢˜

---

## ğŸ¯ ç®—æ³•æ¨å¯¼

### 1. åŠ¨é‡æ–¹æ³•

**æ ‡å‡†åŠ¨é‡ (Momentum)**:

$$
\begin{align}
v_t &= \beta_1 v_{t-1} + (1 - \beta_1) g_t \\
\theta_t &= \theta_{t-1} - \alpha v_t
\end{align}
$$

**ç›´è§‰**ï¼šç´¯ç§¯å†å²æ¢¯åº¦ï¼ŒåŠ é€Ÿæ”¶æ•›ã€‚

---

### 2. RMSProp

**Root Mean Square Propagation**:

$$
\begin{align}
s_t &= \beta_2 s_{t-1} + (1 - \beta_2) g_t^2 \\
\theta_t &= \theta_{t-1} - \frac{\alpha}{\sqrt{s_t} + \epsilon} g_t
\end{align}
$$

**ç›´è§‰**ï¼šä¸ºæ¯ä¸ªå‚æ•°è‡ªé€‚åº”è°ƒæ•´å­¦ä¹ ç‡ã€‚

---

### 3. Adamç®—æ³•

**ç®—æ³• 3.1 (Adam)**:

**è¾“å…¥**ï¼š

- å­¦ä¹ ç‡ $\alpha$ (é»˜è®¤: 0.001)
- ä¸€é˜¶çŸ©è¡°å‡ç‡ $\beta_1$ (é»˜è®¤: 0.9)
- äºŒé˜¶çŸ©è¡°å‡ç‡ $\beta_2$ (é»˜è®¤: 0.999)
- æ•°å€¼ç¨³å®šé¡¹ $\epsilon$ (é»˜è®¤: $10^{-8}$)

**åˆå§‹åŒ–**ï¼š

- $m_0 = 0$ (ä¸€é˜¶çŸ©)
- $v_0 = 0$ (äºŒé˜¶çŸ©)
- $t = 0$ (æ—¶é—´æ­¥)

**æ›´æ–°è§„åˆ™**ï¼š

$$
\begin{align}
t &\leftarrow t + 1 \\
g_t &= \nabla_\theta L(\theta_{t-1}) \\
m_t &= \beta_1 m_{t-1} + (1 - \beta_1) g_t \\
v_t &= \beta_2 v_{t-1} + (1 - \beta_2) g_t^2 \\
\hat{m}_t &= \frac{m_t}{1 - \beta_1^t} \quad \text{(åå·®ä¿®æ­£)} \\
\hat{v}_t &= \frac{v_t}{1 - \beta_2^t} \quad \text{(åå·®ä¿®æ­£)} \\
\theta_t &= \theta_{t-1} - \frac{\alpha}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t
\end{align}
$$

---

## ğŸ“Š ç†è®ºåˆ†æ

### 1. æ”¶æ•›æ€§

**å®šç† 1.1 (Adamæ”¶æ•›æ€§)**:

åœ¨å‡¸æƒ…å†µä¸‹ï¼ŒAdamçš„é—æ†¾ç•Œä¸ºï¼š

$$
R(T) = \sum_{t=1}^{T} [f(\theta_t) - f(\theta^*)] = O(\sqrt{T})
$$

**æ³¨æ„**ï¼šåŸå§‹Adamåœ¨éå‡¸æƒ…å†µä¸‹å¯èƒ½ä¸æ”¶æ•›ï¼ˆè§AMSGradï¼‰ã€‚

---

### 2. åå·®ä¿®æ­£

**ä¸ºä»€ä¹ˆéœ€è¦åå·®ä¿®æ­£ï¼Ÿ**

åˆå§‹æ—¶ $m_0 = 0, v_0 = 0$ï¼Œå¯¼è‡´ä¼°è®¡åå‘é›¶ã€‚

**ä¿®æ­£åçš„æœŸæœ›**ï¼š

$$
\mathbb{E}[\hat{m}_t] = \mathbb{E}[g_t], \quad \mathbb{E}[\hat{v}_t] = \mathbb{E}[g_t^2]
$$

**è¯æ˜**ï¼š

$$
\mathbb{E}[m_t] = \mathbb{E}[g_t](1 - \beta_1^t)
$$

å› æ­¤ï¼š

$$
\mathbb{E}[\hat{m}_t] = \frac{\mathbb{E}[m_t]}{1 - \beta_1^t} = \mathbb{E}[g_t]
$$

---

## ğŸ”§ å˜ä½“ä¸æ”¹è¿›

### 1. AdamW

**æ ¸å¿ƒæ”¹è¿›**ï¼šè§£è€¦æƒé‡è¡°å‡ã€‚

**æ ‡å‡†Adam + L2æ­£åˆ™**ï¼š

$$
\theta_t = \theta_{t-1} - \alpha \left(\frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon} + \lambda \theta_{t-1}\right)
$$

**AdamW**ï¼š

$$
\theta_t = (1 - \alpha \lambda) \theta_{t-1} - \alpha \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
$$

**ä¼˜åŠ¿**ï¼šæ›´å¥½çš„æ³›åŒ–æ€§èƒ½ã€‚

---

### 2. AMSGrad

**é—®é¢˜**ï¼šAdamå¯èƒ½ä¸æ”¶æ•›ï¼ˆæŒ‡æ•°ç§»åŠ¨å¹³å‡å¯èƒ½"å¿˜è®°"å†å²ä¿¡æ¯ï¼‰ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼šä¿ç•™å†å²æœ€å¤§äºŒé˜¶çŸ©ã€‚

$$
\begin{align}
\hat{v}_t &= \max(\hat{v}_{t-1}, v_t) \\
\theta_t &= \theta_{t-1} - \frac{\alpha}{\sqrt{\hat{v}_t} + \epsilon} m_t
\end{align}
$$

**ä¿è¯**ï¼šéå‡¸æƒ…å†µä¸‹çš„æ”¶æ•›æ€§ã€‚

---

## ğŸ’» Pythonå®ç°

```python
import numpy as np

class AdamOptimizer:
    """Adamä¼˜åŒ–å™¨å®ç°"""
    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):
        self.lr = lr
        self.beta1 = beta1
        self.beta2 = beta2
        self.epsilon = epsilon
        
        self.m = None  # ä¸€é˜¶çŸ©
        self.v = None  # äºŒé˜¶çŸ©
        self.t = 0     # æ—¶é—´æ­¥
    
    def update(self, params, grads):
        """
        æ›´æ–°å‚æ•°
        
        Args:
            params: å‚æ•°å­—å…¸ {name: value}
            grads: æ¢¯åº¦å­—å…¸ {name: gradient}
        """
        if self.m is None:
            self.m = {k: np.zeros_like(v) for k, v in params.items()}
            self.v = {k: np.zeros_like(v) for k, v in params.items()}
        
        self.t += 1
        
        for key in params:
            # æ›´æ–°ä¸€é˜¶çŸ©
            self.m[key] = self.beta1 * self.m[key] + (1 - self.beta1) * grads[key]
            
            # æ›´æ–°äºŒé˜¶çŸ©
            self.v[key] = self.beta2 * self.v[key] + (1 - self.beta2) * grads[key]**2
            
            # åå·®ä¿®æ­£
            m_hat = self.m[key] / (1 - self.beta1**self.t)
            v_hat = self.v[key] / (1 - self.beta2**self.t)
            
            # æ›´æ–°å‚æ•°
            params[key] -= self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)
        
        return params


class AdamWOptimizer:
    """AdamWä¼˜åŒ–å™¨ï¼ˆè§£è€¦æƒé‡è¡°å‡ï¼‰"""
    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, weight_decay=0.01):
        self.lr = lr
        self.beta1 = beta1
        self.beta2 = beta2
        self.epsilon = epsilon
        self.weight_decay = weight_decay
        
        self.m = None
        self.v = None
        self.t = 0
    
    def update(self, params, grads):
        """æ›´æ–°å‚æ•°ï¼ˆAdamWï¼‰"""
        if self.m is None:
            self.m = {k: np.zeros_like(v) for k, v in params.items()}
            self.v = {k: np.zeros_like(v) for k, v in params.items()}
        
        self.t += 1
        
        for key in params:
            # æ›´æ–°çŸ©ä¼°è®¡
            self.m[key] = self.beta1 * self.m[key] + (1 - self.beta1) * grads[key]
            self.v[key] = self.beta2 * self.v[key] + (1 - self.beta2) * grads[key]**2
            
            # åå·®ä¿®æ­£
            m_hat = self.m[key] / (1 - self.beta1**self.t)
            v_hat = self.v[key] / (1 - self.beta2**self.t)
            
            # AdamWæ›´æ–°ï¼ˆè§£è€¦æƒé‡è¡°å‡ï¼‰
            params[key] = (1 - self.lr * self.weight_decay) * params[key] - \
                          self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)
        
        return params


# ç¤ºä¾‹ï¼šä¼˜åŒ–Rosenbrockå‡½æ•°
def rosenbrock(x):
    """Rosenbrockå‡½æ•°"""
    return (1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2

def rosenbrock_grad(x):
    """Rosenbrockæ¢¯åº¦"""
    dx0 = -2 * (1 - x[0]) - 400 * x[0] * (x[1] - x[0]**2)
    dx1 = 200 * (x[1] - x[0]**2)
    return np.array([dx0, dx1])

# ä¼˜åŒ–
params = {'x': np.array([-1.0, 1.0])}
optimizer = AdamOptimizer(lr=0.01)

for i in range(1000):
    grads = {'x': rosenbrock_grad(params['x'])}
    params = optimizer.update(params, grads)
    
    if i % 100 == 0:
        loss = rosenbrock(params['x'])
        print(f"Iteration {i}, Loss: {loss:.6f}, x: {params['x']}")
```

---

## ğŸ“š æ ¸å¿ƒè¦ç‚¹

| æ¦‚å¿µ | è¯´æ˜ |
|------|------|
| **ä¸€é˜¶çŸ©** | $m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t$ |
| **äºŒé˜¶çŸ©** | $v_t = \beta_2 v_{t-1} + (1-\beta_2) g_t^2$ |
| **åå·®ä¿®æ­£** | $\hat{m}_t = m_t / (1 - \beta_1^t)$ |
| **è‡ªé€‚åº”å­¦ä¹ ç‡** | $\alpha / \sqrt{\hat{v}_t}$ |

---

## ğŸ“ ç›¸å…³è¯¾ç¨‹

| å¤§å­¦ | è¯¾ç¨‹ |
|------|------|
| **Stanford** | CS231n Deep Learning |
| **MIT** | 6.036 Introduction to ML |
| **CMU** | 10-725 Convex Optimization |

---

## ğŸ“– å‚è€ƒæ–‡çŒ®

1. **Kingma & Ba (2015)**. "Adam: A Method for Stochastic Optimization". *ICLR*.

2. **Loshchilov & Hutter (2019)**. "Decoupled Weight Decay Regularization". *ICLR*.

3. **Reddi et al. (2018)**. "On the Convergence of Adam and Beyond". *ICLR*.

---

*æœ€åæ›´æ–°ï¼š2025å¹´10æœˆ*-
