# Adamä¼˜åŒ–å™¨

> **Adaptive Moment Estimation (Adam)**
>
> æ·±åº¦å­¦ä¹ ä¸­æœ€æµè¡Œçš„ä¼˜åŒ–ç®—æ³•

---

## ç›®å½•

- [Adamä¼˜åŒ–å™¨](#adamä¼˜åŒ–å™¨)
  - [ç›®å½•](#ç›®å½•)
  - [ğŸ“‹ æ ¸å¿ƒæ€æƒ³](#-æ ¸å¿ƒæ€æƒ³)
  - [ğŸ¯ ç®—æ³•æ¨å¯¼](#-ç®—æ³•æ¨å¯¼)
    - [1. åŠ¨é‡æ–¹æ³•](#1-åŠ¨é‡æ–¹æ³•)
    - [2. RMSProp](#2-rmsprop)
    - [3. Adamç®—æ³•](#3-adamç®—æ³•)
  - [ğŸ“Š ç†è®ºåˆ†æ](#-ç†è®ºåˆ†æ)
    - [1. æ”¶æ•›æ€§](#1-æ”¶æ•›æ€§)
    - [ğŸ“ Adamæ”¶æ•›æ€§å®šç†çš„å®Œæ•´åˆ†æ](#-adamæ”¶æ•›æ€§å®šç†çš„å®Œæ•´åˆ†æ)
      - [å®šç† 1.2 (Adamå‡¸æ”¶æ•›æ€§ - Kingma \& Ba 2015)](#å®šç†-12-adamå‡¸æ”¶æ•›æ€§---kingma--ba-2015)
      - [è¯æ˜æ€è·¯ï¼ˆæ ¸å¿ƒæ­¥éª¤ï¼‰](#è¯æ˜æ€è·¯æ ¸å¿ƒæ­¥éª¤)
    - [ğŸš¨ Adamçš„æ”¶æ•›æ€§é—®é¢˜](#-adamçš„æ”¶æ•›æ€§é—®é¢˜)
      - [1. éå‡¸æƒ…å†µçš„åä¾‹ (Reddi et al. 2018)](#1-éå‡¸æƒ…å†µçš„åä¾‹-reddi-et-al-2018)
      - [2. AMSGradä¿®å¤ (Reddi et al. 2018)](#2-amsgradä¿®å¤-reddi-et-al-2018)
      - [3. AMSGradæ”¶æ•›æ€§ä¿è¯](#3-amsgradæ”¶æ•›æ€§ä¿è¯)
    - [ğŸ¯ å®è·µå»ºè®®](#-å®è·µå»ºè®®)
      - [1. ä½•æ—¶ä½¿ç”¨Adam vs AMSGradï¼Ÿ](#1-ä½•æ—¶ä½¿ç”¨adam-vs-amsgrad)
      - [2. Adamè¶…å‚æ•°è°ƒä¼˜](#2-adamè¶…å‚æ•°è°ƒä¼˜)
      - [3. Adam vs SGDé€‰æ‹©](#3-adam-vs-sgdé€‰æ‹©)
    - [ğŸ“Š æ•°å€¼éªŒè¯](#-æ•°å€¼éªŒè¯)
    - [ğŸ”‘ å…³é”®è¦ç‚¹](#-å…³é”®è¦ç‚¹)
    - [2. åå·®ä¿®æ­£](#2-åå·®ä¿®æ­£)
    - [ğŸ“ åå·®ä¿®æ­£çš„å®Œæ•´è¯æ˜](#-åå·®ä¿®æ­£çš„å®Œæ•´è¯æ˜)
      - [è¯æ˜ (1): æœªä¿®æ­£ä¸€é˜¶çŸ©çš„æœŸæœ›](#è¯æ˜-1-æœªä¿®æ­£ä¸€é˜¶çŸ©çš„æœŸæœ›)
      - [è¯æ˜ (2): ä¿®æ­£åä¸€é˜¶çŸ©çš„æ— åæ€§](#è¯æ˜-2-ä¿®æ­£åä¸€é˜¶çŸ©çš„æ— åæ€§)
      - [è¯æ˜ (3): æœªä¿®æ­£äºŒé˜¶çŸ©çš„æœŸæœ›](#è¯æ˜-3-æœªä¿®æ­£äºŒé˜¶çŸ©çš„æœŸæœ›)
      - [è¯æ˜ (4): ä¿®æ­£åäºŒé˜¶çŸ©çš„æ— åæ€§](#è¯æ˜-4-ä¿®æ­£åäºŒé˜¶çŸ©çš„æ— åæ€§)
    - [ğŸ“Š åå·®ä¿®æ­£çš„é‡è¦æ€§åˆ†æ](#-åå·®ä¿®æ­£çš„é‡è¦æ€§åˆ†æ)
      - [1. åˆå§‹é˜¶æ®µçš„åå·®](#1-åˆå§‹é˜¶æ®µçš„åå·®)
      - [2. ä¿®æ­£æ•ˆæœå¯è§†åŒ–](#2-ä¿®æ­£æ•ˆæœå¯è§†åŒ–)
      - [3. å¯¹æ”¶æ•›æ€§çš„å½±å“](#3-å¯¹æ”¶æ•›æ€§çš„å½±å“)
      - [4. æ•°å­¦ç›´è§‰](#4-æ•°å­¦ç›´è§‰)
    - [ğŸ¯ å®è·µå»ºè®®1](#-å®è·µå»ºè®®1)
  - [ğŸ”§ å˜ä½“ä¸æ”¹è¿›](#-å˜ä½“ä¸æ”¹è¿›)
    - [1. AdamW](#1-adamw)
    - [2. AMSGrad](#2-amsgrad)
  - [ğŸ’» Pythonå®ç°](#-pythonå®ç°)
  - [ğŸ“š æ ¸å¿ƒè¦ç‚¹](#-æ ¸å¿ƒè¦ç‚¹)
  - [ğŸ“ ç›¸å…³è¯¾ç¨‹](#-ç›¸å…³è¯¾ç¨‹)
  - [ğŸ“– å‚è€ƒæ–‡çŒ®](#-å‚è€ƒæ–‡çŒ®)

---

## ğŸ“‹ æ ¸å¿ƒæ€æƒ³

**Adam** ç»“åˆäº†**åŠ¨é‡**å’Œ**è‡ªé€‚åº”å­¦ä¹ ç‡**çš„ä¼˜ç‚¹ã€‚

**æ ¸å¿ƒç‰¹æ€§**ï¼š

- ä¸ºæ¯ä¸ªå‚æ•°è‡ªé€‚åº”è°ƒæ•´å­¦ä¹ ç‡
- åˆ©ç”¨ä¸€é˜¶å’ŒäºŒé˜¶çŸ©ä¼°è®¡
- åå·®ä¿®æ­£

**ä¼˜åŠ¿**ï¼š

- å¿«é€Ÿæ”¶æ•›
- å¯¹è¶…å‚æ•°ä¸æ•æ„Ÿ
- é€‚ç”¨äºå¤§è§„æ¨¡é—®é¢˜

---

## ğŸ¯ ç®—æ³•æ¨å¯¼

### 1. åŠ¨é‡æ–¹æ³•

**æ ‡å‡†åŠ¨é‡ (Momentum)**:

$$
\begin{align}
v_t &= \beta_1 v_{t-1} + (1 - \beta_1) g_t \\
\theta_t &= \theta_{t-1} - \alpha v_t
\end{align}
$$

**ç›´è§‰**ï¼šç´¯ç§¯å†å²æ¢¯åº¦ï¼ŒåŠ é€Ÿæ”¶æ•›ã€‚

---

### 2. RMSProp

**Root Mean Square Propagation**:

$$
\begin{align}
s_t &= \beta_2 s_{t-1} + (1 - \beta_2) g_t^2 \\
\theta_t &= \theta_{t-1} - \frac{\alpha}{\sqrt{s_t} + \epsilon} g_t
\end{align}
$$

**ç›´è§‰**ï¼šä¸ºæ¯ä¸ªå‚æ•°è‡ªé€‚åº”è°ƒæ•´å­¦ä¹ ç‡ã€‚

---

### 3. Adamç®—æ³•

**ç®—æ³• 3.1 (Adam)**:

**è¾“å…¥**ï¼š

- å­¦ä¹ ç‡ $\alpha$ (é»˜è®¤: 0.001)
- ä¸€é˜¶çŸ©è¡°å‡ç‡ $\beta_1$ (é»˜è®¤: 0.9)
- äºŒé˜¶çŸ©è¡°å‡ç‡ $\beta_2$ (é»˜è®¤: 0.999)
- æ•°å€¼ç¨³å®šé¡¹ $\epsilon$ (é»˜è®¤: $10^{-8}$)

**åˆå§‹åŒ–**ï¼š

- $m_0 = 0$ (ä¸€é˜¶çŸ©)
- $v_0 = 0$ (äºŒé˜¶çŸ©)
- $t = 0$ (æ—¶é—´æ­¥)

**æ›´æ–°è§„åˆ™**ï¼š

$$
\begin{align}
t &\leftarrow t + 1 \\
g_t &= \nabla_\theta L(\theta_{t-1}) \\
m_t &= \beta_1 m_{t-1} + (1 - \beta_1) g_t \\
v_t &= \beta_2 v_{t-1} + (1 - \beta_2) g_t^2 \\
\hat{m}_t &= \frac{m_t}{1 - \beta_1^t} \quad \text{(åå·®ä¿®æ­£)} \\
\hat{v}_t &= \frac{v_t}{1 - \beta_2^t} \quad \text{(åå·®ä¿®æ­£)} \\
\theta_t &= \theta_{t-1} - \frac{\alpha}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t
\end{align}
$$

---

## ğŸ“Š ç†è®ºåˆ†æ

### 1. æ”¶æ•›æ€§

**å®šç† 1.1 (Adamæ”¶æ•›æ€§)**:

åœ¨å‡¸æƒ…å†µä¸‹ï¼ŒAdamçš„é—æ†¾ç•Œä¸ºï¼š

$$
R(T) = \sum_{t=1}^{T} [f(\theta_t) - f(\theta^*)] = O(\sqrt{T})
$$

**æ³¨æ„**ï¼šåŸå§‹Adamåœ¨éå‡¸æƒ…å†µä¸‹å¯èƒ½ä¸æ”¶æ•›ï¼ˆè§AMSGradï¼‰ã€‚

---

### ğŸ“ Adamæ”¶æ•›æ€§å®šç†çš„å®Œæ•´åˆ†æ

#### å®šç† 1.2 (Adamå‡¸æ”¶æ•›æ€§ - Kingma & Ba 2015)

**å‡è®¾**:

1. **å‡¸æ€§**: æŸå¤±å‡½æ•° $f_t(\theta)$ åœ¨æ¯æ­¥ $t$ æ˜¯å‡¸çš„
2. **æœ‰ç•Œæ¢¯åº¦**: $\|g_t\|_\infty \leq G_\infty$ å¯¹æ‰€æœ‰ $t$
3. **æœ‰ç•Œè·ç¦»**: $\|\theta_t - \theta^*\|_2 \leq D$ å¯¹æ‰€æœ‰ $t$
4. **å…‰æ»‘æ€§**: $\|g_t - g_{t-1}\|_2 \leq \rho$ ï¼ˆæ¢¯åº¦Lipschitzè¿ç»­ï¼‰

**ç»“è®º**: ä½¿ç”¨Adamç®—æ³•ï¼Œé—æ†¾ç•Œä¸ºï¼š

$$
R(T) = \sum_{t=1}^{T} [f_t(\theta_t) - f_t(\theta^*)] \leq \frac{D^2\sum_{i=1}^{d}\sqrt{T}\|\hat{g}_{1:T,i}\|_2}{2\alpha(1-\beta_1)\sqrt{1-\beta_2}} + \frac{\alpha G_\infty}{1-\beta_1}\sum_{i=1}^{d}\sqrt{T\|\hat{g}_{1:T,i}\|_2}
$$

å…¶ä¸­ $\hat{g}_{1:T,i}$ æ˜¯ç¬¬ $i$ ä¸ªåæ ‡çš„æ¢¯åº¦åºåˆ—ã€‚

**ç®€åŒ–**: åœ¨æ¢¯åº¦æœ‰ç•Œæƒ…å†µä¸‹ï¼Œ$R(T) = O(\sqrt{T})$ã€‚

---

#### è¯æ˜æ€è·¯ï¼ˆæ ¸å¿ƒæ­¥éª¤ï¼‰

**Step 1: åœ¨çº¿å‡¸ä¼˜åŒ–æ¡†æ¶**:

Adamå¯ä»¥çœ‹ä½œåœ¨çº¿å‡¸ä¼˜åŒ–ç®—æ³•ï¼Œæ¯æ­¥é¢å¯¹æ–°çš„å‡¸å‡½æ•° $f_t(\theta)$ã€‚

ç´¯ç§¯é—æ†¾ï¼š

$$
R(T) = \sum_{t=1}^{T} [f_t(\theta_t) - f_t(\theta^*)]
$$

---

**Step 2: ä½¿ç”¨å‡¸å‡½æ•°ä¸€é˜¶æ¡ä»¶**:

ç”±å‡¸æ€§ï¼š

$$
f_t(\theta_t) - f_t(\theta^*) \leq g_t^T(\theta_t - \theta^*)
$$

å› æ­¤ï¼š

$$
R(T) \leq \sum_{t=1}^{T} g_t^T(\theta_t - \theta^*)
$$

---

**Step 3: Adamæ›´æ–°è§„åˆ™æ”¹å†™**:

Adamæ›´æ–°å¯ä»¥å†™ä¸ºï¼š

$$
\theta_{t+1} = \theta_t - \frac{\alpha}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t
$$

å®šä¹‰è‡ªé€‚åº”å­¦ä¹ ç‡ï¼š

$$
\eta_{t,i} = \frac{\alpha}{\sqrt{\hat{v}_{t,i}} + \epsilon}
$$

å…¶ä¸­ $i$ è¡¨ç¤ºç¬¬ $i$ ä¸ªåæ ‡ã€‚

---

**Step 4: è·ç¦»é€’æ¨ï¼ˆå…³é”®æŠ€å·§ï¼‰**:

è€ƒè™‘åˆ°æœ€ä¼˜ç‚¹çš„è·ç¦»å˜åŒ–ï¼š

$$
\|\theta_{t+1} - \theta^*\|_2^2 = \|\theta_t - \theta^* - \eta_t \odot \hat{m}_t\|_2^2
$$

å…¶ä¸­ $\odot$ è¡¨ç¤ºé€å…ƒç´ ä¹˜ç§¯ã€‚

å±•å¼€ï¼š

$$
= \|\theta_t - \theta^*\|_2^2 - 2(\theta_t - \theta^*)^T(\eta_t \odot \hat{m}_t) + \|\eta_t \odot \hat{m}_t\|_2^2
$$

---

**Step 5: å¤„ç†å†…ç§¯é¡¹**:

å…³é”®ä¸ç­‰å¼ï¼ˆæ¥è‡ªå‡¸æ€§ï¼‰ï¼š

$$
g_t^T(\theta_t - \theta^*) \leq \frac{1}{2\alpha}[\|\theta_t - \theta^*\|_2^2 - \|\theta_{t+1} - \theta^*\|_2^2] + \text{å…¶ä»–é¡¹}
$$

---

**Step 6: æ±‚å’Œå¹¶åº”ç”¨æœ›è¿œé•œæŠ€å·§**:

å¯¹ $t=1$ åˆ° $T$ æ±‚å’Œï¼š

$$
\sum_{t=1}^{T} g_t^T(\theta_t - \theta^*) \leq \frac{\|\theta_1 - \theta^*\|_2^2}{2\alpha} + \sum_{t=1}^{T} \text{ï¼ˆè‡ªé€‚åº”é¡¹ï¼‰}
$$

---

**Step 7: åˆ†æè‡ªé€‚åº”é¡¹**:

Adamçš„è‡ªé€‚åº”å­¦ä¹ ç‡æ»¡è¶³ï¼š

$$
\sum_{t=1}^{T} \eta_{t,i}^{-1} \geq \sqrt{\sum_{t=1}^{T} g_{t,i}^2}
$$

è¿™å¯¼è‡´æ›´ç´§çš„é—æ†¾ç•Œã€‚

---

**Step 8: æœ€ç»ˆç•Œ**:

ç»“åˆä¸Šè¿°æ­¥éª¤ï¼Œå¾—åˆ°ï¼š

$$
R(T) = O(\sqrt{T})
$$

$\quad \blacksquare$

---

### ğŸš¨ Adamçš„æ”¶æ•›æ€§é—®é¢˜

#### 1. éå‡¸æƒ…å†µçš„åä¾‹ (Reddi et al. 2018)

**é—®é¢˜**: åŸå§‹Adamåœ¨æŸäº›éå‡¸é—®é¢˜ä¸Š**ä¸æ”¶æ•›**ï¼

**åä¾‹**ï¼ˆç®€åŒ–ç‰ˆï¼‰:

è€ƒè™‘ä¸€ç»´ä¼˜åŒ–é—®é¢˜ï¼Œæ¢¯åº¦åºåˆ—ï¼š

$$
g_t = \begin{cases}
1 & t \mod 3 = 0 \\
-1 & \text{otherwise}
\end{cases}
$$

**ç°è±¡**:

- ä¸€é˜¶çŸ© $m_t$ æŒ¯è¡
- äºŒé˜¶çŸ© $v_t$ è¢«å¤§æ¢¯åº¦ä¸»å¯¼
- **å­¦ä¹ ç‡è¡°å‡è¿‡å¿«**ï¼Œå¯¼è‡´æ›´æ–°åœæ»

**æ•°å­¦åŸå› **:

Adamçš„äºŒé˜¶çŸ©æ›´æ–°ï¼š

$$
v_t = \beta_2 v_{t-1} + (1-\beta_2) g_t^2
$$

ä½¿ç”¨æŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼Œå¯èƒ½"å¿˜è®°"å†å²å¤§æ¢¯åº¦ï¼Œå¯¼è‡´ï¼š

$$
\hat{v}_t \ll \max_{1 \leq i \leq t} g_i^2
$$

å­¦ä¹ ç‡ $\frac{\alpha}{\sqrt{\hat{v}_t}}$ è¿‡å¤§ï¼Œå¼•èµ·å‘æ•£ã€‚

---

#### 2. AMSGradä¿®å¤ (Reddi et al. 2018)

**æ ¸å¿ƒæ”¹è¿›**: ä¿ç•™å†å²æœ€å¤§äºŒé˜¶çŸ©

**ç®—æ³•ä¿®æ”¹**:

$$
\hat{v}_t = \max(\hat{v}_{t-1}, v_t)
$$

ä»£æ›¿åŸæ¥çš„ $\hat{v}_t = \frac{v_t}{1-\beta_2^t}$ã€‚

**æ›´æ–°è§„åˆ™**:

$$
\theta_t = \theta_{t-1} - \frac{\alpha}{\sqrt{\hat{v}_t} + \epsilon} m_t
$$

ï¼ˆæ³¨æ„ï¼šä¸éœ€è¦åå·®ä¿®æ­£ $\hat{m}_t$ï¼Œç›´æ¥ç”¨ $m_t$ï¼‰

---

#### 3. AMSGradæ”¶æ•›æ€§ä¿è¯

**å®šç† (AMSGradæ”¶æ•›æ€§)**:

åœ¨éå‡¸æƒ…å†µä¸‹ï¼ŒAMSGradæ»¡è¶³ï¼š

$$
\min_{t \in [T]} \mathbb{E}[\|\nabla f(\theta_t)\|^2] \leq \frac{C}{\sqrt{T}}
$$

å…¶ä¸­ $C$ æ˜¯å¸¸æ•°ï¼Œå–å†³äºé—®é¢˜å‚æ•°ã€‚

**å…³é”®**: $\hat{v}_t$ å•è°ƒé€’å¢ â†’ å­¦ä¹ ç‡å•è°ƒé€’å‡ â†’ ä¿è¯æ”¶æ•›ã€‚

---

### ğŸ¯ å®è·µå»ºè®®

#### 1. ä½•æ—¶ä½¿ç”¨Adam vs AMSGradï¼Ÿ

| åœºæ™¯ | æ¨è | åŸå›  |
|------|------|------|
| **å‡¸ä¼˜åŒ–** | Adam | æ”¶æ•›æ€§æœ‰ä¿è¯ï¼Œé€Ÿåº¦å¿« |
| **æ·±åº¦å­¦ä¹ ï¼ˆä¸€èˆ¬ï¼‰** | Adam | å®è·µä¸­è¡¨ç°å¥½ï¼Œå¾ˆå°‘é‡åˆ°ç—…æ€æƒ…å†µ |
| **å¼ºåŒ–å­¦ä¹ ** | AMSGrad | æ¢¯åº¦ç¨€ç–ï¼Œéœ€è¦ä¿ç•™å†å²ä¿¡æ¯ |
| **å¯¹æŠ—è®­ç»ƒ** | AMSGrad | æ¢¯åº¦å˜åŒ–å‰§çƒˆ |
| **ç†è®ºä¿è¯éœ€æ±‚** | AMSGrad | æœ‰ä¸¥æ ¼æ”¶æ•›æ€§è¯æ˜ |

---

#### 2. Adamè¶…å‚æ•°è°ƒä¼˜

**é»˜è®¤å€¼**ï¼ˆKingma & Ba 2015ï¼‰:

- $\alpha = 0.001$ï¼ˆå­¦ä¹ ç‡ï¼‰
- $\beta_1 = 0.9$ï¼ˆä¸€é˜¶çŸ©è¡°å‡ï¼‰
- $\beta_2 = 0.999$ï¼ˆäºŒé˜¶çŸ©è¡°å‡ï¼‰
- $\epsilon = 10^{-8}$ï¼ˆæ•°å€¼ç¨³å®šé¡¹ï¼‰

**è°ƒä¼˜å»ºè®®**:

1. **å­¦ä¹ ç‡ $\alpha$**:
   - å¦‚æœlossä¸ä¸‹é™ â†’ å‡å° $\alpha$
   - å¦‚æœæ”¶æ•›å¤ªæ…¢ â†’ å¢å¤§ $\alpha$
   - å…¸å‹èŒƒå›´: $[10^{-4}, 10^{-2}]$

2. **$\beta_2$**:
   - æ¢¯åº¦ç¨€ç– â†’ å¢å¤§ $\beta_2$ï¼ˆå¦‚0.999â†’0.9999ï¼‰
   - æ¢¯åº¦å¯†é›† â†’ å‡å° $\beta_2$ï¼ˆå¦‚0.999â†’0.99ï¼‰

3. **$\epsilon$**:
   - å¦‚æœå‡ºç°æ•°å€¼ä¸ç¨³å®š â†’ å¢å¤§ $\epsilon$ï¼ˆå¦‚ $10^{-8}$ â†’ $10^{-4}$ï¼‰

---

#### 3. Adam vs SGDé€‰æ‹©

**Adamä¼˜åŠ¿**:

- è‡ªé€‚åº”å­¦ä¹ ç‡ï¼Œä¸éœ€è¦æ‰‹åŠ¨è°ƒ
- å¯¹è¶…å‚æ•°ä¸æ•æ„Ÿ
- æ”¶æ•›å¿«ï¼ˆå‰æœŸï¼‰

**SGDä¼˜åŠ¿**:

- æ³›åŒ–æ€§èƒ½æ›´å¥½ï¼ˆæŸäº›æƒ…å†µï¼‰
- ç†è®ºæ›´ç®€å•
- æ”¶æ•›åˆ°æ›´sharpçš„æå°å€¼

**å®è·µç»éªŒ**:

- **è®­ç»ƒ**: ç”¨Adamå¿«é€Ÿæ”¶æ•›
- **Fine-tuning**: åˆ‡æ¢åˆ°SGDæå‡æ³›åŒ–

---

### ğŸ“Š æ•°å€¼éªŒè¯

```python
import numpy as np
import matplotlib.pyplot as plt

# ç®€å•éå‡¸å‡½æ•°: f(x) = x^4/4 - x^2/2
def f(x):
    return 0.25 * x**4 - 0.5 * x**2

def grad_f(x):
    return x**3 - x

# Adamå®ç°
def adam_optimizer(x0, lr=0.01, beta1=0.9, beta2=0.999, eps=1e-8, T=1000):
    x = x0
    m = 0
    v = 0
    trajectory = [x]

    for t in range(1, T+1):
        g = grad_f(x) + np.random.randn() * 0.1  # åŠ å™ªå£°

        # æ›´æ–°çŸ©ä¼°è®¡
        m = beta1 * m + (1 - beta1) * g
        v = beta2 * v + (1 - beta2) * g**2

        # åå·®ä¿®æ­£
        m_hat = m / (1 - beta1**t)
        v_hat = v / (1 - beta2**t)

        # æ›´æ–°å‚æ•°
        x = x - lr * m_hat / (np.sqrt(v_hat) + eps)
        trajectory.append(x)

    return trajectory

# AMSGradå®ç°
def amsgrad_optimizer(x0, lr=0.01, beta1=0.9, beta2=0.999, eps=1e-8, T=1000):
    x = x0
    m = 0
    v = 0
    v_hat = 0  # å†å²æœ€å¤§
    trajectory = [x]

    for t in range(1, T+1):
        g = grad_f(x) + np.random.randn() * 0.1

        m = beta1 * m + (1 - beta1) * g
        v = beta2 * v + (1 - beta2) * g**2

        # å…³é”®ï¼šä¿ç•™å†å²æœ€å¤§
        v_hat = max(v_hat, v)

        # æ›´æ–°ï¼ˆæ³¨æ„ï¼šä¸ç”¨åå·®ä¿®æ­£ï¼‰
        x = x - lr * m / (np.sqrt(v_hat) + eps)
        trajectory.append(x)

    return trajectory

# è¿è¡Œå®éªŒ
x0 = 2.0
adam_traj = adam_optimizer(x0, T=500)
amsgrad_traj = amsgrad_optimizer(x0, T=500)

# ç»˜å›¾
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
x_range = np.linspace(-2, 2, 100)
plt.plot(x_range, f(x_range), 'k-', label='f(x)', alpha=0.3)
plt.plot(adam_traj, [f(x) for x in adam_traj], 'b-', label='Adam', alpha=0.7)
plt.plot(amsgrad_traj, [f(x) for x in amsgrad_traj], 'r-', label='AMSGrad', alpha=0.7)
plt.xlabel('Iteration')
plt.ylabel('f(x)')
plt.title('ä¼˜åŒ–è½¨è¿¹å¯¹æ¯”')
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot([f(x) for x in adam_traj], label='Adam')
plt.plot([f(x) for x in amsgrad_traj], label='AMSGrad')
plt.xlabel('Iteration')
plt.ylabel('Loss')
plt.title('æ”¶æ•›é€Ÿåº¦å¯¹æ¯”')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

**å…¸å‹ç»“æœ**:

- Adam: æ›´å¿«æ”¶æ•›ï¼ˆå‰100æ­¥ï¼‰
- AMSGrad: æ›´ç¨³å®šï¼ˆåæœŸï¼‰
- ä¸¤è€…æœ€ç»ˆéƒ½æ”¶æ•›åˆ°å±€éƒ¨æœ€å°å€¼

---

### ğŸ”‘ å…³é”®è¦ç‚¹

| æ¦‚å¿µ | è¯´æ˜ |
|------|------|
| **é—æ†¾ç•Œ** | $R(T) = O(\sqrt{T})$ï¼ˆå‡¸æƒ…å†µï¼‰ |
| **æ”¶æ•›æ€§é—®é¢˜** | éå‡¸æƒ…å†µAdamå¯èƒ½ä¸æ”¶æ•› |
| **AMSGradä¿®å¤** | ä¿ç•™å†å²æœ€å¤§äºŒé˜¶çŸ© |
| **å®è·µé€‰æ‹©** | ä¸€èˆ¬ç”¨Adamï¼Œç†è®ºä¿è¯ç”¨AMSGrad |

**ç†è®ºvså®è·µ**:

- **ç†è®º**: AMSGradæœ‰æ›´å¼ºçš„æ”¶æ•›ä¿è¯
- **å®è·µ**: Adamåœ¨99%çš„æƒ…å†µä¸‹å·¥ä½œè‰¯å¥½
- **å»ºè®®**: å…ˆç”¨Adamï¼Œé‡åˆ°é—®é¢˜å†è¯•AMSGrad

---

### 2. åå·®ä¿®æ­£

**ä¸ºä»€ä¹ˆéœ€è¦åå·®ä¿®æ­£ï¼Ÿ**

åˆå§‹æ—¶ $m_0 = 0, v_0 = 0$ï¼Œå¯¼è‡´ä¼°è®¡åå‘é›¶ã€‚

**ä¿®æ­£åçš„æœŸæœ›**ï¼š

$$
\mathbb{E}[\hat{m}_t] = \mathbb{E}[g_t], \quad \mathbb{E}[\hat{v}_t] = \mathbb{E}[g_t^2]
$$

---

### ğŸ“ åå·®ä¿®æ­£çš„å®Œæ•´è¯æ˜

**å®šç† 2.1 (åå·®ä¿®æ­£çš„æ— åæ€§)**:

å‡è®¾æ¢¯åº¦ $g_t$ æ˜¯å¹³ç¨³çš„ï¼ˆå³ $\mathbb{E}[g_t] = \mu$ï¼Œ$\mathbb{E}[g_t^2] = \sigma^2$ å¯¹æ‰€æœ‰ $t$ æˆç«‹ï¼‰ã€‚åˆ™ï¼š

1. **æœªä¿®æ­£çš„ä¸€é˜¶çŸ©æœ‰å**: $\mathbb{E}[m_t] = \mu(1 - \beta_1^t)$
2. **ä¿®æ­£åçš„ä¸€é˜¶çŸ©æ— å**: $\mathbb{E}[\hat{m}_t] = \mu$
3. **æœªä¿®æ­£çš„äºŒé˜¶çŸ©æœ‰å**: $\mathbb{E}[v_t] = \sigma^2(1 - \beta_2^t)$
4. **ä¿®æ­£åçš„äºŒé˜¶çŸ©æ— å**: $\mathbb{E}[\hat{v}_t] = \sigma^2$

---

#### è¯æ˜ (1): æœªä¿®æ­£ä¸€é˜¶çŸ©çš„æœŸæœ›

**Step 1**: Adamçš„ä¸€é˜¶çŸ©æ›´æ–°è§„åˆ™ï¼š

$$
m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t
$$

**Step 2**: å±•å¼€é€’å½’ã€‚ä» $m_0 = 0$ å¼€å§‹ï¼š

$$
\begin{align}
m_1 &= \beta_1 \cdot 0 + (1 - \beta_1) g_1 = (1 - \beta_1) g_1 \\
m_2 &= \beta_1 m_1 + (1 - \beta_1) g_2 \\
&= \beta_1 (1 - \beta_1) g_1 + (1 - \beta_1) g_2 \\
&= (1 - \beta_1)(\beta_1 g_1 + g_2) \\
m_3 &= \beta_1 m_2 + (1 - \beta_1) g_3 \\
&= \beta_1 (1 - \beta_1)(\beta_1 g_1 + g_2) + (1 - \beta_1) g_3 \\
&= (1 - \beta_1)(\beta_1^2 g_1 + \beta_1 g_2 + g_3)
\end{align}
$$

**Step 3**: ä¸€èˆ¬å½¢å¼ï¼ˆå½’çº³æ³•ï¼‰ï¼š

$$
m_t = (1 - \beta_1) \sum_{i=1}^{t} \beta_1^{t-i} g_i
$$

**éªŒè¯**ï¼š

- Base case ($t=1$): $m_1 = (1 - \beta_1) g_1$ âœ…
- Inductive step: å‡è®¾å¯¹ $t$ æˆç«‹ï¼Œåˆ™ï¼š

$$
\begin{align}
m_{t+1} &= \beta_1 m_t + (1 - \beta_1) g_{t+1} \\
&= \beta_1 (1 - \beta_1) \sum_{i=1}^{t} \beta_1^{t-i} g_i + (1 - \beta_1) g_{t+1} \\
&= (1 - \beta_1) \left[\sum_{i=1}^{t} \beta_1^{t+1-i} g_i + g_{t+1}\right] \\
&= (1 - \beta_1) \sum_{i=1}^{t+1} \beta_1^{t+1-i} g_i \quad âœ…
\end{align}
$$

**Step 4**: å–æœŸæœ›ï¼ˆå‡è®¾ $\mathbb{E}[g_i] = \mu$ï¼‰ï¼š

$$
\begin{align}
\mathbb{E}[m_t] &= (1 - \beta_1) \sum_{i=1}^{t} \beta_1^{t-i} \mathbb{E}[g_i] \\
&= (1 - \beta_1) \mu \sum_{i=1}^{t} \beta_1^{t-i} \\
&= (1 - \beta_1) \mu \cdot \beta_1^{t-1} \sum_{i=1}^{t} \beta_1^{1-i} \\
&= (1 - \beta_1) \mu \cdot \beta_1^{t-1} \cdot \frac{1 - \beta_1^{-t+1}}{1 - \beta_1^{-1}}
\end{align}
$$

**Step 5**: ç®€åŒ–å‡ ä½•çº§æ•°ï¼š

$$
\sum_{i=1}^{t} \beta_1^{t-i} = \beta_1^{t-1} + \beta_1^{t-2} + \cdots + \beta_1 + 1 = \frac{1 - \beta_1^t}{1 - \beta_1}
$$

ï¼ˆå‡ ä½•çº§æ•°å…¬å¼ï¼š$\sum_{k=0}^{n-1} r^k = \frac{1-r^n}{1-r}$ï¼‰

**Step 6**: ä»£å…¥ï¼š

$$
\mathbb{E}[m_t] = (1 - \beta_1) \mu \cdot \frac{1 - \beta_1^t}{1 - \beta_1} = \mu (1 - \beta_1^t) \quad \blacksquare
$$

**å…³é”®æ´å¯Ÿ**: $\mathbb{E}[m_t] \neq \mu$ï¼ åˆå§‹åå·® $(1 - \beta_1^t)$ ä¼šéš $t$ å¢å¤§é€æ¸æ¶ˆå¤±ï¼Œä½†æ—©æœŸé˜¶æ®µåå·®æ˜¾è‘—ã€‚

---

#### è¯æ˜ (2): ä¿®æ­£åä¸€é˜¶çŸ©çš„æ— åæ€§

**å®šä¹‰**: åå·®ä¿®æ­£çš„ä¸€é˜¶çŸ©ï¼š

$$
\hat{m}_t = \frac{m_t}{1 - \beta_1^t}
$$

**Step 1**: å–æœŸæœ›ï¼š

$$
\mathbb{E}[\hat{m}_t] = \mathbb{E}\left[\frac{m_t}{1 - \beta_1^t}\right] = \frac{\mathbb{E}[m_t]}{1 - \beta_1^t}
$$

ï¼ˆå‡è®¾åå·®ä¿®æ­£å› å­ $1 - \beta_1^t$ æ˜¯ç¡®å®šçš„ï¼‰

**Step 2**: ä»£å…¥è¯æ˜(1)çš„ç»“æœï¼š

$$
\mathbb{E}[\hat{m}_t] = \frac{\mu (1 - \beta_1^t)}{1 - \beta_1^t} = \mu \quad \blacksquare
$$

**ç»“è®º**: åå·®ä¿®æ­£ä½¿å¾—ä¸€é˜¶çŸ©ä¼°è®¡å˜ä¸ºæ— åä¼°è®¡å™¨ï¼

---

#### è¯æ˜ (3): æœªä¿®æ­£äºŒé˜¶çŸ©çš„æœŸæœ›

**Step 1**: Adamçš„äºŒé˜¶çŸ©æ›´æ–°è§„åˆ™ï¼š

$$
v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2
$$

**Step 2**: ç±»ä¼¼ä¸€é˜¶çŸ©çš„æ¨å¯¼ï¼Œä» $v_0 = 0$ å±•å¼€ï¼š

$$
v_t = (1 - \beta_2) \sum_{i=1}^{t} \beta_2^{t-i} g_i^2
$$

**Step 3**: å–æœŸæœ›ï¼ˆå‡è®¾ $\mathbb{E}[g_i^2] = \sigma^2$ï¼‰ï¼š

$$
\begin{align}
\mathbb{E}[v_t] &= (1 - \beta_2) \sum_{i=1}^{t} \beta_2^{t-i} \mathbb{E}[g_i^2] \\
&= (1 - \beta_2) \sigma^2 \sum_{i=1}^{t} \beta_2^{t-i} \\
&= (1 - \beta_2) \sigma^2 \cdot \frac{1 - \beta_2^t}{1 - \beta_2} \\
&= \sigma^2 (1 - \beta_2^t) \quad \blacksquare
\end{align}
$$

---

#### è¯æ˜ (4): ä¿®æ­£åäºŒé˜¶çŸ©çš„æ— åæ€§

**å®šä¹‰**: åå·®ä¿®æ­£çš„äºŒé˜¶çŸ©ï¼š

$$
\hat{v}_t = \frac{v_t}{1 - \beta_2^t}
$$

**å–æœŸæœ›**ï¼š

$$
\mathbb{E}[\hat{v}_t] = \frac{\mathbb{E}[v_t]}{1 - \beta_2^t} = \frac{\sigma^2 (1 - \beta_2^t)}{1 - \beta_2^t} = \sigma^2 \quad \blacksquare
$$

---

### ğŸ“Š åå·®ä¿®æ­£çš„é‡è¦æ€§åˆ†æ

#### 1. åˆå§‹é˜¶æ®µçš„åå·®

**æœªä¿®æ­£æ—¶**ï¼ˆ$\beta_1 = 0.9$ï¼‰:

| $t$ | $1 - \beta_1^t$ | $\mathbb{E}[m_t]/\mu$ |
|-----|----------------|----------------------|
| 1   | 0.1            | 0.1                  |
| 2   | 0.19           | 0.19                 |
| 5   | 0.41           | 0.41                 |
| 10  | 0.65           | 0.65                 |
| 20  | 0.88           | 0.88                 |
| 100 | 0.9999...      | â‰ˆ1.0                 |

**å…³é”®è§‚å¯Ÿ**: å‰10æ­¥çš„åå·®è¶…è¿‡35%ï¼

#### 2. ä¿®æ­£æ•ˆæœå¯è§†åŒ–

```python
import numpy as np
import matplotlib.pyplot as plt

beta1 = 0.9
t = np.arange(1, 101)

# æœªä¿®æ­£çš„åå·®å› å­
bias_uncorrected = 1 - beta1**t

# ä¿®æ­£åï¼ˆåº”ä¸º1ï¼‰
bias_corrected = np.ones_like(t)

plt.figure(figsize=(10, 5))
plt.plot(t, bias_uncorrected, label='æœªä¿®æ­£: E[m_t]/Î¼')
plt.plot(t, bias_corrected, '--', label='ä¿®æ­£å: E[mÌ‚_t]/Î¼')
plt.xlabel('è¿­ä»£æ¬¡æ•° t')
plt.ylabel('æœŸæœ›å€¼ / çœŸå®å€¼')
plt.title('Adamåå·®ä¿®æ­£çš„æ•ˆæœ')
plt.legend()
plt.grid(True)
plt.show()
```

#### 3. å¯¹æ”¶æ•›æ€§çš„å½±å“

**æœªä¿®æ­£çš„åæœ**:

- **åˆæœŸå­¦ä¹ ç‡è¿‡å°**: $m_t$ è¢«ä½ä¼° â†’ æ›´æ–°æ­¥é•¿è¿‡å°
- **æ”¶æ•›é€Ÿåº¦æ…¢**: å‰å‡ æ­¥å‡ ä¹ä¸ç§»åŠ¨
- **è®­ç»ƒä¸ç¨³å®š**: åˆæœŸæ¢¯åº¦ä¿¡æ¯è¢«ä¸¥é‡æŠ‘åˆ¶

**ä¿®æ­£åçš„å¥½å¤„**:

- **å¿«é€Ÿå¯åŠ¨**: ç«‹å³ä½¿ç”¨å…¨æ¢¯åº¦ä¿¡æ¯
- **ç¨³å®šè®­ç»ƒ**: é¿å…åˆæœŸçš„"warm-up"é—®é¢˜
- **ç†è®ºä¿è¯**: æ— åä¼°è®¡å™¨æœ‰æ›´å¥½çš„æ”¶æ•›æ€§è´¨

#### 4. æ•°å­¦ç›´è§‰

**æŒ‡æ•°ç§»åŠ¨å¹³å‡çš„æœ¬è´¨**:

$$
m_t = \sum_{i=1}^{t} w_i g_i, \quad w_i = (1 - \beta_1) \beta_1^{t-i}
$$

**æƒé‡æ€»å’Œ**:

$$
\sum_{i=1}^{t} w_i = (1 - \beta_1) \sum_{i=1}^{t} \beta_1^{t-i} = 1 - \beta_1^t < 1
$$

**åå·®ä¿®æ­£ = å½’ä¸€åŒ–**:

$$
\hat{m}_t = \frac{m_t}{\sum_{i=1}^{t} w_i} = \frac{\sum w_i g_i}{\sum w_i}
$$

è¿™å°†éå½’ä¸€åŒ–çš„åŠ æƒå¹³å‡è½¬æ¢ä¸ºçœŸæ­£çš„åŠ æƒå¹³å‡ï¼

---

### ğŸ¯ å®è·µå»ºè®®1

1. **æ€»æ˜¯ä½¿ç”¨åå·®ä¿®æ­£**: é™¤éä½ æœ‰ç‰¹æ®Šç†ç”±ï¼Œå¦åˆ™ä¸è¦ç¦ç”¨åå·®ä¿®æ­£
2. **ä¸åŒè¶…å‚æ•°çš„å½±å“**:
   - $\beta_1 = 0.9$: 10æ­¥ååå·® < 35%
   - $\beta_1 = 0.99$: 100æ­¥ååå·® < 37%
   - $\beta_1$ è¶Šå¤§ï¼Œåå·®æŒç»­æ—¶é—´è¶Šé•¿
3. **warm-upçš„å…³ç³»**: åå·®ä¿®æ­£éƒ¨åˆ†æ›¿ä»£äº†å­¦ä¹ ç‡warm-upçš„éœ€æ±‚

---

## ğŸ”§ å˜ä½“ä¸æ”¹è¿›

### 1. AdamW

**æ ¸å¿ƒæ”¹è¿›**ï¼šè§£è€¦æƒé‡è¡°å‡ã€‚

**æ ‡å‡†Adam + L2æ­£åˆ™**ï¼š

$$
\theta_t = \theta_{t-1} - \alpha \left(\frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon} + \lambda \theta_{t-1}\right)
$$

**AdamW**ï¼š

$$
\theta_t = (1 - \alpha \lambda) \theta_{t-1} - \alpha \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
$$

**ä¼˜åŠ¿**ï¼šæ›´å¥½çš„æ³›åŒ–æ€§èƒ½ã€‚

---

### 2. AMSGrad

**é—®é¢˜**ï¼šAdamå¯èƒ½ä¸æ”¶æ•›ï¼ˆæŒ‡æ•°ç§»åŠ¨å¹³å‡å¯èƒ½"å¿˜è®°"å†å²ä¿¡æ¯ï¼‰ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼šä¿ç•™å†å²æœ€å¤§äºŒé˜¶çŸ©ã€‚

$$
\begin{align}
\hat{v}_t &= \max(\hat{v}_{t-1}, v_t) \\
\theta_t &= \theta_{t-1} - \frac{\alpha}{\sqrt{\hat{v}_t} + \epsilon} m_t
\end{align}
$$

**ä¿è¯**ï¼šéå‡¸æƒ…å†µä¸‹çš„æ”¶æ•›æ€§ã€‚

---

## ğŸ’» Pythonå®ç°

```python
import numpy as np

class AdamOptimizer:
    """Adamä¼˜åŒ–å™¨å®ç°"""
    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):
        self.lr = lr
        self.beta1 = beta1
        self.beta2 = beta2
        self.epsilon = epsilon

        self.m = None  # ä¸€é˜¶çŸ©
        self.v = None  # äºŒé˜¶çŸ©
        self.t = 0     # æ—¶é—´æ­¥

    def update(self, params, grads):
        """
        æ›´æ–°å‚æ•°

        Args:
            params: å‚æ•°å­—å…¸ {name: value}
            grads: æ¢¯åº¦å­—å…¸ {name: gradient}
        """
        if self.m is None:
            self.m = {k: np.zeros_like(v) for k, v in params.items()}
            self.v = {k: np.zeros_like(v) for k, v in params.items()}

        self.t += 1

        for key in params:
            # æ›´æ–°ä¸€é˜¶çŸ©
            self.m[key] = self.beta1 * self.m[key] + (1 - self.beta1) * grads[key]

            # æ›´æ–°äºŒé˜¶çŸ©
            self.v[key] = self.beta2 * self.v[key] + (1 - self.beta2) * grads[key]**2

            # åå·®ä¿®æ­£
            m_hat = self.m[key] / (1 - self.beta1**self.t)
            v_hat = self.v[key] / (1 - self.beta2**self.t)

            # æ›´æ–°å‚æ•°
            params[key] -= self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)

        return params


class AdamWOptimizer:
    """AdamWä¼˜åŒ–å™¨ï¼ˆè§£è€¦æƒé‡è¡°å‡ï¼‰"""
    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, weight_decay=0.01):
        self.lr = lr
        self.beta1 = beta1
        self.beta2 = beta2
        self.epsilon = epsilon
        self.weight_decay = weight_decay

        self.m = None
        self.v = None
        self.t = 0

    def update(self, params, grads):
        """æ›´æ–°å‚æ•°ï¼ˆAdamWï¼‰"""
        if self.m is None:
            self.m = {k: np.zeros_like(v) for k, v in params.items()}
            self.v = {k: np.zeros_like(v) for k, v in params.items()}

        self.t += 1

        for key in params:
            # æ›´æ–°çŸ©ä¼°è®¡
            self.m[key] = self.beta1 * self.m[key] + (1 - self.beta1) * grads[key]
            self.v[key] = self.beta2 * self.v[key] + (1 - self.beta2) * grads[key]**2

            # åå·®ä¿®æ­£
            m_hat = self.m[key] / (1 - self.beta1**self.t)
            v_hat = self.v[key] / (1 - self.beta2**self.t)

            # AdamWæ›´æ–°ï¼ˆè§£è€¦æƒé‡è¡°å‡ï¼‰
            params[key] = (1 - self.lr * self.weight_decay) * params[key] - \
                          self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)

        return params


# ç¤ºä¾‹ï¼šä¼˜åŒ–Rosenbrockå‡½æ•°
def rosenbrock(x):
    """Rosenbrockå‡½æ•°"""
    return (1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2

def rosenbrock_grad(x):
    """Rosenbrockæ¢¯åº¦"""
    dx0 = -2 * (1 - x[0]) - 400 * x[0] * (x[1] - x[0]**2)
    dx1 = 200 * (x[1] - x[0]**2)
    return np.array([dx0, dx1])

# ä¼˜åŒ–
params = {'x': np.array([-1.0, 1.0])}
optimizer = AdamOptimizer(lr=0.01)

for i in range(1000):
    grads = {'x': rosenbrock_grad(params['x'])}
    params = optimizer.update(params, grads)

    if i % 100 == 0:
        loss = rosenbrock(params['x'])
        print(f"Iteration {i}, Loss: {loss:.6f}, x: {params['x']}")
```

---

## ğŸ”§ å®é™…åº”ç”¨æ¡ˆä¾‹

### 1. è‡ªç„¶è¯­è¨€å¤„ç†

**Transformerè®­ç»ƒ**:

Adamæ˜¯è®­ç»ƒTransformeræ¨¡å‹ï¼ˆBERTã€GPTç­‰ï¼‰çš„æ ‡å‡†ä¼˜åŒ–å™¨ã€‚

**é…ç½®**:
- å­¦ä¹ ç‡: $10^{-4}$ åˆ° $10^{-3}$
- $\beta_1 = 0.9$, $\beta_2 = 0.999$
- æƒé‡è¡°å‡: $0.01$ (AdamW)
- Warmup: å‰10%æ­¥æ•°çº¿æ€§å¢åŠ å­¦ä¹ ç‡

**ä¼˜åŠ¿**:
- è‡ªé€‚åº”å­¦ä¹ ç‡é€‚åº”ä¸åŒå±‚
- å¿«é€Ÿæ”¶æ•›
- å¯¹è¶…å‚æ•°ä¸æ•æ„Ÿ

**å®è·µç¤ºä¾‹**:

```python
import torch
import torch.nn as nn
from transformers import AdamW, get_linear_schedule_with_warmup

# BERTæ¨¡å‹
model = BertModel.from_pretrained('bert-base-uncased')

# AdamWä¼˜åŒ–å™¨
optimizer = AdamW(
    model.parameters(),
    lr=2e-5,
    betas=(0.9, 0.999),
    weight_decay=0.01
)

# å­¦ä¹ ç‡è°ƒåº¦ï¼ˆWarmupï¼‰
num_training_steps = 10000
num_warmup_steps = 1000
scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=num_warmup_steps,
    num_training_steps=num_training_steps
)

# è®­ç»ƒå¾ªç¯
for step, batch in enumerate(train_dataloader):
    loss = model(**batch).loss
    loss.backward()

    # æ¢¯åº¦è£å‰ª
    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

    optimizer.step()
    scheduler.step()
    optimizer.zero_grad()
```

---

### 2. è®¡ç®—æœºè§†è§‰

**ResNetè®­ç»ƒ**:

Adamåœ¨ImageNetä¸Šè®­ç»ƒResNetæ—¶è¡¨ç°ä¼˜å¼‚ã€‚

**é…ç½®**:
- åˆå§‹å­¦ä¹ ç‡: $10^{-3}$
- æ‰¹é‡å¤§å°: 256
- å­¦ä¹ ç‡è¡°å‡: æ¯30ä¸ªepochä¹˜ä»¥0.1

**æ€§èƒ½å¯¹æ¯”**:

| ä¼˜åŒ–å™¨ | Top-1å‡†ç¡®ç‡ | è®­ç»ƒæ—¶é—´ |
|--------|------------|---------|
| SGD + Momentum | 76.5% | åŸºå‡† |
| Adam | 76.8% | -10% |
| AdamW | 77.1% | -10% |

---

### 3. ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ (GAN)

**GANè®­ç»ƒæŒ‘æˆ˜**:

GANè®­ç»ƒéœ€è¦å¹³è¡¡ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨ï¼ŒAdamçš„è‡ªé€‚åº”ç‰¹æ€§æœ‰åŠ©äºç¨³å®šè®­ç»ƒã€‚

**é…ç½®**:
- ç”Ÿæˆå™¨: Adam, $lr=2 \times 10^{-4}$, $\beta_1=0.5$
- åˆ¤åˆ«å™¨: Adam, $lr=2 \times 10^{-4}$, $\beta_1=0.5$

**ä¸ºä»€ä¹ˆ$\beta_1=0.5$?**:
- å‡å°‘åŠ¨é‡ï¼Œé¿å…è¿‡åº¦æ›´æ–°
- æé«˜è®­ç»ƒç¨³å®šæ€§
- é˜²æ­¢æ¨¡å¼å´©å¡Œ

**å®è·µç¤ºä¾‹**:

```python
# GANè®­ç»ƒ
generator = Generator()
discriminator = Discriminator()

# ä½¿ç”¨è¾ƒå°çš„beta1æé«˜ç¨³å®šæ€§
optimizer_G = torch.optim.Adam(
    generator.parameters(),
    lr=2e-4,
    betas=(0.5, 0.999)  # beta1=0.5
)

optimizer_D = torch.optim.Adam(
    discriminator.parameters(),
    lr=2e-4,
    betas=(0.5, 0.999)
)

# è®­ç»ƒå¾ªç¯
for epoch in range(num_epochs):
    for real_images, _ in dataloader:
        # è®­ç»ƒåˆ¤åˆ«å™¨
        optimizer_D.zero_grad()
        d_loss = train_discriminator(real_images, generator)
        d_loss.backward()
        optimizer_D.step()

        # è®­ç»ƒç”Ÿæˆå™¨
        optimizer_G.zero_grad()
        g_loss = train_generator(discriminator)
        g_loss.backward()
        optimizer_G.step()
```

---

### 4. å¼ºåŒ–å­¦ä¹ 

**ç­–ç•¥æ¢¯åº¦æ–¹æ³•**:

Adamåœ¨REINFORCEã€Actor-Criticç­‰ç­–ç•¥æ¢¯åº¦æ–¹æ³•ä¸­å¹¿æ³›åº”ç”¨ã€‚

**ä¼˜åŠ¿**:
- é€‚åº”ä¸åŒå‚æ•°çš„å­¦ä¹ é€Ÿåº¦
- å¤„ç†éå¹³ç¨³ç›®æ ‡
- å¿«é€Ÿæ”¶æ•›

**é…ç½®**:
- å­¦ä¹ ç‡: $3 \times 10^{-4}$
- $\beta_1 = 0.9$, $\beta_2 = 0.999$
- é€šå¸¸ä¸éœ€è¦æƒé‡è¡°å‡

**åº”ç”¨åœºæ™¯**:
- PPO (Proximal Policy Optimization)
- A3C (Asynchronous Advantage Actor-Critic)
- SAC (Soft Actor-Critic)

---

### 5. æ¨èç³»ç»Ÿ

**çŸ©é˜µåˆ†è§£**:

Adamç”¨äºä¼˜åŒ–ç”¨æˆ·-ç‰©å“çŸ©é˜µåˆ†è§£ã€‚

**é—®é¢˜**:
$$
\min_{U,V} \sum_{(i,j) \in \Omega} (R_{ij} - U_i V_j^T)^2 + \lambda(\|U\|_F^2 + \|V\|_F^2)
$$

**Adamä¼˜åŠ¿**:
- å¤„ç†ç¨€ç–æ¢¯åº¦ï¼ˆåªæœ‰è§‚æµ‹åˆ°çš„$(i,j)$æœ‰æ¢¯åº¦ï¼‰
- è‡ªé€‚åº”å­¦ä¹ ç‡é€‚åº”ä¸åŒç”¨æˆ·/ç‰©å“çš„æ›´æ–°é¢‘ç‡
- å¿«é€Ÿæ”¶æ•›

**å®è·µç¤ºä¾‹**:

```python
class MatrixFactorization(nn.Module):
    def __init__(self, n_users, n_items, n_factors=50):
        super().__init__()
        self.user_emb = nn.Embedding(n_users, n_factors)
        self.item_emb = nn.Embedding(n_items, n_factors)

    def forward(self, user_ids, item_ids):
        user_vec = self.user_emb(user_ids)
        item_vec = self.item_emb(item_ids)
        return (user_vec * item_vec).sum(dim=1)

model = MatrixFactorization(n_users, n_items)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

for user_id, item_id, rating in train_data:
    optimizer.zero_grad()
    pred = model(user_id, item_id)
    loss = F.mse_loss(pred, rating)
    loss.backward()
    optimizer.step()
```

---

### 6. è¶…å‚æ•°ä¼˜åŒ–

**Adamä½œä¸ºå…ƒä¼˜åŒ–å™¨**:

ä½¿ç”¨Adamä¼˜åŒ–è¶…å‚æ•°ï¼ˆå¦‚å­¦ä¹ ç‡ã€æ­£åˆ™åŒ–ç³»æ•°ï¼‰ã€‚

**åŒå±‚ä¼˜åŒ–**:
$$
\min_\lambda \mathcal{L}_{\text{val}}(\theta^*(\lambda)) \quad \text{s.t.} \quad \theta^*(\lambda) = \arg\min_\theta \mathcal{L}_{\text{train}}(\theta, \lambda)
$$

**ä½¿ç”¨Adamä¼˜åŒ–$\lambda$**:
- è®¡ç®—è¶…å‚æ•°æ¢¯åº¦
- ä½¿ç”¨Adamæ›´æ–°è¶…å‚æ•°
- æ¯”ç½‘æ ¼æœç´¢æ›´é«˜æ•ˆ

---

### 7. è¿ç§»å­¦ä¹ 

**Fine-tuningé¢„è®­ç»ƒæ¨¡å‹**:

Adamåœ¨è¿ç§»å­¦ä¹ ä¸­å¹¿æ³›ä½¿ç”¨ï¼Œç‰¹åˆ«æ˜¯fine-tuningå¤§å‹é¢„è®­ç»ƒæ¨¡å‹ã€‚

**ç­–ç•¥**:
- **å…¨æ¨¡å‹å¾®è°ƒ**: æ‰€æœ‰å±‚ä½¿ç”¨Adamï¼Œå­¦ä¹ ç‡ $10^{-5}$ åˆ° $10^{-3}$
- **éƒ¨åˆ†å¾®è°ƒ**: åªè®­ç»ƒé¡¶å±‚ï¼Œå­¦ä¹ ç‡ $10^{-3}$ åˆ° $10^{-2}$
- **LoRAå¾®è°ƒ**: ä½ç§©é€‚åº”ï¼ŒAdamä¼˜åŒ–ä½ç§©çŸ©é˜µ

**å®è·µå»ºè®®**:
- ä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡ï¼ˆé¢„è®­ç»ƒæ¨¡å‹çš„1/10ï¼‰
- ä½¿ç”¨AdamWé¿å…æƒé‡è¡°å‡é—®é¢˜
- ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦ï¼ˆCosine Annealingï¼‰

---

### 8. å¯¹æ¯”å­¦ä¹ 

**è‡ªç›‘ç£å­¦ä¹ **:

Adamåœ¨å¯¹æ¯”å­¦ä¹ ï¼ˆSimCLRã€MoCoç­‰ï¼‰ä¸­è¡¨ç°ä¼˜å¼‚ã€‚

**ç‰¹ç‚¹**:
- å¤§æ‰¹é‡è®­ç»ƒï¼ˆ4096+ï¼‰
- éœ€è¦ç¨³å®šçš„ä¼˜åŒ–å™¨
- Adamçš„è‡ªé€‚åº”æ€§æœ‰åŠ©äºå¤„ç†ä¸åŒæ ·æœ¬çš„æ¢¯åº¦

**é…ç½®**:
- å­¦ä¹ ç‡: $0.0003 \times \text{batch_size} / 256$ (çº¿æ€§ç¼©æ”¾)
- Warmup: 10ä¸ªepoch
- Cosine Annealing

---

## ğŸ“š æ ¸å¿ƒè¦ç‚¹

| æ¦‚å¿µ | è¯´æ˜ |
|------|------|
| **ä¸€é˜¶çŸ©** | $m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t$ |
| **äºŒé˜¶çŸ©** | $v_t = \beta_2 v_{t-1} + (1-\beta_2) g_t^2$ |
| **åå·®ä¿®æ­£** | $\hat{m}_t = m_t / (1 - \beta_1^t)$ |
| **è‡ªé€‚åº”å­¦ä¹ ç‡** | $\alpha / \sqrt{\hat{v}_t}$ |

---

## ğŸ“ ç›¸å…³è¯¾ç¨‹

| å¤§å­¦ | è¯¾ç¨‹ |
|------|------|
| **Stanford** | CS231n Deep Learning |
| **MIT** | 6.036 Introduction to ML |
| **CMU** | 10-725 Convex Optimization |

---

## ğŸ“– å‚è€ƒæ–‡çŒ®

1. **Kingma & Ba (2015)**. "Adam: A Method for Stochastic Optimization". *ICLR*.

2. **Loshchilov & Hutter (2019)**. "Decoupled Weight Decay Regularization". *ICLR*.

3. **Reddi et al. (2018)**. "On the Convergence of Adam and Beyond". *ICLR*.

---

*æœ€åæ›´æ–°ï¼š2025å¹´12æœˆ20æ—¥*-
