# å‡¸ä¼˜åŒ–è¿›é˜¶ (Advanced Convex Optimization)

> **The Foundation of Efficient Machine Learning Algorithms**
>
> é«˜æ•ˆæœºå™¨å­¦ä¹ ç®—æ³•çš„ç†è®ºåŸºç¡€

---

## ç›®å½•

- [å‡¸ä¼˜åŒ–è¿›é˜¶ (Advanced Convex Optimization)](#å‡¸ä¼˜åŒ–è¿›é˜¶-advanced-convex-optimization)
  - [ç›®å½•](#ç›®å½•)
  - [ğŸ“‹ æ ¸å¿ƒæ€æƒ³](#-æ ¸å¿ƒæ€æƒ³)
  - [ğŸ¯ å‡¸é›†ä¸å‡¸å‡½æ•°](#-å‡¸é›†ä¸å‡¸å‡½æ•°)
    - [1. å‡¸é›†](#1-å‡¸é›†)
    - [2. å‡¸å‡½æ•°](#2-å‡¸å‡½æ•°)
    - [3. å¼ºå‡¸æ€§](#3-å¼ºå‡¸æ€§)
  - [ğŸ“Š å‡¸ä¼˜åŒ–é—®é¢˜](#-å‡¸ä¼˜åŒ–é—®é¢˜)
    - [1. æ ‡å‡†å½¢å¼](#1-æ ‡å‡†å½¢å¼)
    - [2. æœ€ä¼˜æ€§æ¡ä»¶](#2-æœ€ä¼˜æ€§æ¡ä»¶)
    - [3. å¯¹å¶ç†è®º](#3-å¯¹å¶ç†è®º)
  - [ğŸ”¬ å‡¸ä¼˜åŒ–ç®—æ³•](#-å‡¸ä¼˜åŒ–ç®—æ³•)
    - [1. æ¢¯åº¦æŠ•å½±æ³•](#1-æ¢¯åº¦æŠ•å½±æ³•)
    - [2. è¿‘ç«¯æ¢¯åº¦æ³•](#2-è¿‘ç«¯æ¢¯åº¦æ³•)
    - [3. åŠ é€Ÿæ¢¯åº¦æ³•](#3-åŠ é€Ÿæ¢¯åº¦æ³•)
    - [4. ADMMç®—æ³•](#4-admmç®—æ³•)
  - [ğŸ’¡ æ”¶æ•›æ€§åˆ†æ](#-æ”¶æ•›æ€§åˆ†æ)
    - [1. æ¢¯åº¦ä¸‹é™æ”¶æ•›ç‡](#1-æ¢¯åº¦ä¸‹é™æ”¶æ•›ç‡)
    - [2. NesterovåŠ é€Ÿ](#2-nesterovåŠ é€Ÿ)
    - [3. å¼ºå‡¸æƒ…å†µ](#3-å¼ºå‡¸æƒ…å†µ)
  - [ğŸ¨ åœ¨æœºå™¨å­¦ä¹ ä¸­çš„åº”ç”¨](#-åœ¨æœºå™¨å­¦ä¹ ä¸­çš„åº”ç”¨)
    - [1. æ”¯æŒå‘é‡æœº (SVM)](#1-æ”¯æŒå‘é‡æœº-svm)
    - [2. Lassoå›å½’](#2-lassoå›å½’)
    - [3. é€»è¾‘å›å½’](#3-é€»è¾‘å›å½’)
  - [ğŸ’» Pythonå®ç°](#-pythonå®ç°)
  - [ğŸ“š ç»ƒä¹ é¢˜](#-ç»ƒä¹ é¢˜)
    - [ç»ƒä¹ 1ï¼šå‡¸æ€§åˆ¤å®š](#ç»ƒä¹ 1å‡¸æ€§åˆ¤å®š)
    - [ç»ƒä¹ 2ï¼šå¯¹å¶é—®é¢˜](#ç»ƒä¹ 2å¯¹å¶é—®é¢˜)
    - [ç»ƒä¹ 3ï¼šè¿‘ç«¯ç®—å­](#ç»ƒä¹ 3è¿‘ç«¯ç®—å­)
    - [ç»ƒä¹ 4ï¼šADMMåº”ç”¨](#ç»ƒä¹ 4admmåº”ç”¨)
  - [ğŸ“ ç›¸å…³è¯¾ç¨‹](#-ç›¸å…³è¯¾ç¨‹)
  - [ğŸ“– å‚è€ƒæ–‡çŒ®](#-å‚è€ƒæ–‡çŒ®)

---

## ğŸ“‹ æ ¸å¿ƒæ€æƒ³

**å‡¸ä¼˜åŒ–**æ˜¯æœºå™¨å­¦ä¹ ä¸­æœ€é‡è¦çš„ä¼˜åŒ–å·¥å…·ï¼Œå› ä¸ºå‡¸é—®é¢˜æœ‰å…¨å±€æœ€ä¼˜è§£ä¸”å¯ä»¥é«˜æ•ˆæ±‚è§£ã€‚

**ä¸ºä»€ä¹ˆå‡¸ä¼˜åŒ–é‡è¦**:

```text
å‡¸ä¼˜åŒ–çš„ä¼˜åŠ¿:
â”œâ”€ å±€éƒ¨æœ€ä¼˜ = å…¨å±€æœ€ä¼˜
â”œâ”€ é«˜æ•ˆç®—æ³• (å¤šé¡¹å¼æ—¶é—´)
â”œâ”€ ç†è®ºä¿è¯ (æ”¶æ•›æ€§ã€å¤æ‚åº¦)
â””â”€ å¹¿æ³›åº”ç”¨ (SVM, Lasso, é€»è¾‘å›å½’)

æœºå™¨å­¦ä¹ ä¸­çš„å‡¸é—®é¢˜:
â”œâ”€ çº¿æ€§å›å½’ (æœ€å°äºŒä¹˜)
â”œâ”€ é€»è¾‘å›å½’ (å‡¸æŸå¤±)
â”œâ”€ SVM (å‡¸äºŒæ¬¡è§„åˆ’)
â””â”€ Lasso (å‡¸æ­£åˆ™åŒ–)
```

---

## ğŸ¯ å‡¸é›†ä¸å‡¸å‡½æ•°

### 1. å‡¸é›†

**å®šä¹‰ 1.1 (å‡¸é›†)**:

é›†åˆ $C \subseteq \mathbb{R}^n$ æ˜¯å‡¸é›†ï¼Œå¦‚æœå¯¹äºä»»æ„ $x, y \in C$ å’Œ $\theta \in [0, 1]$ï¼š

$$
\theta x + (1 - \theta) y \in C
$$

**å‡ ä½•æ„ä¹‰**ï¼šè¿æ¥é›†åˆä¸­ä»»æ„ä¸¤ç‚¹çš„çº¿æ®µä»åœ¨é›†åˆå†…ã€‚

**ç¤ºä¾‹**:

- âœ… **å‡¸é›†**: è¶…å¹³é¢ã€åŠç©ºé—´ã€çƒã€æ¤­çƒã€å¤šé¢ä½“
- âŒ **éå‡¸é›†**: æœˆç‰™å½¢ã€ç¯å½¢

**å®šç† 1.1 (å‡¸é›†çš„ä¿æŒæ€§)**:

- å‡¸é›†çš„äº¤é›†ä»æ˜¯å‡¸é›†
- å‡¸é›†çš„ä»¿å°„å˜æ¢ä»æ˜¯å‡¸é›†
- å‡¸é›†çš„ç¬›å¡å°”ç§¯ä»æ˜¯å‡¸é›†

---

### 2. å‡¸å‡½æ•°

**å®šä¹‰ 2.1 (å‡¸å‡½æ•°)**:

å‡½æ•° $f: \mathbb{R}^n \to \mathbb{R}$ æ˜¯å‡¸å‡½æ•°ï¼Œå¦‚æœå…¶å®šä¹‰åŸŸ $\text{dom}(f)$ æ˜¯å‡¸é›†ï¼Œä¸”å¯¹äºä»»æ„ $x, y \in \text{dom}(f)$ å’Œ $\theta \in [0, 1]$ï¼š

$$
f(\theta x + (1 - \theta) y) \leq \theta f(x) + (1 - \theta) f(y)
$$

**å‡ ä½•æ„ä¹‰**ï¼šå‡½æ•°å›¾åƒä¸Šä»»æ„ä¸¤ç‚¹ä¹‹é—´çš„å¼¦ä½äºå‡½æ•°å›¾åƒä¸Šæ–¹ã€‚

**ä¸€é˜¶æ¡ä»¶** (å¯å¾®æƒ…å†µ):

$f$ æ˜¯å‡¸å‡½æ•°å½“ä¸”ä»…å½“ï¼š

$$
f(y) \geq f(x) + \nabla f(x)^T (y - x) \quad \forall x, y
$$

**äºŒé˜¶æ¡ä»¶** (äºŒé˜¶å¯å¾®æƒ…å†µ):

$f$ æ˜¯å‡¸å‡½æ•°å½“ä¸”ä»…å½“å…¶HessiançŸ©é˜µåŠæ­£å®šï¼š

$$
\nabla^2 f(x) \succeq 0 \quad \forall x
$$

**ç¤ºä¾‹**:

- âœ… **å‡¸å‡½æ•°**: $\|x\|_2$, $\|x\|_1$, $e^x$, $x^2$, $-\log x$ (x > 0)
- âŒ **éå‡¸å‡½æ•°**: $\sin x$, $x^3$, $\log(1 + e^x)$ (è™½ç„¶æ˜¯å‡¸çš„)

---

### 3. å¼ºå‡¸æ€§

**å®šä¹‰ 3.1 (å¼ºå‡¸å‡½æ•°)**:

å‡½æ•° $f$ æ˜¯ $\mu$-å¼ºå‡¸çš„ï¼Œå¦‚æœå¯¹äºä»»æ„ $x, y$ï¼š

$$
f(y) \geq f(x) + \nabla f(x)^T (y - x) + \frac{\mu}{2} \|y - x\|^2
$$

**ç­‰ä»·æ¡ä»¶**:

$$
\nabla^2 f(x) \succeq \mu I \quad \forall x
$$

**æ„ä¹‰**ï¼šå¼ºå‡¸å‡½æ•°æœ‰æ›´å¥½çš„æ”¶æ•›æ€§è´¨ï¼ˆçº¿æ€§æ”¶æ•›ï¼‰ã€‚

**ç¤ºä¾‹**:

- $f(x) = \frac{1}{2} x^T A x$ æ˜¯ $\lambda_{\min}(A)$-å¼ºå‡¸çš„ï¼ˆå½“ $A \succ 0$ï¼‰
- $f(x) = \|x\|^2$ æ˜¯ 2-å¼ºå‡¸çš„

---

## ğŸ“Š å‡¸ä¼˜åŒ–é—®é¢˜

### 1. æ ‡å‡†å½¢å¼

**å®šä¹‰ 1.1 (å‡¸ä¼˜åŒ–é—®é¢˜)**:

$$
\begin{align}
\min_{x} \quad & f(x) \\
\text{s.t.} \quad & g_i(x) \leq 0, \quad i = 1, \ldots, m \\
& h_j(x) = 0, \quad j = 1, \ldots, p
\end{align}
$$

å…¶ä¸­ $f, g_i$ æ˜¯å‡¸å‡½æ•°ï¼Œ$h_j$ æ˜¯ä»¿å°„å‡½æ•°ã€‚

**ç‰¹æ®Šæƒ…å†µ**:

```text
çº¿æ€§è§„åˆ’ (LP):
    f, g_i, h_j éƒ½æ˜¯ä»¿å°„å‡½æ•°

äºŒæ¬¡è§„åˆ’ (QP):
    f æ˜¯äºŒæ¬¡å‡½æ•°ï¼Œg_i, h_j æ˜¯ä»¿å°„å‡½æ•°

äºŒæ¬¡çº¦æŸäºŒæ¬¡è§„åˆ’ (QCQP):
    f, g_i æ˜¯äºŒæ¬¡å‡½æ•°ï¼Œh_j æ˜¯ä»¿å°„å‡½æ•°
```

---

### 2. æœ€ä¼˜æ€§æ¡ä»¶

**å®šç† 2.1 (KKTæ¡ä»¶)**:

å¯¹äºå‡¸ä¼˜åŒ–é—®é¢˜ï¼Œç‚¹ $x^*$ æ˜¯æœ€ä¼˜è§£å½“ä¸”ä»…å½“å­˜åœ¨ $\lambda^* \geq 0, \nu^*$ ä½¿å¾—ï¼š

1. **å¹³ç¨³æ€§**: $\nabla f(x^*) + \sum_i \lambda_i^* \nabla g_i(x^*) + \sum_j \nu_j^* \nabla h_j(x^*) = 0$
2. **åŸå§‹å¯è¡Œæ€§**: $g_i(x^*) \leq 0$, $h_j(x^*) = 0$
3. **å¯¹å¶å¯è¡Œæ€§**: $\lambda_i^* \geq 0$
4. **äº’è¡¥æ¾å¼›æ€§**: $\lambda_i^* g_i(x^*) = 0$

**æ— çº¦æŸæƒ…å†µ**:

$$
\nabla f(x^*) = 0
$$

---

### 3. å¯¹å¶ç†è®º

**æ‹‰æ ¼æœ—æ—¥å‡½æ•°**:

$$
L(x, \lambda, \nu) = f(x) + \sum_i \lambda_i g_i(x) + \sum_j \nu_j h_j(x)
$$

**å¯¹å¶å‡½æ•°**:

$$
g(\lambda, \nu) = \inf_x L(x, \lambda, \nu)
$$

**å¯¹å¶é—®é¢˜**:

$$
\begin{align}
\max_{\lambda, \nu} \quad & g(\lambda, \nu) \\
\text{s.t.} \quad & \lambda \geq 0
\end{align}
$$

**å®šç† 3.1 (å¼±å¯¹å¶æ€§)**:

$$
g(\lambda, \nu) \leq f(x^*) \quad \forall \lambda \geq 0, \nu
$$

**å®šç† 3.2 (å¼ºå¯¹å¶æ€§)**:

å¯¹äºå‡¸ä¼˜åŒ–é—®é¢˜ï¼Œå¦‚æœSlateræ¡ä»¶æˆç«‹ï¼Œåˆ™å¼ºå¯¹å¶æ€§æˆç«‹ï¼š

$$
g(\lambda^*, \nu^*) = f(x^*)
$$

---

## ğŸ”¬ å‡¸ä¼˜åŒ–ç®—æ³•

### 1. æ¢¯åº¦æŠ•å½±æ³•

**é—®é¢˜**:

$$
\min_{x \in C} f(x)
$$

å…¶ä¸­ $C$ æ˜¯å‡¸é›†ã€‚

**ç®—æ³•**:

$$
x_{t+1} = \Pi_C(x_t - \eta \nabla f(x_t))
$$

å…¶ä¸­ $\Pi_C$ æ˜¯æŠ•å½±ç®—å­ï¼š

$$
\Pi_C(y) = \arg\min_{x \in C} \|x - y\|^2
$$

**æ”¶æ•›æ€§**:

- å‡¸å‡½æ•°ï¼š$O(1/t)$
- å¼ºå‡¸å‡½æ•°ï¼š$O(e^{-\mu \eta t})$

---

### 2. è¿‘ç«¯æ¢¯åº¦æ³•

**é—®é¢˜**:

$$
\min_x f(x) + g(x)
$$

å…¶ä¸­ $f$ å…‰æ»‘ï¼Œ$g$ å¯èƒ½ä¸å…‰æ»‘ä½†æœ‰ç®€å•çš„è¿‘ç«¯ç®—å­ã€‚

**è¿‘ç«¯ç®—å­**:

$$
\text{prox}_{\eta g}(y) = \arg\min_x \left\{ g(x) + \frac{1}{2\eta} \|x - y\|^2 \right\}
$$

**ç®—æ³•**:

$$
x_{t+1} = \text{prox}_{\eta g}(x_t - \eta \nabla f(x_t))
$$

**ç¤ºä¾‹** ($\ell_1$ æ­£åˆ™åŒ–):

$$
\text{prox}_{\eta \lambda \|\cdot\|_1}(y) = \text{sign}(y) \odot \max(|y| - \eta \lambda, 0)
$$

è¿™å°±æ˜¯**è½¯é˜ˆå€¼ç®—å­** (Soft-thresholding)ã€‚

---

### 3. åŠ é€Ÿæ¢¯åº¦æ³•

**NesterovåŠ é€Ÿæ¢¯åº¦æ³•**:

$$
\begin{align}
y_t &= x_t + \frac{t - 1}{t + 2} (x_t - x_{t-1}) \\
x_{t+1} &= y_t - \eta \nabla f(y_t)
\end{align}
$$

**æ”¶æ•›ç‡**:

- æ ‡å‡†æ¢¯åº¦ä¸‹é™ï¼š$O(1/t)$
- NesterovåŠ é€Ÿï¼š$O(1/t^2)$ âœ…

**ç›´è§‰**ï¼šä½¿ç”¨åŠ¨é‡é¡¹åŠ é€Ÿæ”¶æ•›ã€‚

---

### 4. ADMMç®—æ³•

**é—®é¢˜** (å¯åˆ†ç¦»å½¢å¼):

$$
\min_{x, z} f(x) + g(z) \quad \text{s.t.} \quad Ax + Bz = c
$$

**å¢å¹¿æ‹‰æ ¼æœ—æ—¥å‡½æ•°**:

$$
L_\rho(x, z, y) = f(x) + g(z) + y^T(Ax + Bz - c) + \frac{\rho}{2} \|Ax + Bz - c\|^2
$$

**ADMMè¿­ä»£**:

$$
\begin{align}
x_{t+1} &= \arg\min_x L_\rho(x, z_t, y_t) \\
z_{t+1} &= \arg\min_z L_\rho(x_{t+1}, z, y_t) \\
y_{t+1} &= y_t + \rho (Ax_{t+1} + Bz_{t+1} - c)
\end{align}
$$

**ä¼˜åŠ¿**:

- å¯å¤„ç†å¤§è§„æ¨¡é—®é¢˜
- å¯å¹¶è¡ŒåŒ–
- æ”¶æ•›æ€§å¥½

---

## ğŸ’¡ æ”¶æ•›æ€§åˆ†æ

### 1. æ¢¯åº¦ä¸‹é™æ”¶æ•›ç‡

**å®šç† 1.1 (å‡¸å‡½æ•°)**:

å‡è®¾ $f$ æ˜¯ $L$-å…‰æ»‘çš„å‡¸å‡½æ•°ï¼Œä½¿ç”¨å›ºå®šæ­¥é•¿ $\eta = 1/L$ï¼š

$$
f(x_t) - f^* \leq \frac{L \|x_0 - x^*\|^2}{2t}
$$

**æ”¶æ•›ç‡**: $O(1/t)$

---

### 2. NesterovåŠ é€Ÿ

**å®šç† 2.1 (NesterovåŠ é€Ÿ)**:

ä½¿ç”¨NesterovåŠ é€Ÿæ¢¯åº¦æ³•ï¼š

$$
f(x_t) - f^* \leq \frac{2L \|x_0 - x^*\|^2}{(t+1)^2}
$$

**æ”¶æ•›ç‡**: $O(1/t^2)$ âœ… æ¯”æ ‡å‡†æ¢¯åº¦ä¸‹é™å¿«ï¼

---

### 3. å¼ºå‡¸æƒ…å†µ

**å®šç† 3.1 (å¼ºå‡¸å‡½æ•°)**:

å‡è®¾ $f$ æ˜¯ $\mu$-å¼ºå‡¸ä¸” $L$-å…‰æ»‘çš„ï¼Œä½¿ç”¨å›ºå®šæ­¥é•¿ $\eta = 1/L$ï¼š

$$
\|x_t - x^*\|^2 \leq \left(1 - \frac{\mu}{L}\right)^t \|x_0 - x^*\|^2
$$

**æ”¶æ•›ç‡**: $O(e^{-\mu t / L})$ (çº¿æ€§æ”¶æ•›)

**æ¡ä»¶æ•°**:

$$
\kappa = \frac{L}{\mu}
$$

æ¡ä»¶æ•°è¶Šå°ï¼Œæ”¶æ•›è¶Šå¿«ã€‚

---

## ğŸ¨ åœ¨æœºå™¨å­¦ä¹ ä¸­çš„åº”ç”¨

### 1. æ”¯æŒå‘é‡æœº (SVM)

**åŸå§‹é—®é¢˜**:

$$
\min_{w, b} \frac{1}{2} \|w\|^2 \quad \text{s.t.} \quad y_i(w^T x_i + b) \geq 1
$$

**å¯¹å¶é—®é¢˜**:

$$
\max_\alpha \sum_i \alpha_i - \frac{1}{2} \sum_{i,j} \alpha_i \alpha_j y_i y_j x_i^T x_j \quad \text{s.t.} \quad \alpha_i \geq 0, \sum_i \alpha_i y_i = 0
$$

**å‡¸äºŒæ¬¡è§„åˆ’** â†’ å…¨å±€æœ€ä¼˜è§£

---

### 2. Lassoå›å½’

**é—®é¢˜**:

$$
\min_w \frac{1}{2} \|Xw - y\|^2 + \lambda \|w\|_1
$$

**è¿‘ç«¯æ¢¯åº¦æ³•**:

$$
w_{t+1} = \text{prox}_{\eta \lambda \|\cdot\|_1}(w_t - \eta X^T(Xw_t - y))
$$

å…¶ä¸­è¿‘ç«¯ç®—å­æ˜¯è½¯é˜ˆå€¼ï¼š

$$
[\text{prox}_{\eta \lambda \|\cdot\|_1}(w)]_i = \text{sign}(w_i) \max(|w_i| - \eta \lambda, 0)
$$

---

### 3. é€»è¾‘å›å½’

**é—®é¢˜**:

$$
\min_w \sum_i \log(1 + e^{-y_i w^T x_i}) + \frac{\lambda}{2} \|w\|^2
$$

**å‡¸ä¼˜åŒ–** â†’ æ¢¯åº¦ä¸‹é™/ç‰›é¡¿æ³•

**æ¢¯åº¦**:

$$
\nabla f(w) = -\sum_i \frac{y_i x_i}{1 + e^{y_i w^T x_i}} + \lambda w
$$

---

## ğŸ’» Pythonå®ç°

```python
import numpy as np
import matplotlib.pyplot as plt

# 1. æ¢¯åº¦æŠ•å½±æ³•
def gradient_projection(f, grad_f, project, x0, lr=0.01, max_iter=1000, tol=1e-6):
    """æ¢¯åº¦æŠ•å½±æ³•"""
    x = x0.copy()
    trajectory = [x.copy()]
    
    for i in range(max_iter):
        grad = grad_f(x)
        
        if np.linalg.norm(grad) < tol:
            print(f"Converged in {i} iterations")
            break
        
        # æ¢¯åº¦æ­¥
        x_new = x - lr * grad
        
        # æŠ•å½±åˆ°å¯è¡ŒåŸŸ
        x = project(x_new)
        trajectory.append(x.copy())
    
    return x, np.array(trajectory)


# 2. è¿‘ç«¯æ¢¯åº¦æ³•
def proximal_gradient(f, grad_f, prox_g, x0, lr=0.01, max_iter=1000, tol=1e-6):
    """è¿‘ç«¯æ¢¯åº¦æ³•"""
    x = x0.copy()
    trajectory = [x.copy()]
    
    for i in range(max_iter):
        grad = grad_f(x)
        
        # æ¢¯åº¦æ­¥
        x_temp = x - lr * grad
        
        # è¿‘ç«¯ç®—å­
        x = prox_g(x_temp, lr)
        trajectory.append(x.copy())
        
        if np.linalg.norm(x - trajectory[-2]) < tol:
            print(f"Converged in {i} iterations")
            break
    
    return x, np.array(trajectory)


# 3. è½¯é˜ˆå€¼ç®—å­ (L1è¿‘ç«¯ç®—å­)
def soft_threshold(x, lambda_):
    """è½¯é˜ˆå€¼ç®—å­: prox_{lambda ||Â·||_1}"""
    return np.sign(x) * np.maximum(np.abs(x) - lambda_, 0)


# 4. NesterovåŠ é€Ÿæ¢¯åº¦æ³•
def nesterov_accelerated_gradient(f, grad_f, x0, lr=0.01, max_iter=1000, tol=1e-6):
    """NesterovåŠ é€Ÿæ¢¯åº¦æ³•"""
    x = x0.copy()
    x_prev = x0.copy()
    trajectory = [x.copy()]
    
    for t in range(1, max_iter):
        # åŠ¨é‡é¡¹
        momentum = (t - 1) / (t + 2)
        y = x + momentum * (x - x_prev)
        
        grad = grad_f(y)
        
        if np.linalg.norm(grad) < tol:
            print(f"Converged in {t} iterations")
            break
        
        x_prev = x.copy()
        x = y - lr * grad
        trajectory.append(x.copy())
    
    return x, np.array(trajectory)


# 5. ADMMç®—æ³• (Lassoç¤ºä¾‹)
def admm_lasso(X, y, lambda_, rho=1.0, max_iter=100, tol=1e-4):
    """ADMMæ±‚è§£Lasso: min ||Xw - y||^2 + lambda ||w||_1"""
    n, d = X.shape
    
    # åˆå§‹åŒ–
    w = np.zeros(d)
    z = np.zeros(d)
    u = np.zeros(d)
    
    # é¢„è®¡ç®—
    XtX = X.T @ X
    Xty = X.T @ y
    L = XtX + rho * np.eye(d)
    
    for i in range(max_iter):
        # w-update (è§£æè§£)
        w = np.linalg.solve(L, Xty + rho * (z - u))
        
        # z-update (è½¯é˜ˆå€¼)
        z_old = z.copy()
        z = soft_threshold(w + u, lambda_ / rho)
        
        # u-update
        u = u + w - z
        
        # æ£€æŸ¥æ”¶æ•›
        if np.linalg.norm(z - z_old) < tol:
            print(f"ADMM converged in {i+1} iterations")
            break
    
    return w


# ç¤ºä¾‹ï¼šLassoå›å½’
def lasso_example():
    """Lassoå›å½’ç¤ºä¾‹"""
    np.random.seed(42)
    
    # ç”Ÿæˆç¨€ç–æ•°æ®
    n, d = 100, 50
    k = 5  # çœŸå®éé›¶ç³»æ•°æ•°é‡
    
    X = np.random.randn(n, d)
    w_true = np.zeros(d)
    w_true[:k] = np.random.randn(k)
    y = X @ w_true + 0.1 * np.random.randn(n)
    
    # è¿‘ç«¯æ¢¯åº¦æ³•
    lambda_ = 0.1
    
    def f(w):
        return 0.5 * np.sum((X @ w - y)**2)
    
    def grad_f(w):
        return X.T @ (X @ w - y)
    
    def prox_g(w, eta):
        return soft_threshold(w, eta * lambda_)
    
    w0 = np.zeros(d)
    w_prox, traj_prox = proximal_gradient(f, grad_f, prox_g, w0, lr=0.001, max_iter=1000)
    
    # ADMM
    w_admm = admm_lasso(X, y, lambda_, rho=1.0, max_iter=100)
    
    # å¯è§†åŒ–
    plt.figure(figsize=(15, 5))
    
    # çœŸå®ç³»æ•°
    plt.subplot(1, 3, 1)
    plt.stem(w_true)
    plt.title('True Coefficients')
    plt.xlabel('Index')
    plt.ylabel('Value')
    
    # è¿‘ç«¯æ¢¯åº¦æ³•ç»“æœ
    plt.subplot(1, 3, 2)
    plt.stem(w_prox)
    plt.title('Proximal Gradient')
    plt.xlabel('Index')
    plt.ylabel('Value')
    
    # ADMMç»“æœ
    plt.subplot(1, 3, 3)
    plt.stem(w_admm)
    plt.title('ADMM')
    plt.xlabel('Index')
    plt.ylabel('Value')
    
    plt.tight_layout()
    # plt.show()
    
    print(f"True non-zeros: {np.sum(w_true != 0)}")
    print(f"Prox non-zeros: {np.sum(np.abs(w_prox) > 1e-3)}")
    print(f"ADMM non-zeros: {np.sum(np.abs(w_admm) > 1e-3)}")


# ç¤ºä¾‹ï¼šåŠ é€Ÿå¯¹æ¯”
def acceleration_comparison():
    """å¯¹æ¯”æ ‡å‡†æ¢¯åº¦ä¸‹é™ä¸NesterovåŠ é€Ÿ"""
    # å¼ºå‡¸äºŒæ¬¡å‡½æ•°
    A = np.array([[10, 0], [0, 1]])  # æ¡ä»¶æ•° = 10
    b = np.array([1, 1])
    
    def f(x):
        return 0.5 * x @ A @ x - b @ x
    
    def grad_f(x):
        return A @ x - b
    
    x0 = np.array([5.0, 5.0])
    
    # æ ‡å‡†æ¢¯åº¦ä¸‹é™
    from scipy.optimize import minimize_scalar
    
    def gd(x0, lr, max_iter=1000):
        x = x0.copy()
        traj = [x.copy()]
        for _ in range(max_iter):
            x = x - lr * grad_f(x)
            traj.append(x.copy())
        return np.array(traj)
    
    traj_gd = gd(x0, lr=0.1, max_iter=100)
    
    # NesterovåŠ é€Ÿ
    _, traj_nag = nesterov_accelerated_gradient(f, grad_f, x0, lr=0.1, max_iter=100)
    
    # å¯è§†åŒ–
    x_opt = np.linalg.solve(A, b)
    
    plt.figure(figsize=(15, 5))
    
    # ç­‰é«˜çº¿
    x_range = np.linspace(-1, 6, 100)
    y_range = np.linspace(-1, 6, 100)
    X, Y = np.meshgrid(x_range, y_range)
    Z = np.zeros_like(X)
    
    for i in range(X.shape[0]):
        for j in range(X.shape[1]):
            Z[i, j] = f(np.array([X[i, j], Y[i, j]]))
    
    # æ ‡å‡†æ¢¯åº¦ä¸‹é™
    plt.subplot(1, 2, 1)
    plt.contour(X, Y, Z, levels=20, cmap='gray', alpha=0.3)
    plt.plot(traj_gd[:, 0], traj_gd[:, 1], 'r-o', markersize=3, label='GD')
    plt.plot(x_opt[0], x_opt[1], 'g*', markersize=15, label='Optimum')
    plt.title('Standard Gradient Descent')
    plt.xlabel('x1')
    plt.ylabel('x2')
    plt.legend()
    
    # NesterovåŠ é€Ÿ
    plt.subplot(1, 2, 2)
    plt.contour(X, Y, Z, levels=20, cmap='gray', alpha=0.3)
    plt.plot(traj_nag[:, 0], traj_nag[:, 1], 'b-o', markersize=3, label='NAG')
    plt.plot(x_opt[0], x_opt[1], 'g*', markersize=15, label='Optimum')
    plt.title('Nesterov Accelerated Gradient')
    plt.xlabel('x1')
    plt.ylabel('x2')
    plt.legend()
    
    plt.tight_layout()
    # plt.show()
    
    print(f"GD iterations to converge: {len(traj_gd)}")
    print(f"NAG iterations to converge: {len(traj_nag)}")


if __name__ == "__main__":
    print("=== å‡¸ä¼˜åŒ–è¿›é˜¶ç¤ºä¾‹ ===")
    
    print("\n1. Lassoå›å½’ç¤ºä¾‹")
    lasso_example()
    
    print("\n2. åŠ é€Ÿæ¢¯åº¦æ³•å¯¹æ¯”")
    acceleration_comparison()
```

---

## ğŸ“š ç»ƒä¹ é¢˜

### ç»ƒä¹ 1ï¼šå‡¸æ€§åˆ¤å®š

åˆ¤æ–­ä»¥ä¸‹å‡½æ•°æ˜¯å¦ä¸ºå‡¸å‡½æ•°ï¼š

1. $f(x) = e^x$
2. $f(x) = x^4$
3. $f(x) = \log(1 + e^x)$
4. $f(x, y) = x^2 + xy + y^2$

### ç»ƒä¹ 2ï¼šå¯¹å¶é—®é¢˜

æ±‚è§£ä»¥ä¸‹é—®é¢˜çš„å¯¹å¶é—®é¢˜ï¼š

$$
\min_x \frac{1}{2} x^T Q x + c^T x \quad \text{s.t.} \quad Ax = b, \; x \geq 0
$$

### ç»ƒä¹ 3ï¼šè¿‘ç«¯ç®—å­

è®¡ç®—ä»¥ä¸‹å‡½æ•°çš„è¿‘ç«¯ç®—å­ï¼š

1. $g(x) = \lambda \|x\|_1$
2. $g(x) = I_C(x)$ (æŒ‡ç¤ºå‡½æ•°ï¼Œ$C$ æ˜¯å‡¸é›†)

### ç»ƒä¹ 4ï¼šADMMåº”ç”¨

ä½¿ç”¨ADMMæ±‚è§£ä»¥ä¸‹é—®é¢˜ï¼š

$$
\min_{x, z} \frac{1}{2} \|Ax - b\|^2 + \lambda \|z\|_1 \quad \text{s.t.} \quad x = z
$$

---

## ğŸ“ ç›¸å…³è¯¾ç¨‹

| å¤§å­¦ | è¯¾ç¨‹ |
|------|------|
| **Stanford** | EE364A - Convex Optimization I |
| **Stanford** | EE364B - Convex Optimization II |
| **MIT** | 6.255J - Optimization Methods |
| **UC Berkeley** | EECS 127 - Optimization Models |
| **CMU** | 10-725 - Convex Optimization |

---

## ğŸ“– å‚è€ƒæ–‡çŒ®

1. **Boyd & Vandenberghe (2004)**. *Convex Optimization*. Cambridge University Press.

2. **Nesterov, Y. (2004)**. *Introductory Lectures on Convex Optimization*. Springer.

3. **Bertsekas, D. (2009)**. *Convex Optimization Theory*. Athena Scientific.

4. **Parikh & Boyd (2014)**. "Proximal Algorithms". *Foundations and Trends in Optimization*.

5. **Beck & Teboulle (2009)**. "A Fast Iterative Shrinkage-Thresholding Algorithm (FISTA)". *SIAM Journal on Imaging Sciences*.

---

*æœ€åæ›´æ–°ï¼š2025å¹´10æœˆ*-
