# SGDåŠå…¶å˜ä½“ (SGD and Variants)

> **Stochastic Gradient Descent: From Theory to Practice**
>
> æ·±åº¦å­¦ä¹ ä¼˜åŒ–çš„æ ¸å¿ƒç®—æ³•

---

## ç›®å½•

- [SGDåŠå…¶å˜ä½“ (SGD and Variants)](#sgdåŠå…¶å˜ä½“-sgd-and-variants)
  - [ç›®å½•](#ç›®å½•)
  - [ğŸ“‹ æ ¸å¿ƒæ€æƒ³](#-æ ¸å¿ƒæ€æƒ³)
  - [ğŸ¯ ä¼˜åŒ–é—®é¢˜](#-ä¼˜åŒ–é—®é¢˜)
    - [1. ç»éªŒé£é™©æœ€å°åŒ–](#1-ç»éªŒé£é™©æœ€å°åŒ–)
    - [2. æ‰¹é‡æ¢¯åº¦ä¸‹é™çš„é—®é¢˜](#2-æ‰¹é‡æ¢¯åº¦ä¸‹é™çš„é—®é¢˜)
  - [ğŸ“Š éšæœºæ¢¯åº¦ä¸‹é™ (SGD)](#-éšæœºæ¢¯åº¦ä¸‹é™-sgd)
    - [1. ç®—æ³•å®šä¹‰](#1-ç®—æ³•å®šä¹‰)
    - [2. æ”¶æ•›æ€§åˆ†æ](#2-æ”¶æ•›æ€§åˆ†æ)
    - [3. å­¦ä¹ ç‡è°ƒåº¦](#3-å­¦ä¹ ç‡è°ƒåº¦)
  - [ğŸ”¬ åŠ¨é‡æ–¹æ³• (Momentum)](#-åŠ¨é‡æ–¹æ³•-momentum)
    - [1. æ ‡å‡†åŠ¨é‡](#1-æ ‡å‡†åŠ¨é‡)
    - [2. NesterovåŠ é€Ÿæ¢¯åº¦](#2-nesterovåŠ é€Ÿæ¢¯åº¦)
    - [3. åŠ¨é‡çš„å‡ ä½•è§£é‡Š](#3-åŠ¨é‡çš„å‡ ä½•è§£é‡Š)
  - [ğŸ’» è‡ªé€‚åº”å­¦ä¹ ç‡æ–¹æ³•](#-è‡ªé€‚åº”å­¦ä¹ ç‡æ–¹æ³•)
    - [1. AdaGrad](#1-adagrad)
    - [2. RMSprop](#2-rmsprop)
    - [3. Adam](#3-adam)
    - [4. AdamW](#4-adamw)
  - [ğŸ¨ å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥](#-å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥)
    - [1. æ­¥é•¿è¡°å‡](#1-æ­¥é•¿è¡°å‡)
    - [2. ä½™å¼¦é€€ç«](#2-ä½™å¼¦é€€ç«)
    - [3. é¢„çƒ­ (Warmup)](#3-é¢„çƒ­-warmup)
    - [4. å¾ªç¯å­¦ä¹ ç‡](#4-å¾ªç¯å­¦ä¹ ç‡)
  - [ğŸ“ æ‰¹é‡å¤§å°çš„å½±å“](#-æ‰¹é‡å¤§å°çš„å½±å“)
    - [1. æ‰¹é‡å¤§å°ä¸æ³›åŒ–](#1-æ‰¹é‡å¤§å°ä¸æ³›åŒ–)
    - [2. çº¿æ€§ç¼©æ”¾è§„åˆ™](#2-çº¿æ€§ç¼©æ”¾è§„åˆ™)
  - [ğŸ”§ å®è·µæŠ€å·§](#-å®è·µæŠ€å·§)
    - [1. æ¢¯åº¦è£å‰ª](#1-æ¢¯åº¦è£å‰ª)
    - [2. æƒé‡è¡°å‡](#2-æƒé‡è¡°å‡)
    - [3. æ¢¯åº¦ç´¯ç§¯](#3-æ¢¯åº¦ç´¯ç§¯)
  - [ğŸ’¡ Pythonå®ç°](#-pythonå®ç°)
  - [ğŸ“š ä¼˜åŒ–å™¨é€‰æ‹©æŒ‡å—](#-ä¼˜åŒ–å™¨é€‰æ‹©æŒ‡å—)
    - [1. ä¸åŒä»»åŠ¡çš„æ¨è](#1-ä¸åŒä»»åŠ¡çš„æ¨è)
    - [2. è¶…å‚æ•°è°ƒä¼˜](#2-è¶…å‚æ•°è°ƒä¼˜)
  - [ğŸ“ ç›¸å…³è¯¾ç¨‹](#-ç›¸å…³è¯¾ç¨‹)
  - [ğŸ“– å‚è€ƒæ–‡çŒ®](#-å‚è€ƒæ–‡çŒ®)

---

## ğŸ“‹ æ ¸å¿ƒæ€æƒ³

**éšæœºæ¢¯åº¦ä¸‹é™ (SGD)** ä½¿ç”¨**å°æ‰¹é‡æ ·æœ¬**ä¼°è®¡æ¢¯åº¦ï¼Œå®ç°é«˜æ•ˆä¼˜åŒ–ã€‚

**æ ¸å¿ƒåŸç†**ï¼š

```text
æ‰¹é‡æ¢¯åº¦ä¸‹é™ (BGD):
    ä½¿ç”¨å…¨éƒ¨æ•°æ® â†’ å‡†ç¡®ä½†æ…¢

éšæœºæ¢¯åº¦ä¸‹é™ (SGD):
    ä½¿ç”¨å•ä¸ª/å°æ‰¹é‡æ ·æœ¬ â†’ å¿«é€Ÿä½†æœ‰å™ªå£°

å…³é”®æƒè¡¡:
    è®¡ç®—æ•ˆç‡ vs æ¢¯åº¦å‡†ç¡®æ€§
```

---

## ğŸ¯ ä¼˜åŒ–é—®é¢˜

### 1. ç»éªŒé£é™©æœ€å°åŒ–

**ç›®æ ‡**ï¼š

$$
\min_{\theta} \mathcal{L}(\theta) = \frac{1}{n} \sum_{i=1}^{n} \ell(f_\theta(x_i), y_i)
$$

**æ¢¯åº¦**ï¼š

$$
\nabla \mathcal{L}(\theta) = \frac{1}{n} \sum_{i=1}^{n} \nabla \ell(f_\theta(x_i), y_i)
$$

---

### 2. æ‰¹é‡æ¢¯åº¦ä¸‹é™çš„é—®é¢˜

**ç®—æ³•**ï¼š

$$
\theta_{t+1} = \theta_t - \eta \nabla \mathcal{L}(\theta_t)
$$

**é—®é¢˜**ï¼š

- **è®¡ç®—æˆæœ¬é«˜**ï¼šæ¯æ­¥éœ€è¦éå†å…¨éƒ¨æ•°æ®
- **å†…å­˜éœ€æ±‚å¤§**ï¼šéœ€è¦å­˜å‚¨æ‰€æœ‰æ ·æœ¬
- **æ”¶æ•›æ…¢**ï¼šå¤§æ•°æ®é›†ä¸‹ä¸å®ç”¨

**ç¤ºä¾‹**ï¼š

- æ•°æ®é›†ï¼š100ä¸‡æ ·æœ¬
- æ¯ä¸ªepochï¼š100ä¸‡æ¬¡å‰å‘ä¼ æ’­
- è®­ç»ƒ100ä¸ªepochï¼š1äº¿æ¬¡è®¡ç®—ï¼

---

## ğŸ“Š éšæœºæ¢¯åº¦ä¸‹é™ (SGD)

### 1. ç®—æ³•å®šä¹‰

**å®šä¹‰ 1.1 (Mini-batch SGD)**:

åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼š

1. éšæœºé‡‡æ ·å°æ‰¹é‡ $\mathcal{B}_t \subset \{1, \ldots, n\}$ï¼Œ$|\mathcal{B}_t| = b$
2. è®¡ç®—å°æ‰¹é‡æ¢¯åº¦ï¼š
   $$
   g_t = \frac{1}{b} \sum_{i \in \mathcal{B}_t} \nabla \ell(f_{\theta_t}(x_i), y_i)
   $$
3. æ›´æ–°å‚æ•°ï¼š
   $$
   \theta_{t+1} = \theta_t - \eta_t g_t
   $$

**å…³é”®æ€§è´¨**ï¼š

$$
\mathbb{E}[g_t] = \nabla \mathcal{L}(\theta_t) \quad \text{(æ— åä¼°è®¡)}
$$

---

### 2. æ”¶æ•›æ€§åˆ†æ

**å®šç† 2.1 (SGDæ”¶æ•›ç‡, å‡¸æƒ…å†µ)**:

å‡è®¾ $\mathcal{L}$ æ˜¯ $L$-å…‰æ»‘çš„å‡¸å‡½æ•°ï¼Œä½¿ç”¨å›ºå®šå­¦ä¹ ç‡ $\eta \leq 1/L$ï¼š

$$
\mathbb{E}[\mathcal{L}(\bar{\theta}_T)] - \mathcal{L}^* \leq \frac{\|\theta_0 - \theta^*\|^2}{2\eta T} + \frac{\eta \sigma^2}{2b}
$$

å…¶ä¸­ $\bar{\theta}_T = \frac{1}{T} \sum_{t=1}^{T} \theta_t$ï¼Œ$\sigma^2$ æ˜¯æ¢¯åº¦æ–¹å·®ã€‚

**è§£é‡Š**ï¼š

- ç¬¬ä¸€é¡¹ï¼šä¼˜åŒ–è¯¯å·®ï¼Œéš $T$ å‡å°
- ç¬¬äºŒé¡¹ï¼šéšæœºå™ªå£°ï¼Œå–å†³äºæ‰¹é‡å¤§å°

**æ”¶æ•›ç‡**ï¼š$O(1/\sqrt{T})$

---

### 3. å­¦ä¹ ç‡è°ƒåº¦

**å›ºå®šå­¦ä¹ ç‡é—®é¢˜**ï¼š

- å¤ªå¤§ï¼šæŒ¯è¡ï¼Œä¸æ”¶æ•›
- å¤ªå°ï¼šæ”¶æ•›æ…¢

**è§£å†³æ–¹æ¡ˆ**ï¼šå­¦ä¹ ç‡è¡°å‡

**å¸¸è§ç­–ç•¥**ï¼š

1. **æ­¥é•¿è¡°å‡**ï¼š$\eta_t = \eta_0 / (1 + \alpha t)$
2. **æŒ‡æ•°è¡°å‡**ï¼š$\eta_t = \eta_0 \gamma^t$
3. **å¤šé¡¹å¼è¡°å‡**ï¼š$\eta_t = \eta_0 / (1 + t)^p$

---

## ğŸ”¬ åŠ¨é‡æ–¹æ³• (Momentum)

### 1. æ ‡å‡†åŠ¨é‡

**å®šä¹‰ 1.1 (Momentum SGD)**:

$$
v_{t+1} = \beta v_t + g_t
$$

$$
\theta_{t+1} = \theta_t - \eta v_{t+1}
$$

å…¶ä¸­ $\beta \in [0, 1)$ æ˜¯åŠ¨é‡ç³»æ•°ï¼ˆé€šå¸¸ $\beta = 0.9$ï¼‰ã€‚

**å±•å¼€å½¢å¼**ï¼š

$$
v_{t+1} = g_t + \beta g_{t-1} + \beta^2 g_{t-2} + \cdots
$$

**ç›´è§‰**ï¼š

- ç´¯ç§¯å†å²æ¢¯åº¦
- åŠ é€Ÿä¸€è‡´æ–¹å‘
- æŠ‘åˆ¶æŒ¯è¡

---

### 2. NesterovåŠ é€Ÿæ¢¯åº¦

**å®šä¹‰ 2.1 (Nesterov Accelerated Gradient, NAG)**:

$$
v_{t+1} = \beta v_t + \nabla \mathcal{L}(\theta_t - \eta \beta v_t)
$$

$$
\theta_{t+1} = \theta_t - \eta v_{t+1}
$$

**å…³é”®æ€æƒ³**ï¼šå…ˆ"é¢„æµ‹"ä¸€æ­¥ï¼Œå†è®¡ç®—æ¢¯åº¦

**ä¼˜åŠ¿**ï¼š

- æ›´å¥½çš„æ”¶æ•›æ€§
- å‡¸æƒ…å†µï¼š$O(1/T^2)$ vs æ ‡å‡†åŠ¨é‡çš„ $O(1/T)$

---

### 3. åŠ¨é‡çš„å‡ ä½•è§£é‡Š

**ç‰©ç†ç±»æ¯”**ï¼š

```text
æ¢¯åº¦ = åŠ›
åŠ¨é‡ = é€Ÿåº¦
å‚æ•° = ä½ç½®

ç‰©ç†æ–¹ç¨‹:
    v_{t+1} = Î²v_t + F_t  (ç‰›é¡¿ç¬¬äºŒå®šå¾‹)
    x_{t+1} = x_t + v_{t+1}
```

**æ•ˆæœ**ï¼š

- åœ¨å¹³å¦åŒºåŸŸåŠ é€Ÿ
- åœ¨é™¡å³­åŒºåŸŸå‡é€Ÿ
- è·¨è¶Šå±€éƒ¨æå°å€¼

---

## ğŸ’» è‡ªé€‚åº”å­¦ä¹ ç‡æ–¹æ³•

### 1. AdaGrad

**å®šä¹‰ 1.1 (AdaGrad)**:

$$
G_t = G_{t-1} + g_t \odot g_t
$$

$$
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{G_t + \epsilon}} \odot g_t
$$

**ç‰¹ç‚¹**ï¼š

- é¢‘ç¹æ›´æ–°çš„å‚æ•° â†’ å°å­¦ä¹ ç‡
- ç¨€ç–æ›´æ–°çš„å‚æ•° â†’ å¤§å­¦ä¹ ç‡

**é—®é¢˜**ï¼šå­¦ä¹ ç‡å•è°ƒé€’å‡ï¼Œå¯èƒ½è¿‡æ—©åœæ­¢

---

### 2. RMSprop

**å®šä¹‰ 2.1 (RMSprop)**:

$$
v_t = \beta v_{t-1} + (1 - \beta) g_t \odot g_t
$$

$$
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{v_t + \epsilon}} \odot g_t
$$

**æ”¹è¿›**ï¼š

- ä½¿ç”¨æŒ‡æ•°ç§»åŠ¨å¹³å‡
- é¿å…å­¦ä¹ ç‡å•è°ƒé€’å‡

**è¶…å‚æ•°**ï¼š$\beta = 0.9$, $\eta = 0.001$

---

### 3. Adam

**å®šä¹‰ 3.1 (Adam - Adaptive Moment Estimation)**:

$$
m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t \quad \text{(ä¸€é˜¶çŸ©)}
$$

$$
v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t \odot g_t \quad \text{(äºŒé˜¶çŸ©)}
$$

**åå·®ä¿®æ­£**ï¼š

$$
\hat{m}_t = \frac{m_t}{1 - \beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1 - \beta_2^t}
$$

**æ›´æ–°**ï¼š

$$
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t
$$

**é»˜è®¤è¶…å‚æ•°**ï¼š

- $\beta_1 = 0.9$
- $\beta_2 = 0.999$
- $\eta = 0.001$
- $\epsilon = 10^{-8}$

---

### 4. AdamW

**å®šä¹‰ 4.1 (AdamW - Adam with Weight Decay)**:

$$
\theta_{t+1} = \theta_t - \eta \left( \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon} + \lambda \theta_t \right)
$$

**å…³é”®æ”¹è¿›**ï¼š

- å°†æƒé‡è¡°å‡ä»æ¢¯åº¦ä¸­è§£è€¦
- æ›´å¥½çš„æ­£åˆ™åŒ–æ•ˆæœ

**æ¨è**ï¼šç°ä»£æ·±åº¦å­¦ä¹ çš„é¦–é€‰

---

## ğŸ¨ å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥

### 1. æ­¥é•¿è¡°å‡

**å®šä¹‰**ï¼š

$$
\eta_t = \eta_0 \cdot \gamma^{\lfloor t / s \rfloor}
$$

å…¶ä¸­ $s$ æ˜¯æ­¥é•¿å¤§å°ï¼Œ$\gamma$ æ˜¯è¡°å‡å› å­ï¼ˆå¦‚0.1ï¼‰ã€‚

**ç¤ºä¾‹**ï¼š

- åˆå§‹ï¼š$\eta_0 = 0.1$
- æ¯30ä¸ªepochï¼š$\eta \times 0.1$

---

### 2. ä½™å¼¦é€€ç«

**å®šä¹‰ 2.1 (Cosine Annealing)**:

$$
\eta_t = \eta_{\min} + \frac{1}{2}(\eta_{\max} - \eta_{\min}) \left(1 + \cos\left(\frac{t}{T} \pi\right)\right)
$$

**ç‰¹ç‚¹**ï¼š

- å¹³æ»‘è¡°å‡
- æ— éœ€æ‰‹åŠ¨è°ƒæ•´æ­¥é•¿

**å˜ä½“ï¼šCosine Annealing with Warm Restarts**:

$$
\eta_t = \eta_{\min} + \frac{1}{2}(\eta_{\max} - \eta_{\min}) \left(1 + \cos\left(\frac{T_{\text{cur}}}{T_i} \pi\right)\right)
$$

---

### 3. é¢„çƒ­ (Warmup)

**å®šä¹‰ 3.1 (Linear Warmup)**:

$$
\eta_t = \begin{cases}
\frac{t}{T_{\text{warmup}}} \eta_0 & \text{if } t \leq T_{\text{warmup}} \\
\eta_0 & \text{otherwise}
\end{cases}
$$

**ä½œç”¨**ï¼š

- é¿å…åˆæœŸå¤§å­¦ä¹ ç‡å¯¼è‡´çš„ä¸ç¨³å®š
- ç‰¹åˆ«é€‚ç”¨äºTransformerè®­ç»ƒ

---

### 4. å¾ªç¯å­¦ä¹ ç‡

**å®šä¹‰ 4.1 (Cyclical Learning Rate)**:

$$
\eta_t = \eta_{\min} + (\eta_{\max} - \eta_{\min}) \cdot \max(0, 1 - |t \bmod (2s) - s| / s)
$$

**ç‰¹ç‚¹**ï¼š

- å‘¨æœŸæ€§å˜åŒ–
- å¸®åŠ©è·³å‡ºå±€éƒ¨æå°å€¼

---

## ğŸ“ æ‰¹é‡å¤§å°çš„å½±å“

### 1. æ‰¹é‡å¤§å°ä¸æ³›åŒ–

**è§‚å¯Ÿ**ï¼š

- **å°æ‰¹é‡**ï¼šæ³›åŒ–æ›´å¥½ï¼Œä½†è®­ç»ƒæ…¢
- **å¤§æ‰¹é‡**ï¼šè®­ç»ƒå¿«ï¼Œä½†æ³›åŒ–å·®

**ç†è®ºè§£é‡Š**ï¼š

- å°æ‰¹é‡ï¼šæ¢¯åº¦å™ªå£° â†’ éšå¼æ­£åˆ™åŒ–
- å¤§æ‰¹é‡ï¼šæ”¶æ•›åˆ°å°–é”æå°å€¼

---

### 2. çº¿æ€§ç¼©æ”¾è§„åˆ™

**å®šç† 2.1 (Linear Scaling Rule, Goyal et al. 2017)**:

å½“æ‰¹é‡å¤§å°å¢åŠ  $k$ å€æ—¶ï¼Œå­¦ä¹ ç‡ä¹Ÿåº”å¢åŠ  $k$ å€ï¼š

$$
\eta_{\text{new}} = k \cdot \eta_{\text{old}}
$$

**å‰æ**ï¼š

- ä½¿ç”¨é¢„çƒ­
- æ‰¹é‡å¤§å°ä¸èƒ½å¤ªå¤§

**ç¤ºä¾‹**ï¼š

- æ‰¹é‡256ï¼Œå­¦ä¹ ç‡0.1
- æ‰¹é‡1024 â†’ å­¦ä¹ ç‡0.4

---

## ğŸ”§ å®è·µæŠ€å·§

### 1. æ¢¯åº¦è£å‰ª

**æŒ‰èŒƒæ•°è£å‰ª**ï¼š

$$
g = \begin{cases}
\frac{c}{\|g\|} g & \text{if } \|g\| > c \\
g & \text{otherwise}
\end{cases}
$$

**ä½œç”¨**ï¼šé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸

---

### 2. æƒé‡è¡°å‡

**L2æ­£åˆ™åŒ–**ï¼š

$$
\mathcal{L}_{\text{reg}} = \mathcal{L} + \frac{\lambda}{2} \|\theta\|^2
$$

**ç­‰ä»·äº**ï¼š

$$
\theta_{t+1} = (1 - \eta \lambda) \theta_t - \eta g_t
$$

---

### 3. æ¢¯åº¦ç´¯ç§¯

**åŠ¨æœº**ï¼šæ¨¡æ‹Ÿå¤§æ‰¹é‡ï¼ŒèŠ‚çœå†…å­˜

**æ–¹æ³•**ï¼š

```python
for i in range(accumulation_steps):
    loss = compute_loss(batch[i])
    loss.backward()  # ç´¯ç§¯æ¢¯åº¦

optimizer.step()  # æ›´æ–°å‚æ•°
optimizer.zero_grad()
```

**ç­‰ä»·æ‰¹é‡å¤§å°**ï¼š$b_{\text{eff}} = b \times \text{accumulation\_steps}$

---

## ğŸ’¡ Pythonå®ç°

```python
import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt

# 1. ä»é›¶å®ç°SGD with Momentum
class SGDMomentum:
    """SGD with Momentum"""
    def __init__(self, params, lr=0.01, momentum=0.9):
        self.params = list(params)
        self.lr = lr
        self.momentum = momentum
        self.velocities = [torch.zeros_like(p) for p in self.params]
    
    def step(self):
        for i, param in enumerate(self.params):
            if param.grad is None:
                continue
            
            # æ›´æ–°é€Ÿåº¦
            self.velocities[i] = self.momentum * self.velocities[i] + param.grad
            
            # æ›´æ–°å‚æ•°
            param.data -= self.lr * self.velocities[i]
    
    def zero_grad(self):
        for param in self.params:
            if param.grad is not None:
                param.grad.zero_()


# 2. ä»é›¶å®ç°Adam
class Adam:
    """Adam Optimizer"""
    def __init__(self, params, lr=0.001, betas=(0.9, 0.999), eps=1e-8):
        self.params = list(params)
        self.lr = lr
        self.beta1, self.beta2 = betas
        self.eps = eps
        
        self.m = [torch.zeros_like(p) for p in self.params]  # ä¸€é˜¶çŸ©
        self.v = [torch.zeros_like(p) for p in self.params]  # äºŒé˜¶çŸ©
        self.t = 0  # æ—¶é—´æ­¥
    
    def step(self):
        self.t += 1
        
        for i, param in enumerate(self.params):
            if param.grad is None:
                continue
            
            grad = param.grad
            
            # æ›´æ–°ä¸€é˜¶çŸ©å’ŒäºŒé˜¶çŸ©
            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad
            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * grad ** 2
            
            # åå·®ä¿®æ­£
            m_hat = self.m[i] / (1 - self.beta1 ** self.t)
            v_hat = self.v[i] / (1 - self.beta2 ** self.t)
            
            # æ›´æ–°å‚æ•°
            param.data -= self.lr * m_hat / (torch.sqrt(v_hat) + self.eps)
    
    def zero_grad(self):
        for param in self.params:
            if param.grad is not None:
                param.grad.zero_()


# 3. å­¦ä¹ ç‡è°ƒåº¦å™¨
class CosineAnnealingLR:
    """ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦"""
    def __init__(self, optimizer, T_max, eta_min=0):
        self.optimizer = optimizer
        self.T_max = T_max
        self.eta_min = eta_min
        self.base_lr = optimizer.lr
        self.t = 0
    
    def step(self):
        self.t += 1
        lr = self.eta_min + (self.base_lr - self.eta_min) * \
             (1 + np.cos(np.pi * self.t / self.T_max)) / 2
        self.optimizer.lr = lr
        return lr


# 4. å¯¹æ¯”ä¸åŒä¼˜åŒ–å™¨
def compare_optimizers():
    """å¯¹æ¯”ä¸åŒä¼˜åŒ–å™¨çš„æ€§èƒ½"""
    # å®šä¹‰ä¸€ä¸ªç®€å•çš„ä¼˜åŒ–é—®é¢˜
    def rosenbrock(x, y):
        return (1 - x)**2 + 100 * (y - x**2)**2
    
    # åˆå§‹ç‚¹
    x0, y0 = -1.5, 2.0
    
    # ä¸åŒä¼˜åŒ–å™¨
    optimizers = {
        'SGD': lambda p: torch.optim.SGD(p, lr=0.001),
        'SGD+Momentum': lambda p: torch.optim.SGD(p, lr=0.001, momentum=0.9),
        'Adam': lambda p: torch.optim.Adam(p, lr=0.01),
        'RMSprop': lambda p: torch.optim.RMSprop(p, lr=0.01),
    }
    
    trajectories = {}
    
    for name, opt_fn in optimizers.items():
        x = torch.tensor([x0, y0], requires_grad=True)
        optimizer = opt_fn([x])
        
        trajectory = [x.detach().numpy().copy()]
        
        for _ in range(200):
            optimizer.zero_grad()
            loss = rosenbrock(x[0], x[1])
            loss.backward()
            optimizer.step()
            
            trajectory.append(x.detach().numpy().copy())
        
        trajectories[name] = np.array(trajectory)
    
    # å¯è§†åŒ–
    plt.figure(figsize=(12, 8))
    
    # ç»˜åˆ¶ç­‰é«˜çº¿
    x_range = np.linspace(-2, 2, 100)
    y_range = np.linspace(-1, 3, 100)
    X, Y = np.meshgrid(x_range, y_range)
    Z = rosenbrock(X, Y)
    
    plt.contour(X, Y, Z, levels=np.logspace(-1, 3, 20), cmap='gray', alpha=0.3)
    
    # ç»˜åˆ¶è½¨è¿¹
    colors = ['red', 'blue', 'green', 'orange']
    for (name, traj), color in zip(trajectories.items(), colors):
        plt.plot(traj[:, 0], traj[:, 1], '-o', label=name, color=color, 
                 markersize=2, linewidth=1.5)
    
    plt.plot(1, 1, 'r*', markersize=15, label='Optimum')
    plt.xlabel('x')
    plt.ylabel('y')
    plt.title('Optimizer Comparison on Rosenbrock Function')
    plt.legend()
    plt.grid(True, alpha=0.3)
    # plt.show()


# 5. å­¦ä¹ ç‡è°ƒåº¦å¯è§†åŒ–
def visualize_lr_schedules():
    """å¯è§†åŒ–ä¸åŒçš„å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥"""
    T = 100
    eta_0 = 0.1
    
    schedules = {
        'Constant': [eta_0] * T,
        'Step Decay': [eta_0 * (0.5 ** (t // 30)) for t in range(T)],
        'Exponential': [eta_0 * (0.95 ** t) for t in range(T)],
        'Cosine': [eta_0 * (1 + np.cos(np.pi * t / T)) / 2 for t in range(T)],
        'Linear Warmup': [min(t / 10, 1) * eta_0 for t in range(T)],
    }
    
    plt.figure(figsize=(12, 6))
    for name, schedule in schedules.items():
        plt.plot(schedule, label=name, linewidth=2)
    
    plt.xlabel('Iteration')
    plt.ylabel('Learning Rate')
    plt.title('Learning Rate Schedules')
    plt.legend()
    plt.grid(True, alpha=0.3)
    # plt.show()


# ç¤ºä¾‹ä½¿ç”¨
if __name__ == "__main__":
    print("=== å¯¹æ¯”ä¼˜åŒ–å™¨ ===")
    compare_optimizers()
    
    print("\n=== å­¦ä¹ ç‡è°ƒåº¦ ===")
    visualize_lr_schedules()
    
    # æµ‹è¯•è‡ªå®šä¹‰Adam
    print("\n=== æµ‹è¯•è‡ªå®šä¹‰Adam ===")
    model = nn.Linear(10, 1)
    optimizer = Adam(model.parameters(), lr=0.001)
    
    x = torch.randn(32, 10)
    y = torch.randn(32, 1)
    
    for epoch in range(10):
        optimizer.zero_grad()
        loss = nn.MSELoss()(model(x), y)
        loss.backward()
        optimizer.step()
        print(f"Epoch {epoch+1}, Loss: {loss.item():.4f}")
```

---

## ğŸ“š ä¼˜åŒ–å™¨é€‰æ‹©æŒ‡å—

### 1. ä¸åŒä»»åŠ¡çš„æ¨è

| ä»»åŠ¡ | æ¨èä¼˜åŒ–å™¨ | å­¦ä¹ ç‡ |
|------|-----------|--------|
| **å›¾åƒåˆ†ç±»** | SGD+Momentum | 0.1 |
| **ç›®æ ‡æ£€æµ‹** | SGD+Momentum | 0.02 |
| **è¯­è¨€æ¨¡å‹** | Adam/AdamW | 1e-4 |
| **Transformer** | AdamW + Warmup | 1e-4 |
| **GAN** | Adam | 2e-4 |
| **å¼ºåŒ–å­¦ä¹ ** | Adam | 3e-4 |

---

### 2. è¶…å‚æ•°è°ƒä¼˜

**å­¦ä¹ ç‡**ï¼š

- ä»å¤§åˆ°å°å°è¯•ï¼š$[1, 0.1, 0.01, 0.001, 0.0001]$
- ä½¿ç”¨å­¦ä¹ ç‡æŸ¥æ‰¾å™¨

**æ‰¹é‡å¤§å°**ï¼š

- ä»å°å¼€å§‹ï¼š32, 64, 128, 256
- å—å†…å­˜é™åˆ¶

**åŠ¨é‡**ï¼š

- é»˜è®¤0.9é€šå¸¸æœ‰æ•ˆ
- å¯å°è¯•0.95, 0.99

---

## ğŸ“ ç›¸å…³è¯¾ç¨‹

| å¤§å­¦ | è¯¾ç¨‹ |
|------|------|
| **Stanford** | CS229 Machine Learning |
| **MIT** | 6.255J Optimization Methods |
| **CMU** | 10-725 Convex Optimization |
| **UC Berkeley** | CS189 Introduction to Machine Learning |

---

## ğŸ“– å‚è€ƒæ–‡çŒ®

1. **Robbins & Monro (1951)**. "A Stochastic Approximation Method". *Annals of Mathematical Statistics*.

2. **Polyak (1964)**. "Some methods of speeding up the convergence of iteration methods". *USSR Computational Mathematics and Mathematical Physics*.

3. **Nesterov (1983)**. "A method for solving the convex programming problem with convergence rate O(1/k^2)". *Soviet Mathematics Doklady*.

4. **Duchi et al. (2011)**. "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization". *JMLR*. (AdaGrad)

5. **Kingma & Ba (2015)**. "Adam: A Method for Stochastic Optimization". *ICLR*.

6. **Loshchilov & Hutter (2019)**. "Decoupled Weight Decay Regularization". *ICLR*. (AdamW)

7. **Goyal et al. (2017)**. "Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour". *arXiv*. (Linear Scaling Rule)

---

*æœ€åæ›´æ–°ï¼š2025å¹´10æœˆ*-
