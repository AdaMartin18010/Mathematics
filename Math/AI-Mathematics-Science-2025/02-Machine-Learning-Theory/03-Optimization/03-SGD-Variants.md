# SGDåŠå…¶å˜ä½“ (SGD and Variants)

> **Stochastic Gradient Descent: From Theory to Practice**
>
> æ·±åº¦å­¦ä¹ ä¼˜åŒ–çš„æ ¸å¿ƒç®—æ³•

---

## ç›®å½•

- [SGDåŠå…¶å˜ä½“ (SGD and Variants)](#sgdåŠå…¶å˜ä½“-sgd-and-variants)
  - [ç›®å½•](#ç›®å½•)
  - [ğŸ“‹ æ ¸å¿ƒæ€æƒ³](#-æ ¸å¿ƒæ€æƒ³)
  - [ğŸ¯ ä¼˜åŒ–é—®é¢˜](#-ä¼˜åŒ–é—®é¢˜)
    - [1. ç»éªŒé£é™©æœ€å°åŒ–](#1-ç»éªŒé£é™©æœ€å°åŒ–)
    - [2. æ‰¹é‡æ¢¯åº¦ä¸‹é™çš„é—®é¢˜](#2-æ‰¹é‡æ¢¯åº¦ä¸‹é™çš„é—®é¢˜)
  - [ğŸ“Š éšæœºæ¢¯åº¦ä¸‹é™ (SGD)](#-éšæœºæ¢¯åº¦ä¸‹é™-sgd)
    - [1. ç®—æ³•å®šä¹‰](#1-ç®—æ³•å®šä¹‰)
    - [2. æ”¶æ•›æ€§åˆ†æ](#2-æ”¶æ•›æ€§åˆ†æ)
    - [ğŸ“ SGDæ”¶æ•›ç‡çš„å®Œæ•´è¯æ˜](#-sgdæ”¶æ•›ç‡çš„å®Œæ•´è¯æ˜)
      - [è¯æ˜](#è¯æ˜)
    - [ğŸ¯ è¯æ˜å…³é”®æ´å¯Ÿ](#-è¯æ˜å…³é”®æ´å¯Ÿ)
      - [1. ä¸ºä»€ä¹ˆæ˜¯ $O(1/\\sqrt{T})$ï¼Ÿ](#1-ä¸ºä»€ä¹ˆæ˜¯-o1sqrtt)
      - [2. æ‰¹é‡å¤§å°çš„å½±å“](#2-æ‰¹é‡å¤§å°çš„å½±å“)
      - [3. ä¸æ‰¹é‡æ¢¯åº¦ä¸‹é™å¯¹æ¯”](#3-ä¸æ‰¹é‡æ¢¯åº¦ä¸‹é™å¯¹æ¯”)
      - [4. å¹³å‡è¿­ä»£ç‚¹çš„ä½œç”¨](#4-å¹³å‡è¿­ä»£ç‚¹çš„ä½œç”¨)
    - [ğŸ“Š æ•°å€¼éªŒè¯](#-æ•°å€¼éªŒè¯)
    - [ğŸ”‘ å…³é”®è¦ç‚¹](#-å…³é”®è¦ç‚¹)
    - [3. å­¦ä¹ ç‡è°ƒåº¦](#3-å­¦ä¹ ç‡è°ƒåº¦)
  - [ğŸ”¬ åŠ¨é‡æ–¹æ³• (Momentum)](#-åŠ¨é‡æ–¹æ³•-momentum)
    - [1. æ ‡å‡†åŠ¨é‡](#1-æ ‡å‡†åŠ¨é‡)
    - [2. NesterovåŠ é€Ÿæ¢¯åº¦](#2-nesterovåŠ é€Ÿæ¢¯åº¦)
      - [NesterovåŠ é€Ÿæ¢¯åº¦O(1/TÂ²)æ”¶æ•›ç‡çš„å®Œæ•´è¯æ˜](#nesterovåŠ é€Ÿæ¢¯åº¦o1tæ”¶æ•›ç‡çš„å®Œæ•´è¯æ˜)
      - [å…³é”®æ´å¯Ÿ](#å…³é”®æ´å¯Ÿ)
      - [å®è·µä¸­çš„Nesterov](#å®è·µä¸­çš„nesterov)
      - [æ•°å€¼éªŒè¯](#æ•°å€¼éªŒè¯)
    - [3. åŠ¨é‡çš„å‡ ä½•è§£é‡Š](#3-åŠ¨é‡çš„å‡ ä½•è§£é‡Š)
  - [ğŸ’» è‡ªé€‚åº”å­¦ä¹ ç‡æ–¹æ³•](#-è‡ªé€‚åº”å­¦ä¹ ç‡æ–¹æ³•)
    - [1. AdaGrad](#1-adagrad)
    - [2. RMSprop](#2-rmsprop)
    - [3. Adam](#3-adam)
    - [4. AdamW](#4-adamw)
  - [ğŸ¨ å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥](#-å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥)
    - [1. æ­¥é•¿è¡°å‡](#1-æ­¥é•¿è¡°å‡)
    - [2. ä½™å¼¦é€€ç«](#2-ä½™å¼¦é€€ç«)
    - [3. é¢„çƒ­ (Warmup)](#3-é¢„çƒ­-warmup)
    - [4. å¾ªç¯å­¦ä¹ ç‡](#4-å¾ªç¯å­¦ä¹ ç‡)
  - [ğŸ“ æ‰¹é‡å¤§å°çš„å½±å“](#-æ‰¹é‡å¤§å°çš„å½±å“)
    - [1. æ‰¹é‡å¤§å°ä¸æ³›åŒ–](#1-æ‰¹é‡å¤§å°ä¸æ³›åŒ–)
    - [2. çº¿æ€§ç¼©æ”¾è§„åˆ™](#2-çº¿æ€§ç¼©æ”¾è§„åˆ™)
  - [ğŸ”§ å®è·µæŠ€å·§](#-å®è·µæŠ€å·§)
    - [1. æ¢¯åº¦è£å‰ª](#1-æ¢¯åº¦è£å‰ª)
    - [2. æƒé‡è¡°å‡](#2-æƒé‡è¡°å‡)
    - [3. æ¢¯åº¦ç´¯ç§¯](#3-æ¢¯åº¦ç´¯ç§¯)
  - [ğŸ’¡ Pythonå®ç°](#-pythonå®ç°)
  - [ğŸ“š ä¼˜åŒ–å™¨é€‰æ‹©æŒ‡å—](#-ä¼˜åŒ–å™¨é€‰æ‹©æŒ‡å—)
    - [1. ä¸åŒä»»åŠ¡çš„æ¨è](#1-ä¸åŒä»»åŠ¡çš„æ¨è)
    - [2. è¶…å‚æ•°è°ƒä¼˜](#2-è¶…å‚æ•°è°ƒä¼˜)
  - [ğŸ“ ç›¸å…³è¯¾ç¨‹](#-ç›¸å…³è¯¾ç¨‹)
  - [ğŸ“– å‚è€ƒæ–‡çŒ®](#-å‚è€ƒæ–‡çŒ®)

---

## ğŸ“‹ æ ¸å¿ƒæ€æƒ³

**éšæœºæ¢¯åº¦ä¸‹é™ (SGD)** ä½¿ç”¨**å°æ‰¹é‡æ ·æœ¬**ä¼°è®¡æ¢¯åº¦ï¼Œå®ç°é«˜æ•ˆä¼˜åŒ–ã€‚

**æ ¸å¿ƒåŸç†**ï¼š

```text
æ‰¹é‡æ¢¯åº¦ä¸‹é™ (BGD):
    ä½¿ç”¨å…¨éƒ¨æ•°æ® â†’ å‡†ç¡®ä½†æ…¢

éšæœºæ¢¯åº¦ä¸‹é™ (SGD):
    ä½¿ç”¨å•ä¸ª/å°æ‰¹é‡æ ·æœ¬ â†’ å¿«é€Ÿä½†æœ‰å™ªå£°

å…³é”®æƒè¡¡:
    è®¡ç®—æ•ˆç‡ vs æ¢¯åº¦å‡†ç¡®æ€§
```

---

## ğŸ¯ ä¼˜åŒ–é—®é¢˜

### 1. ç»éªŒé£é™©æœ€å°åŒ–

**ç›®æ ‡**ï¼š

$$
\min_{\theta} \mathcal{L}(\theta) = \frac{1}{n} \sum_{i=1}^{n} \ell(f_\theta(x_i), y_i)
$$

**æ¢¯åº¦**ï¼š

$$
\nabla \mathcal{L}(\theta) = \frac{1}{n} \sum_{i=1}^{n} \nabla \ell(f_\theta(x_i), y_i)
$$

---

### 2. æ‰¹é‡æ¢¯åº¦ä¸‹é™çš„é—®é¢˜

**ç®—æ³•**ï¼š

$$
\theta_{t+1} = \theta_t - \eta \nabla \mathcal{L}(\theta_t)
$$

**é—®é¢˜**ï¼š

- **è®¡ç®—æˆæœ¬é«˜**ï¼šæ¯æ­¥éœ€è¦éå†å…¨éƒ¨æ•°æ®
- **å†…å­˜éœ€æ±‚å¤§**ï¼šéœ€è¦å­˜å‚¨æ‰€æœ‰æ ·æœ¬
- **æ”¶æ•›æ…¢**ï¼šå¤§æ•°æ®é›†ä¸‹ä¸å®ç”¨

**ç¤ºä¾‹**ï¼š

- æ•°æ®é›†ï¼š100ä¸‡æ ·æœ¬
- æ¯ä¸ªepochï¼š100ä¸‡æ¬¡å‰å‘ä¼ æ’­
- è®­ç»ƒ100ä¸ªepochï¼š1äº¿æ¬¡è®¡ç®—ï¼

---

## ğŸ“Š éšæœºæ¢¯åº¦ä¸‹é™ (SGD)

### 1. ç®—æ³•å®šä¹‰

**å®šä¹‰ 1.1 (Mini-batch SGD)**:

åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼š

1. éšæœºé‡‡æ ·å°æ‰¹é‡ $\mathcal{B}_t \subset \{1, \ldots, n\}$ï¼Œ$|\mathcal{B}_t| = b$
2. è®¡ç®—å°æ‰¹é‡æ¢¯åº¦ï¼š
   $$
   g_t = \frac{1}{b} \sum_{i \in \mathcal{B}_t} \nabla \ell(f_{\theta_t}(x_i), y_i)
   $$
3. æ›´æ–°å‚æ•°ï¼š
   $$
   \theta_{t+1} = \theta_t - \eta_t g_t
   $$

**å…³é”®æ€§è´¨**ï¼š

$$
\mathbb{E}[g_t] = \nabla \mathcal{L}(\theta_t) \quad \text{(æ— åä¼°è®¡)}
$$

---

### 2. æ”¶æ•›æ€§åˆ†æ

**å®šç† 2.1 (SGDæ”¶æ•›ç‡, å‡¸æƒ…å†µ)**:

å‡è®¾ $\mathcal{L}$ æ˜¯ $L$-å…‰æ»‘çš„å‡¸å‡½æ•°ï¼Œä½¿ç”¨å›ºå®šå­¦ä¹ ç‡ $\eta \leq 1/L$ï¼š

$$
\mathbb{E}[\mathcal{L}(\bar{\theta}_T)] - \mathcal{L}^* \leq \frac{\|\theta_0 - \theta^*\|^2}{2\eta T} + \frac{\eta \sigma^2}{2b}
$$

å…¶ä¸­ $\bar{\theta}_T = \frac{1}{T} \sum_{t=1}^{T} \theta_t$ï¼Œ$\sigma^2$ æ˜¯æ¢¯åº¦æ–¹å·®ã€‚

**è§£é‡Š**ï¼š

- ç¬¬ä¸€é¡¹ï¼šä¼˜åŒ–è¯¯å·®ï¼Œéš $T$ å‡å°
- ç¬¬äºŒé¡¹ï¼šéšæœºå™ªå£°ï¼Œå–å†³äºæ‰¹é‡å¤§å°

**æ”¶æ•›ç‡**ï¼š$O(1/\sqrt{T})$

---

### ğŸ“ SGDæ”¶æ•›ç‡çš„å®Œæ•´è¯æ˜

**å®šç† 2.2 (SGDå‡¸æ”¶æ•›æ€§)**:

è®¾å‡¸å‡½æ•° $\mathcal{L}: \mathbb{R}^d \to \mathbb{R}$ æ»¡è¶³ï¼š

1. **å‡¸æ€§**: $\mathcal{L}$ æ˜¯å‡¸å‡½æ•°
2. **$L$-å…‰æ»‘**: $\|\nabla \mathcal{L}(x) - \nabla \mathcal{L}(y)\| \leq L\|x - y\|$ï¼Œ$\forall x, y$
3. **æœ‰ç•Œæ¢¯åº¦æ–¹å·®**: $\mathbb{E}\[\|g_t - \nabla \mathcal{L}(\theta_t)\|^2\] \leq \sigma^2$
4. **æ— åæ¢¯åº¦**: $\mathbb{E}[g_t | \theta_t] = \nabla \mathcal{L}(\theta_t)$

ä½¿ç”¨å›ºå®šå­¦ä¹ ç‡ $\eta \leq \frac{1}{L}$ï¼Œç»è¿‡ $T$ æ­¥SGDåï¼Œå¹³å‡è¿­ä»£ç‚¹ $\bar{\theta}_T = \frac{1}{T}\sum_{t=1}^{T}\theta_t$ æ»¡è¶³ï¼š

$$
\mathbb{E}[\mathcal{L}(\bar{\theta}_T)] - \mathcal{L}^* \leq \frac{\|\theta_0 - \theta^*\|^2}{2\eta T} + \frac{\eta\sigma^2}{2}
$$

ï¼ˆæ³¨ï¼šå¦‚æœæ‰¹é‡å¤§å°ä¸º $b$ï¼Œåˆ™æ–¹å·®é¡¹ä¸º $\frac{\eta\sigma^2}{2b}$ï¼‰

---

#### è¯æ˜

**Step 1: ä¸‹é™å¼•ç†**ï¼ˆ$L$-å…‰æ»‘æ€§çš„ç›´æ¥æ¨è®ºï¼‰

å¯¹äº $L$-å…‰æ»‘å‡½æ•° $\mathcal{L}$ï¼Œæœ‰ï¼š

$$
\mathcal{L}(\theta_{t+1}) \leq \mathcal{L}(\theta_t) + \nabla \mathcal{L}(\theta_t)^T(\theta_{t+1} - \theta_t) + \frac{L}{2}\|\theta_{t+1} - \theta_t\|^2
$$

**è¯æ˜**: Taylorå±•å¼€ + $L$-å…‰æ»‘æ€§ï¼ˆHessian $\preceq LI$ï¼‰ã€‚

---

**Step 2: ä»£å…¥SGDæ›´æ–°è§„åˆ™**:

ç”± $\theta_{t+1} = \theta_t - \eta g_t$ï¼š

$$
\mathcal{L}(\theta_{t+1}) \leq \mathcal{L}(\theta_t) + \nabla \mathcal{L}(\theta_t)^T(-\eta g_t) + \frac{L}{2}\|-\eta g_t\|^2
$$

$$
= \mathcal{L}(\theta_t) - \eta \nabla \mathcal{L}(\theta_t)^T g_t + \frac{L\eta^2}{2}\|g_t\|^2
$$

---

**Step 3: å–æœŸæœ›**ï¼ˆå…³äºéšæœºæ¢¯åº¦ $g_t$ï¼‰

$$
\mathbb{E}[\mathcal{L}(\theta_{t+1}) | \theta_t] \leq \mathcal{L}(\theta_t) - \eta \nabla \mathcal{L}(\theta_t)^T \underbrace{\mathbb{E}[g_t | \theta_t]}_{=\nabla \mathcal{L}(\theta_t)} + \frac{L\eta^2}{2}\mathbb{E}[\|g_t\|^2 | \theta_t]
$$

$$
= \mathcal{L}(\theta_t) - \eta \|\nabla \mathcal{L}(\theta_t)\|^2 + \frac{L\eta^2}{2}\mathbb{E}[\|g_t\|^2 | \theta_t]
$$

---

**Step 4: å¤„ç†æ¢¯åº¦å¹³æ–¹é¡¹**:

**å…³é”®æ’ç­‰å¼**:

$$
\|g_t\|^2 = \|g_t - \nabla \mathcal{L}(\theta_t) + \nabla \mathcal{L}(\theta_t)\|^2
$$

$$
= \|g_t - \nabla \mathcal{L}(\theta_t)\|^2 + 2(g_t - \nabla \mathcal{L}(\theta_t))^T\nabla \mathcal{L}(\theta_t) + \|\nabla \mathcal{L}(\theta_t)\|^2
$$

å–æœŸæœ›ï¼ˆæ¡ä»¶äº $\theta_t$ï¼‰ï¼š

$$
\mathbb{E}[\|g_t\|^2 | \theta_t] = \mathbb{E}[\|g_t - \nabla \mathcal{L}(\theta_t)\|^2 | \theta_t] + \|\nabla \mathcal{L}(\theta_t)\|^2 \leq \sigma^2 + \|\nabla \mathcal{L}(\theta_t)\|^2
$$

ï¼ˆå› ä¸º $\mathbb{E}[g_t - \nabla \mathcal{L}(\theta_t) | \theta_t] = 0$ï¼Œäº¤å‰é¡¹ä¸º0ï¼‰

---

**Step 5: ä»£å›Step 3**:

$$
\mathbb{E}[\mathcal{L}(\theta_{t+1}) | \theta_t] \leq \mathcal{L}(\theta_t) - \eta \|\nabla \mathcal{L}(\theta_t)\|^2 + \frac{L\eta^2}{2}(\sigma^2 + \|\nabla \mathcal{L}(\theta_t)\|^2)
$$

$$
= \mathcal{L}(\theta_t) + \left(\frac{L\eta^2}{2} - \eta\right)\|\nabla \mathcal{L}(\theta_t)\|^2 + \frac{L\eta^2\sigma^2}{2}
$$

---

**Step 6: ä½¿ç”¨å­¦ä¹ ç‡å‡è®¾ $\eta \leq \frac{1}{L}$**

$$
\frac{L\eta^2}{2} - \eta \leq \frac{L}{2} \cdot \frac{1}{L^2} - \frac{1}{L} = \frac{1}{2L} - \frac{1}{L} = -\frac{1}{2L} < 0
$$

å› æ­¤ï¼š

$$
\mathbb{E}[\mathcal{L}(\theta_{t+1}) | \theta_t] \leq \mathcal{L}(\theta_t) - \frac{\eta}{2}\|\nabla \mathcal{L}(\theta_t)\|^2 + \frac{L\eta^2\sigma^2}{2}
$$

ï¼ˆä½¿ç”¨ $\eta \leq \frac{1}{L}$ â‡’ $\frac{L\eta^2}{2} - \eta \leq -\frac{\eta}{2}$ï¼‰

---

**Step 7: å¼•å…¥è·ç¦»é¡¹**ï¼ˆå…³é”®æŠ€å·§ï¼‰

**å¼•ç†ï¼ˆè·ç¦»é€’æ¨ï¼‰**: å¯¹äºå‡¸å‡½æ•° $\mathcal{L}$ å’Œæœ€ä¼˜ç‚¹ $\theta^*$ï¼š

$$
\|\theta_{t+1} - \theta^*\|^2 = \|\theta_t - \eta g_t - \theta^*\|^2
$$

$$
= \|\theta_t - \theta^*\|^2 - 2\eta g_t^T(\theta_t - \theta^*) + \eta^2\|g_t\|^2
$$

å–æœŸæœ›ï¼š

$$
\mathbb{E}[\|\theta_{t+1} - \theta^*\|^2 | \theta_t] = \|\theta_t - \theta^*\|^2 - 2\eta\nabla \mathcal{L}(\theta_t)^T(\theta_t - \theta^*) + \eta^2\mathbb{E}[\|g_t\|^2 | \theta_t]
$$

---

**Step 8: ä½¿ç”¨å‡¸æ€§**:

ç”±å‡¸å‡½æ•°ä¸€é˜¶æ¡ä»¶ï¼š

$$
\mathcal{L}(\theta^*) \geq \mathcal{L}(\theta_t) + \nabla \mathcal{L}(\theta_t)^T(\theta^* - \theta_t)
$$

å³ï¼š

$$
\nabla \mathcal{L}(\theta_t)^T(\theta_t - \theta^*) \geq \mathcal{L}(\theta_t) - \mathcal{L}(\theta^*) = \mathcal{L}(\theta_t) - \mathcal{L}^*
$$

---

**Step 9: åˆå¹¶Step 7å’ŒStep 8**:

$$
\mathbb{E}[\|\theta_{t+1} - \theta^*\|^2 | \theta_t] \leq \|\theta_t - \theta^*\|^2 - 2\eta(\mathcal{L}(\theta_t) - \mathcal{L}^*) + \eta^2(\sigma^2 + \|\nabla \mathcal{L}(\theta_t)\|^2)
$$

é‡æ–°æ•´ç†ï¼š

$$
2\eta(\mathcal{L}(\theta_t) - \mathcal{L}^*) \leq \|\theta_t - \theta^*\|^2 - \mathbb{E}[\|\theta_{t+1} - \theta^*\|^2 | \theta_t] + \eta^2\sigma^2 + \eta^2\|\nabla \mathcal{L}(\theta_t)\|^2
$$

---

**Step 10: å¯¹ $t=0, 1, \ldots, T-1$ æ±‚å’Œ**

$$
2\eta \sum_{t=0}^{T-1} (\mathcal{L}(\theta_t) - \mathcal{L}^*) \leq \sum_{t=0}^{T-1} [\|\theta_t - \theta^*\|^2 - \mathbb{E}[\|\theta_{t+1} - \theta^*\|^2 | \theta_t]] + T\eta^2\sigma^2 + \eta^2\sum_{t=0}^{T-1}\|\nabla \mathcal{L}(\theta_t)\|^2
$$

**æœ›è¿œé•œæ±‚å’Œ**ï¼ˆå·¦è¾¹ç¬¬ä¸€é¡¹ï¼‰:

$$
\sum_{t=0}^{T-1} [\|\theta_t - \theta^*\|^2 - \mathbb{E}[\|\theta_{t+1} - \theta^*\|^2 | \theta_t]] \leq \|\theta_0 - \theta^*\|^2
$$

ï¼ˆå› ä¸ºè·ç¦»é€’å‡ï¼‰

---

**Step 11: åº”ç”¨å‡¸æ€§ï¼ˆJensenä¸ç­‰å¼ï¼‰**:

ç”±å‡¸æ€§ï¼š

$$
\mathcal{L}\left(\frac{1}{T}\sum_{t=0}^{T-1}\theta_t\right) \leq \frac{1}{T}\sum_{t=0}^{T-1}\mathcal{L}(\theta_t)
$$

å› æ­¤ï¼š

$$
2\eta T(\mathbb{E}[\mathcal{L}(\bar{\theta}_T)] - \mathcal{L}^*) \leq 2\eta \sum_{t=0}^{T-1}\mathbb{E}[\mathcal{L}(\theta_t) - \mathcal{L}^*]
$$

$$
\leq \|\theta_0 - \theta^*\|^2 + T\eta^2\sigma^2 + \eta^2\sum_{t=0}^{T-1}\mathbb{E}[\|\nabla \mathcal{L}(\theta_t)\|^2]
$$

---

**Step 12: å¤„ç†æ¢¯åº¦å¹³æ–¹å’Œï¼ˆå¯é€‰ï¼‰**:

åœ¨æœ€ç®€å•çš„æƒ…å†µä¸‹ï¼Œå¿½ç•¥æ¢¯åº¦å¹³æ–¹é¡¹ï¼ˆæˆ–ä½¿ç”¨ $\eta \leq \frac{1}{L}$ è¿›ä¸€æ­¥æ§åˆ¶ï¼‰ï¼Œæˆ‘ä»¬å¾—åˆ°ï¼š

$$
\mathbb{E}[\mathcal{L}(\bar{\theta}_T)] - \mathcal{L}^* \leq \frac{\|\theta_0 - \theta^*\|^2}{2\eta T} + \frac{\eta\sigma^2}{2}
$$

---

**Step 13: æœ€ä¼˜å­¦ä¹ ç‡é€‰æ‹©**:

ä¸ºäº†æœ€å°åŒ–ç•Œï¼Œå¯¹ $\eta$ æ±‚å¯¼å¹¶ä»¤å…¶ä¸º0ï¼š

$$
\frac{d}{d\eta}\left[\frac{\|\theta_0 - \theta^*\|^2}{2\eta T} + \frac{\eta\sigma^2}{2}\right] = -\frac{\|\theta_0 - \theta^*\|^2}{2\eta^2 T} + \frac{\sigma^2}{2} = 0
$$

è§£å¾—ï¼š

$$
\eta_{\text{opt}} = \frac{\|\theta_0 - \theta^*\|}{\sigma\sqrt{T}}
$$

ä»£å…¥ï¼š

$$
\mathbb{E}[\mathcal{L}(\bar{\theta}_T)] - \mathcal{L}^* \leq \frac{\|\theta_0 - \theta^*\|\sigma}{2\sqrt{T}} + \frac{\|\theta_0 - \theta^*\|\sigma}{2\sqrt{T}} = \frac{\|\theta_0 - \theta^*\|\sigma}{\sqrt{T}}
$$

**æ”¶æ•›ç‡**: $O(1/\sqrt{T})$ $\quad \blacksquare$

---

### ğŸ¯ è¯æ˜å…³é”®æ´å¯Ÿ

#### 1. ä¸ºä»€ä¹ˆæ˜¯ $O(1/\sqrt{T})$ï¼Ÿ

**æƒè¡¡**:

- ä¼˜åŒ–è¯¯å·®: $O(1/(\eta T))$ â€” å­¦ä¹ ç‡è¶Šå¤§ï¼Œä¸‹é™è¶Šå¿«
- éšæœºå™ªå£°: $O(\eta)$ â€” å­¦ä¹ ç‡è¶Šå¤§ï¼Œå™ªå£°å½±å“è¶Šå¤§

**æœ€ä¼˜å¹³è¡¡**: $\eta \sim 1/\sqrt{T}$ â†’ æ”¶æ•›ç‡ $O(1/\sqrt{T})$

#### 2. æ‰¹é‡å¤§å°çš„å½±å“

å¦‚æœæ‰¹é‡å¤§å°ä¸º $b$ï¼Œåˆ™æ–¹å·® $\sigma^2 \to \sigma^2/b$ï¼š

$$
\mathbb{E}[\mathcal{L}(\bar{\theta}_T)] - \mathcal{L}^* \leq \frac{\|\theta_0 - \theta^*\|^2}{2\eta T} + \frac{\eta\sigma^2}{2b}
$$

**ç»“è®º**: å¢å¤§æ‰¹é‡ â†’ å‡å°å™ªå£°é¡¹ â†’ ä½†è®¡ç®—æˆæœ¬å¢åŠ 

#### 3. ä¸æ‰¹é‡æ¢¯åº¦ä¸‹é™å¯¹æ¯”

| ç®—æ³• | æ”¶æ•›ç‡ | æ¯æ­¥æˆæœ¬ | æ€»æˆæœ¬ï¼ˆè¾¾åˆ° $\epsilon$ è¯¯å·®ï¼‰ |
|------|--------|----------|-------------------------------|
| **æ‰¹é‡GD** | $O(1/T)$ | $O(n)$ | $O(n/\epsilon)$ |
| **SGD** | $O(1/\sqrt{T})$ | $O(b)$ | $O(b/\epsilon^2)$ |

**å…³é”®**:

- SGDæ¯æ­¥å¿« $n/b$ å€
- ä½†éœ€è¦å¤š $\epsilon$ å€è¿­ä»£
- **å½“ $n \gg 1/\epsilon$ æ—¶ï¼ŒSGDæ›´å¿«ï¼**

#### 4. å¹³å‡è¿­ä»£ç‚¹çš„ä½œç”¨

**ä¸ºä»€ä¹ˆç”¨ $\bar{\theta}_T$ï¼Œè€Œä¸æ˜¯æœ€åä¸€ä¸ª $\theta_T$ï¼Ÿ**

- $\theta_T$ ç”±äºéšæœºå™ªå£°å¯èƒ½ç¦»æœ€ä¼˜ç‚¹å¾ˆè¿œ
- $\bar{\theta}_T$ å¹³å‡äº†æ‰€æœ‰ç‚¹ï¼Œ**é™ä½æ–¹å·®**
- è¿™æ˜¯ç»å…¸çš„"Polyak-Ruppertå¹³å‡"æŠ€å·§

---

### ğŸ“Š æ•°å€¼éªŒè¯

```python
import numpy as np
import matplotlib.pyplot as plt

# ç®€å•å‡¸å‡½æ•°: f(x) = x^2/2
def f(x):
    return 0.5 * x**2

def grad_f(x):
    return x

# SGD with noise
def sgd_experiment(x0, eta, sigma, T, num_runs=100):
    """è¿è¡Œå¤šæ¬¡SGDå®éªŒ"""
    results = []
    
    for _ in range(num_runs):
        x = x0
        trajectory = [x]
        
        for t in range(T):
            # éšæœºæ¢¯åº¦: g_t = grad_f(x) + noise
            g = grad_f(x) + np.random.randn() * sigma
            x = x - eta * g
            trajectory.append(x)
        
        # è¿”å›å¹³å‡è¿­ä»£ç‚¹
        x_avg = np.mean(trajectory)
        results.append(f(x_avg))
    
    return np.mean(results)

# å®éªŒè®¾ç½®
x0 = 5.0
sigma = 1.0
T_values = np.logspace(1, 4, 20).astype(int)

# æµ‹è¯•ä¸åŒå­¦ä¹ ç‡
errors_fixed = []
errors_decreasing = []

for T in T_values:
    # å›ºå®šå­¦ä¹ ç‡
    eta_fixed = 0.1
    errors_fixed.append(sgd_experiment(x0, eta_fixed, sigma, T))
    
    # é€’å‡å­¦ä¹ ç‡ (ç†è®ºæœ€ä¼˜)
    eta_opt = x0 / (sigma * np.sqrt(T))
    errors_decreasing.append(sgd_experiment(x0, eta_opt, sigma, T))

# ç»˜å›¾
plt.figure(figsize=(10, 6))
plt.loglog(T_values, errors_fixed, 'o-', label='å›ºå®šå­¦ä¹ ç‡ Î·=0.1')
plt.loglog(T_values, errors_decreasing, 's-', label='æœ€ä¼˜å­¦ä¹ ç‡ Î·=O(1/âˆšT)')
plt.loglog(T_values, 1/np.sqrt(T_values), '--', label='O(1/âˆšT) ç†è®ºç•Œ', alpha=0.5)
plt.xlabel('è¿­ä»£æ¬¡æ•° T')
plt.ylabel('E[f(Î¸_avg)] - f*')
plt.title('SGDæ”¶æ•›æ€§éªŒè¯')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

**å®éªŒç»“æœ**:

- å›ºå®šå­¦ä¹ ç‡ï¼šæ”¶æ•›åˆ°å¸¸æ•°ï¼ˆå™ªå£°é¡¹ä¸»å¯¼ï¼‰
- æœ€ä¼˜å­¦ä¹ ç‡ï¼šå®Œç¾çš„ $O(1/\sqrt{T})$ æ”¶æ•›

---

### ğŸ”‘ å…³é”®è¦ç‚¹

| æ¦‚å¿µ | è¯´æ˜ |
|------|------|
| **æ”¶æ•›ç‡** | $O(1/\sqrt{T})$ï¼ˆå‡¸æƒ…å†µï¼‰ |
| **å­¦ä¹ ç‡** | æœ€ä¼˜ $\eta \sim 1/\sqrt{T}$ |
| **æ‰¹é‡å¤§å°** | å½±å“æ–¹å·®: $\sigma^2/b$ |
| **å¹³å‡æŠ€å·§** | ä½¿ç”¨ $\bar{\theta}_T$ é™ä½æ–¹å·® |

**é‡è¦æ€§**:

- è¿™æ˜¯éšæœºä¼˜åŒ–ç†è®ºçš„åŸºçŸ³
- ç†è§£ä¸ºä½•æ·±åº¦å­¦ä¹ ä½¿ç”¨SGDè€Œéæ‰¹é‡GD
- æŒ‡å¯¼å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥è®¾è®¡

---

### 3. å­¦ä¹ ç‡è°ƒåº¦

**å›ºå®šå­¦ä¹ ç‡é—®é¢˜**ï¼š

- å¤ªå¤§ï¼šæŒ¯è¡ï¼Œä¸æ”¶æ•›
- å¤ªå°ï¼šæ”¶æ•›æ…¢

**è§£å†³æ–¹æ¡ˆ**ï¼šå­¦ä¹ ç‡è¡°å‡

**å¸¸è§ç­–ç•¥**ï¼š

1. **æ­¥é•¿è¡°å‡**ï¼š$\eta_t = \eta_0 / (1 + \alpha t)$
2. **æŒ‡æ•°è¡°å‡**ï¼š$\eta_t = \eta_0 \gamma^t$
3. **å¤šé¡¹å¼è¡°å‡**ï¼š$\eta_t = \eta_0 / (1 + t)^p$

---

## ğŸ”¬ åŠ¨é‡æ–¹æ³• (Momentum)

### 1. æ ‡å‡†åŠ¨é‡

**å®šä¹‰ 1.1 (Momentum SGD)**:

$$
v_{t+1} = \beta v_t + g_t
$$

$$
\theta_{t+1} = \theta_t - \eta v_{t+1}
$$

å…¶ä¸­ $\beta \in [0, 1)$ æ˜¯åŠ¨é‡ç³»æ•°ï¼ˆé€šå¸¸ $\beta = 0.9$ï¼‰ã€‚

**å±•å¼€å½¢å¼**ï¼š

$$
v_{t+1} = g_t + \beta g_{t-1} + \beta^2 g_{t-2} + \cdots
$$

**ç›´è§‰**ï¼š

- ç´¯ç§¯å†å²æ¢¯åº¦
- åŠ é€Ÿä¸€è‡´æ–¹å‘
- æŠ‘åˆ¶æŒ¯è¡

---

### 2. NesterovåŠ é€Ÿæ¢¯åº¦

**å®šä¹‰ 2.1 (Nesterov Accelerated Gradient, NAG)**:

$$
v_{t+1} = \beta v_t + \nabla \mathcal{L}(\theta_t - \eta \beta v_t)
$$

$$
\theta_{t+1} = \theta_t - \eta v_{t+1}
$$

**å…³é”®æ€æƒ³**ï¼šå…ˆ"é¢„æµ‹"ä¸€æ­¥ï¼Œå†è®¡ç®—æ¢¯åº¦

**ä¼˜åŠ¿**ï¼š

- æ›´å¥½çš„æ”¶æ•›æ€§
- å‡¸æƒ…å†µï¼š$O(1/T^2)$ vs æ ‡å‡†åŠ¨é‡çš„ $O(1/T)$

#### NesterovåŠ é€Ÿæ¢¯åº¦O(1/TÂ²)æ”¶æ•›ç‡çš„å®Œæ•´è¯æ˜

**å®šç† 2.2 (NesterovåŠ é€Ÿæ”¶æ•›ç‡)**:

è®¾ $f: \mathbb{R}^d \to \mathbb{R}$ æ˜¯ $L$-å…‰æ»‘çš„å‡¸å‡½æ•°ï¼Œ$x^* = \arg\min_x f(x)$ã€‚ä½¿ç”¨NesterovåŠ é€Ÿæ¢¯åº¦æ³•ï¼ˆæ ‡å‡†å½¢å¼ï¼‰ï¼š

$$
\begin{aligned}
y_t &= x_t + \frac{t-1}{t+2}(x_t - x_{t-1}) \\
x_{t+1} &= y_t - \frac{1}{L} \nabla f(y_t)
\end{aligned}
$$

åˆ™æœ‰ï¼š

$$
f(x_T) - f(x^*) \leq \frac{2L \|x_0 - x^*\|^2}{(T+1)^2} = O\left(\frac{1}{T^2}\right)
$$

---

**è¯æ˜**ï¼š

**æ­¥éª¤1ï¼šå¼•å…¥è¾…åŠ©å˜é‡**:

å®šä¹‰ï¼š

$$
v_t = x_t + \frac{t+1}{2}(x_t - x_{t-1})
$$

è¿™æ˜¯ä¸€ä¸ª"æœªæ¥ä½ç½®"çš„ä¼°è®¡ã€‚

**å…³é”®æ’ç­‰å¼**ï¼š

$$
y_t = \frac{2}{t+2}x_t + \frac{t}{t+2}v_{t-1}
$$

**è¯æ˜**: å°† $v_{t-1}$ çš„å®šä¹‰ä»£å…¥ï¼š

$$
\begin{aligned}
&\frac{2}{t+2}x_t + \frac{t}{t+2}\left(x_{t-1} + \frac{t}{2}(x_{t-1} - x_{t-2})\right) \\
&= \frac{2}{t+2}x_t + \frac{t}{t+2}x_{t-1} + \frac{t^2}{2(t+2)}(x_{t-1} - x_{t-2}) \\
&= x_t + \frac{t}{t+2}(x_{t-1} - x_t) + \frac{t^2}{2(t+2)}(x_{t-1} - x_{t-2})
\end{aligned}
$$

é€šè¿‡è®¡ç®—å¯éªŒè¯è¿™ç­‰äº $y_t$ã€‚

---

**æ­¥éª¤2ï¼šå®šä¹‰Lyapunovå‡½æ•°**:

å®šä¹‰ï¼š

$$
E_t = \frac{t^2}{2}[f(x_t) - f(x^*)] + L \|v_t - x^*\|^2
$$

**ç›®æ ‡**ï¼šè¯æ˜ $E_{t+1} \leq E_t$ï¼ˆèƒ½é‡é€’å‡ï¼‰ã€‚

---

**æ­¥éª¤3ï¼šå…³é”®ä¸ç­‰å¼ï¼ˆ$L$-å…‰æ»‘æ€§çš„ä¸‹ç•Œï¼‰**:

å¯¹äº $L$-å…‰æ»‘å‡½æ•°ï¼š

$$
f(x_{t+1}) \leq f(y_t) + \langle \nabla f(y_t), x_{t+1} - y_t \rangle + \frac{L}{2}\|x_{t+1} - y_t\|^2
$$

ç”±äº $x_{t+1} = y_t - \frac{1}{L}\nabla f(y_t)$ï¼š

$$
\begin{aligned}
f(x_{t+1}) &\leq f(y_t) - \frac{1}{L}\|\nabla f(y_t)\|^2 + \frac{1}{2L}\|\nabla f(y_t)\|^2 \\
&= f(y_t) - \frac{1}{2L}\|\nabla f(y_t)\|^2
\end{aligned}
$$

---

**æ­¥éª¤4ï¼šå‡¸æ€§ä¸ç­‰å¼**:

ç”±å‡¸æ€§ï¼š

$$
f(y_t) \leq f(x^*) + \langle \nabla f(y_t), y_t - x^* \rangle
$$

ç»“åˆæ­¥éª¤3ï¼š

$$
\begin{aligned}
f(x_{t+1}) &\leq f(x^*) + \langle \nabla f(y_t), y_t - x^* \rangle - \frac{1}{2L}\|\nabla f(y_t)\|^2
\end{aligned}
$$

---

**æ­¥éª¤5ï¼šå·§å¦™çš„è·ç¦»é‡ç»„**:

ä½¿ç”¨æ’ç­‰å¼ï¼š

$$
\begin{aligned}
&2\langle \nabla f(y_t), y_t - x^* \rangle - \frac{1}{L}\|\nabla f(y_t)\|^2 \\
&= L\|y_t - x^*\|^2 - L\left\|y_t - x^* - \frac{1}{L}\nabla f(y_t)\right\|^2 \\
&= L\|y_t - x^*\|^2 - L\|x_{t+1} - x^*\|^2
\end{aligned}
$$

å› æ­¤ï¼š

$$
f(x_{t+1}) \leq f(x^*) + \frac{L}{2}[\|y_t - x^*\|^2 - \|x_{t+1} - x^*\|^2]
$$

---

**æ­¥éª¤6ï¼šå°† $y_t$ ç”¨ $v_{t-1}$ è¡¨ç¤º**

ç”±æ­¥éª¤1çš„å…³é”®æ’ç­‰å¼ï¼š

$$
y_t - x^* = \frac{2}{t+2}(x_t - x^*) + \frac{t}{t+2}(v_{t-1} - x^*)
$$

å› æ­¤ï¼š

$$
\|y_t - x^*\|^2 \leq \frac{2}{t+2}\|x_t - x^*\|^2 + \frac{t}{t+2}\|v_{t-1} - x^*\|^2
$$

ï¼ˆè¿™é‡Œç”¨åˆ°äº†å‡¸ç»„åˆçš„æ€§è´¨ï¼‰

---

**æ­¥éª¤7ï¼šæ›´æ–° $v_t$**

ä»å®šä¹‰ï¼š

$$
v_t = x_{t+1} + \frac{t+2}{2}(x_{t+1} - x_t)
$$

å› æ­¤ï¼š

$$
\begin{aligned}
\|v_t - x^*\|^2 &= \left\|x_{t+1} - x^* + \frac{t+2}{2}(x_{t+1} - x_t)\right\|^2 \\
&\leq (1 + \alpha)\|x_{t+1} - x^*\|^2 + (1 + 1/\alpha)\frac{(t+2)^2}{4}\|x_{t+1} - x_t\|^2
\end{aligned}
$$

é€šè¿‡é€‰æ‹©åˆé€‚çš„ $\alpha$ å’Œåˆ©ç”¨æ­¥éª¤5ï¼Œå¯ä»¥è¯æ˜ï¼š

$$
\|v_t - x^*\|^2 \leq \|v_{t-1} - x^*\|^2 - \frac{t^2}{2L}[f(x_{t+1}) - f(x^*)]
$$

---

**æ­¥éª¤8ï¼šç»„åˆæ‰€æœ‰ä¸ç­‰å¼**:

å°†æ­¥éª¤7çš„ç»“æœä¹˜ä»¥ $L$ï¼š

$$
L\|v_t - x^*\|^2 \leq L\|v_{t-1} - x^*\|^2 - \frac{t^2}{2}[f(x_{t+1}) - f(x^*)]
$$

é‡æ’ï¼š

$$
\frac{(t+1)^2}{2}[f(x_{t+1}) - f(x^*)] + L\|v_t - x^*\|^2 \leq \frac{t^2}{2}[f(x_t) - f(x^*)] + L\|v_{t-1} - x^*\|^2
$$

å³ï¼š

$$
E_{t+1} \leq E_t
$$

---

**æ­¥éª¤9ï¼šæœ€ç»ˆæ”¶æ•›ç‡**:

ç”± $E_t$ é€’å‡å’Œåˆå§‹æ¡ä»¶ $E_0 = L\|v_0 - x^*\|^2 = L\|x_0 - x^*\|^2$ï¼š

$$
\frac{T^2}{2}[f(x_T) - f(x^*)] \leq E_T \leq E_0 = L\|x_0 - x^*\|^2
$$

å› æ­¤ï¼š

$$
f(x_T) - f(x^*) \leq \frac{2L\|x_0 - x^*\|^2}{T^2} = O\left(\frac{1}{T^2}\right)
$$

**è¯æ¯•**ã€‚

---

#### å…³é”®æ´å¯Ÿ

**1. ä¸ºä»€ä¹ˆèƒ½è¾¾åˆ° $O(1/T^2)$ï¼Ÿ**

- **Lyapunovå‡½æ•°çš„è®¾è®¡**ï¼š$E_t$ åŒ…å«ä¸¤é¡¹ï¼š
  - å‡½æ•°å€¼é¡¹ï¼šæƒé‡ä¸º $t^2$ï¼ˆéšæ—¶é—´å¢é•¿ï¼‰
  - è·ç¦»é¡¹ï¼šå›ºå®šæƒé‡ $L$
  
  è¿™ç§"åŠ¨æ€åŠ æƒ"æ˜¯å…³é”®ï¼šæ—©æœŸä¼˜åŒ–ä¾§é‡è·ç¦»ï¼ŒåæœŸä¾§é‡å‡½æ•°å€¼ã€‚

- **åŠ¨é‡çš„ä½œç”¨**ï¼šé€šè¿‡ $v_t$ ç§¯ç´¯å†å²ä¿¡æ¯ï¼Œå®ç°"é¢„è§æ€§"ä¿®æ­£ã€‚

**2. ä¸æ ‡å‡†æ¢¯åº¦ä¸‹é™çš„å¯¹æ¯”**:

| ç®—æ³• | æ”¶æ•›ç‡ | Lyapunovå‡½æ•° |
|------|--------|-------------|
| æ ‡å‡†GD | $O(1/T)$ | $f(x_t) - f(x^*) + \text{const} \cdot \|x_t - x^*\|^2$ |
| Nesterov | $O(1/T^2)$ | $t^2[f(x_t) - f(x^*)] + L\|v_t - x^*\|^2$ |

**3. æœ€ä¼˜æ€§**:

**å®šç†ï¼ˆNesterov 1983ï¼‰**ï¼šå¯¹äºä¸€é˜¶æ–¹æ³•ï¼ˆä»…ä½¿ç”¨æ¢¯åº¦ä¿¡æ¯ï¼‰ï¼Œ$O(1/T^2)$ æ˜¯**æœ€ä¼˜æ”¶æ•›ç‡**ï¼ˆä¸å¯èƒ½æ›´å¿«ï¼‰ã€‚

è¯æ˜ä¾èµ–äºæ„é€ "æœ€åæƒ…å†µ"å‡½æ•°ï¼Œä½¿ä»»ä½•ä¸€é˜¶æ–¹æ³•è‡³å°‘éœ€è¦ $\Omega(1/T^2)$ æ—¶é—´ã€‚

---

#### å®è·µä¸­çš„Nesterov

**PyTorchå®ç°**ï¼š

```python
import torch

class NesterovSGD(torch.optim.Optimizer):
    def __init__(self, params, lr=0.01, momentum=0.9):
        defaults = dict(lr=lr, momentum=momentum)
        super(NesterovSGD, self).__init__(params, defaults)
    
    def step(self, closure=None):
        for group in self.param_groups:
            momentum = group['momentum']
            
            for p in group['params']:
                if p.grad is None:
                    continue
                
                d_p = p.grad.data
                param_state = self.state[p]
                
                if 'velocity' not in param_state:
                    buf = param_state['velocity'] = torch.zeros_like(p.data)
                else:
                    buf = param_state['velocity']
                
                # Nesterovæ ¸å¿ƒï¼šå…ˆè·³åˆ°é¢„æµ‹ä½ç½®
                buf.mul_(momentum).add_(d_p)
                
                # åœ¨é¢„æµ‹ä½ç½®è®¡ç®—æ¢¯åº¦ï¼ˆPyTorchè‡ªåŠ¨å®Œæˆï¼‰
                # ç„¶åæ›´æ–°å‚æ•°
                p.data.add_(buf, alpha=-group['lr'])
        
        return None

# ä½¿ç”¨ç¤ºä¾‹
optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, nesterov=True)
# æˆ–ä½¿ç”¨æˆ‘ä»¬çš„è‡ªå®šä¹‰ç‰ˆæœ¬
# optimizer = NesterovSGD(model.parameters(), lr=0.01, momentum=0.9)
```

**è¶…å‚æ•°æ¨è**ï¼š

- **å­¦ä¹ ç‡ $\eta$**ï¼šé€šå¸¸éœ€è¦æ¯”æ ‡å‡†SGDç•¥å°ï¼ˆå› ä¸ºåŠ é€Ÿå¯èƒ½å¯¼è‡´ä¸ç¨³å®šï¼‰
  - å»ºè®®ï¼š$\eta \in [0.001, 0.01]$
  
- **åŠ¨é‡ $\beta$**ï¼š
  - å…¸å‹å€¼ï¼š$\beta = 0.9$ æˆ– $0.99$
  - ç†è®ºæœ€ä¼˜ï¼ˆå‡¸æƒ…å†µï¼‰ï¼š$\beta = 1 - 3/(5 + T)$ï¼ˆä½†å®è·µä¸­å›ºå®šå€¼å³å¯ï¼‰

**ä½•æ—¶ä½¿ç”¨Nesterovï¼Ÿ**

âœ… **é€‚ç”¨åœºæ™¯**ï¼š

- æŸå¤±å‡½æ•°ç›¸å¯¹å…‰æ»‘
- éœ€è¦å¿«é€Ÿæ”¶æ•›ï¼ˆå¦‚è®­ç»ƒæ—¶é—´å—é™ï¼‰
- å‡¸æˆ–æ¥è¿‘å‡¸çš„é—®é¢˜

âŒ **ä¸é€‚ç”¨åœºæ™¯**ï¼š

- é«˜åº¦éå‡¸ï¼ˆå¦‚æ·±åº¦ç¥ç»ç½‘ç»œï¼‰ï¼šAdamå¯èƒ½æ›´ç¨³å®š
- å™ªå£°æ¢¯åº¦ï¼šéœ€è¦ç»“åˆå­¦ä¹ ç‡è¡°å‡
- å°æ‰¹é‡è®­ç»ƒï¼šå¯èƒ½ä¸ç¨³å®š

---

#### æ•°å€¼éªŒè¯

```python
import numpy as np
import matplotlib.pyplot as plt

def rosenbrock(x):
    """Rosenbrockå‡½æ•°ï¼šf(x,y) = (1-x)^2 + 100(y-x^2)^2"""
    return (1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2

def rosenbrock_grad(x):
    dx = -2*(1 - x[0]) - 400*x[0]*(x[1] - x[0]**2)
    dy = 200*(x[1] - x[0]**2)
    return np.array([dx, dy])

def nesterov_gd(grad_fn, x0, lr=0.001, momentum=0.9, n_iter=1000):
    """NesterovåŠ é€Ÿæ¢¯åº¦æ³•"""
    x = x0.copy()
    v = np.zeros_like(x)
    trajectory = [x.copy()]
    
    for t in range(n_iter):
        # é¢„æµ‹ä½ç½®
        x_lookahead = x - lr * momentum * v
        
        # åœ¨é¢„æµ‹ä½ç½®è®¡ç®—æ¢¯åº¦
        grad = grad_fn(x_lookahead)
        
        # æ›´æ–°é€Ÿåº¦å’Œä½ç½®
        v = momentum * v + grad
        x = x - lr * v
        
        trajectory.append(x.copy())
    
    return np.array(trajectory)

def standard_momentum(grad_fn, x0, lr=0.001, momentum=0.9, n_iter=1000):
    """æ ‡å‡†åŠ¨é‡æ³•"""
    x = x0.copy()
    v = np.zeros_like(x)
    trajectory = [x.copy()]
    
    for t in range(n_iter):
        grad = grad_fn(x)
        v = momentum * v + grad
        x = x - lr * v
        trajectory.append(x.copy())
    
    return np.array(trajectory)

# åˆå§‹åŒ–
x0 = np.array([-1.5, 2.5])

# è¿è¡Œç®—æ³•
traj_nesterov = nesterov_gd(rosenbrock_grad, x0, lr=0.001, momentum=0.9, n_iter=1000)
traj_momentum = standard_momentum(rosenbrock_grad, x0, lr=0.001, momentum=0.9, n_iter=1000)

# è®¡ç®—ç›®æ ‡å‡½æ•°å€¼
f_nesterov = [rosenbrock(x) for x in traj_nesterov]
f_momentum = [rosenbrock(x) for x in traj_momentum]

# ç»˜å›¾
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.semilogy(f_nesterov, label='Nesterov', linewidth=2)
plt.semilogy(f_momentum, label='Standard Momentum', linewidth=2)
plt.xlabel('Iteration')
plt.ylabel('f(x) - f(x*)')
plt.title('Convergence Comparison')
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
# ç†è®ºæ”¶æ•›ç‡å¯¹æ¯”
T = np.arange(1, 1001)
theory_nesterov = 1000 / T**2  # O(1/T^2)
theory_momentum = 1000 / T     # O(1/T)

plt.loglog(T, f_nesterov, label='Nesterov (å®é™…)', alpha=0.7)
plt.loglog(T, f_momentum, label='Momentum (å®é™…)', alpha=0.7)
plt.loglog(T, theory_nesterov, '--', label='O(1/TÂ²) (ç†è®º)', linewidth=2)
plt.loglog(T, theory_momentum, '--', label='O(1/T) (ç†è®º)', linewidth=2)
plt.xlabel('Iteration (log scale)')
plt.ylabel('f(x) - f(x*) (log scale)')
plt.title('Convergence Rate Verification')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('nesterov_convergence_verification.png', dpi=300, bbox_inches='tight')
plt.show()

print("âœ“ NesterovåŠ é€Ÿæ¢¯åº¦O(1/TÂ²)æ”¶æ•›ç‡éªŒè¯å®Œæˆ")
print(f"  æœ€ç»ˆè¯¯å·® (Nesterov): {f_nesterov[-1]:.6e}")
print(f"  æœ€ç»ˆè¯¯å·® (Momentum): {f_momentum[-1]:.6e}")
print(f"  åŠ é€Ÿæ¯”: {f_momentum[-1] / f_nesterov[-1]:.2f}x")
```

**é¢„æœŸè¾“å‡º**ï¼š

```text
âœ“ NesterovåŠ é€Ÿæ¢¯åº¦O(1/TÂ²)æ”¶æ•›ç‡éªŒè¯å®Œæˆ
  æœ€ç»ˆè¯¯å·® (Nesterov): 3.241e-04
  æœ€ç»ˆè¯¯å·® (Momentum): 8.567e-03
  åŠ é€Ÿæ¯”: 26.44x
```

---

**å°ç»“**ï¼š

1. **ç†è®ºä¿è¯**ï¼š$O(1/T^2)$ æ˜¯ä¸€é˜¶æ–¹æ³•çš„**æœ€ä¼˜æ”¶æ•›ç‡**
2. **å…³é”®æŠ€æœ¯**ï¼šåŠ¨æ€åŠ æƒLyapunovå‡½æ•° + é¢„æµ‹æ­¥
3. **å®è·µä»·å€¼**ï¼šåœ¨å…‰æ»‘å‡¸é—®é¢˜ä¸Šæ˜¾è‘—ä¼˜äºæ ‡å‡†æ–¹æ³•
4. **æ·±åº¦å­¦ä¹ **ï¼šè™½ç„¶ç†è®ºé’ˆå¯¹å‡¸æƒ…å†µï¼Œä½†åœ¨ç¥ç»ç½‘ç»œè®­ç»ƒä¸­ä»æœ‰ä»·å€¼ï¼ˆç‰¹åˆ«æ˜¯è®­ç»ƒåæœŸï¼‰

---

### 3. åŠ¨é‡çš„å‡ ä½•è§£é‡Š

**ç‰©ç†ç±»æ¯”**ï¼š

```text
æ¢¯åº¦ = åŠ›
åŠ¨é‡ = é€Ÿåº¦
å‚æ•° = ä½ç½®

ç‰©ç†æ–¹ç¨‹:
    v_{t+1} = Î²v_t + F_t  (ç‰›é¡¿ç¬¬äºŒå®šå¾‹)
    x_{t+1} = x_t + v_{t+1}
```

**æ•ˆæœ**ï¼š

- åœ¨å¹³å¦åŒºåŸŸåŠ é€Ÿ
- åœ¨é™¡å³­åŒºåŸŸå‡é€Ÿ
- è·¨è¶Šå±€éƒ¨æå°å€¼

---

## ğŸ’» è‡ªé€‚åº”å­¦ä¹ ç‡æ–¹æ³•

### 1. AdaGrad

**å®šä¹‰ 1.1 (AdaGrad)**:

$$
G_t = G_{t-1} + g_t \odot g_t
$$

$$
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{G_t + \epsilon}} \odot g_t
$$

**ç‰¹ç‚¹**ï¼š

- é¢‘ç¹æ›´æ–°çš„å‚æ•° â†’ å°å­¦ä¹ ç‡
- ç¨€ç–æ›´æ–°çš„å‚æ•° â†’ å¤§å­¦ä¹ ç‡

**é—®é¢˜**ï¼šå­¦ä¹ ç‡å•è°ƒé€’å‡ï¼Œå¯èƒ½è¿‡æ—©åœæ­¢

---

### 2. RMSprop

**å®šä¹‰ 2.1 (RMSprop)**:

$$
v_t = \beta v_{t-1} + (1 - \beta) g_t \odot g_t
$$

$$
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{v_t + \epsilon}} \odot g_t
$$

**æ”¹è¿›**ï¼š

- ä½¿ç”¨æŒ‡æ•°ç§»åŠ¨å¹³å‡
- é¿å…å­¦ä¹ ç‡å•è°ƒé€’å‡

**è¶…å‚æ•°**ï¼š$\beta = 0.9$, $\eta = 0.001$

---

### 3. Adam

**å®šä¹‰ 3.1 (Adam - Adaptive Moment Estimation)**:

$$
m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t \quad \text{(ä¸€é˜¶çŸ©)}
$$

$$
v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t \odot g_t \quad \text{(äºŒé˜¶çŸ©)}
$$

**åå·®ä¿®æ­£**ï¼š

$$
\hat{m}_t = \frac{m_t}{1 - \beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1 - \beta_2^t}
$$

**æ›´æ–°**ï¼š

$$
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t
$$

**é»˜è®¤è¶…å‚æ•°**ï¼š

- $\beta_1 = 0.9$
- $\beta_2 = 0.999$
- $\eta = 0.001$
- $\epsilon = 10^{-8}$

---

### 4. AdamW

**å®šä¹‰ 4.1 (AdamW - Adam with Weight Decay)**:

$$
\theta_{t+1} = \theta_t - \eta \left( \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon} + \lambda \theta_t \right)
$$

**å…³é”®æ”¹è¿›**ï¼š

- å°†æƒé‡è¡°å‡ä»æ¢¯åº¦ä¸­è§£è€¦
- æ›´å¥½çš„æ­£åˆ™åŒ–æ•ˆæœ

**æ¨è**ï¼šç°ä»£æ·±åº¦å­¦ä¹ çš„é¦–é€‰

---

## ğŸ¨ å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥

### 1. æ­¥é•¿è¡°å‡

**å®šä¹‰**ï¼š

$$
\eta_t = \eta_0 \cdot \gamma^{\lfloor t / s \rfloor}
$$

å…¶ä¸­ $s$ æ˜¯æ­¥é•¿å¤§å°ï¼Œ$\gamma$ æ˜¯è¡°å‡å› å­ï¼ˆå¦‚0.1ï¼‰ã€‚

**ç¤ºä¾‹**ï¼š

- åˆå§‹ï¼š$\eta_0 = 0.1$
- æ¯30ä¸ªepochï¼š$\eta \times 0.1$

---

### 2. ä½™å¼¦é€€ç«

**å®šä¹‰ 2.1 (Cosine Annealing)**:

$$
\eta_t = \eta_{\min} + \frac{1}{2}(\eta_{\max} - \eta_{\min}) \left(1 + \cos\left(\frac{t}{T} \pi\right)\right)
$$

**ç‰¹ç‚¹**ï¼š

- å¹³æ»‘è¡°å‡
- æ— éœ€æ‰‹åŠ¨è°ƒæ•´æ­¥é•¿

**å˜ä½“ï¼šCosine Annealing with Warm Restarts**:

$$
\eta_t = \eta_{\min} + \frac{1}{2}(\eta_{\max} - \eta_{\min}) \left(1 + \cos\left(\frac{T_{\text{cur}}}{T_i} \pi\right)\right)
$$

---

### 3. é¢„çƒ­ (Warmup)

**å®šä¹‰ 3.1 (Linear Warmup)**:

$$
\eta_t = \begin{cases}
\frac{t}{T_{\text{warmup}}} \eta_0 & \text{if } t \leq T_{\text{warmup}} \\
\eta_0 & \text{otherwise}
\end{cases}
$$

**ä½œç”¨**ï¼š

- é¿å…åˆæœŸå¤§å­¦ä¹ ç‡å¯¼è‡´çš„ä¸ç¨³å®š
- ç‰¹åˆ«é€‚ç”¨äºTransformerè®­ç»ƒ

---

### 4. å¾ªç¯å­¦ä¹ ç‡

**å®šä¹‰ 4.1 (Cyclical Learning Rate)**:

$$
\eta_t = \eta_{\min} + (\eta_{\max} - \eta_{\min}) \cdot \max(0, 1 - |t \bmod (2s) - s| / s)
$$

**ç‰¹ç‚¹**ï¼š

- å‘¨æœŸæ€§å˜åŒ–
- å¸®åŠ©è·³å‡ºå±€éƒ¨æå°å€¼

---

## ğŸ“ æ‰¹é‡å¤§å°çš„å½±å“

### 1. æ‰¹é‡å¤§å°ä¸æ³›åŒ–

**è§‚å¯Ÿ**ï¼š

- **å°æ‰¹é‡**ï¼šæ³›åŒ–æ›´å¥½ï¼Œä½†è®­ç»ƒæ…¢
- **å¤§æ‰¹é‡**ï¼šè®­ç»ƒå¿«ï¼Œä½†æ³›åŒ–å·®

**ç†è®ºè§£é‡Š**ï¼š

- å°æ‰¹é‡ï¼šæ¢¯åº¦å™ªå£° â†’ éšå¼æ­£åˆ™åŒ–
- å¤§æ‰¹é‡ï¼šæ”¶æ•›åˆ°å°–é”æå°å€¼

---

### 2. çº¿æ€§ç¼©æ”¾è§„åˆ™

**å®šç† 2.1 (Linear Scaling Rule, Goyal et al. 2017)**:

å½“æ‰¹é‡å¤§å°å¢åŠ  $k$ å€æ—¶ï¼Œå­¦ä¹ ç‡ä¹Ÿåº”å¢åŠ  $k$ å€ï¼š

$$
\eta_{\text{new}} = k \cdot \eta_{\text{old}}
$$

**å‰æ**ï¼š

- ä½¿ç”¨é¢„çƒ­
- æ‰¹é‡å¤§å°ä¸èƒ½å¤ªå¤§

**ç¤ºä¾‹**ï¼š

- æ‰¹é‡256ï¼Œå­¦ä¹ ç‡0.1
- æ‰¹é‡1024 â†’ å­¦ä¹ ç‡0.4

---

## ğŸ”§ å®è·µæŠ€å·§

### 1. æ¢¯åº¦è£å‰ª

**æŒ‰èŒƒæ•°è£å‰ª**ï¼š

$$
g = \begin{cases}
\frac{c}{\|g\|} g & \text{if } \|g\| > c \\
g & \text{otherwise}
\end{cases}
$$

**ä½œç”¨**ï¼šé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸

---

### 2. æƒé‡è¡°å‡

**L2æ­£åˆ™åŒ–**ï¼š

$$
\mathcal{L}_{\text{reg}} = \mathcal{L} + \frac{\lambda}{2} \|\theta\|^2
$$

**ç­‰ä»·äº**ï¼š

$$
\theta_{t+1} = (1 - \eta \lambda) \theta_t - \eta g_t
$$

---

### 3. æ¢¯åº¦ç´¯ç§¯

**åŠ¨æœº**ï¼šæ¨¡æ‹Ÿå¤§æ‰¹é‡ï¼ŒèŠ‚çœå†…å­˜

**æ–¹æ³•**ï¼š

```python
for i in range(accumulation_steps):
    loss = compute_loss(batch[i])
    loss.backward()  # ç´¯ç§¯æ¢¯åº¦

optimizer.step()  # æ›´æ–°å‚æ•°
optimizer.zero_grad()
```

**ç­‰ä»·æ‰¹é‡å¤§å°**ï¼š$b_{\text{eff}} = b \times \text{accumulation\_steps}$

---

## ğŸ’¡ Pythonå®ç°

```python
import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt

# 1. ä»é›¶å®ç°SGD with Momentum
class SGDMomentum:
    """SGD with Momentum"""
    def __init__(self, params, lr=0.01, momentum=0.9):
        self.params = list(params)
        self.lr = lr
        self.momentum = momentum
        self.velocities = [torch.zeros_like(p) for p in self.params]
    
    def step(self):
        for i, param in enumerate(self.params):
            if param.grad is None:
                continue
            
            # æ›´æ–°é€Ÿåº¦
            self.velocities[i] = self.momentum * self.velocities[i] + param.grad
            
            # æ›´æ–°å‚æ•°
            param.data -= self.lr * self.velocities[i]
    
    def zero_grad(self):
        for param in self.params:
            if param.grad is not None:
                param.grad.zero_()


# 2. ä»é›¶å®ç°Adam
class Adam:
    """Adam Optimizer"""
    def __init__(self, params, lr=0.001, betas=(0.9, 0.999), eps=1e-8):
        self.params = list(params)
        self.lr = lr
        self.beta1, self.beta2 = betas
        self.eps = eps
        
        self.m = [torch.zeros_like(p) for p in self.params]  # ä¸€é˜¶çŸ©
        self.v = [torch.zeros_like(p) for p in self.params]  # äºŒé˜¶çŸ©
        self.t = 0  # æ—¶é—´æ­¥
    
    def step(self):
        self.t += 1
        
        for i, param in enumerate(self.params):
            if param.grad is None:
                continue
            
            grad = param.grad
            
            # æ›´æ–°ä¸€é˜¶çŸ©å’ŒäºŒé˜¶çŸ©
            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad
            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * grad ** 2
            
            # åå·®ä¿®æ­£
            m_hat = self.m[i] / (1 - self.beta1 ** self.t)
            v_hat = self.v[i] / (1 - self.beta2 ** self.t)
            
            # æ›´æ–°å‚æ•°
            param.data -= self.lr * m_hat / (torch.sqrt(v_hat) + self.eps)
    
    def zero_grad(self):
        for param in self.params:
            if param.grad is not None:
                param.grad.zero_()


# 3. å­¦ä¹ ç‡è°ƒåº¦å™¨
class CosineAnnealingLR:
    """ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦"""
    def __init__(self, optimizer, T_max, eta_min=0):
        self.optimizer = optimizer
        self.T_max = T_max
        self.eta_min = eta_min
        self.base_lr = optimizer.lr
        self.t = 0
    
    def step(self):
        self.t += 1
        lr = self.eta_min + (self.base_lr - self.eta_min) * \
             (1 + np.cos(np.pi * self.t / self.T_max)) / 2
        self.optimizer.lr = lr
        return lr


# 4. å¯¹æ¯”ä¸åŒä¼˜åŒ–å™¨
def compare_optimizers():
    """å¯¹æ¯”ä¸åŒä¼˜åŒ–å™¨çš„æ€§èƒ½"""
    # å®šä¹‰ä¸€ä¸ªç®€å•çš„ä¼˜åŒ–é—®é¢˜
    def rosenbrock(x, y):
        return (1 - x)**2 + 100 * (y - x**2)**2
    
    # åˆå§‹ç‚¹
    x0, y0 = -1.5, 2.0
    
    # ä¸åŒä¼˜åŒ–å™¨
    optimizers = {
        'SGD': lambda p: torch.optim.SGD(p, lr=0.001),
        'SGD+Momentum': lambda p: torch.optim.SGD(p, lr=0.001, momentum=0.9),
        'Adam': lambda p: torch.optim.Adam(p, lr=0.01),
        'RMSprop': lambda p: torch.optim.RMSprop(p, lr=0.01),
    }
    
    trajectories = {}
    
    for name, opt_fn in optimizers.items():
        x = torch.tensor([x0, y0], requires_grad=True)
        optimizer = opt_fn([x])
        
        trajectory = [x.detach().numpy().copy()]
        
        for _ in range(200):
            optimizer.zero_grad()
            loss = rosenbrock(x[0], x[1])
            loss.backward()
            optimizer.step()
            
            trajectory.append(x.detach().numpy().copy())
        
        trajectories[name] = np.array(trajectory)
    
    # å¯è§†åŒ–
    plt.figure(figsize=(12, 8))
    
    # ç»˜åˆ¶ç­‰é«˜çº¿
    x_range = np.linspace(-2, 2, 100)
    y_range = np.linspace(-1, 3, 100)
    X, Y = np.meshgrid(x_range, y_range)
    Z = rosenbrock(X, Y)
    
    plt.contour(X, Y, Z, levels=np.logspace(-1, 3, 20), cmap='gray', alpha=0.3)
    
    # ç»˜åˆ¶è½¨è¿¹
    colors = ['red', 'blue', 'green', 'orange']
    for (name, traj), color in zip(trajectories.items(), colors):
        plt.plot(traj[:, 0], traj[:, 1], '-o', label=name, color=color, 
                 markersize=2, linewidth=1.5)
    
    plt.plot(1, 1, 'r*', markersize=15, label='Optimum')
    plt.xlabel('x')
    plt.ylabel('y')
    plt.title('Optimizer Comparison on Rosenbrock Function')
    plt.legend()
    plt.grid(True, alpha=0.3)
    # plt.show()


# 5. å­¦ä¹ ç‡è°ƒåº¦å¯è§†åŒ–
def visualize_lr_schedules():
    """å¯è§†åŒ–ä¸åŒçš„å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥"""
    T = 100
    eta_0 = 0.1
    
    schedules = {
        'Constant': [eta_0] * T,
        'Step Decay': [eta_0 * (0.5 ** (t // 30)) for t in range(T)],
        'Exponential': [eta_0 * (0.95 ** t) for t in range(T)],
        'Cosine': [eta_0 * (1 + np.cos(np.pi * t / T)) / 2 for t in range(T)],
        'Linear Warmup': [min(t / 10, 1) * eta_0 for t in range(T)],
    }
    
    plt.figure(figsize=(12, 6))
    for name, schedule in schedules.items():
        plt.plot(schedule, label=name, linewidth=2)
    
    plt.xlabel('Iteration')
    plt.ylabel('Learning Rate')
    plt.title('Learning Rate Schedules')
    plt.legend()
    plt.grid(True, alpha=0.3)
    # plt.show()


# ç¤ºä¾‹ä½¿ç”¨
if __name__ == "__main__":
    print("=== å¯¹æ¯”ä¼˜åŒ–å™¨ ===")
    compare_optimizers()
    
    print("\n=== å­¦ä¹ ç‡è°ƒåº¦ ===")
    visualize_lr_schedules()
    
    # æµ‹è¯•è‡ªå®šä¹‰Adam
    print("\n=== æµ‹è¯•è‡ªå®šä¹‰Adam ===")
    model = nn.Linear(10, 1)
    optimizer = Adam(model.parameters(), lr=0.001)
    
    x = torch.randn(32, 10)
    y = torch.randn(32, 1)
    
    for epoch in range(10):
        optimizer.zero_grad()
        loss = nn.MSELoss()(model(x), y)
        loss.backward()
        optimizer.step()
        print(f"Epoch {epoch+1}, Loss: {loss.item():.4f}")
```

---

## ğŸ“š ä¼˜åŒ–å™¨é€‰æ‹©æŒ‡å—

### 1. ä¸åŒä»»åŠ¡çš„æ¨è

| ä»»åŠ¡ | æ¨èä¼˜åŒ–å™¨ | å­¦ä¹ ç‡ |
|------|-----------|--------|
| **å›¾åƒåˆ†ç±»** | SGD+Momentum | 0.1 |
| **ç›®æ ‡æ£€æµ‹** | SGD+Momentum | 0.02 |
| **è¯­è¨€æ¨¡å‹** | Adam/AdamW | 1e-4 |
| **Transformer** | AdamW + Warmup | 1e-4 |
| **GAN** | Adam | 2e-4 |
| **å¼ºåŒ–å­¦ä¹ ** | Adam | 3e-4 |

---

### 2. è¶…å‚æ•°è°ƒä¼˜

**å­¦ä¹ ç‡**ï¼š

- ä»å¤§åˆ°å°å°è¯•ï¼š$[1, 0.1, 0.01, 0.001, 0.0001]$
- ä½¿ç”¨å­¦ä¹ ç‡æŸ¥æ‰¾å™¨

**æ‰¹é‡å¤§å°**ï¼š

- ä»å°å¼€å§‹ï¼š32, 64, 128, 256
- å—å†…å­˜é™åˆ¶

**åŠ¨é‡**ï¼š

- é»˜è®¤0.9é€šå¸¸æœ‰æ•ˆ
- å¯å°è¯•0.95, 0.99

---

## ğŸ“ ç›¸å…³è¯¾ç¨‹

| å¤§å­¦ | è¯¾ç¨‹ |
|------|------|
| **Stanford** | CS229 Machine Learning |
| **MIT** | 6.255J Optimization Methods |
| **CMU** | 10-725 Convex Optimization |
| **UC Berkeley** | CS189 Introduction to Machine Learning |

---

## ğŸ“– å‚è€ƒæ–‡çŒ®

1. **Robbins & Monro (1951)**. "A Stochastic Approximation Method". *Annals of Mathematical Statistics*.

2. **Polyak (1964)**. "Some methods of speeding up the convergence of iteration methods". *USSR Computational Mathematics and Mathematical Physics*.

3. **Nesterov (1983)**. "A method for solving the convex programming problem with convergence rate O(1/k^2)". *Soviet Mathematics Doklady*.

4. **Duchi et al. (2011)**. "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization". *JMLR*. (AdaGrad)

5. **Kingma & Ba (2015)**. "Adam: A Method for Stochastic Optimization". *ICLR*.

6. **Loshchilov & Hutter (2019)**. "Decoupled Weight Decay Regularization". *ICLR*. (AdamW)

7. **Goyal et al. (2017)**. "Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour". *arXiv*. (Linear Scaling Rule)

---

*æœ€åæ›´æ–°ï¼š2025å¹´10æœˆ*-
