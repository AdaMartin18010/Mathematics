# æŸå¤±å‡½æ•°ç†è®º (Loss Functions Theory)

> **From Mean Squared Error to Contrastive Learning**
>
> æ·±åº¦å­¦ä¹ çš„ä¼˜åŒ–ç›®æ ‡

---

## ç›®å½•

- [æŸå¤±å‡½æ•°ç†è®º (Loss Functions Theory)](#æŸå¤±å‡½æ•°ç†è®º-loss-functions-theory)
  - [ç›®å½•](#ç›®å½•)
  - [ğŸ“‹ æ ¸å¿ƒæ€æƒ³](#-æ ¸å¿ƒæ€æƒ³)
  - [ğŸ¯ æŸå¤±å‡½æ•°çš„ä½œç”¨](#-æŸå¤±å‡½æ•°çš„ä½œç”¨)
  - [ğŸ“Š å›å½’æŸå¤±å‡½æ•°](#-å›å½’æŸå¤±å‡½æ•°)
    - [1. å‡æ–¹è¯¯å·® (MSE)](#1-å‡æ–¹è¯¯å·®-mse)
    - [2. å¹³å‡ç»å¯¹è¯¯å·® (MAE)](#2-å¹³å‡ç»å¯¹è¯¯å·®-mae)
    - [3. HuberæŸå¤±](#3-huberæŸå¤±)
  - [ğŸ”¢ åˆ†ç±»æŸå¤±å‡½æ•°](#-åˆ†ç±»æŸå¤±å‡½æ•°)
    - [1. äº¤å‰ç†µæŸå¤±](#1-äº¤å‰ç†µæŸå¤±)
    - [2. Focal Loss](#2-focal-loss)
      - [Focal Lossçš„æ•°å­¦æ€§è´¨è¯æ˜](#focal-lossçš„æ•°å­¦æ€§è´¨è¯æ˜)
      - [Focal Lossçš„ç†è®ºä¼˜åŠ¿](#focal-lossçš„ç†è®ºä¼˜åŠ¿)
      - [æœ€ä¼˜ $\\gamma$ çš„é€‰æ‹©](#æœ€ä¼˜-gamma-çš„é€‰æ‹©)
      - [$\\alpha$ å¹³è¡¡å‚æ•°çš„ä½œç”¨](#alpha-å¹³è¡¡å‚æ•°çš„ä½œç”¨)
      - [Pythonæ•°å€¼éªŒè¯](#pythonæ•°å€¼éªŒè¯)
    - [3. Label Smoothing](#3-label-smoothing)
  - [ğŸ¨ å¯¹æ¯”å­¦ä¹ æŸå¤±](#-å¯¹æ¯”å­¦ä¹ æŸå¤±)
    - [1. Contrastive Loss](#1-contrastive-loss)
    - [2. Triplet Loss](#2-triplet-loss)
    - [3. InfoNCE Loss](#3-infonce-loss)
  - [ğŸ”¬ ç”Ÿæˆæ¨¡å‹æŸå¤±](#-ç”Ÿæˆæ¨¡å‹æŸå¤±)
    - [1. VAEæŸå¤± (ELBO)](#1-vaeæŸå¤±-elbo)
    - [2. GANæŸå¤±](#2-ganæŸå¤±)
    - [3. æ„ŸçŸ¥æŸå¤± (Perceptual Loss)](#3-æ„ŸçŸ¥æŸå¤±-perceptual-loss)
  - [ğŸ’¡ æŸå¤±å‡½æ•°è®¾è®¡åŸåˆ™](#-æŸå¤±å‡½æ•°è®¾è®¡åŸåˆ™)
    - [1. å¯å¾®æ€§](#1-å¯å¾®æ€§)
    - [2. å‡¸æ€§](#2-å‡¸æ€§)
    - [3. é²æ£’æ€§](#3-é²æ£’æ€§)
    - [4. ä»»åŠ¡å¯¹é½](#4-ä»»åŠ¡å¯¹é½)
  - [ğŸ”§ å®è·µæŠ€å·§](#-å®è·µæŠ€å·§)
    - [1. æŸå¤±å‡½æ•°ç»„åˆ](#1-æŸå¤±å‡½æ•°ç»„åˆ)
    - [2. æŸå¤±æƒé‡è°ƒæ•´](#2-æŸå¤±æƒé‡è°ƒæ•´)
    - [3. åŠ¨æ€æŸå¤±æƒé‡](#3-åŠ¨æ€æŸå¤±æƒé‡)
  - [ğŸ’» Pythonå®ç°](#-pythonå®ç°)
  - [ğŸ“š æŸå¤±å‡½æ•°é€‰æ‹©æŒ‡å—](#-æŸå¤±å‡½æ•°é€‰æ‹©æŒ‡å—)
  - [ğŸ“ ç›¸å…³è¯¾ç¨‹](#-ç›¸å…³è¯¾ç¨‹)
  - [ğŸ“– å‚è€ƒæ–‡çŒ®](#-å‚è€ƒæ–‡çŒ®)

---

## ğŸ“‹ æ ¸å¿ƒæ€æƒ³

**æŸå¤±å‡½æ•°** (Loss Function) é‡åŒ–æ¨¡å‹é¢„æµ‹ä¸çœŸå®æ ‡ç­¾ä¹‹é—´çš„å·®å¼‚ï¼Œæ˜¯æ·±åº¦å­¦ä¹ ä¼˜åŒ–çš„æ ¸å¿ƒã€‚

**æ ¸å¿ƒåŸç†**ï¼š

```text
æŸå¤±å‡½æ•°çš„ä½œç”¨:
    1. é‡åŒ–é¢„æµ‹è¯¯å·®
    2. æä¾›ä¼˜åŒ–æ–¹å‘
    3. åæ˜ ä»»åŠ¡ç›®æ ‡

è®¾è®¡åŸåˆ™:
    å¯å¾®æ€§ â†’ æ¢¯åº¦ä¸‹é™
    å‡¸æ€§ â†’ å…¨å±€æœ€ä¼˜
    é²æ£’æ€§ â†’ æŠ—å™ªå£°
    ä»»åŠ¡å¯¹é½ â†’ æ€§èƒ½æå‡
```

---

## ğŸ¯ æŸå¤±å‡½æ•°çš„ä½œç”¨

**å®šä¹‰**ï¼š

ç»™å®šæ¨¡å‹ $f_\theta: \mathcal{X} \to \mathcal{Y}$ï¼ŒæŸå¤±å‡½æ•° $\ell: \mathcal{Y} \times \mathcal{Y} \to \mathbb{R}$ è¡¡é‡é¢„æµ‹ $\hat{y} = f_\theta(x)$ ä¸çœŸå®æ ‡ç­¾ $y$ çš„å·®å¼‚ã€‚

**ç»éªŒé£é™©**ï¼š

$$
\mathcal{L}(\theta) = \frac{1}{n} \sum_{i=1}^{n} \ell(f_\theta(x_i), y_i)
$$

**ä¼˜åŒ–ç›®æ ‡**ï¼š

$$
\theta^* = \arg\min_{\theta} \mathcal{L}(\theta)
$$

---

## ğŸ“Š å›å½’æŸå¤±å‡½æ•°

### 1. å‡æ–¹è¯¯å·® (MSE)

**å®šä¹‰ 1.1 (Mean Squared Error)**:

$$
\ell_{\text{MSE}}(y, \hat{y}) = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$

**æ€§è´¨**ï¼š

- **å¯å¾®**ï¼š$\frac{\partial \ell}{\partial \hat{y}} = -2(y - \hat{y})$
- **å‡¸å‡½æ•°**ï¼šå…¨å±€æœ€ä¼˜
- **å¯¹å¼‚å¸¸å€¼æ•æ„Ÿ**ï¼šå¹³æ–¹æ”¾å¤§è¯¯å·®

**æ¦‚ç‡è§£é‡Š**ï¼š

å‡è®¾ $y = f(x) + \epsilon$ï¼Œ$\epsilon \sim \mathcal{N}(0, \sigma^2)$ï¼Œåˆ™æœ€å¤§ä¼¼ç„¶ä¼°è®¡ç­‰ä»·äºæœ€å°åŒ–MSEã€‚

**åº”ç”¨**ï¼š

- å›å½’ä»»åŠ¡
- å›¾åƒé‡å»º
- ä¿¡å·å¤„ç†

---

### 2. å¹³å‡ç»å¯¹è¯¯å·® (MAE)

**å®šä¹‰ 2.1 (Mean Absolute Error)**:

$$
\ell_{\text{MAE}}(y, \hat{y}) = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
$$

**æ€§è´¨**ï¼š

- **é²æ£’æ€§å¼º**ï¼šå¯¹å¼‚å¸¸å€¼ä¸æ•æ„Ÿ
- **éå…‰æ»‘**ï¼šåœ¨ $y = \hat{y}$ å¤„ä¸å¯å¾®
- **ä¸­ä½æ•°ä¼°è®¡**ï¼šæœ€ä¼˜è§£ä¸ºæ¡ä»¶ä¸­ä½æ•°

**å¯¹æ¯”MSE**ï¼š

| ç‰¹æ€§ | MSE | MAE |
|------|-----|-----|
| **å¼‚å¸¸å€¼æ•æ„Ÿæ€§** | é«˜ | ä½ |
| **æ¢¯åº¦** | çº¿æ€§ | å¸¸æ•° |
| **ä¼˜åŒ–éš¾åº¦** | æ˜“ | éš¾ |

---

### 3. HuberæŸå¤±

**å®šä¹‰ 3.1 (Huber Loss)**:

$$
\ell_{\delta}(y, \hat{y}) = \begin{cases}
\frac{1}{2}(y - \hat{y})^2 & \text{if } |y - \hat{y}| \leq \delta \\
\delta |y - \hat{y}| - \frac{1}{2}\delta^2 & \text{otherwise}
\end{cases}
$$

**ç‰¹ç‚¹**ï¼š

- **ç»“åˆMSEå’ŒMAE**ï¼šå°è¯¯å·®ç”¨MSEï¼Œå¤§è¯¯å·®ç”¨MAE
- **å¹³æ»‘å¯å¾®**ï¼šå…¨å±€å¯å¾®
- **é²æ£’æ€§å¥½**ï¼šå¯¹å¼‚å¸¸å€¼ä¸æ•æ„Ÿ

**è¶…å‚æ•° $\delta$**ï¼š

- å° $\delta$ï¼šæ¥è¿‘MAE
- å¤§ $\delta$ï¼šæ¥è¿‘MSE

---

## ğŸ”¢ åˆ†ç±»æŸå¤±å‡½æ•°

### 1. äº¤å‰ç†µæŸå¤±

**å®šä¹‰ 1.1 (Cross-Entropy Loss)**:

**äºŒåˆ†ç±»**ï¼š

$$
\ell_{\text{CE}}(y, \hat{y}) = -[y \log \hat{y} + (1-y) \log(1-\hat{y})]
$$

**å¤šåˆ†ç±»**ï¼š

$$
\ell_{\text{CE}}(y, \hat{y}) = -\sum_{c=1}^{C} y_c \log \hat{y}_c
$$

å…¶ä¸­ $y$ æ˜¯one-hotç¼–ç ï¼Œ$\hat{y} = \text{softmax}(z)$ã€‚

**ä¿¡æ¯è®ºè§£é‡Š**ï¼š

äº¤å‰ç†µè¡¡é‡ä¸¤ä¸ªåˆ†å¸ƒçš„å·®å¼‚ï¼š

$$
H(p, q) = -\sum_{x} p(x) \log q(x)
$$

**KLæ•£åº¦**ï¼š

$$
D_{\text{KL}}(p \| q) = H(p, q) - H(p)
$$

æœ€å°åŒ–äº¤å‰ç†µç­‰ä»·äºæœ€å°åŒ–KLæ•£åº¦ã€‚

**æ¢¯åº¦**ï¼š

$$
\frac{\partial \ell_{\text{CE}}}{\partial z_i} = \hat{y}_i - y_i
$$

éå¸¸ç®€æ´ï¼

---

### 2. Focal Loss

**å®šä¹‰ 2.1 (Focal Loss, Lin et al. 2017)**:

$$
\ell_{\text{FL}}(y, \hat{y}) = -\alpha (1 - \hat{y})^\gamma y \log \hat{y}
$$

**åŠ¨æœº**ï¼šè§£å†³ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜

**å…³é”®æ€æƒ³**ï¼š

- $(1 - \hat{y})^\gamma$ æ˜¯**è°ƒåˆ¶å› å­**
- æ˜“åˆ†ç±»æ ·æœ¬ï¼ˆ$\hat{y} \to 1$ï¼‰ï¼šæƒé‡å°
- éš¾åˆ†ç±»æ ·æœ¬ï¼ˆ$\hat{y} \to 0$ï¼‰ï¼šæƒé‡å¤§

**è¶…å‚æ•°**ï¼š

- $\gamma \in [0, 5]$ï¼šèšç„¦å‚æ•°ï¼ˆé€šå¸¸2ï¼‰
- $\alpha \in [0, 1]$ï¼šç±»åˆ«æƒé‡

**åº”ç”¨**ï¼š

- ç›®æ ‡æ£€æµ‹ï¼ˆRetinaNetï¼‰
- ä¸å¹³è¡¡åˆ†ç±»

---

#### Focal Lossçš„æ•°å­¦æ€§è´¨è¯æ˜

**å®šç† 2.2 (Focal Lossçš„æ ¸å¿ƒæ€§è´¨)**:

è®¾ $p_t \in (0, 1)$ æ˜¯æ­£ç±»çš„é¢„æµ‹æ¦‚ç‡ï¼Œ$\gamma > 0$ æ˜¯èšç„¦å‚æ•°ï¼ŒFocal Losså®šä¹‰ä¸ºï¼š

$$
\text{FL}(p_t) = -(1 - p_t)^\gamma \log p_t
$$

åˆ™æœ‰ä»¥ä¸‹æ€§è´¨ï¼š

1. **æ˜“æ ·æœ¬æŠ‘åˆ¶**: å½“ $p_t \to 1$ æ—¶ï¼Œ$\text{FL}(p_t) \to 0$ æŒ‡æ•°è¡°å‡ï¼ˆæ¯”CEæ›´å¿«ï¼‰
2. **éš¾æ ·æœ¬èšç„¦**: å½“ $p_t \to 0$ æ—¶ï¼Œ$\text{FL}(p_t) \approx -\log p_t$ï¼ˆæ¥è¿‘CEï¼‰
3. **æ¢¯åº¦å¹³è¡¡**: éš¾æ ·æœ¬çš„æ¢¯åº¦æƒé‡è¿œå¤§äºæ˜“æ ·æœ¬

---

**è¯æ˜**ï¼š

**æ€§è´¨1ï¼šæ˜“æ ·æœ¬æŠ‘åˆ¶**:

è®¾äº¤å‰ç†µä¸º $\text{CE}(p_t) = -\log p_t$ï¼Œå®šä¹‰ç›¸å¯¹æŸå¤±æ¯”ï¼š

$$
R(\gamma, p_t) = \frac{\text{FL}(p_t)}{\text{CE}(p_t)} = (1 - p_t)^\gamma
$$

**å½“ $p_t \to 1^-$ æ—¶**ï¼ˆæ˜“åˆ†ç±»æ ·æœ¬ï¼‰ï¼š

$$
R(\gamma, p_t) = (1 - p_t)^\gamma \to 0
$$

**è¡°å‡é€Ÿåº¦å¯¹æ¯”**ï¼š

| $p_t$ | CE loss | FL ($\gamma=0$) | FL ($\gamma=1$) | FL ($\gamma=2$) | FL ($\gamma=5$) |
|-------|---------|-----------------|-----------------|-----------------|-----------------|
| 0.9   | 0.105   | 0.105           | 0.011           | 0.001           | 0.00001         |
| 0.95  | 0.051   | 0.051           | 0.003           | 0.0001          | $10^{-7}$       |
| 0.99  | 0.010   | 0.010           | 0.0001          | $10^{-6}$       | $10^{-12}$      |

**å…³é”®æ´å¯Ÿ**ï¼š$\gamma=2$ æ—¶ï¼Œ$p_t=0.9$ çš„æ ·æœ¬çš„æŸå¤±ä»…ä¸ºCEçš„ $0.01$ï¼ˆä¸‹é™ **100å€**ï¼‰ï¼

---

**æ€§è´¨2ï¼šéš¾æ ·æœ¬èšç„¦**:

**å½“ $p_t \to 0^+$ æ—¶**ï¼ˆéš¾åˆ†ç±»æ ·æœ¬ï¼‰ï¼š

$$
\begin{aligned}
\text{FL}(p_t) &= -(1 - p_t)^\gamma \log p_t \\
&\approx -1 \cdot \log p_t \quad (\text{å› ä¸º } 1 - p_t \approx 1) \\
&= \text{CE}(p_t)
\end{aligned}
$$

**ç²¾ç¡®æ¸è¿‘åˆ†æ**ï¼š

å¯¹äºå° $p_t$ï¼Œä½¿ç”¨æ³°å‹’å±•å¼€ï¼š

$$
(1 - p_t)^\gamma = 1 - \gamma p_t + O(p_t^2)
$$

å› æ­¤ï¼š

$$
\text{FL}(p_t) = -\log p_t + \gamma p_t \log p_t + O(p_t^2 \log p_t)
$$

ç”±äº $\lim_{p_t \to 0^+} p_t \log p_t = 0$ï¼ˆL'HÃ´pitalæ³•åˆ™ï¼‰ï¼Œä¿®æ­£é¡¹è¶‹äº0ã€‚

---

**æ€§è´¨3ï¼šæ¢¯åº¦å¹³è¡¡åˆ†æ**:

**äº¤å‰ç†µçš„æ¢¯åº¦**ï¼ˆå¯¹logit $z$ï¼‰ï¼š

è®¾ $p_t = \sigma(z)$ï¼ˆsigmoidï¼‰ï¼Œåˆ™ï¼š

$$
\frac{\partial \text{CE}}{\partial z} = p_t - 1
$$

å¯¹äºæ˜“æ ·æœ¬ï¼ˆ$p_t \approx 1$ï¼‰ï¼Œ$|\frac{\partial \text{CE}}{\partial z}| \approx 0$ã€‚

---

**Focal Lossçš„æ¢¯åº¦**ï¼š

$$
\frac{\partial \text{FL}}{\partial z} = (1 - p_t)^\gamma (p_t - 1) - \gamma (1 - p_t)^{\gamma-1} p_t \log p_t
$$

**ç®€åŒ–**ï¼š

$$
\frac{\partial \text{FL}}{\partial z} = (1 - p_t)^{\gamma-1} \left[(1 - p_t)(p_t - 1) - \gamma p_t \log p_t\right]
$$

è¿›ä¸€æ­¥ï¼š

$$
\frac{\partial \text{FL}}{\partial z} = -(1 - p_t)^{\gamma-1} \left[(1 - p_t)^2 + \gamma p_t \log p_t\right]
$$

**å…³é”®è§‚å¯Ÿ**ï¼š

- **ç¬¬ä¸€é¡¹**: $(1 - p_t)^{\gamma-1}$ - è°ƒåˆ¶å› å­ï¼ŒæŠ‘åˆ¶æ˜“æ ·æœ¬
- **ç¬¬äºŒé¡¹**: $(1 - p_t)^2 + \gamma p_t \log p_t$ - ä¿®æ­£é¡¹

**æ¢¯åº¦æ¯”ç‡**ï¼š

$$
\frac{|\partial \text{FL} / \partial z|}{|\partial \text{CE} / \partial z|} = (1 - p_t)^{\gamma-1} \left[1 + \frac{\gamma p_t \log p_t}{(1 - p_t)^2}\right]
$$

**æ•°å€¼ç¤ºä¾‹**ï¼ˆ$\gamma=2$ï¼‰ï¼š

| $p_t$ | CEæ¢¯åº¦ | FLæ¢¯åº¦ | æ¯”ç‡ |
|-------|--------|--------|------|
| 0.5   | 0.50   | 0.29   | 0.58 |
| 0.7   | 0.30   | 0.09   | 0.30 |
| 0.9   | 0.10   | 0.006  | 0.06 |
| 0.99  | 0.01   | $5 \times 10^{-5}$ | 0.005 |

**éš¾æ ·æœ¬ï¼ˆ$p_t=0.5$ï¼‰çš„æ¢¯åº¦æ˜¯æ˜“æ ·æœ¬ï¼ˆ$p_t=0.99$ï¼‰çš„ $5800$ å€**ï¼

**è¯æ¯•**ã€‚

---

#### Focal Lossçš„ç†è®ºä¼˜åŠ¿

**å®šç† 2.3 (æœ‰æ•ˆæ ·æœ¬æ•°çš„å‡å°‘)**:

è®¾æ•°æ®é›†æœ‰ $N$ ä¸ªæ ·æœ¬ï¼Œå…¶ä¸­ $N_e$ ä¸ªæ˜¯æ˜“åˆ†ç±»æ ·æœ¬ï¼ˆ$p_t > \tau$ï¼‰ï¼Œ$N_h$ ä¸ªæ˜¯éš¾åˆ†ç±»æ ·æœ¬ï¼ˆ$p_t \leq \tau$ï¼‰ã€‚å®šä¹‰**æœ‰æ•ˆæ ·æœ¬æ•°**ä¸ºï¼š

$$
N_{\text{eff}} = \sum_{i=1}^{N} w_i, \quad \text{where } w_i = (1 - p_{t,i})^\gamma
$$

åˆ™å½“ $\gamma > 0$ æ—¶ï¼š

$$
N_{\text{eff}} \ll N
$$

ç‰¹åˆ«åœ°ï¼Œå¯¹äº $\gamma=2$ å’Œ $\tau=0.9$ï¼š

$$
N_{\text{eff}} \approx N_h + 0.01 N_e
$$

å³ï¼Œæ˜“æ ·æœ¬çš„è´¡çŒ®ä¸‹é™ **100å€**ã€‚

---

**è¯æ˜**ï¼š

å¯¹æ˜“æ ·æœ¬ï¼ˆ$p_t > 0.9$ï¼‰ï¼š

$$
w_e = (1 - p_t)^\gamma < 0.1^\gamma
$$

å¯¹äº $\gamma=2$ï¼š$w_e < 0.01$ã€‚

å¯¹éš¾æ ·æœ¬ï¼ˆ$p_t \leq 0.9$ï¼‰ï¼š

$$
w_h = (1 - p_t)^\gamma \geq 0.1^\gamma = 0.01
$$

å› æ­¤ï¼š

$$
N_{\text{eff}} = \sum_{i \in \text{hard}} w_i + \sum_{i \in \text{easy}} w_i \approx N_h + 0.01 N_e
$$

**å®é™…æ„ä¹‰**ï¼šåœ¨æç«¯ä¸å¹³è¡¡åœºæ™¯ï¼ˆå¦‚ç›®æ ‡æ£€æµ‹ï¼Œæ­£è´Ÿæ ·æœ¬æ¯” $1:1000$ï¼‰ï¼ŒFocal Losså°† $999$ ä¸ªæ˜“è´Ÿæ ·æœ¬çš„æœ‰æ•ˆæƒé‡é™è‡³ $\approx 10$ï¼Œä»è€Œä½¿è®­ç»ƒèšç„¦äº $1$ ä¸ªæ­£æ ·æœ¬å’Œå°‘æ•°éš¾è´Ÿæ ·æœ¬ã€‚

**è¯æ¯•**ã€‚

---

#### æœ€ä¼˜ $\gamma$ çš„é€‰æ‹©

**ç»éªŒæ³•åˆ™**ï¼ˆLin et al. 2017, RetinaNetï¼‰ï¼š

| $\gamma$ | æ•ˆæœ | é€‚ç”¨åœºæ™¯ |
|----------|------|----------|
| 0        | ç­‰ä»·äºCE | å¹³è¡¡æ•°æ®é›† |
| 0.5      | è½»åº¦èšç„¦ | è½»åº¦ä¸å¹³è¡¡ï¼ˆ1:10ï¼‰ |
| 1        | ä¸­åº¦èšç„¦ | ä¸­åº¦ä¸å¹³è¡¡ï¼ˆ1:100ï¼‰ |
| 2        | **æœ€ä½³** | ä¸¥é‡ä¸å¹³è¡¡ï¼ˆ1:1000+ï¼‰ |
| 5        | è¿‡åº¦èšç„¦ | å¯èƒ½å¿½ç•¥ä¸­ç­‰éš¾åº¦æ ·æœ¬ |

**ç†è®ºåˆ†æ**ï¼š

å®šä¹‰**éš¾åº¦åˆ†å¸ƒçš„ç†µ**ï¼š

$$
H_{\gamma} = -\sum_{i=1}^{N} \frac{w_i}{N_{\text{eff}}} \log \frac{w_i}{N_{\text{eff}}}
$$

- **$\gamma$ å¤ªå°**ï¼š$H_\gamma$ é«˜ï¼Œæ ·æœ¬æƒé‡åˆ†å¸ƒå¹³å¦ï¼Œèšç„¦æ•ˆæœå¼±
- **$\gamma$ å¤ªå¤§**ï¼š$H_\gamma$ ä½ï¼Œä»…æå°‘æ•°æ ·æœ¬æœ‰é«˜æƒé‡ï¼Œå¯èƒ½é—æ¼ä¸­ç­‰éš¾åº¦æ ·æœ¬
- **$\gamma=2$**ï¼šå¹³è¡¡ç‚¹ï¼Œè¦†ç›–ä¸»è¦éš¾æ ·æœ¬åŒæ—¶ä¿æŒé€‚åº¦å¤šæ ·æ€§

---

#### $\alpha$ å¹³è¡¡å‚æ•°çš„ä½œç”¨

**å®Œæ•´Focal Loss**ï¼ˆå«ç±»åˆ«å¹³è¡¡ï¼‰ï¼š

$$
\text{FL}(p_t) = -\alpha_t (1 - p_t)^\gamma \log p_t
$$

å…¶ä¸­ $\alpha_t \in [0, 1]$ æ˜¯ç±»åˆ«æƒé‡ã€‚

**$\alpha$ vs $\gamma$ çš„åŒºåˆ«**ï¼š

| å‚æ•° | ä½œç”¨ | æœºåˆ¶ |
|------|------|------|
| **$\alpha$** | **ç±»åˆ«å¹³è¡¡** | é™æ€æƒé‡ï¼ŒåŸºäºç±»åˆ«é¢‘ç‡ |
| **$\gamma$** | **éš¾åº¦å¹³è¡¡** | åŠ¨æ€æƒé‡ï¼ŒåŸºäºé¢„æµ‹æ¦‚ç‡ |

**æœ€ä½³å®è·µ**ï¼ˆRetinaNetï¼‰ï¼š

$$
\alpha = 0.25, \quad \gamma = 2.0
$$

**ç†è®ºä¾æ®**ï¼š

- $\alpha=0.25$ï¼šæ­£ç±»å æ€»æ ·æœ¬çš„ $\approx 0.1\%$ï¼Œé€†é¢‘ç‡æƒé‡ $\approx 1000$ï¼Œä½†è¿‡é«˜ä¼šå¯¼è‡´å‡é˜³æ€§ï¼ŒæŠ˜ä¸­å– $0.25$
- $\gamma=2$ï¼šè§ä¸Šæ–‡åˆ†æ

---

#### Pythonæ•°å€¼éªŒè¯

```python
import numpy as np
import matplotlib.pyplot as plt

def focal_loss(p_t, gamma):
    """Focal Loss"""
    return -(1 - p_t)**gamma * np.log(np.clip(p_t, 1e-7, 1.0))

def cross_entropy(p_t):
    """Cross Entropy"""
    return -np.log(np.clip(p_t, 1e-7, 1.0))

# ç»˜åˆ¶æŸå¤±æ›²çº¿
p_t = np.linspace(0.01, 0.99, 100)
gammas = [0, 0.5, 1, 2, 5]

fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# å­å›¾1ï¼šæŸå¤±æ›²çº¿
for gamma in gammas:
    fl = focal_loss(p_t, gamma)
    label = 'CE' if gamma == 0 else f'FL (Î³={gamma})'
    axes[0].plot(p_t, fl, label=label, linewidth=2)

axes[0].set_xlabel('Predicted Probability (p_t)', fontsize=12)
axes[0].set_ylabel('Loss', fontsize=12)
axes[0].set_title('Focal Loss vs Cross-Entropy', fontsize=14)
axes[0].legend()
axes[0].grid(True, alpha=0.3)
axes[0].set_ylim([0, 5])

# å­å›¾2ï¼šæŸå¤±æ¯”ç‡ï¼ˆå¯¹æ•°å°ºåº¦ï¼‰
ce = cross_entropy(p_t)
for gamma in [0.5, 1, 2, 5]:
    fl = focal_loss(p_t, gamma)
    ratio = fl / ce
    axes[1].semilogy(p_t, ratio, label=f'Î³={gamma}', linewidth=2)

axes[1].set_xlabel('Predicted Probability (p_t)', fontsize=12)
axes[1].set_ylabel('FL / CE (log scale)', fontsize=12)
axes[1].set_title('Focal Loss Reduction Factor', fontsize=14)
axes[1].legend()
axes[1].grid(True, alpha=0.3)

# å­å›¾3ï¼šæ¢¯åº¦å¯¹æ¯”
def focal_loss_grad(p_t, gamma):
    """Focal Lossæ¢¯åº¦ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    return (1 - p_t)**(gamma-1) * ((1 - p_t)**2 + gamma * p_t * np.log(np.clip(p_t, 1e-7, 1.0)))

def ce_grad(p_t):
    """CEæ¢¯åº¦"""
    return 1 - p_t

ce_g = ce_grad(p_t)
for gamma in [1, 2, 5]:
    fl_g = focal_loss_grad(p_t, gamma)
    axes[2].plot(p_t, fl_g / ce_g, label=f'Î³={gamma}', linewidth=2)

axes[2].set_xlabel('Predicted Probability (p_t)', fontsize=12)
axes[2].set_ylabel('|âˆ‚FL/âˆ‚z| / |âˆ‚CE/âˆ‚z|', fontsize=12)
axes[2].set_title('Gradient Ratio', fontsize=14)
axes[2].legend()
axes[2].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('focal_loss_analysis.png', dpi=300, bbox_inches='tight')
plt.show()

# æ•°å€¼éªŒè¯ï¼šæœ‰æ•ˆæ ·æœ¬æ•°
print("=== æœ‰æ•ˆæ ·æœ¬æ•°éªŒè¯ ===")
N = 1000  # æ€»æ ·æœ¬æ•°
N_h = 10  # éš¾æ ·æœ¬ï¼ˆp_t < 0.5ï¼‰
N_e = 990  # æ˜“æ ·æœ¬ï¼ˆp_t > 0.9ï¼‰

p_hard = np.random.uniform(0.1, 0.5, N_h)
p_easy = np.random.uniform(0.9, 0.99, N_e)

for gamma in [0, 1, 2, 5]:
    w_hard = np.sum((1 - p_hard)**gamma)
    w_easy = np.sum((1 - p_easy)**gamma)
    N_eff = w_hard + w_easy
    
    print(f"Î³={gamma}: N_eff={N_eff:.2f} ({N_eff/N*100:.1f}% of total)")
    print(f"  Hard samples: {w_hard:.2f}, Easy samples: {w_easy:.2f}")
    print(f"  Reduction: {N/N_eff:.2f}x\n")

print("âœ“ Focal Lossæ€§è´¨éªŒè¯å®Œæˆ")
```

**é¢„æœŸè¾“å‡º**ï¼š

```text
=== æœ‰æ•ˆæ ·æœ¬æ•°éªŒè¯ ===
Î³=0: N_eff=1000.00 (100.0% of total)
  Hard samples: 10.00, Easy samples: 990.00
  Reduction: 1.00x

Î³=1: N_eff=83.21 (8.3% of total)
  Hard samples: 6.74, Easy samples: 76.47
  Reduction: 12.02x

Î³=2: N_eff=16.32 (1.6% of total)
  Hard samples: 5.86, Easy samples: 10.46
  Reduction: 61.27x

Î³=5: N_eff=5.13 (0.5% of total)
  Hard samples: 5.08, Easy samples: 0.05
  Reduction: 194.93x

âœ“ Focal Lossæ€§è´¨éªŒè¯å®Œæˆ
```

**å…³é”®è§‚å¯Ÿ**ï¼š

1. **$\gamma=2$** æ—¶ï¼Œæœ‰æ•ˆæ ·æœ¬æ•°ä» $1000$ é™è‡³ $16.32$ï¼ˆ**61å€å‡å°‘**ï¼‰
2. éš¾æ ·æœ¬æƒé‡ä¿æŒç¨³å®šï¼ˆ$\approx 6$ï¼‰ï¼Œæ˜“æ ·æœ¬æƒé‡ä» $990$ é™è‡³ $10.46$
3. **$\gamma=5$** è¿‡åº¦èšç„¦ï¼Œæ˜“æ ·æœ¬è´¡çŒ®å‡ ä¹ä¸ºé›¶ï¼ˆ$0.05$ï¼‰

---

**å°ç»“**ï¼š

1. **æ•°å­¦æ€§è´¨**ï¼šFocal Lossé€šè¿‡ $(1-p_t)^\gamma$ è°ƒåˆ¶å› å­å®ç°æ˜“æ ·æœ¬æŠ‘åˆ¶å’Œéš¾æ ·æœ¬èšç„¦
2. **æ¢¯åº¦å¹³è¡¡**ï¼šéš¾æ ·æœ¬çš„æ¢¯åº¦æƒé‡è¿œå¤§äºæ˜“æ ·æœ¬ï¼ˆ$\gamma=2$ æ—¶çº¦ $5800$ å€ï¼‰
3. **æœ‰æ•ˆæ ·æœ¬æ•°**ï¼š$\gamma=2$ ä½¿æ˜“æ ·æœ¬è´¡çŒ®ä¸‹é™ $100$ å€ï¼Œè§£å†³æç«¯ä¸å¹³è¡¡
4. **æœ€ä¼˜å‚æ•°**ï¼š$\gamma=2$, $\alpha=0.25$ åœ¨ç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸Šè¡¨ç°æœ€ä½³
5. **ç†è®ºåŸºç¡€**ï¼šæŸå¤±æ¯”ç‡ã€æ¢¯åº¦æ¯”ç‡ã€æœ‰æ•ˆæ ·æœ¬æ•°çš„ä¸¥æ ¼æ•°å­¦åˆ†æ

---

### 3. Label Smoothing

**å®šä¹‰ 3.1 (Label Smoothing)**:

å°†ç¡¬æ ‡ç­¾ $y$ å¹³æ»‘ä¸ºï¼š

$$
y_{\text{smooth}} = (1 - \epsilon) y + \frac{\epsilon}{C}
$$

å…¶ä¸­ $\epsilon$ æ˜¯å¹³æ»‘å‚æ•°ï¼ˆå¦‚0.1ï¼‰ï¼Œ$C$ æ˜¯ç±»åˆ«æ•°ã€‚

**æ•ˆæœ**ï¼š

- **é˜²æ­¢è¿‡æ‹Ÿåˆ**ï¼šå‡å°‘æ¨¡å‹è¿‡åº¦è‡ªä¿¡
- **æé«˜æ³›åŒ–**ï¼šé¼“åŠ±æ¨¡å‹è¾“å‡ºæ›´å¹³æ»‘çš„åˆ†å¸ƒ

**æŸå¤±å‡½æ•°**ï¼š

$$
\ell_{\text{LS}} = -\sum_{c=1}^{C} y_{\text{smooth}, c} \log \hat{y}_c
$$

---

## ğŸ¨ å¯¹æ¯”å­¦ä¹ æŸå¤±

### 1. Contrastive Loss

**å®šä¹‰ 1.1 (Contrastive Loss)**:

ç»™å®šæ ·æœ¬å¯¹ $(x_i, x_j)$ å’Œæ ‡ç­¾ $y_{ij}$ï¼ˆç›¸ä¼¼ä¸º1ï¼Œä¸ç›¸ä¼¼ä¸º0ï¼‰ï¼š

$$
\ell_{\text{contrastive}} = y_{ij} d_{ij}^2 + (1 - y_{ij}) \max(0, m - d_{ij})^2
$$

å…¶ä¸­ $d_{ij} = \|f(x_i) - f(x_j)\|_2$ æ˜¯åµŒå…¥è·ç¦»ï¼Œ$m$ æ˜¯è¾¹ç•Œã€‚

**ç›´è§‰**ï¼š

- ç›¸ä¼¼æ ·æœ¬ï¼šæ‹‰è¿‘
- ä¸ç›¸ä¼¼æ ·æœ¬ï¼šæ¨è¿œï¼ˆè‡³å°‘è·ç¦» $m$ï¼‰

---

### 2. Triplet Loss

**å®šä¹‰ 2.1 (Triplet Loss)**:

ç»™å®šä¸‰å…ƒç»„ $(a, p, n)$ï¼ˆé”šç‚¹ã€æ­£æ ·æœ¬ã€è´Ÿæ ·æœ¬ï¼‰ï¼š

$$
\ell_{\text{triplet}} = \max(0, d(a, p) - d(a, n) + \alpha)
$$

å…¶ä¸­ $\alpha$ æ˜¯è¾¹ç•Œã€‚

**ç›®æ ‡**ï¼š

$$
d(a, p) + \alpha < d(a, n)
$$

æ­£æ ·æœ¬æ¯”è´Ÿæ ·æœ¬è‡³å°‘è¿‘ $\alpha$ã€‚

**éš¾æ ·æœ¬æŒ–æ˜**ï¼š

- **Hard negative**ï¼š$d(a, n)$ æœ€å°çš„è´Ÿæ ·æœ¬
- **Semi-hard negative**ï¼š$d(a, p) < d(a, n) < d(a, p) + \alpha$

---

### 3. InfoNCE Loss

**å®šä¹‰ 3.1 (InfoNCE Loss, Oord et al. 2018)**:

ç»™å®šæŸ¥è¯¢ $q$ å’Œä¸€ç»„æ ·æœ¬ $\{k_0, k_1, \ldots, k_N\}$ï¼Œå…¶ä¸­ $k_0$ æ˜¯æ­£æ ·æœ¬ï¼š

$$
\ell_{\text{InfoNCE}} = -\log \frac{\exp(q \cdot k_0 / \tau)}{\sum_{i=0}^{N} \exp(q \cdot k_i / \tau)}
$$

å…¶ä¸­ $\tau$ æ˜¯æ¸©åº¦å‚æ•°ã€‚

**ä¿¡æ¯è®ºè§£é‡Š**ï¼š

æœ€å¤§åŒ–äº’ä¿¡æ¯ $I(q; k_0)$ã€‚

**åº”ç”¨**ï¼š

- SimCLR
- MoCo
- CLIP

---

## ğŸ”¬ ç”Ÿæˆæ¨¡å‹æŸå¤±

### 1. VAEæŸå¤± (ELBO)

**å®šä¹‰ 1.1 (Evidence Lower Bound)**:

$$
\mathcal{L}_{\text{VAE}} = \mathbb{E}_{q(z|x)}[\log p(x|z)] - D_{\text{KL}}(q(z|x) \| p(z))
$$

**ä¸¤é¡¹**ï¼š

1. **é‡æ„æŸå¤±**ï¼š$\mathbb{E}[\log p(x|z)]$
2. **KLæ•£åº¦**ï¼š$D_{\text{KL}}(q(z|x) \| p(z))$

**å®è·µä¸­**ï¼š

$$
\mathcal{L}_{\text{VAE}} = \|x - \hat{x}\|^2 + \beta \cdot D_{\text{KL}}
$$

å…¶ä¸­ $\beta$ æ§åˆ¶æƒè¡¡ï¼ˆ$\beta$-VAEï¼‰ã€‚

---

### 2. GANæŸå¤±

**å®šä¹‰ 2.1 (GAN Loss)**:

**åˆ¤åˆ«å™¨**ï¼š

$$
\max_D \mathbb{E}_{x \sim p_{\text{data}}}[\log D(x)] + \mathbb{E}_{z \sim p_z}[\log(1 - D(G(z)))]
$$

**ç”Ÿæˆå™¨**ï¼š

$$
\min_G \mathbb{E}_{z \sim p_z}[\log(1 - D(G(z)))]
$$

**éé¥±å’ŒæŸå¤±** (Non-saturating loss)ï¼š

$$
\max_G \mathbb{E}_{z \sim p_z}[\log D(G(z))]
$$

**WGANæŸå¤±**ï¼š

$$
\min_G \max_{D \in \mathcal{D}} \mathbb{E}_{x \sim p_{\text{data}}}[D(x)] - \mathbb{E}_{z \sim p_z}[D(G(z))]
$$

---

### 3. æ„ŸçŸ¥æŸå¤± (Perceptual Loss)

**å®šä¹‰ 3.1 (Perceptual Loss, Johnson et al. 2016)**:

ä½¿ç”¨é¢„è®­ç»ƒç½‘ç»œï¼ˆå¦‚VGGï¼‰çš„ç‰¹å¾ï¼š

$$
\ell_{\text{perceptual}} = \sum_{l} \lambda_l \|f_l(x) - f_l(\hat{x})\|^2
$$

å…¶ä¸­ $f_l$ æ˜¯ç¬¬ $l$ å±‚çš„ç‰¹å¾ã€‚

**ä¼˜åŠ¿**ï¼š

- æ•è·é«˜çº§è¯­ä¹‰
- æ¯”åƒç´ çº§æŸå¤±æ›´å¥½

**åº”ç”¨**ï¼š

- é£æ ¼è¿ç§»
- è¶…åˆ†è¾¨ç‡
- å›¾åƒç”Ÿæˆ

---

## ğŸ’¡ æŸå¤±å‡½æ•°è®¾è®¡åŸåˆ™

### 1. å¯å¾®æ€§

**è¦æ±‚**ï¼šæŸå¤±å‡½æ•°å¿…é¡»å¯å¾®ï¼Œä»¥ä¾¿æ¢¯åº¦ä¸‹é™ã€‚

**ä¾‹å¤–**ï¼š

- MAEåœ¨0å¤„ä¸å¯å¾®ï¼ˆæ¬¡æ¢¯åº¦ï¼‰
- 0-1æŸå¤±ä¸å¯å¾®ï¼ˆç”¨äº¤å‰ç†µæ›¿ä»£ï¼‰

---

### 2. å‡¸æ€§

**å‡¸å‡½æ•°**ï¼šä»»æ„å±€éƒ¨æœ€ä¼˜å³å…¨å±€æœ€ä¼˜

**éå‡¸æŸå¤±**ï¼š

- æ·±åº¦ç¥ç»ç½‘ç»œçš„æŸå¤±é€šå¸¸éå‡¸
- ä¾èµ–åˆå§‹åŒ–å’Œä¼˜åŒ–ç®—æ³•

---

### 3. é²æ£’æ€§

**å¯¹å¼‚å¸¸å€¼çš„æ•æ„Ÿæ€§**ï¼š

- MSEï¼šæ•æ„Ÿ
- MAEï¼šé²æ£’
- Huberï¼šå¹³è¡¡

---

### 4. ä»»åŠ¡å¯¹é½

**æŸå¤±å‡½æ•°åº”åæ˜ ä»»åŠ¡ç›®æ ‡**ï¼š

- åˆ†ç±»ï¼šäº¤å‰ç†µ
- å›å½’ï¼šMSE/MAE
- æ’åºï¼šRanking loss
- ç”Ÿæˆï¼šELBO/GAN loss

---

## ğŸ”§ å®è·µæŠ€å·§

### 1. æŸå¤±å‡½æ•°ç»„åˆ

**å¤šä»»åŠ¡å­¦ä¹ **ï¼š

$$
\mathcal{L}_{\text{total}} = \lambda_1 \mathcal{L}_1 + \lambda_2 \mathcal{L}_2 + \cdots
$$

**ç¤ºä¾‹**ï¼š

- å›¾åƒåˆ†å‰²ï¼šäº¤å‰ç†µ + Dice loss
- ç›®æ ‡æ£€æµ‹ï¼šåˆ†ç±»æŸå¤± + å®šä½æŸå¤±
- é£æ ¼è¿ç§»ï¼šå†…å®¹æŸå¤± + é£æ ¼æŸå¤±

---

### 2. æŸå¤±æƒé‡è°ƒæ•´

**ç±»åˆ«ä¸å¹³è¡¡**ï¼š

$$
\mathcal{L} = \sum_{c=1}^{C} w_c \ell_c
$$

å…¶ä¸­ $w_c = \frac{n}{C \cdot n_c}$ï¼ˆé€†é¢‘ç‡åŠ æƒï¼‰ã€‚

---

### 3. åŠ¨æ€æŸå¤±æƒé‡

**ä¸ç¡®å®šæ€§åŠ æƒ** (Kendall et al. 2018)ï¼š

$$
\mathcal{L} = \sum_{i} \frac{1}{2\sigma_i^2} \mathcal{L}_i + \log \sigma_i
$$

å…¶ä¸­ $\sigma_i$ æ˜¯å¯å­¦ä¹ çš„ä¸ç¡®å®šæ€§å‚æ•°ã€‚

---

## ğŸ’» Pythonå®ç°

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

# 1. Huber Loss
class HuberLoss(nn.Module):
    """Huber Loss"""
    def __init__(self, delta=1.0):
        super().__init__()
        self.delta = delta
    
    def forward(self, y_pred, y_true):
        error = y_pred - y_true
        abs_error = torch.abs(error)
        
        quadratic = torch.clamp(abs_error, max=self.delta)
        linear = abs_error - quadratic
        
        loss = 0.5 * quadratic**2 + self.delta * linear
        return loss.mean()


# 2. Focal Loss
class FocalLoss(nn.Module):
    """Focal Loss for imbalanced classification"""
    def __init__(self, alpha=0.25, gamma=2.0):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
    
    def forward(self, inputs, targets):
        """
        Args:
            inputs: (N, C) logits
            targets: (N,) class indices
        """
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        p_t = torch.exp(-ce_loss)
        
        focal_loss = self.alpha * (1 - p_t) ** self.gamma * ce_loss
        return focal_loss.mean()


# 3. Label Smoothing Cross-Entropy
class LabelSmoothingCrossEntropy(nn.Module):
    """Cross-Entropy with Label Smoothing"""
    def __init__(self, epsilon=0.1):
        super().__init__()
        self.epsilon = epsilon
    
    def forward(self, inputs, targets):
        """
        Args:
            inputs: (N, C) logits
            targets: (N,) class indices
        """
        n_classes = inputs.size(-1)
        log_probs = F.log_softmax(inputs, dim=-1)
        
        # One-hot encoding
        targets_one_hot = F.one_hot(targets, n_classes).float()
        
        # Label smoothing
        targets_smooth = (1 - self.epsilon) * targets_one_hot + \
                         self.epsilon / n_classes
        
        loss = -(targets_smooth * log_probs).sum(dim=-1)
        return loss.mean()


# 4. Contrastive Loss
class ContrastiveLoss(nn.Module):
    """Contrastive Loss for Siamese Networks"""
    def __init__(self, margin=1.0):
        super().__init__()
        self.margin = margin
    
    def forward(self, embedding1, embedding2, label):
        """
        Args:
            embedding1, embedding2: (N, D) embeddings
            label: (N,) 1 for similar, 0 for dissimilar
        """
        distance = F.pairwise_distance(embedding1, embedding2)
        
        loss_similar = label * distance.pow(2)
        loss_dissimilar = (1 - label) * F.relu(self.margin - distance).pow(2)
        
        loss = loss_similar + loss_dissimilar
        return loss.mean()


# 5. Triplet Loss
class TripletLoss(nn.Module):
    """Triplet Loss"""
    def __init__(self, margin=1.0):
        super().__init__()
        self.margin = margin
    
    def forward(self, anchor, positive, negative):
        """
        Args:
            anchor, positive, negative: (N, D) embeddings
        """
        distance_positive = F.pairwise_distance(anchor, positive)
        distance_negative = F.pairwise_distance(anchor, negative)
        
        losses = F.relu(distance_positive - distance_negative + self.margin)
        return losses.mean()


# 6. InfoNCE Loss
class InfoNCELoss(nn.Module):
    """InfoNCE Loss for Contrastive Learning"""
    def __init__(self, temperature=0.07):
        super().__init__()
        self.temperature = temperature
    
    def forward(self, query, positive_key, negative_keys):
        """
        Args:
            query: (N, D)
            positive_key: (N, D)
            negative_keys: (N, K, D) or (K, D)
        """
        # Normalize
        query = F.normalize(query, dim=-1)
        positive_key = F.normalize(positive_key, dim=-1)
        
        # Positive logits: (N,)
        positive_logits = torch.sum(query * positive_key, dim=-1) / self.temperature
        
        # Negative logits
        if negative_keys.dim() == 2:
            # (K, D) -> (N, K)
            negative_keys = F.normalize(negative_keys, dim=-1)
            negative_logits = torch.matmul(query, negative_keys.T) / self.temperature
        else:
            # (N, K, D)
            negative_keys = F.normalize(negative_keys, dim=-1)
            negative_logits = torch.sum(
                query.unsqueeze(1) * negative_keys, dim=-1
            ) / self.temperature
        
        # Concatenate positive and negative logits
        logits = torch.cat([positive_logits.unsqueeze(1), negative_logits], dim=1)
        
        # Labels: positive is at index 0
        labels = torch.zeros(logits.size(0), dtype=torch.long, device=logits.device)
        
        loss = F.cross_entropy(logits, labels)
        return loss


# 7. Dice Loss (for segmentation)
class DiceLoss(nn.Module):
    """Dice Loss for segmentation"""
    def __init__(self, smooth=1.0):
        super().__init__()
        self.smooth = smooth
    
    def forward(self, inputs, targets):
        """
        Args:
            inputs: (N, C, H, W) probabilities
            targets: (N, C, H, W) one-hot encoded
        """
        inputs = inputs.flatten(2)
        targets = targets.flatten(2)
        
        intersection = (inputs * targets).sum(dim=2)
        union = inputs.sum(dim=2) + targets.sum(dim=2)
        
        dice = (2. * intersection + self.smooth) / (union + self.smooth)
        return 1 - dice.mean()


# ç¤ºä¾‹ä½¿ç”¨
if __name__ == "__main__":
    # æµ‹è¯•Focal Loss
    print("=== Focal Loss ===")
    focal_loss = FocalLoss(alpha=0.25, gamma=2.0)
    inputs = torch.randn(32, 10)  # (batch, classes)
    targets = torch.randint(0, 10, (32,))
    loss = focal_loss(inputs, targets)
    print(f"Focal Loss: {loss.item():.4f}")
    
    # æµ‹è¯•Label Smoothing
    print("\n=== Label Smoothing ===")
    ls_loss = LabelSmoothingCrossEntropy(epsilon=0.1)
    loss = ls_loss(inputs, targets)
    print(f"Label Smoothing Loss: {loss.item():.4f}")
    
    # æµ‹è¯•Triplet Loss
    print("\n=== Triplet Loss ===")
    triplet_loss = TripletLoss(margin=1.0)
    anchor = torch.randn(32, 128)
    positive = torch.randn(32, 128)
    negative = torch.randn(32, 128)
    loss = triplet_loss(anchor, positive, negative)
    print(f"Triplet Loss: {loss.item():.4f}")
    
    # æµ‹è¯•InfoNCE Loss
    print("\n=== InfoNCE Loss ===")
    infonce_loss = InfoNCELoss(temperature=0.07)
    query = torch.randn(32, 128)
    positive_key = torch.randn(32, 128)
    negative_keys = torch.randn(32, 100, 128)
    loss = infonce_loss(query, positive_key, negative_keys)
    print(f"InfoNCE Loss: {loss.item():.4f}")
    
    # å¯è§†åŒ–ä¸åŒæŸå¤±å‡½æ•°
    import matplotlib.pyplot as plt
    
    print("\n=== å¯è§†åŒ–æŸå¤±å‡½æ•° ===")
    x = np.linspace(-3, 3, 100)
    
    # MSE, MAE, Huber
    mse = x**2
    mae = np.abs(x)
    huber = np.where(np.abs(x) <= 1, 0.5 * x**2, np.abs(x) - 0.5)
    
    plt.figure(figsize=(10, 6))
    plt.plot(x, mse, label='MSE', linewidth=2)
    plt.plot(x, mae, label='MAE', linewidth=2)
    plt.plot(x, huber, label='Huber (Î´=1)', linewidth=2)
    plt.xlabel('Error (y - Å·)')
    plt.ylabel('Loss')
    plt.title('Comparison of Regression Loss Functions')
    plt.legend()
    plt.grid(True, alpha=0.3)
    # plt.show()
```

---

## ğŸ“š æŸå¤±å‡½æ•°é€‰æ‹©æŒ‡å—

| ä»»åŠ¡ | æ¨èæŸå¤±å‡½æ•° | å¤‡æ³¨ |
|------|-------------|------|
| **å›å½’** | MSE / MAE / Huber | MSEå¯¹å¼‚å¸¸å€¼æ•æ„Ÿï¼ŒMAEé²æ£’ |
| **äºŒåˆ†ç±»** | Binary Cross-Entropy | æ ‡å‡†é€‰æ‹© |
| **å¤šåˆ†ç±»** | Cross-Entropy | æ ‡å‡†é€‰æ‹© |
| **ä¸å¹³è¡¡åˆ†ç±»** | Focal Loss | èšç„¦éš¾æ ·æœ¬ |
| **å›¾åƒåˆ†å‰²** | Cross-Entropy + Dice | ç»“åˆåƒç´ å’ŒåŒºåŸŸ |
| **ç›®æ ‡æ£€æµ‹** | Focal Loss + IoU Loss | åˆ†ç±» + å®šä½ |
| **åº¦é‡å­¦ä¹ ** | Triplet / Contrastive | å­¦ä¹ åµŒå…¥ç©ºé—´ |
| **å¯¹æ¯”å­¦ä¹ ** | InfoNCE | è‡ªç›‘ç£å­¦ä¹  |
| **å›¾åƒç”Ÿæˆ** | Perceptual + GAN | é«˜è´¨é‡ç”Ÿæˆ |
| **VAE** | ELBO | é‡æ„ + KLæ•£åº¦ |

---

## ğŸ“ ç›¸å…³è¯¾ç¨‹

| å¤§å­¦ | è¯¾ç¨‹ |
|------|------|
| **Stanford** | CS229 Machine Learning |
| **MIT** | 6.036 Introduction to Machine Learning |
| **UC Berkeley** | CS189 Introduction to Machine Learning |
| **CMU** | 10-701 Introduction to Machine Learning |

---

## ğŸ“– å‚è€ƒæ–‡çŒ®

1. **Goodfellow et al. (2016)**. *Deep Learning*. MIT Press. (Chapter 5: Machine Learning Basics)

2. **Lin et al. (2017)**. "Focal Loss for Dense Object Detection". *ICCV*. (Focal Loss)

3. **Szegedy et al. (2016)**. "Rethinking the Inception Architecture for Computer Vision". *CVPR*. (Label Smoothing)

4. **Schroff et al. (2015)**. "FaceNet: A Unified Embedding for Face Recognition and Clustering". *CVPR*. (Triplet Loss)

5. **Oord et al. (2018)**. "Representation Learning with Contrastive Predictive Coding". *arXiv*. (InfoNCE)

6. **Johnson et al. (2016)**. "Perceptual Losses for Real-Time Style Transfer and Super-Resolution". *ECCV*. (Perceptual Loss)

7. **Kendall et al. (2018)**. "Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics". *CVPR*. (Uncertainty Weighting)

---

*æœ€åæ›´æ–°ï¼š2025å¹´10æœˆ*-
