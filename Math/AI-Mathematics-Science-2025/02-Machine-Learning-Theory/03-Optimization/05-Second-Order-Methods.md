# äºŒé˜¶ä¼˜åŒ–æ–¹æ³• (Second-Order Optimization Methods)

> **Beyond Gradient Descent: Leveraging Curvature Information**
>
> è¶…è¶Šæ¢¯åº¦ä¸‹é™ï¼šåˆ©ç”¨æ›²ç‡ä¿¡æ¯

---

## ç›®å½•

- [äºŒé˜¶ä¼˜åŒ–æ–¹æ³• (Second-Order Optimization Methods)](#äºŒé˜¶ä¼˜åŒ–æ–¹æ³•-second-order-optimization-methods)
  - [ç›®å½•](#ç›®å½•)
  - [ğŸ“‹ æ ¸å¿ƒæ€æƒ³](#-æ ¸å¿ƒæ€æƒ³)
  - [ğŸ¯ Newtonæ³•](#-newtonæ³•)
    - [1. åŸºæœ¬Newtonæ³•](#1-åŸºæœ¬newtonæ³•)
    - [2. é˜»å°¼Newtonæ³•](#2-é˜»å°¼newtonæ³•)
    - [3. ä¿¡èµ–åŸŸNewtonæ³•](#3-ä¿¡èµ–åŸŸnewtonæ³•)
  - [ğŸ“Š æ‹ŸNewtonæ³•](#-æ‹Ÿnewtonæ³•)
    - [1. BFGSç®—æ³•](#1-bfgsç®—æ³•)
    - [2. L-BFGSç®—æ³•](#2-l-bfgsç®—æ³•)
    - [3. DFPç®—æ³•](#3-dfpç®—æ³•)
  - [ğŸ”¬ å…±è½­æ¢¯åº¦æ³•](#-å…±è½­æ¢¯åº¦æ³•)
    - [1. çº¿æ€§å…±è½­æ¢¯åº¦](#1-çº¿æ€§å…±è½­æ¢¯åº¦)
    - [2. éçº¿æ€§å…±è½­æ¢¯åº¦](#2-éçº¿æ€§å…±è½­æ¢¯åº¦)
    - [3. é¢„æ¡ä»¶å…±è½­æ¢¯åº¦](#3-é¢„æ¡ä»¶å…±è½­æ¢¯åº¦)
  - [ğŸ’¡ Gauss-Newtonæ³•](#-gauss-newtonæ³•)
    - [1. åŸºæœ¬Gauss-Newtonæ³•](#1-åŸºæœ¬gauss-newtonæ³•)
    - [2. Levenberg-Marquardtç®—æ³•](#2-levenberg-marquardtç®—æ³•)
  - [ğŸ¨ åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„åº”ç”¨](#-åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„åº”ç”¨)
    - [1. è‡ªç„¶æ¢¯åº¦](#1-è‡ªç„¶æ¢¯åº¦)
    - [2. K-FAC](#2-k-fac)
    - [3. Shampoo](#3-shampoo)
  - [ğŸ”§ å®ç”¨æŠ€å·§](#-å®ç”¨æŠ€å·§)
    - [1. Hessianè¿‘ä¼¼](#1-hessianè¿‘ä¼¼)
    - [2. çº¿æœç´¢ç­–ç•¥](#2-çº¿æœç´¢ç­–ç•¥)
    - [3. æ”¶æ•›æ€§åˆ†æ](#3-æ”¶æ•›æ€§åˆ†æ)
  - [ğŸ’» Pythonå®ç°](#-pythonå®ç°)
  - [ğŸ“š ç»ƒä¹ é¢˜](#-ç»ƒä¹ é¢˜)
    - [ç»ƒä¹ 1ï¼šNewtonæ³•](#ç»ƒä¹ 1newtonæ³•)
    - [ç»ƒä¹ 2ï¼šBFGS](#ç»ƒä¹ 2bfgs)
    - [ç»ƒä¹ 3ï¼šå…±è½­æ¢¯åº¦](#ç»ƒä¹ 3å…±è½­æ¢¯åº¦)
    - [ç»ƒä¹ 4ï¼šGauss-Newton](#ç»ƒä¹ 4gauss-newton)
  - [ğŸ“ ç›¸å…³è¯¾ç¨‹](#-ç›¸å…³è¯¾ç¨‹)
  - [ğŸ“– å‚è€ƒæ–‡çŒ®](#-å‚è€ƒæ–‡çŒ®)

---

## ğŸ“‹ æ ¸å¿ƒæ€æƒ³

**äºŒé˜¶ä¼˜åŒ–æ–¹æ³•**åˆ©ç”¨ç›®æ ‡å‡½æ•°çš„äºŒé˜¶å¯¼æ•°ä¿¡æ¯ï¼ˆHessiançŸ©é˜µï¼‰ï¼Œç›¸æ¯”ä¸€é˜¶æ–¹æ³•ï¼ˆå¦‚æ¢¯åº¦ä¸‹é™ï¼‰å…·æœ‰æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ã€‚

**ä¸ºä»€ä¹ˆä½¿ç”¨äºŒé˜¶æ–¹æ³•**:

```text
ä¼˜åŠ¿:
â”œâ”€ æ”¶æ•›é€Ÿåº¦å¿«: äºŒæ¬¡æ”¶æ•› vs çº¿æ€§æ”¶æ•›
â”œâ”€ è‡ªé€‚åº”æ­¥é•¿: è‡ªåŠ¨è°ƒæ•´å­¦ä¹ ç‡
â”œâ”€ æ›²ç‡ä¿¡æ¯: åˆ©ç”¨HessiançŸ©é˜µ
â””â”€ ç†è®ºä¿è¯: å¼ºå‡¸å‡½æ•°å±€éƒ¨äºŒæ¬¡æ”¶æ•›

æŒ‘æˆ˜:
â”œâ”€ è®¡ç®—æˆæœ¬: O(nÂ³) Hessianæ±‚é€†
â”œâ”€ å­˜å‚¨éœ€æ±‚: O(nÂ²) HessiançŸ©é˜µ
â”œâ”€ éå‡¸é—®é¢˜: å¯èƒ½æ”¶æ•›åˆ°éç‚¹
â””â”€ æ·±åº¦å­¦ä¹ : å‚æ•°é‡å·¨å¤§
```

---

## ğŸ¯ Newtonæ³•

### 1. åŸºæœ¬Newtonæ³•

**ç®—æ³•æ€æƒ³**:

åœ¨å½“å‰ç‚¹ $x_k$ å¤„ï¼Œç”¨äºŒé˜¶Taylorå±•å¼€é€¼è¿‘ç›®æ ‡å‡½æ•°ï¼š

$$
f(x) \approx f(x_k) + \nabla f(x_k)^T (x - x_k) + \frac{1}{2} (x - x_k)^T H_k (x - x_k)
$$

å…¶ä¸­ $H_k = \nabla^2 f(x_k)$ æ˜¯HessiançŸ©é˜µã€‚

**æœ€ä¼˜åŒ–æ¡ä»¶**:

$$
\nabla f(x) \approx \nabla f(x_k) + H_k (x - x_k) = 0
$$

**Newtonæ–¹å‘**:

$$
d_k = -H_k^{-1} \nabla f(x_k)
$$

**æ›´æ–°è§„åˆ™**:

$$
x_{k+1} = x_k + d_k = x_k - H_k^{-1} \nabla f(x_k)
$$

---

**ç®—æ³• 1.1 (Newtonæ³•)**:

```text
è¾“å…¥: åˆå§‹ç‚¹ xâ‚€, å®¹å·® Îµ
è¾“å‡º: æœ€ä¼˜è§£ x*

1. for k = 0, 1, 2, ... do
2.     è®¡ç®—æ¢¯åº¦ g_k = âˆ‡f(x_k)
3.     if ||g_k|| < Îµ then
4.         return x_k
5.     è®¡ç®—Hessian H_k = âˆ‡Â²f(x_k)
6.     æ±‚è§£ H_k d_k = -g_k  (Newtonæ–¹ç¨‹)
7.     x_{k+1} = x_k + d_k
8. end for
```

---

**æ”¶æ•›æ€§**:

**å®šç† 1.1 (Newtonæ³•æ”¶æ•›æ€§)**:

è®¾ $f$ æ˜¯å¼ºå‡¸å‡½æ•°ï¼Œ$\nabla^2 f$ æ˜¯Lipschitzè¿ç»­çš„ã€‚å¦‚æœåˆå§‹ç‚¹ $x_0$ è¶³å¤Ÿæ¥è¿‘æœ€ä¼˜è§£ $x^*$ï¼Œåˆ™Newtonæ³•**äºŒæ¬¡æ”¶æ•›**ï¼š

$$
\|x_{k+1} - x^*\| \leq C \|x_k - x^*\|^2
$$

---

### 2. é˜»å°¼Newtonæ³•

**é—®é¢˜**: åŸºæœ¬Newtonæ³•å¯èƒ½ä¸æ”¶æ•›ï¼ˆè¿œç¦»æœ€ä¼˜è§£æ—¶ï¼‰ã€‚

**è§£å†³**: å¼•å…¥æ­¥é•¿ $\alpha_k$ï¼š

$$
x_{k+1} = x_k - \alpha_k H_k^{-1} \nabla f(x_k)
$$

**Armijoçº¿æœç´¢**:

é€‰æ‹© $\alpha_k$ ä½¿å¾—ï¼š

$$
f(x_k + \alpha_k d_k) \leq f(x_k) + c_1 \alpha_k \nabla f(x_k)^T d_k
$$

å…¶ä¸­ $c_1 \in (0, 1)$ï¼ˆé€šå¸¸å–0.0001ï¼‰ã€‚

---

### 3. ä¿¡èµ–åŸŸNewtonæ³•

**æ€æƒ³**: åœ¨ä¿¡èµ–åŸŸå†…æœ€å°åŒ–äºŒæ¬¡æ¨¡å‹ã€‚

**å­é—®é¢˜**:

$$
\min_{d} \quad m_k(d) = f(x_k) + \nabla f(x_k)^T d + \frac{1}{2} d^T H_k d
$$

$$
\text{s.t.} \quad \|d\| \leq \Delta_k
$$

å…¶ä¸­ $\Delta_k$ æ˜¯ä¿¡èµ–åŸŸåŠå¾„ã€‚

**æ›´æ–°è§„åˆ™**:

æ ¹æ®å®é™…ä¸‹é™ä¸é¢„æµ‹ä¸‹é™çš„æ¯”å€¼è°ƒæ•´ä¿¡èµ–åŸŸï¼š

$$
\rho_k = \frac{f(x_k) - f(x_k + d_k)}{m_k(0) - m_k(d_k)}
$$

- å¦‚æœ $\rho_k > 0.75$ï¼šå¢å¤§ $\Delta_{k+1}$
- å¦‚æœ $\rho_k < 0.25$ï¼šå‡å° $\Delta_{k+1}$

---

## ğŸ“Š æ‹ŸNewtonæ³•

**æ ¸å¿ƒæ€æƒ³**: é¿å…è®¡ç®—HessiançŸ©é˜µï¼Œç”¨è¿‘ä¼¼çŸ©é˜µ $B_k \approx H_k$ ä»£æ›¿ã€‚

### 1. BFGSç®—æ³•

**Broyden-Fletcher-Goldfarb-Shannoç®—æ³•**:

**æ‹ŸNewtonæ¡ä»¶** (Secant equation):

$$
B_{k+1} s_k = y_k
$$

å…¶ä¸­ï¼š

- $s_k = x_{k+1} - x_k$
- $y_k = \nabla f(x_{k+1}) - \nabla f(x_k)$

**BFGSæ›´æ–°å…¬å¼**:

$$
B_{k+1} = B_k - \frac{B_k s_k s_k^T B_k}{s_k^T B_k s_k} + \frac{y_k y_k^T}{y_k^T s_k}
$$

æˆ–è€…ç›´æ¥æ›´æ–°é€†çŸ©é˜µ $H_k = B_k^{-1}$ï¼š

$$
H_{k+1} = \left(I - \frac{s_k y_k^T}{y_k^T s_k}\right) H_k \left(I - \frac{y_k s_k^T}{y_k^T s_k}\right) + \frac{s_k s_k^T}{y_k^T s_k}
$$

---

**ç®—æ³• 1.2 (BFGS)**:

```text
è¾“å…¥: åˆå§‹ç‚¹ xâ‚€, åˆå§‹Hessiané€† Hâ‚€ = I
è¾“å‡º: æœ€ä¼˜è§£ x*

1. for k = 0, 1, 2, ... do
2.     è®¡ç®—æ¢¯åº¦ g_k = âˆ‡f(x_k)
3.     è®¡ç®—æœç´¢æ–¹å‘ d_k = -H_k g_k
4.     çº¿æœç´¢: é€‰æ‹© Î±_k æ»¡è¶³Wolfeæ¡ä»¶
5.     x_{k+1} = x_k + Î±_k d_k
6.     s_k = x_{k+1} - x_k
7.     y_k = âˆ‡f(x_{k+1}) - âˆ‡f(x_k)
8.     æ›´æ–° H_{k+1} (BFGSå…¬å¼)
9. end for
```

---

### 2. L-BFGSç®—æ³•

**Limited-memory BFGS**:

**é—®é¢˜**: BFGSéœ€è¦å­˜å‚¨ $n \times n$ çŸ©é˜µï¼Œå¯¹äºå¤§è§„æ¨¡é—®é¢˜ä¸å¯è¡Œã€‚

**è§£å†³**: åªå­˜å‚¨æœ€è¿‘ $m$ æ¬¡è¿­ä»£çš„ $(s_i, y_i)$ å¯¹ï¼ˆé€šå¸¸ $m = 5 \sim 20$ï¼‰ã€‚

**ä¸¤å¾ªç¯é€’å½’**:

ä¸æ˜¾å¼å­˜å‚¨ $H_k$ï¼Œè€Œæ˜¯é€šè¿‡é€’å½’è®¡ç®— $H_k g_k$ã€‚

**å­˜å‚¨éœ€æ±‚**: $O(mn)$ vs $O(n^2)$

---

**ç®—æ³• 1.3 (L-BFGSä¸¤å¾ªç¯é€’å½’)**:

```text
è¾“å…¥: æ¢¯åº¦ g, å†å² {(s_i, y_i)}_{i=k-m}^{k-1}
è¾“å‡º: H_k g

1. q = g
2. for i = k-1, k-2, ..., k-m do
3.     Î±_i = Ï_i s_i^T q, where Ï_i = 1/(y_i^T s_i)
4.     q = q - Î±_i y_i
5. end for
6. r = H_0 q  (é€šå¸¸ H_0 = Î³I, Î³ = s_{k-1}^T y_{k-1} / y_{k-1}^T y_{k-1})
7. for i = k-m, k-m+1, ..., k-1 do
8.     Î² = Ï_i y_i^T r
9.     r = r + s_i (Î±_i - Î²)
10. end for
11. return r
```

---

### 3. DFPç®—æ³•

**Davidon-Fletcher-Powellç®—æ³•**:

**æ›´æ–°å…¬å¼**:

$$
H_{k+1} = H_k - \frac{H_k y_k y_k^T H_k}{y_k^T H_k y_k} + \frac{s_k s_k^T}{y_k^T s_k}
$$

**ä¸BFGSçš„å…³ç³»**: DFPæ˜¯BFGSçš„å¯¹å¶å½¢å¼ã€‚

---

## ğŸ”¬ å…±è½­æ¢¯åº¦æ³•

### 1. çº¿æ€§å…±è½­æ¢¯åº¦

**é—®é¢˜**: æ±‚è§£çº¿æ€§ç³»ç»Ÿ $Ax = b$ï¼Œå…¶ä¸­ $A$ æ˜¯å¯¹ç§°æ­£å®šçŸ©é˜µã€‚

ç­‰ä»·äºæœ€å°åŒ–äºŒæ¬¡å‡½æ•°ï¼š

$$
f(x) = \frac{1}{2} x^T A x - b^T x
$$

**å…±è½­æ–¹å‘**:

æ–¹å‘ $d_i$ å’Œ $d_j$ å…³äº $A$ å…±è½­ï¼Œå¦‚æœï¼š

$$
d_i^T A d_j = 0, \quad i \neq j
$$

---

**ç®—æ³• 1.4 (çº¿æ€§å…±è½­æ¢¯åº¦)**:

```text
è¾“å…¥: A, b, xâ‚€
è¾“å‡º: è§£ x

1. râ‚€ = b - Axâ‚€
2. dâ‚€ = râ‚€
3. for k = 0, 1, 2, ... do
4.     Î±_k = (r_k^T r_k) / (d_k^T A d_k)
5.     x_{k+1} = x_k + Î±_k d_k
6.     r_{k+1} = r_k - Î±_k A d_k
7.     if ||r_{k+1}|| < Îµ then
8.         return x_{k+1}
9.     Î²_k = (r_{k+1}^T r_{k+1}) / (r_k^T r_k)
10.    d_{k+1} = r_{k+1} + Î²_k d_k
11. end for
```

**æ”¶æ•›æ€§**: æœ€å¤š $n$ æ­¥ç²¾ç¡®æ”¶æ•›ï¼ˆç†è®ºä¸Šï¼‰ã€‚

---

### 2. éçº¿æ€§å…±è½­æ¢¯åº¦

**æ‰©å±•åˆ°éçº¿æ€§ä¼˜åŒ–**:

$$
x_{k+1} = x_k + \alpha_k d_k
$$

$$
d_k = -\nabla f(x_k) + \beta_k d_{k-1}
$$

**$\beta_k$ çš„é€‰æ‹©**:

- **Fletcher-Reeves**:
  $$
  \beta_k^{FR} = \frac{\|\nabla f(x_k)\|^2}{\|\nabla f(x_{k-1})\|^2}
  $$

- **Polak-RibiÃ¨re**:
  $$
  \beta_k^{PR} = \frac{\nabla f(x_k)^T (\nabla f(x_k) - \nabla f(x_{k-1}))}{\|\nabla f(x_{k-1})\|^2}
  $$

- **Hestenes-Stiefel**:
  $$
  \beta_k^{HS} = \frac{\nabla f(x_k)^T (\nabla f(x_k) - \nabla f(x_{k-1}))}{d_{k-1}^T (\nabla f(x_k) - \nabla f(x_{k-1}))}
  $$

---

### 3. é¢„æ¡ä»¶å…±è½­æ¢¯åº¦

**æ€æƒ³**: ç”¨é¢„æ¡ä»¶çŸ©é˜µ $M$ åŠ é€Ÿæ”¶æ•›ã€‚

æ±‚è§£ $M^{-1} A x = M^{-1} b$ è€Œä¸æ˜¯ $Ax = b$ã€‚

**é€‰æ‹© $M$**:

- $M \approx A$ï¼ˆæ˜“äºæ±‚é€†ï¼‰
- æ”¹å–„æ¡ä»¶æ•° $\kappa(M^{-1}A) < \kappa(A)$

---

## ğŸ’¡ Gauss-Newtonæ³•

### 1. åŸºæœ¬Gauss-Newtonæ³•

**é—®é¢˜**: éçº¿æ€§æœ€å°äºŒä¹˜

$$
\min_x \quad f(x) = \frac{1}{2} \|r(x)\|^2 = \frac{1}{2} \sum_{i=1}^m r_i(x)^2
$$

å…¶ä¸­ $r(x) = [r_1(x), \ldots, r_m(x)]^T$ æ˜¯æ®‹å·®å‘é‡ã€‚

**æ¢¯åº¦**:

$$
\nabla f(x) = J(x)^T r(x)
$$

å…¶ä¸­ $J(x)$ æ˜¯æ®‹å·®çš„JacobiançŸ©é˜µã€‚

**Hessianè¿‘ä¼¼**:

$$
\nabla^2 f(x) = J(x)^T J(x) + \sum_{i=1}^m r_i(x) \nabla^2 r_i(x)
$$

å¿½ç•¥äºŒé˜¶é¡¹ï¼š

$$
H \approx J(x)^T J(x)
$$

**Gauss-Newtonæ–¹å‘**:

$$
d = -(J^T J)^{-1} J^T r
$$

---

### 2. Levenberg-Marquardtç®—æ³•

**æ”¹è¿›**: ç»“åˆæ¢¯åº¦ä¸‹é™å’ŒGauss-Newtonã€‚

**æ›´æ–°è§„åˆ™**:

$$
(J^T J + \lambda I) d = -J^T r
$$

- $\lambda$ å¤§ï¼šæ¥è¿‘æ¢¯åº¦ä¸‹é™
- $\lambda$ å°ï¼šæ¥è¿‘Gauss-Newton

**è‡ªé€‚åº”è°ƒæ•´ $\lambda$**:

- å¦‚æœè¿­ä»£æˆåŠŸï¼šå‡å° $\lambda$
- å¦‚æœè¿­ä»£å¤±è´¥ï¼šå¢å¤§ $\lambda$

---

## ğŸ¨ åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„åº”ç”¨

### 1. è‡ªç„¶æ¢¯åº¦

**Fisherä¿¡æ¯çŸ©é˜µ**:

$$
F = \mathbb{E}[\nabla \log p(y|x; \theta) \nabla \log p(y|x; \theta)^T]
$$

**è‡ªç„¶æ¢¯åº¦**:

$$
\tilde{\nabla} L = F^{-1} \nabla L
$$

**æ›´æ–°è§„åˆ™**:

$$
\theta_{t+1} = \theta_t - \eta F^{-1} \nabla L(\theta_t)
$$

**ä¼˜åŠ¿**: å‚æ•°ç©ºé—´ä¸å˜æ€§ã€‚

---

### 2. K-FAC

**Kronecker-Factored Approximate Curvature**:

**æ€æƒ³**: ç”¨Kroneckerç§¯è¿‘ä¼¼Fisherä¿¡æ¯çŸ©é˜µã€‚

å¯¹äºå±‚ $l$ï¼š

$$
F_l \approx A_l \otimes G_l
$$

å…¶ä¸­ï¼š

- $A_l$: æ¿€æ´»çš„äºŒé˜¶ç»Ÿè®¡é‡
- $G_l$: æ¢¯åº¦çš„äºŒé˜¶ç»Ÿè®¡é‡

**è®¡ç®—å¤æ‚åº¦**: $O(n^{3/2})$ vs $O(n^3)$

---

### 3. Shampoo

**Scalable Higher-order Adaptive Methods for Parallel and Distributed Optimization**:

**æ€æƒ³**: å¯¹æ¯å±‚ä½¿ç”¨ç‹¬ç«‹çš„é¢„æ¡ä»¶çŸ©é˜µã€‚

**æ›´æ–°è§„åˆ™**:

$$
\theta_{t+1} = \theta_t - \eta (L_t \otimes R_t)^{-1/4} \nabla L(\theta_t)
$$

å…¶ä¸­ $L_t$ å’Œ $R_t$ æ˜¯å·¦å³é¢„æ¡ä»¶çŸ©é˜µã€‚

---

## ğŸ”§ å®ç”¨æŠ€å·§

### 1. Hessianè¿‘ä¼¼

**æœ‰é™å·®åˆ†**:

$$
\frac{\partial^2 f}{\partial x_i \partial x_j} \approx \frac{f(x + \epsilon e_i + \epsilon e_j) - f(x + \epsilon e_i) - f(x + \epsilon e_j) + f(x)}{\ epsilon^2}
$$

**è‡ªåŠ¨å¾®åˆ†**: ä½¿ç”¨åå‘æ¨¡å¼è‡ªåŠ¨å¾®åˆ†è®¡ç®—Hessian-å‘é‡ç§¯ã€‚

---

### 2. çº¿æœç´¢ç­–ç•¥

**Wolfeæ¡ä»¶**:

1. **å……åˆ†ä¸‹é™æ¡ä»¶** (Armijo):
   $$
   f(x_k + \alpha d_k) \leq f(x_k) + c_1 \alpha \nabla f(x_k)^T d_k
   $$

2. **æ›²ç‡æ¡ä»¶**:
   $$
   \nabla f(x_k + \alpha d_k)^T d_k \geq c_2 \nabla f(x_k)^T d_k
   $$

å…¶ä¸­ $0 < c_1 < c_2 < 1$ï¼ˆé€šå¸¸ $c_1 = 10^{-4}$, $c_2 = 0.9$ï¼‰ã€‚

---

### 3. æ”¶æ•›æ€§åˆ†æ

**æ”¶æ•›é€Ÿåº¦æ¯”è¾ƒ**:

| æ–¹æ³• | æ”¶æ•›é€Ÿåº¦ | æ¯æ­¥æˆæœ¬ |
|------|----------|----------|
| æ¢¯åº¦ä¸‹é™ | çº¿æ€§ | $O(n)$ |
| å…±è½­æ¢¯åº¦ | è¶…çº¿æ€§ | $O(n)$ |
| BFGS | è¶…çº¿æ€§ | $O(n^2)$ |
| L-BFGS | è¶…çº¿æ€§ | $O(mn)$ |
| Newton | äºŒæ¬¡ | $O(n^3)$ |

---

## ğŸ’» Pythonå®ç°

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import line_search
from scipy.linalg import cho_factor, cho_solve

# 1. Newtonæ³•
def newton_method(f, grad_f, hess_f, x0, max_iter=100, tol=1e-6):
    """Newtonæ³•"""
    x = x0.copy()
    trajectory = [x.copy()]
    
    for k in range(max_iter):
        g = grad_f(x)
        
        if np.linalg.norm(g) < tol:
            break
        
        H = hess_f(x)
        
        # æ±‚è§£Newtonæ–¹ç¨‹: H * d = -g
        try:
            d = -np.linalg.solve(H, g)
        except np.linalg.LinAlgError:
            print("Hessian is singular, using gradient descent")
            d = -g
        
        # çº¿æœç´¢
        alpha = backtracking_line_search(f, grad_f, x, d)
        
        x = x + alpha * d
        trajectory.append(x.copy())
    
    return x, np.array(trajectory)


def backtracking_line_search(f, grad_f, x, d, alpha=1.0, rho=0.5, c=1e-4):
    """Armijoå›æº¯çº¿æœç´¢"""
    f_x = f(x)
    grad_f_x = grad_f(x)
    
    while f(x + alpha * d) > f_x + c * alpha * np.dot(grad_f_x, d):
        alpha *= rho
        if alpha < 1e-10:
            break
    
    return alpha


# 2. BFGSç®—æ³•
def bfgs(f, grad_f, x0, max_iter=100, tol=1e-6):
    """BFGSç®—æ³•"""
    n = len(x0)
    x = x0.copy()
    H = np.eye(n)  # åˆå§‹Hessiané€†è¿‘ä¼¼
    trajectory = [x.copy()]
    
    for k in range(max_iter):
        g = grad_f(x)
        
        if np.linalg.norm(g) < tol:
            break
        
        # æœç´¢æ–¹å‘
        d = -H @ g
        
        # çº¿æœç´¢
        alpha = backtracking_line_search(f, grad_f, x, d)
        
        # æ›´æ–°
        s = alpha * d
        x_new = x + s
        y = grad_f(x_new) - g
        
        # BFGSæ›´æ–°H
        rho = 1.0 / (y @ s)
        if rho > 0:  # ç¡®ä¿æ­£å®šæ€§
            I = np.eye(n)
            H = (I - rho * np.outer(s, y)) @ H @ (I - rho * np.outer(y, s)) + rho * np.outer(s, s)
        
        x = x_new
        trajectory.append(x.copy())
    
    return x, np.array(trajectory)


# 3. L-BFGSç®—æ³•
class LBFGS:
    """L-BFGSç®—æ³•"""
    
    def __init__(self, m=10):
        self.m = m  # å†å²å¤§å°
        self.s_list = []
        self.y_list = []
        
    def two_loop_recursion(self, g):
        """ä¸¤å¾ªç¯é€’å½’è®¡ç®—H*g"""
        q = g.copy()
        alpha_list = []
        
        # ç¬¬ä¸€ä¸ªå¾ªç¯
        for s, y in zip(reversed(self.s_list), reversed(self.y_list)):
            rho = 1.0 / (y @ s)
            alpha = rho * (s @ q)
            q = q - alpha * y
            alpha_list.append(alpha)
        
        alpha_list.reverse()
        
        # åˆå§‹Hessiané€†è¿‘ä¼¼
        if len(self.s_list) > 0:
            s = self.s_list[-1]
            y = self.y_list[-1]
            gamma = (s @ y) / (y @ y)
        else:
            gamma = 1.0
        
        r = gamma * q
        
        # ç¬¬äºŒä¸ªå¾ªç¯
        for s, y, alpha in zip(self.s_list, self.y_list, alpha_list):
            rho = 1.0 / (y @ s)
            beta = rho * (y @ r)
            r = r + s * (alpha - beta)
        
        return r
    
    def update(self, s, y):
        """æ›´æ–°å†å²"""
        if len(self.s_list) >= self.m:
            self.s_list.pop(0)
            self.y_list.pop(0)
        
        self.s_list.append(s)
        self.y_list.append(y)
    
    def optimize(self, f, grad_f, x0, max_iter=100, tol=1e-6):
        """ä¼˜åŒ–"""
        x = x0.copy()
        trajectory = [x.copy()]
        
        for k in range(max_iter):
            g = grad_f(x)
            
            if np.linalg.norm(g) < tol:
                break
            
            # è®¡ç®—æœç´¢æ–¹å‘
            d = -self.two_loop_recursion(g)
            
            # çº¿æœç´¢
            alpha = backtracking_line_search(f, grad_f, x, d)
            
            # æ›´æ–°
            s = alpha * d
            x_new = x + s
            y = grad_f(x_new) - g
            
            # æ›´æ–°å†å²
            if y @ s > 0:  # ç¡®ä¿æ­£å®šæ€§
                self.update(s, y)
            
            x = x_new
            trajectory.append(x.copy())
        
        return x, np.array(trajectory)


# 4. å…±è½­æ¢¯åº¦æ³•
def conjugate_gradient(A, b, x0=None, max_iter=None, tol=1e-6):
    """çº¿æ€§å…±è½­æ¢¯åº¦æ³•"""
    n = len(b)
    if x0 is None:
        x = np.zeros(n)
    else:
        x = x0.copy()
    
    if max_iter is None:
        max_iter = n
    
    r = b - A @ x
    d = r.copy()
    
    trajectory = [x.copy()]
    
    for k in range(max_iter):
        if np.linalg.norm(r) < tol:
            break
        
        Ad = A @ d
        alpha = (r @ r) / (d @ Ad)
        
        x = x + alpha * d
        r_new = r - alpha * Ad
        
        beta = (r_new @ r_new) / (r @ r)
        d = r_new + beta * d
        
        r = r_new
        trajectory.append(x.copy())
    
    return x, np.array(trajectory)


# 5. Gauss-Newtonæ³•
def gauss_newton(residual, jacobian, x0, max_iter=100, tol=1e-6):
    """Gauss-Newtonæ³•"""
    x = x0.copy()
    trajectory = [x.copy()]
    
    for k in range(max_iter):
        r = residual(x)
        J = jacobian(x)
        
        if np.linalg.norm(r) < tol:
            break
        
        # æ±‚è§£æ­£è§„æ–¹ç¨‹: (J^T J) d = -J^T r
        d = -np.linalg.solve(J.T @ J, J.T @ r)
        
        # çº¿æœç´¢
        def f(x):
            return 0.5 * np.sum(residual(x)**2)
        
        def grad_f(x):
            return jacobian(x).T @ residual(x)
        
        alpha = backtracking_line_search(f, grad_f, x, d)
        
        x = x + alpha * d
        trajectory.append(x.copy())
    
    return x, np.array(trajectory)


# 6. å¯è§†åŒ–æ¯”è¾ƒ
def compare_methods():
    """æ¯”è¾ƒä¸åŒä¼˜åŒ–æ–¹æ³•"""
    # Rosenbrockå‡½æ•°
    def f(x):
        return (1 - x[0])**2 + 100*(x[1] - x[0]**2)**2
    
    def grad_f(x):
        return np.array([
            -2*(1 - x[0]) - 400*x[0]*(x[1] - x[0]**2),
            200*(x[1] - x[0]**2)
        ])
    
    def hess_f(x):
        return np.array([
            [2 - 400*(x[1] - x[0]**2) + 800*x[0]**2, -400*x[0]],
            [-400*x[0], 200]
        ])
    
    x0 = np.array([-1.0, 1.0])
    
    # è¿è¡Œä¸åŒæ–¹æ³•
    methods = {
        'Newton': lambda: newton_method(f, grad_f, hess_f, x0, max_iter=50),
        'BFGS': lambda: bfgs(f, grad_f, x0, max_iter=50),
        'L-BFGS': lambda: LBFGS(m=5).optimize(f, grad_f, x0, max_iter=50)
    }
    
    # ç»˜åˆ¶
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # ç­‰é«˜çº¿å›¾
    x = np.linspace(-1.5, 1.5, 100)
    y = np.linspace(-0.5, 2.5, 100)
    X, Y = np.meshgrid(x, y)
    Z = np.array([[f(np.array([x, y])) for x in x] for y in y])
    
    ax1.contour(X, Y, Z, levels=np.logspace(-1, 3, 20), cmap='viridis')
    
    colors = ['red', 'blue', 'green']
    for (name, method), color in zip(methods.items(), colors):
        x_opt, traj = method()
        ax1.plot(traj[:, 0], traj[:, 1], 'o-', color=color, 
                label=f'{name} ({len(traj)} iters)', markersize=4, linewidth=2)
        
        # æ”¶æ•›æ›²çº¿
        f_vals = [f(x) for x in traj]
        ax2.semilogy(f_vals, 'o-', color=color, label=name, linewidth=2)
    
    ax1.plot(1, 1, 'r*', markersize=15, label='Optimum')
    ax1.set_xlabel('xâ‚')
    ax1.set_ylabel('xâ‚‚')
    ax1.set_title('Optimization Trajectories')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    ax2.set_xlabel('Iteration')
    ax2.set_ylabel('Function Value')
    ax2.set_title('Convergence')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    # plt.show()


if __name__ == "__main__":
    print("=" * 60)
    print("äºŒé˜¶ä¼˜åŒ–æ–¹æ³•ç¤ºä¾‹")
    print("=" * 60 + "\n")
    
    print("æ¯”è¾ƒä¸åŒä¼˜åŒ–æ–¹æ³•...")
    compare_methods()
    
    print("\næ‰€æœ‰ç¤ºä¾‹å®Œæˆï¼")
```

---

## ğŸ“š ç»ƒä¹ é¢˜

### ç»ƒä¹ 1ï¼šNewtonæ³•

å®ç°Newtonæ³•æ±‚è§£ $f(x) = x^4 - 3x^3 + 2$ çš„æœ€å°å€¼ã€‚

### ç»ƒä¹ 2ï¼šBFGS

ä½¿ç”¨BFGSç®—æ³•æœ€å°åŒ–Rosenbrockå‡½æ•°ã€‚

### ç»ƒä¹ 3ï¼šå…±è½­æ¢¯åº¦

ç”¨å…±è½­æ¢¯åº¦æ³•æ±‚è§£çº¿æ€§ç³»ç»Ÿ $Ax = b$ï¼Œå…¶ä¸­ $A$ æ˜¯å¤§å‹ç¨€ç–çŸ©é˜µã€‚

### ç»ƒä¹ 4ï¼šGauss-Newton

ä½¿ç”¨Gauss-Newtonæ³•æ‹Ÿåˆéçº¿æ€§æ¨¡å‹ã€‚

---

## ğŸ“ ç›¸å…³è¯¾ç¨‹

| å¤§å­¦ | è¯¾ç¨‹ |
|------|------|
| **Stanford** | EE364B - Convex Optimization II |
| **MIT** | 6.255J - Optimization Methods |
| **CMU** | 10-725 - Convex Optimization |
| **UC Berkeley** | EECS227C - Convex Optimization |

---

## ğŸ“– å‚è€ƒæ–‡çŒ®

1. **Nocedal & Wright (2006)**. *Numerical Optimization*. Springer.

2. **Boyd & Vandenberghe (2004)**. *Convex Optimization*. Cambridge University Press.

3. **Martens & Grosse (2015)**. *Optimizing Neural Networks with Kronecker-factored Approximate Curvature*. ICML.

4. **Gupta et al. (2018)**. *Shampoo: Preconditioned Stochastic Tensor Optimization*. ICML.

5. **Amari (1998)**. *Natural Gradient Works Efficiently in Learning*. Neural Computation.

---

*æœ€åæ›´æ–°ï¼š2025å¹´10æœˆ*-
