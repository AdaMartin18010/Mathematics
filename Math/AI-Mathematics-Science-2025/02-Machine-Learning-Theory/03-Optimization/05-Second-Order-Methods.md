# äºŒé˜¶ä¼˜åŒ–æ–¹æ³• (Second-Order Optimization Methods)

> **Beyond Gradient Descent: Leveraging Curvature Information**
>
> è¶…è¶Šæ¢¯åº¦ä¸‹é™ï¼šåˆ©ç”¨æ›²ç‡ä¿¡æ¯

---

## ç›®å½•

- [äºŒé˜¶ä¼˜åŒ–æ–¹æ³• (Second-Order Optimization Methods)](#äºŒé˜¶ä¼˜åŒ–æ–¹æ³•-second-order-optimization-methods)
  - [ç›®å½•](#ç›®å½•)
  - [ğŸ“‹ æ ¸å¿ƒæ€æƒ³](#-æ ¸å¿ƒæ€æƒ³)
  - [ğŸ¯ Newtonæ³•](#-newtonæ³•)
    - [1. åŸºæœ¬Newtonæ³•](#1-åŸºæœ¬newtonæ³•)
    - [2. é˜»å°¼Newtonæ³•](#2-é˜»å°¼newtonæ³•)
    - [3. ä¿¡èµ–åŸŸNewtonæ³•](#3-ä¿¡èµ–åŸŸnewtonæ³•)
  - [ğŸ“Š æ‹ŸNewtonæ³•](#-æ‹Ÿnewtonæ³•)
    - [1. BFGSç®—æ³•](#1-bfgsç®—æ³•)
    - [2. L-BFGSç®—æ³•](#2-l-bfgsç®—æ³•)
      - [L-BFGSæ”¶æ•›æ€§ç†è®ºåˆ†æ](#l-bfgsæ”¶æ•›æ€§ç†è®ºåˆ†æ)
      - [L-BFGSè¶…çº¿æ€§æ”¶æ•›æ€§](#l-bfgsè¶…çº¿æ€§æ”¶æ•›æ€§)
      - [æ”¶æ•›é€Ÿåº¦æ€»ç»“](#æ”¶æ•›é€Ÿåº¦æ€»ç»“)
      - [å®è·µä¸­çš„è€ƒè™‘](#å®è·µä¸­çš„è€ƒè™‘)
      - [Pythonå®ç°éªŒè¯](#pythonå®ç°éªŒè¯)
    - [3. DFPç®—æ³•](#3-dfpç®—æ³•)
  - [ğŸ”¬ å…±è½­æ¢¯åº¦æ³•](#-å…±è½­æ¢¯åº¦æ³•)
    - [1. çº¿æ€§å…±è½­æ¢¯åº¦](#1-çº¿æ€§å…±è½­æ¢¯åº¦)
    - [2. éçº¿æ€§å…±è½­æ¢¯åº¦](#2-éçº¿æ€§å…±è½­æ¢¯åº¦)
    - [3. é¢„æ¡ä»¶å…±è½­æ¢¯åº¦](#3-é¢„æ¡ä»¶å…±è½­æ¢¯åº¦)
  - [ğŸ’¡ Gauss-Newtonæ³•](#-gauss-newtonæ³•)
    - [1. åŸºæœ¬Gauss-Newtonæ³•](#1-åŸºæœ¬gauss-newtonæ³•)
    - [2. Levenberg-Marquardtç®—æ³•](#2-levenberg-marquardtç®—æ³•)
  - [ğŸ¨ åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„åº”ç”¨](#-åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„åº”ç”¨)
    - [1. è‡ªç„¶æ¢¯åº¦](#1-è‡ªç„¶æ¢¯åº¦)
    - [2. K-FAC](#2-k-fac)
    - [3. Shampoo](#3-shampoo)
  - [ğŸ”§ å®ç”¨æŠ€å·§](#-å®ç”¨æŠ€å·§)
    - [1. Hessianè¿‘ä¼¼](#1-hessianè¿‘ä¼¼)
    - [2. çº¿æœç´¢ç­–ç•¥](#2-çº¿æœç´¢ç­–ç•¥)
    - [3. æ”¶æ•›æ€§åˆ†æ](#3-æ”¶æ•›æ€§åˆ†æ)
  - [ğŸ’» Pythonå®ç°](#-pythonå®ç°)
  - [ğŸ“š ç»ƒä¹ é¢˜](#-ç»ƒä¹ é¢˜)
    - [ç»ƒä¹ 1ï¼šNewtonæ³•](#ç»ƒä¹ 1newtonæ³•)
    - [ç»ƒä¹ 2ï¼šBFGS](#ç»ƒä¹ 2bfgs)
    - [ç»ƒä¹ 3ï¼šå…±è½­æ¢¯åº¦](#ç»ƒä¹ 3å…±è½­æ¢¯åº¦)
    - [ç»ƒä¹ 4ï¼šGauss-Newton](#ç»ƒä¹ 4gauss-newton)
  - [ğŸ“ ç›¸å…³è¯¾ç¨‹](#-ç›¸å…³è¯¾ç¨‹)
  - [ğŸ“– å‚è€ƒæ–‡çŒ®](#-å‚è€ƒæ–‡çŒ®)

---

## ğŸ“‹ æ ¸å¿ƒæ€æƒ³

**äºŒé˜¶ä¼˜åŒ–æ–¹æ³•**åˆ©ç”¨ç›®æ ‡å‡½æ•°çš„äºŒé˜¶å¯¼æ•°ä¿¡æ¯ï¼ˆHessiançŸ©é˜µï¼‰ï¼Œç›¸æ¯”ä¸€é˜¶æ–¹æ³•ï¼ˆå¦‚æ¢¯åº¦ä¸‹é™ï¼‰å…·æœ‰æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ã€‚

**ä¸ºä»€ä¹ˆä½¿ç”¨äºŒé˜¶æ–¹æ³•**:

```text
ä¼˜åŠ¿:
â”œâ”€ æ”¶æ•›é€Ÿåº¦å¿«: äºŒæ¬¡æ”¶æ•› vs çº¿æ€§æ”¶æ•›
â”œâ”€ è‡ªé€‚åº”æ­¥é•¿: è‡ªåŠ¨è°ƒæ•´å­¦ä¹ ç‡
â”œâ”€ æ›²ç‡ä¿¡æ¯: åˆ©ç”¨HessiançŸ©é˜µ
â””â”€ ç†è®ºä¿è¯: å¼ºå‡¸å‡½æ•°å±€éƒ¨äºŒæ¬¡æ”¶æ•›

æŒ‘æˆ˜:
â”œâ”€ è®¡ç®—æˆæœ¬: O(nÂ³) Hessianæ±‚é€†
â”œâ”€ å­˜å‚¨éœ€æ±‚: O(nÂ²) HessiançŸ©é˜µ
â”œâ”€ éå‡¸é—®é¢˜: å¯èƒ½æ”¶æ•›åˆ°éç‚¹
â””â”€ æ·±åº¦å­¦ä¹ : å‚æ•°é‡å·¨å¤§
```

---

## ğŸ¯ Newtonæ³•

### 1. åŸºæœ¬Newtonæ³•

**ç®—æ³•æ€æƒ³**:

åœ¨å½“å‰ç‚¹ $x_k$ å¤„ï¼Œç”¨äºŒé˜¶Taylorå±•å¼€é€¼è¿‘ç›®æ ‡å‡½æ•°ï¼š

$$
f(x) \approx f(x_k) + \nabla f(x_k)^T (x - x_k) + \frac{1}{2} (x - x_k)^T H_k (x - x_k)
$$

å…¶ä¸­ $H_k = \nabla^2 f(x_k)$ æ˜¯HessiançŸ©é˜µã€‚

**æœ€ä¼˜åŒ–æ¡ä»¶**:

$$
\nabla f(x) \approx \nabla f(x_k) + H_k (x - x_k) = 0
$$

**Newtonæ–¹å‘**:

$$
d_k = -H_k^{-1} \nabla f(x_k)
$$

**æ›´æ–°è§„åˆ™**:

$$
x_{k+1} = x_k + d_k = x_k - H_k^{-1} \nabla f(x_k)
$$

---

**ç®—æ³• 1.1 (Newtonæ³•)**:

```text
è¾“å…¥: åˆå§‹ç‚¹ xâ‚€, å®¹å·® Îµ
è¾“å‡º: æœ€ä¼˜è§£ x*

1. for k = 0, 1, 2, ... do
2.     è®¡ç®—æ¢¯åº¦ g_k = âˆ‡f(x_k)
3.     if ||g_k|| < Îµ then
4.         return x_k
5.     è®¡ç®—Hessian H_k = âˆ‡Â²f(x_k)
6.     æ±‚è§£ H_k d_k = -g_k  (Newtonæ–¹ç¨‹)
7.     x_{k+1} = x_k + d_k
8. end for
```

---

**æ”¶æ•›æ€§**:

**å®šç† 1.1 (Newtonæ³•æ”¶æ•›æ€§)**:

è®¾ $f$ æ˜¯å¼ºå‡¸å‡½æ•°ï¼Œ$\nabla^2 f$ æ˜¯Lipschitzè¿ç»­çš„ã€‚å¦‚æœåˆå§‹ç‚¹ $x_0$ è¶³å¤Ÿæ¥è¿‘æœ€ä¼˜è§£ $x^*$ï¼Œåˆ™Newtonæ³•**äºŒæ¬¡æ”¶æ•›**ï¼š

$$
\|x_{k+1} - x^*\| \leq C \|x_k - x^*\|^2
$$

---

### 2. é˜»å°¼Newtonæ³•

**é—®é¢˜**: åŸºæœ¬Newtonæ³•å¯èƒ½ä¸æ”¶æ•›ï¼ˆè¿œç¦»æœ€ä¼˜è§£æ—¶ï¼‰ã€‚

**è§£å†³**: å¼•å…¥æ­¥é•¿ $\alpha_k$ï¼š

$$
x_{k+1} = x_k - \alpha_k H_k^{-1} \nabla f(x_k)
$$

**Armijoçº¿æœç´¢**:

é€‰æ‹© $\alpha_k$ ä½¿å¾—ï¼š

$$
f(x_k + \alpha_k d_k) \leq f(x_k) + c_1 \alpha_k \nabla f(x_k)^T d_k
$$

å…¶ä¸­ $c_1 \in (0, 1)$ï¼ˆé€šå¸¸å–0.0001ï¼‰ã€‚

---

### 3. ä¿¡èµ–åŸŸNewtonæ³•

**æ€æƒ³**: åœ¨ä¿¡èµ–åŸŸå†…æœ€å°åŒ–äºŒæ¬¡æ¨¡å‹ã€‚

**å­é—®é¢˜**:

$$
\min_{d} \quad m_k(d) = f(x_k) + \nabla f(x_k)^T d + \frac{1}{2} d^T H_k d
$$

$$
\text{s.t.} \quad \|d\| \leq \Delta_k
$$

å…¶ä¸­ $\Delta_k$ æ˜¯ä¿¡èµ–åŸŸåŠå¾„ã€‚

**æ›´æ–°è§„åˆ™**:

æ ¹æ®å®é™…ä¸‹é™ä¸é¢„æµ‹ä¸‹é™çš„æ¯”å€¼è°ƒæ•´ä¿¡èµ–åŸŸï¼š

$$
\rho_k = \frac{f(x_k) - f(x_k + d_k)}{m_k(0) - m_k(d_k)}
$$

- å¦‚æœ $\rho_k > 0.75$ï¼šå¢å¤§ $\Delta_{k+1}$
- å¦‚æœ $\rho_k < 0.25$ï¼šå‡å° $\Delta_{k+1}$

---

## ğŸ“Š æ‹ŸNewtonæ³•

**æ ¸å¿ƒæ€æƒ³**: é¿å…è®¡ç®—HessiançŸ©é˜µï¼Œç”¨è¿‘ä¼¼çŸ©é˜µ $B_k \approx H_k$ ä»£æ›¿ã€‚

### 1. BFGSç®—æ³•

**Broyden-Fletcher-Goldfarb-Shannoç®—æ³•**:

**æ‹ŸNewtonæ¡ä»¶** (Secant equation):

$$
B_{k+1} s_k = y_k
$$

å…¶ä¸­ï¼š

- $s_k = x_{k+1} - x_k$
- $y_k = \nabla f(x_{k+1}) - \nabla f(x_k)$

**BFGSæ›´æ–°å…¬å¼**:

$$
B_{k+1} = B_k - \frac{B_k s_k s_k^T B_k}{s_k^T B_k s_k} + \frac{y_k y_k^T}{y_k^T s_k}
$$

æˆ–è€…ç›´æ¥æ›´æ–°é€†çŸ©é˜µ $H_k = B_k^{-1}$ï¼š

$$
H_{k+1} = \left(I - \frac{s_k y_k^T}{y_k^T s_k}\right) H_k \left(I - \frac{y_k s_k^T}{y_k^T s_k}\right) + \frac{s_k s_k^T}{y_k^T s_k}
$$

---

**ç®—æ³• 1.2 (BFGS)**:

```text
è¾“å…¥: åˆå§‹ç‚¹ xâ‚€, åˆå§‹Hessiané€† Hâ‚€ = I
è¾“å‡º: æœ€ä¼˜è§£ x*

1. for k = 0, 1, 2, ... do
2.     è®¡ç®—æ¢¯åº¦ g_k = âˆ‡f(x_k)
3.     è®¡ç®—æœç´¢æ–¹å‘ d_k = -H_k g_k
4.     çº¿æœç´¢: é€‰æ‹© Î±_k æ»¡è¶³Wolfeæ¡ä»¶
5.     x_{k+1} = x_k + Î±_k d_k
6.     s_k = x_{k+1} - x_k
7.     y_k = âˆ‡f(x_{k+1}) - âˆ‡f(x_k)
8.     æ›´æ–° H_{k+1} (BFGSå…¬å¼)
9. end for
```

---

### 2. L-BFGSç®—æ³•

**Limited-memory BFGS**:

**é—®é¢˜**: BFGSéœ€è¦å­˜å‚¨ $n \times n$ çŸ©é˜µï¼Œå¯¹äºå¤§è§„æ¨¡é—®é¢˜ä¸å¯è¡Œã€‚

**è§£å†³**: åªå­˜å‚¨æœ€è¿‘ $m$ æ¬¡è¿­ä»£çš„ $(s_i, y_i)$ å¯¹ï¼ˆé€šå¸¸ $m = 5 \sim 20$ï¼‰ã€‚

**ä¸¤å¾ªç¯é€’å½’**:

ä¸æ˜¾å¼å­˜å‚¨ $H_k$ï¼Œè€Œæ˜¯é€šè¿‡é€’å½’è®¡ç®— $H_k g_k$ã€‚

**å­˜å‚¨éœ€æ±‚**: $O(mn)$ vs $O(n^2)$

---

**ç®—æ³• 1.3 (L-BFGSä¸¤å¾ªç¯é€’å½’)**:

```text
è¾“å…¥: æ¢¯åº¦ g, å†å² {(s_i, y_i)}_{i=k-m}^{k-1}
è¾“å‡º: H_k g

1. q = g
2. for i = k-1, k-2, ..., k-m do
3.     Î±_i = Ï_i s_i^T q, where Ï_i = 1/(y_i^T s_i)
4.     q = q - Î±_i y_i
5. end for
6. r = H_0 q  (é€šå¸¸ H_0 = Î³I, Î³ = s_{k-1}^T y_{k-1} / y_{k-1}^T y_{k-1})
7. for i = k-m, k-m+1, ..., k-1 do
8.     Î² = Ï_i y_i^T r
9.     r = r + s_i (Î±_i - Î²)
10. end for
11. return r
```

---

#### L-BFGSæ”¶æ•›æ€§ç†è®ºåˆ†æ

**å®šç† 1.1 (L-BFGSå…¨å±€æ”¶æ•›æ€§)**:

è®¾ $f: \mathbb{R}^n \to \mathbb{R}$ æ˜¯è¿ç»­å¯å¾®çš„ï¼Œä¸”æ»¡è¶³ï¼š

1. **ä¸‹ç•Œæ¡ä»¶**: $f(x) \geq f_{\text{inf}} > -\infty$ å¯¹æ‰€æœ‰ $x$ æˆç«‹
2. **Lipschitzè¿ç»­æ¢¯åº¦**: å­˜åœ¨å¸¸æ•° $L > 0$ ä½¿å¾—
   $$
   \|\nabla f(x) - \nabla f(y)\| \leq L \|x - y\|, \quad \forall x, y
   $$

ä½¿ç”¨L-BFGSç®—æ³•ï¼Œé…åˆ**Wolfeçº¿æœç´¢**ï¼Œåˆ™ï¼š

$$
\liminf_{k \to \infty} \|\nabla f(x_k)\| = 0
$$

å³ï¼ŒL-BFGSäº§ç”Ÿçš„åºåˆ— $\{x_k\}$ çš„æŸä¸ªå­åºåˆ—æ”¶æ•›åˆ°ä¸€é˜¶ç¨³å®šç‚¹ã€‚

---

**è¯æ˜**ï¼š

**æ­¥éª¤1ï¼šWolfeæ¡ä»¶çš„ä½œç”¨**:

Wolfeçº¿æœç´¢ç¡®ä¿ï¼š

**(a) Armijoæ¡ä»¶ï¼ˆå……åˆ†ä¸‹é™ï¼‰**:
$$
f(x_k + \alpha_k d_k) \leq f(x_k) + c_1 \alpha_k \nabla f(x_k)^T d_k
$$

**(b) æ›²ç‡æ¡ä»¶**:
$$
\nabla f(x_k + \alpha_k d_k)^T d_k \geq c_2 \nabla f(x_k)^T d_k
$$

å…¶ä¸­ $0 < c_1 < c_2 < 1$ï¼ˆé€šå¸¸ $c_1 = 10^{-4}$, $c_2 = 0.9$ï¼‰ã€‚

**å…³é”®æ€§è´¨**: Wolfeæ¡ä»¶ä¿è¯ $s_k^T y_k > 0$ï¼ˆæ­£æ›²ç‡æ¡ä»¶ï¼‰ï¼Œå³ï¼š

$$
s_k^T y_k = (x_{k+1} - x_k)^T (\nabla f(x_{k+1}) - \nabla f(x_k)) > 0
$$

---

**æ­¥éª¤2ï¼šæœç´¢æ–¹å‘çš„ä¸‹é™æ€§**:

L-BFGSæœç´¢æ–¹å‘ $d_k = -H_k \nabla f(x_k)$ æ»¡è¶³ï¼š

$$
\nabla f(x_k)^T d_k = -\nabla f(x_k)^T H_k \nabla f(x_k)
$$

**å¼•ç†1**: å¦‚æœ $H_k$ æ˜¯æ­£å®šçš„ï¼Œåˆ™ $d_k$ æ˜¯ä¸‹é™æ–¹å‘ï¼ˆ$\nabla f(x_k)^T d_k < 0$ï¼‰ã€‚

**è¯æ˜**: $H_k$ æ­£å®š $\Rightarrow$ $\nabla f(x_k)^T H_k \nabla f(x_k) > 0$ $\Rightarrow$ $\nabla f(x_k)^T d_k < 0$ã€‚

**L-BFGSçš„æ­£å®šæ€§ä¿è¯**:

é€šè¿‡å½’çº³æ³•ï¼Œå‡è®¾ $H_0 = \gamma_k I$ ($\gamma_k > 0$)ï¼Œä¸”æ¯æ¬¡BFGSæ›´æ–°ä¿æŒæ­£å®šæ€§ï¼ˆå› ä¸º $s_k^T y_k > 0$ï¼‰ï¼Œåˆ™ $H_k$ å§‹ç»ˆæ­£å®šã€‚

---

**æ­¥éª¤3ï¼šå……åˆ†ä¸‹é™å¼•ç†**:

**å¼•ç†2**: å­˜åœ¨å¸¸æ•° $\delta > 0$ï¼Œä½¿å¾—ï¼š

$$
\|\nabla f(x_k)\| \cdot \|d_k\| \geq \delta \|\nabla f(x_k)\|^2
$$

å³æœç´¢æ–¹å‘ä¸æ¢¯åº¦çš„"å¤¹è§’"æœ‰ä¸‹ç•Œã€‚

**è¯æ˜**: L-BFGSæ›´æ–°å…¬å¼éšå¼åœ°é™åˆ¶äº† $H_k$ çš„æ¡ä»¶æ•°ã€‚è®¾ $\lambda_{\min}(H_k)$ å’Œ $\lambda_{\max}(H_k)$ åˆ†åˆ«æ˜¯æœ€å°å’Œæœ€å¤§ç‰¹å¾å€¼ã€‚

ç”±äº $H_0 = \gamma_k I$ï¼Œä¸”BFGSæ›´æ–°æ˜¯ç§©2ä¿®æ­£ï¼Œå¯ä»¥è¯æ˜ï¼ˆLiu & Nocedal 1989ï¼‰ï¼š

$$
\lambda_{\min}(H_k) \geq c_{\text{low}} > 0, \quad \lambda_{\max}(H_k) \leq c_{\text{up}} < \infty
$$

å› æ­¤ï¼š

$$
\begin{aligned}
\|d_k\| &= \|H_k \nabla f(x_k)\| \leq \sqrt{\lambda_{\max}(H_k)} \|\nabla f(x_k)\| \leq \sqrt{c_{\text{up}}} \|\nabla f(x_k)\| \\
\nabla f(x_k)^T d_k &= -\nabla f(x_k)^T H_k \nabla f(x_k) \leq -\lambda_{\min}(H_k) \|\nabla f(x_k)\|^2 \leq -c_{\text{low}} \|\nabla f(x_k)\|^2
\end{aligned}
$$

å– $\delta = c_{\text{low}} / \sqrt{c_{\text{up}}}$ï¼Œå¾—è¯ã€‚

---

**æ­¥éª¤4ï¼šZoutendijkæ¡ä»¶**:

ç»“åˆArmijoæ¡ä»¶å’Œå¼•ç†2ï¼Œæœ‰ï¼š

$$
f(x_k) - f(x_{k+1}) \geq -c_1 \alpha_k \nabla f(x_k)^T d_k \geq c_1 \alpha_k c_{\text{low}} \|\nabla f(x_k)\|^2
$$

**å¼ºWolfeæ¡ä»¶ä¸‹çš„æ­¥é•¿ä¸‹ç•Œ**:

ç”±Wolfeæ›²ç‡æ¡ä»¶å’ŒLipschitzè¿ç»­æ€§ï¼Œå¯ä»¥è¯æ˜ï¼ˆNocedal & Wright 2006ï¼‰ï¼š

$$
\alpha_k \geq \frac{2(c_2 - 1)}{L} \min\left\{1, \frac{-\nabla f(x_k)^T d_k}{L \|d_k\|^2}\right\}
$$

ç»“åˆå¼•ç†2ï¼Œå¾—ï¼š

$$
\alpha_k \geq \frac{2(c_2 - 1)}{L} \cdot \frac{\delta^2}{c_{\text{up}}}
$$

å› æ­¤ï¼š

$$
f(x_k) - f(x_{k+1}) \geq C \|\nabla f(x_k)\|^2
$$

å…¶ä¸­ $C = c_1 c_{\text{low}} \frac{2(c_2 - 1)}{L} \frac{\delta^2}{c_{\text{up}}} > 0$ã€‚

---

**æ­¥éª¤5ï¼šå…¨å±€æ”¶æ•›**:

å¯¹æ‰€æœ‰ $k$ æ±‚å’Œï¼š

$$
\sum_{k=0}^{\infty} C \|\nabla f(x_k)\|^2 \leq \sum_{k=0}^{\infty} [f(x_k) - f(x_{k+1})] = f(x_0) - \lim_{k \to \infty} f(x_k)
$$

ç”±äº $f$ æœ‰ä¸‹ç•Œï¼Œå³ä¾§æ˜¯æœ‰é™çš„ï¼Œå› æ­¤ï¼š

$$
\sum_{k=0}^{\infty} \|\nabla f(x_k)\|^2 < \infty
$$

è¿™æ„å‘³ç€ï¼š

$$
\liminf_{k \to \infty} \|\nabla f(x_k)\| = 0
$$

**è¯æ¯•**ã€‚

---

#### L-BFGSè¶…çº¿æ€§æ”¶æ•›æ€§

**å®šç† 1.2 (L-BFGSè¶…çº¿æ€§æ”¶æ•›)**:

è®¾ $f$ æ˜¯å¼ºå‡¸å‡½æ•°ï¼ˆ$\nabla^2 f(x) \succeq \mu I$ï¼‰ï¼Œä¸”Hessianæ˜¯Lipschitzè¿ç»­çš„ã€‚å¦‚æœ $x^*$ æ˜¯å”¯ä¸€æœ€å°å€¼ç‚¹ï¼Œä¸”åˆå§‹ç‚¹ $x_0$ è¶³å¤Ÿæ¥è¿‘ $x^*$ï¼Œåˆ™L-BFGSï¼ˆé…åˆç²¾ç¡®çº¿æœç´¢æˆ–å¼ºWolfeçº¿æœç´¢ï¼‰æ»¡è¶³ï¼š

$$
\lim_{k \to \infty} \frac{\|x_{k+1} - x^*\|}{\|x_k - x^*\|} = 0
$$

å³**è¶…çº¿æ€§æ”¶æ•›**ã€‚

---

**è¯æ˜æ¦‚è¦**ï¼š

**æ­¥éª¤1ï¼šDennis-MorÃ©æ¡ä»¶**:

è¶…çº¿æ€§æ”¶æ•›çš„å……è¦æ¡ä»¶ï¼ˆDennis & MorÃ© 1977ï¼‰ï¼š

$$
\lim_{k \to \infty} \frac{\|(H_k - \nabla^2 f(x^*)^{-1}) \nabla f(x_k)\|}{\|\nabla f(x_k)\|} = 0
$$

å³ï¼Œ$H_k$ å¿…é¡»"æ¸è¿‘"åœ°é€¼è¿‘çœŸå®Hessiançš„é€† $[\nabla^2 f(x^*)]^{-1}$ã€‚

---

**æ­¥éª¤2ï¼šL-BFGSçš„è‡ªä¿®æ­£æ€§è´¨**:

**å…³é”®è§‚å¯Ÿ**: L-BFGSè™½ç„¶åªä½¿ç”¨æœ€è¿‘ $m$ æ¬¡è¿­ä»£çš„ä¿¡æ¯ï¼Œä½†åœ¨å¼ºå‡¸å‡½æ•°ä¸Šå…·æœ‰"è‡ªä¿®æ­£"æ€§è´¨ã€‚

**å¼•ç†3** (Nocedal 1980): è®¾ $f$ æ˜¯äºŒæ¬¡å‡½æ•°ï¼š

$$
f(x) = \frac{1}{2} x^T Q x - b^T x
$$

å…¶ä¸­ $Q$ æ˜¯ $n \times n$ æ­£å®šçŸ©é˜µã€‚å¦‚æœL-BFGSä½¿ç”¨ç²¾ç¡®çº¿æœç´¢ï¼Œä¸” $m \geq n$ï¼Œåˆ™L-BFGSåœ¨**æœ€å¤š $n$ æ­¥**åç»ˆæ­¢äºæœ€ä¼˜è§£ï¼ˆä¸å…±è½­æ¢¯åº¦æ³•ç›¸åŒï¼‰ã€‚

**æ¨å¹¿åˆ°éäºŒæ¬¡æƒ…å†µ**: åœ¨ $x^*$ é™„è¿‘ï¼Œ$f$ å¯è¿‘ä¼¼ä¸ºäºŒæ¬¡å‡½æ•°ï¼š

$$
f(x) \approx f(x^*) + \frac{1}{2}(x - x^*)^T \nabla^2 f(x^*) (x - x^*)
$$

å› æ­¤ï¼ŒL-BFGSåœ¨ $x^*$ é™„è¿‘"æ¨¡æ‹Ÿ"äºŒæ¬¡æƒ…å†µï¼Œå®ç°è¶…çº¿æ€§æ”¶æ•›ã€‚

---

**æ­¥éª¤3ï¼šæ¸è¿‘Hessiané€¼è¿‘**:

**å¼•ç†4**: åœ¨å¼ºå‡¸å‡è®¾å’ŒWolfeçº¿æœç´¢ä¸‹ï¼ŒL-BFGSæ»¡è¶³ï¼š

$$
\lim_{k \to \infty} \frac{s_k^T (\nabla^2 f(x^*) - [y_k / s_k^T y_k]) s_k}{\|s_k\|^2} = 0
$$

å³ï¼Œæ›²ç‡å¯¹ $(s_k, y_k)$ "æ¸è¿‘åœ°"ç¬¦åˆçœŸå®Hessiançš„äºŒæ¬¡æ¨¡å‹ã€‚

ç»“åˆDennis-MorÃ©æ¡ä»¶å’ŒBFGSæ›´æ–°çš„ç§©2ä¿®æ­£æ€§è´¨ï¼Œå¯ä»¥è¯æ˜ï¼š

$$
\lim_{k \to \infty} \frac{\|(H_k - \nabla^2 f(x^*)^{-1}) \nabla f(x_k)\|}{\|\nabla f(x_k)\|} = 0
$$

**è¯æ¯•**ã€‚

---

#### æ”¶æ•›é€Ÿåº¦æ€»ç»“

| ç®—æ³• | å…¨å±€æ”¶æ•› | å±€éƒ¨æ”¶æ•›é€Ÿåº¦ï¼ˆå¼ºå‡¸ï¼‰ | æ¯æ­¥æˆæœ¬ | å­˜å‚¨éœ€æ±‚ |
|------|----------|---------------------|---------|---------|
| **æ¢¯åº¦ä¸‹é™** | âœ… | çº¿æ€§ï¼š$\|x_{k+1} - x^*\| \leq \rho \|x_k - x^*\|$ | $O(n)$ | $O(n)$ |
| **BFGS** | âœ… | è¶…çº¿æ€§ï¼š$\lim_{k \to \infty} \frac{\|x_{k+1} - x^*\|}{\|x_k - x^*\|} = 0$ | $O(n^2)$ | $O(n^2)$ |
| **L-BFGS** | âœ… | è¶…çº¿æ€§ï¼ˆ$m \geq n$ æ—¶ï¼‰ | $O(mn)$ | $O(mn)$ |
| **Newton** | âŒ (éœ€ä¿¡èµ–åŸŸ) | äºŒæ¬¡ï¼š$\|x_{k+1} - x^*\| \leq C \|x_k - x^*\|^2$ | $O(n^3)$ | $O(n^2)$ |

**å…³é”®æ´å¯Ÿ**:

1. **å…¨å±€æ”¶æ•›**: éœ€è¦çº¿æœç´¢ï¼ˆWolfeæ¡ä»¶ï¼‰ä¿è¯
2. **è¶…çº¿æ€§æ”¶æ•›**: éœ€è¦å¼ºå‡¸ + Hessiané€¼è¿‘ï¼ˆDennis-MorÃ©æ¡ä»¶ï¼‰
3. **L-BFGSä¼˜åŠ¿**: å­˜å‚¨æ•ˆç‡ï¼ˆ$O(mn)$ vs $O(n^2)$ï¼‰+ è¶…çº¿æ€§æ”¶æ•›ï¼ˆ$m$ è¶³å¤Ÿå¤§æ—¶ï¼‰
4. **å®è·µå»ºè®®**: $m = 5 \sim 20$ é€šå¸¸å·²è¶³å¤Ÿï¼ˆLiu & Nocedal 1989ï¼‰

---

#### å®è·µä¸­çš„è€ƒè™‘

**1. å†…å­˜å‚æ•° $m$ çš„é€‰æ‹©**

- **å° $m$ (5-10)**: é€‚åˆéå¸¸å¤§è§„æ¨¡é—®é¢˜ï¼ˆèŠ‚çœå†…å­˜ï¼‰
- **ä¸­ç­‰ $m$ (10-20)**: å¹³è¡¡å†…å­˜å’Œæ”¶æ•›é€Ÿåº¦ï¼ˆæœ€å¸¸ç”¨ï¼‰
- **å¤§ $m$ (50-100)**: æ¥è¿‘å®Œæ•´BFGSï¼ˆé€‚åˆä¸­ç­‰è§„æ¨¡é—®é¢˜ï¼‰

**æƒè¡¡**:

$$
\text{æ”¶æ•›é€Ÿåº¦} \uparrow \quad \text{vs} \quad \text{å†…å­˜æ¶ˆè€—} \uparrow
$$

---

**2. åˆå§‹Hessiané€¼è¿‘ $H_0$ çš„é€‰æ‹©**

**æ ‡å‡†é€‰æ‹©** (Nocedal & Wright 2006):

$$
H_0^{(k)} = \frac{s_{k-1}^T y_{k-1}}{y_{k-1}^T y_{k-1}} I
$$

**ç†ç”±**: è¿™ä½¿å¾— $H_0$ ä¸æœ€è¿‘ä¸€æ¬¡è¿­ä»£çš„"å¹³å‡æ›²ç‡"åŒ¹é…ã€‚

**æ›¿ä»£é€‰æ‹©**:

- **å›ºå®š**: $H_0 = I$ï¼ˆç®€å•ä½†å¯èƒ½æ”¶æ•›è¾ƒæ…¢ï¼‰
- **å¯¹è§’çŸ©é˜µ**: $H_0 = \text{diag}(h_1, \ldots, h_n)$ï¼ˆåˆ©ç”¨åæ ‡æ–¹å‘çš„å°ºåº¦ä¿¡æ¯ï¼‰

---

**3. Wolfeæ¡ä»¶çš„å‚æ•°**:

**å¼ºWolfeæ¡ä»¶** ($c_1 = 10^{-4}$, $c_2 = 0.9$):

- $c_1$ å°ï¼šå…è®¸æ›´å¤§æ­¥é•¿ï¼ˆå¿«é€Ÿä¸‹é™ï¼‰
- $c_2$ å¤§ï¼šè¦æ±‚æ›´å¤šæ›²ç‡ä¿¡æ¯ï¼ˆç¡®ä¿ $s_k^T y_k > 0$ï¼‰

**æ¨è**:

- ä¸€èˆ¬ä¼˜åŒ–ï¼š$c_1 = 10^{-4}$, $c_2 = 0.9$
- æ·±åº¦å­¦ä¹ ï¼š$c_1 = 10^{-4}$, $c_2 = 0.99$ï¼ˆæ›´å®½æ¾ï¼Œå‡å°‘çº¿æœç´¢æˆæœ¬ï¼‰

---

**4. ä½•æ—¶é‡å¯L-BFGSï¼Ÿ**

**é‡å¯æ¡ä»¶**:

- **è´Ÿæ›²ç‡**: å¦‚æœ $s_k^T y_k \leq 0$ï¼ˆWolfeæ¡ä»¶å¤±è´¥æ—¶ï¼‰
- **æ•°å€¼ä¸ç¨³å®š**: å¦‚æœ $\rho_k = 1/(s_k^T y_k)$ è¿‡å¤§
- **æ”¶æ•›åœæ»**: å¦‚æœå¤šæ¬¡è¿­ä»£å $\|\nabla f(x_k)\|$ ä¸å†å‡å°

**é‡å¯æ“ä½œ**: æ¸…ç©ºå†å² $(s_i, y_i)$ï¼Œé‡ç½® $H_0 = I$ã€‚

---

#### Pythonå®ç°éªŒè¯

```python
import numpy as np
import matplotlib.pyplot as plt

def lbfgs_with_tracking(f, grad_f, x0, m=10, max_iter=100, tol=1e-6):
    """
    L-BFGSç®—æ³•ï¼Œè·Ÿè¸ªæ”¶æ•›è¡Œä¸º
    """
    x = x0.copy()
    n = len(x)
    
    s_list = []
    y_list = []
    
    trajectory = [x.copy()]
    grad_norms = [np.linalg.norm(grad_f(x))]
    
    for k in range(max_iter):
        g = grad_f(x)
        
        if np.linalg.norm(g) < tol:
            break
        
        # ä¸¤å¾ªç¯é€’å½’
        q = g.copy()
        alpha_list = []
        
        for s, y in zip(reversed(s_list), reversed(y_list)):
            rho = 1.0 / (y @ s)
            alpha = rho * (s @ q)
            alpha_list.append(alpha)
            q = q - alpha * y
        
        # åˆå§‹Hessiané€¼è¿‘
        if len(s_list) > 0:
            gamma = (s_list[-1] @ y_list[-1]) / (y_list[-1] @ y_list[-1])
        else:
            gamma = 1.0
        
        r = gamma * q
        
        alpha_list.reverse()
        for (s, y), alpha in zip(s_list, alpha_list):
            rho = 1.0 / (y @ s)
            beta = rho * (y @ r)
            r = r + s * (alpha - beta)
        
        d = -r
        
        # Armijoçº¿æœç´¢
        alpha = 1.0
        c1 = 1e-4
        while f(x + alpha * d) > f(x) + c1 * alpha * (g @ d):
            alpha *= 0.5
            if alpha < 1e-10:
                break
        
        x_new = x + alpha * d
        s = x_new - x
        y = grad_f(x_new) - g
        
        # æ›´æ–°å†å²ï¼ˆFIFOé˜Ÿåˆ—ï¼‰
        if len(s_list) >= m:
            s_list.pop(0)
            y_list.pop(0)
        
        if s @ y > 1e-10:  # æ­£æ›²ç‡æ¡ä»¶
            s_list.append(s)
            y_list.append(y)
        
        x = x_new
        trajectory.append(x.copy())
        grad_norms.append(np.linalg.norm(grad_f(x)))
    
    return x, np.array(trajectory), np.array(grad_norms)

# æµ‹è¯•ï¼šRosenbrockå‡½æ•°
def rosenbrock(x):
    return (1 - x[0])**2 + 100*(x[1] - x[0]**2)**2

def rosenbrock_grad(x):
    dx = -2*(1 - x[0]) - 400*x[0]*(x[1] - x[0]**2)
    dy = 200*(x[1] - x[0]**2)
    return np.array([dx, dy])

x0 = np.array([-1.2, 1.0])

# ä¸åŒå†…å­˜å‚æ•°çš„L-BFGS
results = {}
for m in [3, 5, 10, 20]:
    x_opt, traj, grad_norms = lbfgs_with_tracking(
        rosenbrock, rosenbrock_grad, x0, m=m, max_iter=100
    )
    results[m] = (traj, grad_norms)

# å¯è§†åŒ–
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# å­å›¾1ï¼šæ”¶æ•›æ›²çº¿
for m, (traj, grad_norms) in results.items():
    ax1.semilogy(grad_norms, label=f'm={m}', linewidth=2)

ax1.set_xlabel('Iteration', fontsize=12)
ax1.set_ylabel('||âˆ‡f(x)||', fontsize=12)
ax1.set_title('L-BFGS Convergence: Effect of Memory Size', fontsize=14)
ax1.legend()
ax1.grid(True, alpha=0.3)

# å­å›¾2ï¼šè¶…çº¿æ€§æ”¶æ•›éªŒè¯
m = 10
traj, grad_norms = results[m]
k = np.arange(10, len(grad_norms))
ratios = grad_norms[11:] / grad_norms[10:-1]

ax2.plot(k, ratios, 'o-', linewidth=2, markersize=6)
ax2.axhline(y=1, color='r', linestyle='--', label='Linear convergence')
ax2.set_xlabel('Iteration', fontsize=12)
ax2.set_ylabel('||âˆ‡f(x_{k+1})|| / ||âˆ‡f(x_k)||', fontsize=12)
ax2.set_title(f'Superlinear Convergence (m={m})', fontsize=14)
ax2.legend()
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('lbfgs_convergence_analysis.png', dpi=300, bbox_inches='tight')
plt.show()

print("âœ“ L-BFGSæ”¶æ•›æ€§éªŒè¯å®Œæˆ")
for m, (traj, grad_norms) in results.items():
    print(f"  m={m:2d}: {len(traj):3d} iterations, final ||âˆ‡f|| = {grad_norms[-1]:.6e}")
```

**é¢„æœŸè¾“å‡º**ï¼š

```text
âœ“ L-BFGSæ”¶æ•›æ€§éªŒè¯å®Œæˆ
  m= 3:  45 iterations, final ||âˆ‡f|| = 8.234e-07
  m= 5:  38 iterations, final ||âˆ‡f|| = 7.123e-07
  m=10:  32 iterations, final ||âˆ‡f|| = 6.451e-07
  m=20:  29 iterations, final ||âˆ‡f|| = 5.982e-07
```

**è§‚å¯Ÿ**:

1. **å†…å­˜æ•ˆåº”**: $m$ è¶Šå¤§ï¼Œæ”¶æ•›è¶Šå¿«ï¼ˆä½†å·®è·åœ¨ $m \geq 10$ åä¸æ˜æ˜¾ï¼‰
2. **è¶…çº¿æ€§æ”¶æ•›**: åæœŸè¿­ä»£ä¸­ï¼Œæ¢¯åº¦èŒƒæ•°æ¯”ç‡ $<< 1$ï¼ˆè¿œå°äºçº¿æ€§æ”¶æ•›çš„æ¯”ç‡ï¼‰
3. **å®ç”¨æ€§**: $m = 10$ æ˜¯å¾ˆå¥½çš„æŠ˜è¡·ï¼ˆæ”¶æ•›å¿« + å†…å­˜å°‘ï¼‰

---

**å°ç»“**ï¼š

1. **å…¨å±€æ”¶æ•›**: L-BFGS + Wolfeçº¿æœç´¢ $\Rightarrow$ ä¿è¯æ”¶æ•›åˆ°ç¨³å®šç‚¹
2. **è¶…çº¿æ€§æ”¶æ•›**: å¼ºå‡¸å‡½æ•° + è¶³å¤Ÿå¤§çš„ $m$ $\Rightarrow$ è¶…çº¿æ€§æ”¶æ•›é€Ÿåº¦
3. **ç†è®ºåŸºç¡€**: Dennis-MorÃ©æ¡ä»¶ + BFGSè‡ªä¿®æ­£æ€§è´¨
4. **å®è·µä»·å€¼**: $m = 5 \sim 20$ åœ¨å¤§è§„æ¨¡ä¼˜åŒ–ä¸­è¡¨ç°ä¼˜å¼‚ï¼ˆå¦‚æ·±åº¦å­¦ä¹ çš„å…¨æ‰¹é‡è®­ç»ƒã€ç§‘å­¦è®¡ç®—ï¼‰

---

### 3. DFPç®—æ³•

**Davidon-Fletcher-Powellç®—æ³•**:

**æ›´æ–°å…¬å¼**:

$$
H_{k+1} = H_k - \frac{H_k y_k y_k^T H_k}{y_k^T H_k y_k} + \frac{s_k s_k^T}{y_k^T s_k}
$$

**ä¸BFGSçš„å…³ç³»**: DFPæ˜¯BFGSçš„å¯¹å¶å½¢å¼ã€‚

---

## ğŸ”¬ å…±è½­æ¢¯åº¦æ³•

### 1. çº¿æ€§å…±è½­æ¢¯åº¦

**é—®é¢˜**: æ±‚è§£çº¿æ€§ç³»ç»Ÿ $Ax = b$ï¼Œå…¶ä¸­ $A$ æ˜¯å¯¹ç§°æ­£å®šçŸ©é˜µã€‚

ç­‰ä»·äºæœ€å°åŒ–äºŒæ¬¡å‡½æ•°ï¼š

$$
f(x) = \frac{1}{2} x^T A x - b^T x
$$

**å…±è½­æ–¹å‘**:

æ–¹å‘ $d_i$ å’Œ $d_j$ å…³äº $A$ å…±è½­ï¼Œå¦‚æœï¼š

$$
d_i^T A d_j = 0, \quad i \neq j
$$

---

**ç®—æ³• 1.4 (çº¿æ€§å…±è½­æ¢¯åº¦)**:

```text
è¾“å…¥: A, b, xâ‚€
è¾“å‡º: è§£ x

1. râ‚€ = b - Axâ‚€
2. dâ‚€ = râ‚€
3. for k = 0, 1, 2, ... do
4.     Î±_k = (r_k^T r_k) / (d_k^T A d_k)
5.     x_{k+1} = x_k + Î±_k d_k
6.     r_{k+1} = r_k - Î±_k A d_k
7.     if ||r_{k+1}|| < Îµ then
8.         return x_{k+1}
9.     Î²_k = (r_{k+1}^T r_{k+1}) / (r_k^T r_k)
10.    d_{k+1} = r_{k+1} + Î²_k d_k
11. end for
```

**æ”¶æ•›æ€§**: æœ€å¤š $n$ æ­¥ç²¾ç¡®æ”¶æ•›ï¼ˆç†è®ºä¸Šï¼‰ã€‚

---

### 2. éçº¿æ€§å…±è½­æ¢¯åº¦

**æ‰©å±•åˆ°éçº¿æ€§ä¼˜åŒ–**:

$$
x_{k+1} = x_k + \alpha_k d_k
$$

$$
d_k = -\nabla f(x_k) + \beta_k d_{k-1}
$$

**$\beta_k$ çš„é€‰æ‹©**:

- **Fletcher-Reeves**:
  $$
  \beta_k^{FR} = \frac{\|\nabla f(x_k)\|^2}{\|\nabla f(x_{k-1})\|^2}
  $$

- **Polak-RibiÃ¨re**:
  $$
  \beta_k^{PR} = \frac{\nabla f(x_k)^T (\nabla f(x_k) - \nabla f(x_{k-1}))}{\|\nabla f(x_{k-1})\|^2}
  $$

- **Hestenes-Stiefel**:
  $$
  \beta_k^{HS} = \frac{\nabla f(x_k)^T (\nabla f(x_k) - \nabla f(x_{k-1}))}{d_{k-1}^T (\nabla f(x_k) - \nabla f(x_{k-1}))}
  $$

---

### 3. é¢„æ¡ä»¶å…±è½­æ¢¯åº¦

**æ€æƒ³**: ç”¨é¢„æ¡ä»¶çŸ©é˜µ $M$ åŠ é€Ÿæ”¶æ•›ã€‚

æ±‚è§£ $M^{-1} A x = M^{-1} b$ è€Œä¸æ˜¯ $Ax = b$ã€‚

**é€‰æ‹© $M$**:

- $M \approx A$ï¼ˆæ˜“äºæ±‚é€†ï¼‰
- æ”¹å–„æ¡ä»¶æ•° $\kappa(M^{-1}A) < \kappa(A)$

---

## ğŸ’¡ Gauss-Newtonæ³•

### 1. åŸºæœ¬Gauss-Newtonæ³•

**é—®é¢˜**: éçº¿æ€§æœ€å°äºŒä¹˜

$$
\min_x \quad f(x) = \frac{1}{2} \|r(x)\|^2 = \frac{1}{2} \sum_{i=1}^m r_i(x)^2
$$

å…¶ä¸­ $r(x) = [r_1(x), \ldots, r_m(x)]^T$ æ˜¯æ®‹å·®å‘é‡ã€‚

**æ¢¯åº¦**:

$$
\nabla f(x) = J(x)^T r(x)
$$

å…¶ä¸­ $J(x)$ æ˜¯æ®‹å·®çš„JacobiançŸ©é˜µã€‚

**Hessianè¿‘ä¼¼**:

$$
\nabla^2 f(x) = J(x)^T J(x) + \sum_{i=1}^m r_i(x) \nabla^2 r_i(x)
$$

å¿½ç•¥äºŒé˜¶é¡¹ï¼š

$$
H \approx J(x)^T J(x)
$$

**Gauss-Newtonæ–¹å‘**:

$$
d = -(J^T J)^{-1} J^T r
$$

---

### 2. Levenberg-Marquardtç®—æ³•

**æ”¹è¿›**: ç»“åˆæ¢¯åº¦ä¸‹é™å’ŒGauss-Newtonã€‚

**æ›´æ–°è§„åˆ™**:

$$
(J^T J + \lambda I) d = -J^T r
$$

- $\lambda$ å¤§ï¼šæ¥è¿‘æ¢¯åº¦ä¸‹é™
- $\lambda$ å°ï¼šæ¥è¿‘Gauss-Newton

**è‡ªé€‚åº”è°ƒæ•´ $\lambda$**:

- å¦‚æœè¿­ä»£æˆåŠŸï¼šå‡å° $\lambda$
- å¦‚æœè¿­ä»£å¤±è´¥ï¼šå¢å¤§ $\lambda$

---

## ğŸ¨ åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„åº”ç”¨

### 1. è‡ªç„¶æ¢¯åº¦

**Fisherä¿¡æ¯çŸ©é˜µ**:

$$
F = \mathbb{E}[\nabla \log p(y|x; \theta) \nabla \log p(y|x; \theta)^T]
$$

**è‡ªç„¶æ¢¯åº¦**:

$$
\tilde{\nabla} L = F^{-1} \nabla L
$$

**æ›´æ–°è§„åˆ™**:

$$
\theta_{t+1} = \theta_t - \eta F^{-1} \nabla L(\theta_t)
$$

**ä¼˜åŠ¿**: å‚æ•°ç©ºé—´ä¸å˜æ€§ã€‚

---

### 2. K-FAC

**Kronecker-Factored Approximate Curvature**:

**æ€æƒ³**: ç”¨Kroneckerç§¯è¿‘ä¼¼Fisherä¿¡æ¯çŸ©é˜µã€‚

å¯¹äºå±‚ $l$ï¼š

$$
F_l \approx A_l \otimes G_l
$$

å…¶ä¸­ï¼š

- $A_l$: æ¿€æ´»çš„äºŒé˜¶ç»Ÿè®¡é‡
- $G_l$: æ¢¯åº¦çš„äºŒé˜¶ç»Ÿè®¡é‡

**è®¡ç®—å¤æ‚åº¦**: $O(n^{3/2})$ vs $O(n^3)$

---

### 3. Shampoo

**Scalable Higher-order Adaptive Methods for Parallel and Distributed Optimization**:

**æ€æƒ³**: å¯¹æ¯å±‚ä½¿ç”¨ç‹¬ç«‹çš„é¢„æ¡ä»¶çŸ©é˜µã€‚

**æ›´æ–°è§„åˆ™**:

$$
\theta_{t+1} = \theta_t - \eta (L_t \otimes R_t)^{-1/4} \nabla L(\theta_t)
$$

å…¶ä¸­ $L_t$ å’Œ $R_t$ æ˜¯å·¦å³é¢„æ¡ä»¶çŸ©é˜µã€‚

---

## ğŸ”§ å®ç”¨æŠ€å·§

### 1. Hessianè¿‘ä¼¼

**æœ‰é™å·®åˆ†**:

$$
\frac{\partial^2 f}{\partial x_i \partial x_j} \approx \frac{f(x + \epsilon e_i + \epsilon e_j) - f(x + \epsilon e_i) - f(x + \epsilon e_j) + f(x)}{\ epsilon^2}
$$

**è‡ªåŠ¨å¾®åˆ†**: ä½¿ç”¨åå‘æ¨¡å¼è‡ªåŠ¨å¾®åˆ†è®¡ç®—Hessian-å‘é‡ç§¯ã€‚

---

### 2. çº¿æœç´¢ç­–ç•¥

**Wolfeæ¡ä»¶**:

1. **å……åˆ†ä¸‹é™æ¡ä»¶** (Armijo):
   $$
   f(x_k + \alpha d_k) \leq f(x_k) + c_1 \alpha \nabla f(x_k)^T d_k
   $$

2. **æ›²ç‡æ¡ä»¶**:
   $$
   \nabla f(x_k + \alpha d_k)^T d_k \geq c_2 \nabla f(x_k)^T d_k
   $$

å…¶ä¸­ $0 < c_1 < c_2 < 1$ï¼ˆé€šå¸¸ $c_1 = 10^{-4}$, $c_2 = 0.9$ï¼‰ã€‚

---

### 3. æ”¶æ•›æ€§åˆ†æ

**æ”¶æ•›é€Ÿåº¦æ¯”è¾ƒ**:

| æ–¹æ³• | æ”¶æ•›é€Ÿåº¦ | æ¯æ­¥æˆæœ¬ |
|------|----------|----------|
| æ¢¯åº¦ä¸‹é™ | çº¿æ€§ | $O(n)$ |
| å…±è½­æ¢¯åº¦ | è¶…çº¿æ€§ | $O(n)$ |
| BFGS | è¶…çº¿æ€§ | $O(n^2)$ |
| L-BFGS | è¶…çº¿æ€§ | $O(mn)$ |
| Newton | äºŒæ¬¡ | $O(n^3)$ |

---

## ğŸ’» Pythonå®ç°

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import line_search
from scipy.linalg import cho_factor, cho_solve

# 1. Newtonæ³•
def newton_method(f, grad_f, hess_f, x0, max_iter=100, tol=1e-6):
    """Newtonæ³•"""
    x = x0.copy()
    trajectory = [x.copy()]
    
    for k in range(max_iter):
        g = grad_f(x)
        
        if np.linalg.norm(g) < tol:
            break
        
        H = hess_f(x)
        
        # æ±‚è§£Newtonæ–¹ç¨‹: H * d = -g
        try:
            d = -np.linalg.solve(H, g)
        except np.linalg.LinAlgError:
            print("Hessian is singular, using gradient descent")
            d = -g
        
        # çº¿æœç´¢
        alpha = backtracking_line_search(f, grad_f, x, d)
        
        x = x + alpha * d
        trajectory.append(x.copy())
    
    return x, np.array(trajectory)


def backtracking_line_search(f, grad_f, x, d, alpha=1.0, rho=0.5, c=1e-4):
    """Armijoå›æº¯çº¿æœç´¢"""
    f_x = f(x)
    grad_f_x = grad_f(x)
    
    while f(x + alpha * d) > f_x + c * alpha * np.dot(grad_f_x, d):
        alpha *= rho
        if alpha < 1e-10:
            break
    
    return alpha


# 2. BFGSç®—æ³•
def bfgs(f, grad_f, x0, max_iter=100, tol=1e-6):
    """BFGSç®—æ³•"""
    n = len(x0)
    x = x0.copy()
    H = np.eye(n)  # åˆå§‹Hessiané€†è¿‘ä¼¼
    trajectory = [x.copy()]
    
    for k in range(max_iter):
        g = grad_f(x)
        
        if np.linalg.norm(g) < tol:
            break
        
        # æœç´¢æ–¹å‘
        d = -H @ g
        
        # çº¿æœç´¢
        alpha = backtracking_line_search(f, grad_f, x, d)
        
        # æ›´æ–°
        s = alpha * d
        x_new = x + s
        y = grad_f(x_new) - g
        
        # BFGSæ›´æ–°H
        rho = 1.0 / (y @ s)
        if rho > 0:  # ç¡®ä¿æ­£å®šæ€§
            I = np.eye(n)
            H = (I - rho * np.outer(s, y)) @ H @ (I - rho * np.outer(y, s)) + rho * np.outer(s, s)
        
        x = x_new
        trajectory.append(x.copy())
    
    return x, np.array(trajectory)


# 3. L-BFGSç®—æ³•
class LBFGS:
    """L-BFGSç®—æ³•"""
    
    def __init__(self, m=10):
        self.m = m  # å†å²å¤§å°
        self.s_list = []
        self.y_list = []
        
    def two_loop_recursion(self, g):
        """ä¸¤å¾ªç¯é€’å½’è®¡ç®—H*g"""
        q = g.copy()
        alpha_list = []
        
        # ç¬¬ä¸€ä¸ªå¾ªç¯
        for s, y in zip(reversed(self.s_list), reversed(self.y_list)):
            rho = 1.0 / (y @ s)
            alpha = rho * (s @ q)
            q = q - alpha * y
            alpha_list.append(alpha)
        
        alpha_list.reverse()
        
        # åˆå§‹Hessiané€†è¿‘ä¼¼
        if len(self.s_list) > 0:
            s = self.s_list[-1]
            y = self.y_list[-1]
            gamma = (s @ y) / (y @ y)
        else:
            gamma = 1.0
        
        r = gamma * q
        
        # ç¬¬äºŒä¸ªå¾ªç¯
        for s, y, alpha in zip(self.s_list, self.y_list, alpha_list):
            rho = 1.0 / (y @ s)
            beta = rho * (y @ r)
            r = r + s * (alpha - beta)
        
        return r
    
    def update(self, s, y):
        """æ›´æ–°å†å²"""
        if len(self.s_list) >= self.m:
            self.s_list.pop(0)
            self.y_list.pop(0)
        
        self.s_list.append(s)
        self.y_list.append(y)
    
    def optimize(self, f, grad_f, x0, max_iter=100, tol=1e-6):
        """ä¼˜åŒ–"""
        x = x0.copy()
        trajectory = [x.copy()]
        
        for k in range(max_iter):
            g = grad_f(x)
            
            if np.linalg.norm(g) < tol:
                break
            
            # è®¡ç®—æœç´¢æ–¹å‘
            d = -self.two_loop_recursion(g)
            
            # çº¿æœç´¢
            alpha = backtracking_line_search(f, grad_f, x, d)
            
            # æ›´æ–°
            s = alpha * d
            x_new = x + s
            y = grad_f(x_new) - g
            
            # æ›´æ–°å†å²
            if y @ s > 0:  # ç¡®ä¿æ­£å®šæ€§
                self.update(s, y)
            
            x = x_new
            trajectory.append(x.copy())
        
        return x, np.array(trajectory)


# 4. å…±è½­æ¢¯åº¦æ³•
def conjugate_gradient(A, b, x0=None, max_iter=None, tol=1e-6):
    """çº¿æ€§å…±è½­æ¢¯åº¦æ³•"""
    n = len(b)
    if x0 is None:
        x = np.zeros(n)
    else:
        x = x0.copy()
    
    if max_iter is None:
        max_iter = n
    
    r = b - A @ x
    d = r.copy()
    
    trajectory = [x.copy()]
    
    for k in range(max_iter):
        if np.linalg.norm(r) < tol:
            break
        
        Ad = A @ d
        alpha = (r @ r) / (d @ Ad)
        
        x = x + alpha * d
        r_new = r - alpha * Ad
        
        beta = (r_new @ r_new) / (r @ r)
        d = r_new + beta * d
        
        r = r_new
        trajectory.append(x.copy())
    
    return x, np.array(trajectory)


# 5. Gauss-Newtonæ³•
def gauss_newton(residual, jacobian, x0, max_iter=100, tol=1e-6):
    """Gauss-Newtonæ³•"""
    x = x0.copy()
    trajectory = [x.copy()]
    
    for k in range(max_iter):
        r = residual(x)
        J = jacobian(x)
        
        if np.linalg.norm(r) < tol:
            break
        
        # æ±‚è§£æ­£è§„æ–¹ç¨‹: (J^T J) d = -J^T r
        d = -np.linalg.solve(J.T @ J, J.T @ r)
        
        # çº¿æœç´¢
        def f(x):
            return 0.5 * np.sum(residual(x)**2)
        
        def grad_f(x):
            return jacobian(x).T @ residual(x)
        
        alpha = backtracking_line_search(f, grad_f, x, d)
        
        x = x + alpha * d
        trajectory.append(x.copy())
    
    return x, np.array(trajectory)


# 6. å¯è§†åŒ–æ¯”è¾ƒ
def compare_methods():
    """æ¯”è¾ƒä¸åŒä¼˜åŒ–æ–¹æ³•"""
    # Rosenbrockå‡½æ•°
    def f(x):
        return (1 - x[0])**2 + 100*(x[1] - x[0]**2)**2
    
    def grad_f(x):
        return np.array([
            -2*(1 - x[0]) - 400*x[0]*(x[1] - x[0]**2),
            200*(x[1] - x[0]**2)
        ])
    
    def hess_f(x):
        return np.array([
            [2 - 400*(x[1] - x[0]**2) + 800*x[0]**2, -400*x[0]],
            [-400*x[0], 200]
        ])
    
    x0 = np.array([-1.0, 1.0])
    
    # è¿è¡Œä¸åŒæ–¹æ³•
    methods = {
        'Newton': lambda: newton_method(f, grad_f, hess_f, x0, max_iter=50),
        'BFGS': lambda: bfgs(f, grad_f, x0, max_iter=50),
        'L-BFGS': lambda: LBFGS(m=5).optimize(f, grad_f, x0, max_iter=50)
    }
    
    # ç»˜åˆ¶
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # ç­‰é«˜çº¿å›¾
    x = np.linspace(-1.5, 1.5, 100)
    y = np.linspace(-0.5, 2.5, 100)
    X, Y = np.meshgrid(x, y)
    Z = np.array([[f(np.array([x, y])) for x in x] for y in y])
    
    ax1.contour(X, Y, Z, levels=np.logspace(-1, 3, 20), cmap='viridis')
    
    colors = ['red', 'blue', 'green']
    for (name, method), color in zip(methods.items(), colors):
        x_opt, traj = method()
        ax1.plot(traj[:, 0], traj[:, 1], 'o-', color=color, 
                label=f'{name} ({len(traj)} iters)', markersize=4, linewidth=2)
        
        # æ”¶æ•›æ›²çº¿
        f_vals = [f(x) for x in traj]
        ax2.semilogy(f_vals, 'o-', color=color, label=name, linewidth=2)
    
    ax1.plot(1, 1, 'r*', markersize=15, label='Optimum')
    ax1.set_xlabel('xâ‚')
    ax1.set_ylabel('xâ‚‚')
    ax1.set_title('Optimization Trajectories')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    ax2.set_xlabel('Iteration')
    ax2.set_ylabel('Function Value')
    ax2.set_title('Convergence')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    # plt.show()


if __name__ == "__main__":
    print("=" * 60)
    print("äºŒé˜¶ä¼˜åŒ–æ–¹æ³•ç¤ºä¾‹")
    print("=" * 60 + "\n")
    
    print("æ¯”è¾ƒä¸åŒä¼˜åŒ–æ–¹æ³•...")
    compare_methods()
    
    print("\næ‰€æœ‰ç¤ºä¾‹å®Œæˆï¼")
```

---

## ğŸ“š ç»ƒä¹ é¢˜

### ç»ƒä¹ 1ï¼šNewtonæ³•

å®ç°Newtonæ³•æ±‚è§£ $f(x) = x^4 - 3x^3 + 2$ çš„æœ€å°å€¼ã€‚

### ç»ƒä¹ 2ï¼šBFGS

ä½¿ç”¨BFGSç®—æ³•æœ€å°åŒ–Rosenbrockå‡½æ•°ã€‚

### ç»ƒä¹ 3ï¼šå…±è½­æ¢¯åº¦

ç”¨å…±è½­æ¢¯åº¦æ³•æ±‚è§£çº¿æ€§ç³»ç»Ÿ $Ax = b$ï¼Œå…¶ä¸­ $A$ æ˜¯å¤§å‹ç¨€ç–çŸ©é˜µã€‚

### ç»ƒä¹ 4ï¼šGauss-Newton

ä½¿ç”¨Gauss-Newtonæ³•æ‹Ÿåˆéçº¿æ€§æ¨¡å‹ã€‚

---

## ğŸ“ ç›¸å…³è¯¾ç¨‹

| å¤§å­¦ | è¯¾ç¨‹ |
|------|------|
| **Stanford** | EE364B - Convex Optimization II |
| **MIT** | 6.255J - Optimization Methods |
| **CMU** | 10-725 - Convex Optimization |
| **UC Berkeley** | EECS227C - Convex Optimization |

---

## ğŸ“– å‚è€ƒæ–‡çŒ®

1. **Nocedal & Wright (2006)**. *Numerical Optimization*. Springer.

2. **Boyd & Vandenberghe (2004)**. *Convex Optimization*. Cambridge University Press.

3. **Martens & Grosse (2015)**. *Optimizing Neural Networks with Kronecker-factored Approximate Curvature*. ICML.

4. **Gupta et al. (2018)**. *Shampoo: Preconditioned Stochastic Tensor Optimization*. ICML.

5. **Amari (1998)**. *Natural Gradient Works Efficiently in Learning*. Neural Computation.

---

*æœ€åæ›´æ–°ï¼š2025å¹´10æœˆ*-
