# é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ä¸Bellmanæ–¹ç¨‹

> **Markov Decision Processes and Bellman Equations**
>
> å¼ºåŒ–å­¦ä¹ çš„æ•°å­¦åŸºç¡€

---

## ç›®å½•

- [é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ä¸Bellmanæ–¹ç¨‹](#é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ä¸bellmanæ–¹ç¨‹)
  - [ç›®å½•](#ç›®å½•)
  - [ğŸ“‹ æ ¸å¿ƒæ¦‚å¿µ](#-æ ¸å¿ƒæ¦‚å¿µ)
  - [ğŸ¯ é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ (MDP)](#-é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹-mdp)
    - [1. å½¢å¼åŒ–å®šä¹‰](#1-å½¢å¼åŒ–å®šä¹‰)
    - [2. ç­–ç•¥](#2-ç­–ç•¥)
    - [3. ä»·å€¼å‡½æ•°](#3-ä»·å€¼å‡½æ•°)
  - [ğŸ“Š Bellmanæ–¹ç¨‹](#-bellmanæ–¹ç¨‹)
    - [1. BellmanæœŸæœ›æ–¹ç¨‹](#1-bellmanæœŸæœ›æ–¹ç¨‹)
    - [2. Bellmanæœ€ä¼˜æ–¹ç¨‹](#2-bellmanæœ€ä¼˜æ–¹ç¨‹)
    - [3. æœ€ä¼˜ç­–ç•¥](#3-æœ€ä¼˜ç­–ç•¥)
  - [ğŸ”§ æ±‚è§£æ–¹æ³•](#-æ±‚è§£æ–¹æ³•)
    - [1. åŠ¨æ€è§„åˆ’](#1-åŠ¨æ€è§„åˆ’)
    - [2. è’™ç‰¹å¡æ´›æ–¹æ³•](#2-è’™ç‰¹å¡æ´›æ–¹æ³•)
    - [3. æ—¶åºå·®åˆ†å­¦ä¹ ](#3-æ—¶åºå·®åˆ†å­¦ä¹ )
  - [ğŸ¤– æ·±åº¦å¼ºåŒ–å­¦ä¹ ](#-æ·±åº¦å¼ºåŒ–å­¦ä¹ )
  - [ğŸ’» Pythonå®ç°](#-pythonå®ç°)
  - [ğŸ“š æ ¸å¿ƒå®šç†æ€»ç»“](#-æ ¸å¿ƒå®šç†æ€»ç»“)
  - [ğŸ“ ç›¸å…³è¯¾ç¨‹](#-ç›¸å…³è¯¾ç¨‹)
  - [ğŸ“– å‚è€ƒæ–‡çŒ®](#-å‚è€ƒæ–‡çŒ®)

---

## ğŸ“‹ æ ¸å¿ƒæ¦‚å¿µ

**å¼ºåŒ–å­¦ä¹ **ç ”ç©¶æ™ºèƒ½ä½“å¦‚ä½•é€šè¿‡ä¸ç¯å¢ƒäº¤äº’å­¦ä¹ æœ€ä¼˜è¡Œä¸ºç­–ç•¥ã€‚

**æ ¸å¿ƒè¦ç´ **ï¼š
- **çŠ¶æ€ (State)**ï¼šç¯å¢ƒçš„æè¿°
- **åŠ¨ä½œ (Action)**ï¼šæ™ºèƒ½ä½“çš„é€‰æ‹©
- **å¥–åŠ± (Reward)**ï¼šå³æ—¶åé¦ˆ
- **ç­–ç•¥ (Policy)**ï¼šçŠ¶æ€åˆ°åŠ¨ä½œçš„æ˜ å°„
- **ä»·å€¼ (Value)**ï¼šé•¿æœŸç´¯ç§¯å¥–åŠ±

---

## ğŸ¯ é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ (MDP)

### 1. å½¢å¼åŒ–å®šä¹‰

**å®šä¹‰ 1.1 (MDP)**:

MDPæ˜¯ä¸€ä¸ªäº”å…ƒç»„ $(\mathcal{S}, \mathcal{A}, P, R, \gamma)$ï¼š

- $\mathcal{S}$ï¼šçŠ¶æ€ç©ºé—´
- $\mathcal{A}$ï¼šåŠ¨ä½œç©ºé—´
- $P(s'|s, a)$ï¼šçŠ¶æ€è½¬ç§»æ¦‚ç‡
- $R(s, a, s')$ï¼šå¥–åŠ±å‡½æ•°
- $\gamma \in [0, 1)$ï¼šæŠ˜æ‰£å› å­

**é©¬å°”å¯å¤«æ€§è´¨**ï¼š

$$
\mathbb{P}(s_{t+1} | s_t, a_t, s_{t-1}, a_{t-1}, \ldots) = \mathbb{P}(s_{t+1} | s_t, a_t)
$$

---

### 2. ç­–ç•¥

**å®šä¹‰ 2.1 (ç­–ç•¥)**:

ç­–ç•¥ $\pi$ æ˜¯ä»çŠ¶æ€åˆ°åŠ¨ä½œçš„æ˜ å°„ï¼š

- **ç¡®å®šæ€§ç­–ç•¥**ï¼š$\pi : \mathcal{S} \to \mathcal{A}$
- **éšæœºç­–ç•¥**ï¼š$\pi(a|s) = \mathbb{P}(a_t = a | s_t = s)$

---

### 3. ä»·å€¼å‡½æ•°

**å®šä¹‰ 3.1 (çŠ¶æ€ä»·å€¼å‡½æ•°)**:

$$
V^\pi(s) = \mathbb{E}_\pi\left[\sum_{t=0}^{\infty} \gamma^t R_{t+1} \mid S_0 = s\right]
$$

**å®šä¹‰ 3.2 (åŠ¨ä½œä»·å€¼å‡½æ•°)**:

$$
Q^\pi(s, a) = \mathbb{E}_\pi\left[\sum_{t=0}^{\infty} \gamma^t R_{t+1} \mid S_0 = s, A_0 = a\right]
$$

**å…³ç³»**ï¼š

$$
V^\pi(s) = \sum_{a \in \mathcal{A}} \pi(a|s) Q^\pi(s, a)
$$

---

## ğŸ“Š Bellmanæ–¹ç¨‹

### 1. BellmanæœŸæœ›æ–¹ç¨‹

**å®šç† 1.1 (BellmanæœŸæœ›æ–¹ç¨‹)**:

å¯¹äºä»»æ„ç­–ç•¥ $\pi$ï¼š

$$
V^\pi(s) = \sum_{a} \pi(a|s) \sum_{s'} P(s'|s, a) [R(s, a, s') + \gamma V^\pi(s')]
$$

$$
Q^\pi(s, a) = \sum_{s'} P(s'|s, a) [R(s, a, s') + \gamma \sum_{a'} \pi(a'|s') Q^\pi(s', a')]
$$

**çŸ©é˜µå½¢å¼**ï¼š

$$
V^\pi = R^\pi + \gamma P^\pi V^\pi
$$

**è§£æè§£**ï¼š

$$
V^\pi = (I - \gamma P^\pi)^{-1} R^\pi
$$

---

### 2. Bellmanæœ€ä¼˜æ–¹ç¨‹

**å®šä¹‰ 2.1 (æœ€ä¼˜ä»·å€¼å‡½æ•°)**:

$$
V^*(s) = \max_\pi V^\pi(s)
$$

$$
Q^*(s, a) = \max_\pi Q^\pi(s, a)
$$

**å®šç† 2.2 (Bellmanæœ€ä¼˜æ–¹ç¨‹)**:

$$
V^*(s) = \max_{a \in \mathcal{A}} \sum_{s'} P(s'|s, a) [R(s, a, s') + \gamma V^*(s')]
$$

$$
Q^*(s, a) = \sum_{s'} P(s'|s, a) [R(s, a, s') + \gamma \max_{a'} Q^*(s', a')]
$$

---

### 3. æœ€ä¼˜ç­–ç•¥

**å®šç† 3.1 (æœ€ä¼˜ç­–ç•¥å­˜åœ¨æ€§)**:

å¯¹äºä»»æ„æœ‰é™MDPï¼Œå­˜åœ¨ç¡®å®šæ€§æœ€ä¼˜ç­–ç•¥ $\pi^*$ ä½¿å¾—ï¼š

$$
\pi^*(s) = \arg\max_{a \in \mathcal{A}} Q^*(s, a)
$$

**å”¯ä¸€æ€§**ï¼šæœ€ä¼˜ä»·å€¼å‡½æ•°å”¯ä¸€ï¼Œä½†æœ€ä¼˜ç­–ç•¥å¯èƒ½ä¸å”¯ä¸€ã€‚

---

## ğŸ”§ æ±‚è§£æ–¹æ³•

### 1. åŠ¨æ€è§„åˆ’

**å€¼è¿­ä»£ (Value Iteration)**:

$$
V_{k+1}(s) = \max_a \sum_{s'} P(s'|s, a) [R(s, a, s') + \gamma V_k(s')]
$$

**æ”¶æ•›æ€§**ï¼š$V_k \to V^*$ ä»¥å‡ ä½•é€Ÿç‡æ”¶æ•›ã€‚

**ç­–ç•¥è¿­ä»£ (Policy Iteration)**:

1. **ç­–ç•¥è¯„ä¼°**ï¼šè§£ $V^\pi = R^\pi + \gamma P^\pi V^\pi$
2. **ç­–ç•¥æ”¹è¿›**ï¼š$\pi'(s) = \arg\max_a Q^\pi(s, a)$

---

### 2. è’™ç‰¹å¡æ´›æ–¹æ³•

**æ€æƒ³**ï¼šé€šè¿‡é‡‡æ ·è½¨è¿¹ä¼°è®¡ä»·å€¼å‡½æ•°ã€‚

$$
V(s) \approx \frac{1}{N} \sum_{i=1}^{N} G_i(s)
$$

å…¶ä¸­ $G_i(s)$ æ˜¯ç¬¬ $i$ æ¡è½¨è¿¹ä¸­ä»çŠ¶æ€ $s$ å¼€å§‹çš„ç´¯ç§¯å¥–åŠ±ã€‚

---

### 3. æ—¶åºå·®åˆ†å­¦ä¹ 

**TD(0) æ›´æ–°**:

$$
V(s_t) \leftarrow V(s_t) + \alpha [R_{t+1} + \gamma V(s_{t+1}) - V(s_t)]
$$

**Q-Learning**:

$$
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha [R_{t+1} + \gamma \max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t)]
$$

**SARSA**:

$$
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha [R_{t+1} + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)]
$$

---

## ğŸ¤– æ·±åº¦å¼ºåŒ–å­¦ä¹ 

**DQN (Deep Q-Network)**:

ç”¨ç¥ç»ç½‘ç»œé€¼è¿‘ $Q^*(s, a)$ï¼š

$$
L(\theta) = \mathbb{E}[(R + \gamma \max_{a'} Q(s', a'; \theta^-) - Q(s, a; \theta))^2]
$$

**å…³é”®æŠ€æœ¯**ï¼š
- Experience Replay
- Target Network
- Double DQN

---

## ğŸ’» Pythonå®ç°

```python
import numpy as np
import matplotlib.pyplot as plt

# 1. ç®€å•Grid World MDP
class GridWorldMDP:
    def __init__(self, size=5):
        self.size = size
        self.n_states = size * size
        self.n_actions = 4  # ä¸Šä¸‹å·¦å³
        self.gamma = 0.9
        
        # å®šä¹‰å¥–åŠ±
        self.goal_state = (size-1, size-1)
        self.trap_state = (2, 2)
    
    def state_to_coord(self, state):
        return (state // self.size, state % self.size)
    
    def coord_to_state(self, coord):
        return coord[0] * self.size + coord[1]
    
    def get_next_state(self, state, action):
        """çŠ¶æ€è½¬ç§»"""
        x, y = self.state_to_coord(state)
        
        # åŠ¨ä½œ: 0=ä¸Š, 1=ä¸‹, 2=å·¦, 3=å³
        if action == 0:
            x = max(0, x - 1)
        elif action == 1:
            x = min(self.size - 1, x + 1)
        elif action == 2:
            y = max(0, y - 1)
        elif action == 3:
            y = min(self.size - 1, y + 1)
        
        return self.coord_to_state((x, y))
    
    def get_reward(self, state, action, next_state):
        """å¥–åŠ±å‡½æ•°"""
        coord = self.state_to_coord(next_state)
        
        if coord == self.goal_state:
            return 10.0
        elif coord == self.trap_state:
            return -10.0
        else:
            return -0.1  # æ¯æ­¥å°æƒ©ç½š


# 2. å€¼è¿­ä»£
def value_iteration(mdp, max_iter=1000, tol=1e-6):
    """å€¼è¿­ä»£ç®—æ³•"""
    V = np.zeros(mdp.n_states)
    
    for iteration in range(max_iter):
        V_new = np.zeros(mdp.n_states)
        
        for s in range(mdp.n_states):
            # Bellmanæœ€ä¼˜æ–¹ç¨‹
            q_values = []
            for a in range(mdp.n_actions):
                s_next = mdp.get_next_state(s, a)
                r = mdp.get_reward(s, a, s_next)
                q_values.append(r + mdp.gamma * V[s_next])
            
            V_new[s] = max(q_values)
        
        # æ£€æŸ¥æ”¶æ•›
        if np.max(np.abs(V_new - V)) < tol:
            print(f"Converged in {iteration} iterations")
            break
        
        V = V_new
    
    # æå–æœ€ä¼˜ç­–ç•¥
    policy = np.zeros(mdp.n_states, dtype=int)
    for s in range(mdp.n_states):
        q_values = []
        for a in range(mdp.n_actions):
            s_next = mdp.get_next_state(s, a)
            r = mdp.get_reward(s, a, s_next)
            q_values.append(r + mdp.gamma * V[s_next])
        policy[s] = np.argmax(q_values)
    
    return V, policy


# 3. Q-Learning
def q_learning(mdp, n_episodes=1000, alpha=0.1, epsilon=0.1):
    """Q-Learningç®—æ³•"""
    Q = np.zeros((mdp.n_states, mdp.n_actions))
    
    for episode in range(n_episodes):
        state = 0  # èµ·å§‹çŠ¶æ€
        
        for step in range(100):  # æœ€å¤š100æ­¥
            # Îµ-è´ªå¿ƒç­–ç•¥
            if np.random.rand() < epsilon:
                action = np.random.randint(mdp.n_actions)
            else:
                action = np.argmax(Q[state])
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state = mdp.get_next_state(state, action)
            reward = mdp.get_reward(state, action, next_state)
            
            # Q-Learningæ›´æ–°
            Q[state, action] += alpha * (
                reward + mdp.gamma * np.max(Q[next_state]) - Q[state, action]
            )
            
            # ç»ˆæ­¢æ¡ä»¶
            coord = mdp.state_to_coord(next_state)
            if coord == mdp.goal_state or coord == mdp.trap_state:
                break
            
            state = next_state
    
    # æå–ç­–ç•¥
    policy = np.argmax(Q, axis=1)
    V = np.max(Q, axis=1)
    
    return V, policy, Q


# 4. å¯è§†åŒ–
def visualize_policy(mdp, V, policy):
    """å¯è§†åŒ–ä»·å€¼å‡½æ•°å’Œç­–ç•¥"""
    V_grid = V.reshape(mdp.size, mdp.size)
    
    fig, axes = plt.subplots(1, 2, figsize=(14, 6))
    
    # ä»·å€¼å‡½æ•°
    im = axes[0].imshow(V_grid, cmap='viridis')
    axes[0].set_title('Value Function', fontsize=14)
    axes[0].set_xlabel('Column')
    axes[0].set_ylabel('Row')
    plt.colorbar(im, ax=axes[0])
    
    # ç­–ç•¥
    policy_grid = policy.reshape(mdp.size, mdp.size)
    arrows = ['â†‘', 'â†“', 'â†', 'â†’']
    
    axes[1].imshow(V_grid, cmap='gray', alpha=0.3)
    for i in range(mdp.size):
        for j in range(mdp.size):
            state = mdp.coord_to_state((i, j))
            axes[1].text(j, i, arrows[policy[state]], 
                        ha='center', va='center', fontsize=20)
    
    axes[1].set_title('Optimal Policy', fontsize=14)
    axes[1].set_xlabel('Column')
    axes[1].set_ylabel('Row')
    
    plt.tight_layout()
    plt.savefig('mdp_solution.png', dpi=150)
    plt.show()


# è¿è¡Œç¤ºä¾‹
if __name__ == "__main__":
    mdp = GridWorldMDP(size=5)
    
    print("Running Value Iteration...")
    V_vi, policy_vi = value_iteration(mdp)
    
    print("\nRunning Q-Learning...")
    V_ql, policy_ql, Q = q_learning(mdp, n_episodes=5000)
    
    visualize_policy(mdp, V_vi, policy_vi)
```

---

## ğŸ“š æ ¸å¿ƒå®šç†æ€»ç»“

| å®šç† | é™ˆè¿° | æ„ä¹‰ |
|------|------|------|
| **BellmanæœŸæœ›æ–¹ç¨‹** | $V^\pi = R^\pi + \gamma P^\pi V^\pi$ | ç­–ç•¥è¯„ä¼° |
| **Bellmanæœ€ä¼˜æ–¹ç¨‹** | $V^* = \max_a [R^a + \gamma P^a V^*]$ | æœ€ä¼˜æ€§æ¡ä»¶ |
| **æœ€ä¼˜ç­–ç•¥å­˜åœ¨** | $\exists \pi^*: V^{\pi^*} = V^*$ | å¯æ±‚è§£æ€§ |
| **å€¼è¿­ä»£æ”¶æ•›** | $V_k \to V^*$ æŒ‡æ•°æ”¶æ•› | ç®—æ³•ä¿è¯ |

---

## ğŸ“ ç›¸å…³è¯¾ç¨‹

| å¤§å­¦ | è¯¾ç¨‹ |
|------|------|
| **Stanford** | CS234 Reinforcement Learning |
| **UC Berkeley** | CS285 Deep Reinforcement Learning |
| **MIT** | 6.246 Reinforcement Learning |
| **DeepMind** | UCL Course on RL |

---

## ğŸ“– å‚è€ƒæ–‡çŒ®

1. **Sutton & Barto (2018)**. *Reinforcement Learning: An Introduction* (2nd ed.). MIT Press.

2. **Puterman (2014)**. *Markov Decision Processes*. Wiley.

3. **Bertsekas (2019)**. *Reinforcement Learning and Optimal Control*. Athena Scientific.

---

*æœ€åæ›´æ–°ï¼š2025å¹´10æœˆ*
