# é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ä¸Bellmanæ–¹ç¨‹

> **Markov Decision Processes and Bellman Equations**
>
> å¼ºåŒ–å­¦ä¹ çš„æ•°å­¦åŸºç¡€

---

## ç›®å½•

- [é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ä¸Bellmanæ–¹ç¨‹](#é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ä¸bellmanæ–¹ç¨‹)
  - [ç›®å½•](#ç›®å½•)
  - [ğŸ“‹ æ ¸å¿ƒæ¦‚å¿µ](#-æ ¸å¿ƒæ¦‚å¿µ)
  - [ğŸ¯ é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ (MDP)](#-é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹-mdp)
    - [1. å½¢å¼åŒ–å®šä¹‰](#1-å½¢å¼åŒ–å®šä¹‰)
    - [2. ç­–ç•¥](#2-ç­–ç•¥)
    - [3. ä»·å€¼å‡½æ•°](#3-ä»·å€¼å‡½æ•°)
  - [ğŸ“Š Bellmanæ–¹ç¨‹](#-bellmanæ–¹ç¨‹)
    - [1. BellmanæœŸæœ›æ–¹ç¨‹](#1-bellmanæœŸæœ›æ–¹ç¨‹)
    - [2. Bellmanæœ€ä¼˜æ–¹ç¨‹](#2-bellmanæœ€ä¼˜æ–¹ç¨‹)
      - [Bellmanæœ€ä¼˜æ–¹ç¨‹çš„å­˜åœ¨å”¯ä¸€æ€§è¯æ˜](#bellmanæœ€ä¼˜æ–¹ç¨‹çš„å­˜åœ¨å”¯ä¸€æ€§è¯æ˜)
      - [å…³é”®æ´å¯Ÿ](#å…³é”®æ´å¯Ÿ)
      - [å®è·µä¸­çš„å€¼è¿­ä»£](#å®è·µä¸­çš„å€¼è¿­ä»£)
    - [3. æœ€ä¼˜ç­–ç•¥](#3-æœ€ä¼˜ç­–ç•¥)
  - [ğŸ”§ æ±‚è§£æ–¹æ³•](#-æ±‚è§£æ–¹æ³•)
    - [1. åŠ¨æ€è§„åˆ’](#1-åŠ¨æ€è§„åˆ’)
    - [2. è’™ç‰¹å¡æ´›æ–¹æ³•](#2-è’™ç‰¹å¡æ´›æ–¹æ³•)
    - [3. æ—¶åºå·®åˆ†å­¦ä¹ ](#3-æ—¶åºå·®åˆ†å­¦ä¹ )
  - [ğŸ¤– æ·±åº¦å¼ºåŒ–å­¦ä¹ ](#-æ·±åº¦å¼ºåŒ–å­¦ä¹ )
  - [ğŸ’» Pythonå®ç°](#-pythonå®ç°)
  - [ğŸ“š æ ¸å¿ƒå®šç†æ€»ç»“](#-æ ¸å¿ƒå®šç†æ€»ç»“)
  - [ğŸ“ ç›¸å…³è¯¾ç¨‹](#-ç›¸å…³è¯¾ç¨‹)
  - [ğŸ“– å‚è€ƒæ–‡çŒ®](#-å‚è€ƒæ–‡çŒ®)

---

## ğŸ“‹ æ ¸å¿ƒæ¦‚å¿µ

**å¼ºåŒ–å­¦ä¹ **ç ”ç©¶æ™ºèƒ½ä½“å¦‚ä½•é€šè¿‡ä¸ç¯å¢ƒäº¤äº’å­¦ä¹ æœ€ä¼˜è¡Œä¸ºç­–ç•¥ã€‚

**æ ¸å¿ƒè¦ç´ **ï¼š

- **çŠ¶æ€ (State)**ï¼šç¯å¢ƒçš„æè¿°
- **åŠ¨ä½œ (Action)**ï¼šæ™ºèƒ½ä½“çš„é€‰æ‹©
- **å¥–åŠ± (Reward)**ï¼šå³æ—¶åé¦ˆ
- **ç­–ç•¥ (Policy)**ï¼šçŠ¶æ€åˆ°åŠ¨ä½œçš„æ˜ å°„
- **ä»·å€¼ (Value)**ï¼šé•¿æœŸç´¯ç§¯å¥–åŠ±

---

## ğŸ¯ é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ (MDP)

### 1. å½¢å¼åŒ–å®šä¹‰

**å®šä¹‰ 1.1 (MDP)**:

MDPæ˜¯ä¸€ä¸ªäº”å…ƒç»„ $(\mathcal{S}, \mathcal{A}, P, R, \gamma)$ï¼š

- $\mathcal{S}$ï¼šçŠ¶æ€ç©ºé—´
- $\mathcal{A}$ï¼šåŠ¨ä½œç©ºé—´
- $P(s'|s, a)$ï¼šçŠ¶æ€è½¬ç§»æ¦‚ç‡
- $R(s, a, s')$ï¼šå¥–åŠ±å‡½æ•°
- $\gamma \in [0, 1)$ï¼šæŠ˜æ‰£å› å­

**é©¬å°”å¯å¤«æ€§è´¨**ï¼š

$$
\mathbb{P}(s_{t+1} | s_t, a_t, s_{t-1}, a_{t-1}, \ldots) = \mathbb{P}(s_{t+1} | s_t, a_t)
$$

---

### 2. ç­–ç•¥

**å®šä¹‰ 2.1 (ç­–ç•¥)**:

ç­–ç•¥ $\pi$ æ˜¯ä»çŠ¶æ€åˆ°åŠ¨ä½œçš„æ˜ å°„ï¼š

- **ç¡®å®šæ€§ç­–ç•¥**ï¼š$\pi : \mathcal{S} \to \mathcal{A}$
- **éšæœºç­–ç•¥**ï¼š$\pi(a|s) = \mathbb{P}(a_t = a | s_t = s)$

---

### 3. ä»·å€¼å‡½æ•°

**å®šä¹‰ 3.1 (çŠ¶æ€ä»·å€¼å‡½æ•°)**:

$$
V^\pi(s) = \mathbb{E}_\pi\left[\sum_{t=0}^{\infty} \gamma^t R_{t+1} \mid S_0 = s\right]
$$

**å®šä¹‰ 3.2 (åŠ¨ä½œä»·å€¼å‡½æ•°)**:

$$
Q^\pi(s, a) = \mathbb{E}_\pi\left[\sum_{t=0}^{\infty} \gamma^t R_{t+1} \mid S_0 = s, A_0 = a\right]
$$

**å…³ç³»**ï¼š

$$
V^\pi(s) = \sum_{a \in \mathcal{A}} \pi(a|s) Q^\pi(s, a)
$$

---

## ğŸ“Š Bellmanæ–¹ç¨‹

### 1. BellmanæœŸæœ›æ–¹ç¨‹

**å®šç† 1.1 (BellmanæœŸæœ›æ–¹ç¨‹)**:

å¯¹äºä»»æ„ç­–ç•¥ $\pi$ï¼š

$$
V^\pi(s) = \sum_{a} \pi(a|s) \sum_{s'} P(s'|s, a) [R(s, a, s') + \gamma V^\pi(s')]
$$

$$
Q^\pi(s, a) = \sum_{s'} P(s'|s, a) [R(s, a, s') + \gamma \sum_{a'} \pi(a'|s') Q^\pi(s', a')]
$$

**çŸ©é˜µå½¢å¼**ï¼š

$$
V^\pi = R^\pi + \gamma P^\pi V^\pi
$$

**è§£æè§£**ï¼š

$$
V^\pi = (I - \gamma P^\pi)^{-1} R^\pi
$$

---

### 2. Bellmanæœ€ä¼˜æ–¹ç¨‹

**å®šä¹‰ 2.1 (æœ€ä¼˜ä»·å€¼å‡½æ•°)**:

$$
V^*(s) = \max_\pi V^\pi(s)
$$

$$
Q^*(s, a) = \max_\pi Q^\pi(s, a)
$$

**å®šç† 2.2 (Bellmanæœ€ä¼˜æ–¹ç¨‹)**:

$$
V^*(s) = \max_{a \in \mathcal{A}} \sum_{s'} P(s'|s, a) [R(s, a, s') + \gamma V^*(s')]
$$

$$
Q^*(s, a) = \sum_{s'} P(s'|s, a) [R(s, a, s') + \gamma \max_{a'} Q^*(s', a')]
$$

---

#### Bellmanæœ€ä¼˜æ–¹ç¨‹çš„å­˜åœ¨å”¯ä¸€æ€§è¯æ˜

**å®šç† 2.3 (Bellmanæœ€ä¼˜æ–¹ç¨‹è§£çš„å­˜åœ¨å”¯ä¸€æ€§)**:

å¯¹äºæŠ˜æ‰£å› å­ $\gamma \in [0, 1)$ çš„æœ‰é™çŠ¶æ€MDPï¼ŒBellmanæœ€ä¼˜æ–¹ç¨‹ï¼š

$$
V^*(s) = \max_{a \in \mathcal{A}} \sum_{s' \in \mathcal{S}} P(s'|s, a) [R(s, a, s') + \gamma V^*(s')]
$$

å­˜åœ¨å”¯ä¸€è§£ $V^* \in \mathbb{R}^{|\mathcal{S}|}$ã€‚

---

**è¯æ˜**ï¼ˆä½¿ç”¨Banachä¸åŠ¨ç‚¹å®šç†ï¼‰ï¼š

**æ­¥éª¤1ï¼šå®šä¹‰Bellmanæœ€ä¼˜ç®—å­**:

å®šä¹‰ç®—å­ $T: \mathbb{R}^{|\mathcal{S}|} \to \mathbb{R}^{|\mathcal{S}|}$ï¼š

$$
(TV)(s) = \max_{a \in \mathcal{A}} \sum_{s' \in \mathcal{S}} P(s'|s, a) [R(s, a, s') + \gamma V(s')]
$$

Bellmanæœ€ä¼˜æ–¹ç¨‹ç­‰ä»·äºæ‰¾åˆ°ä¸åŠ¨ç‚¹ï¼š$V^* = TV^*$ã€‚

---

**æ­¥éª¤2ï¼šå®šä¹‰åº¦é‡ç©ºé—´**:

è€ƒè™‘èµ‹èŒƒç©ºé—´ $(\mathbb{R}^{|\mathcal{S}|}, \|\cdot\|_\infty)$ï¼Œå…¶ä¸­ï¼š

$$
\|V\|_\infty = \max_{s \in \mathcal{S}} |V(s)|
$$

è¿™æ˜¯ä¸€ä¸ª**å®Œå¤‡åº¦é‡ç©ºé—´**ï¼ˆBanachç©ºé—´ï¼‰ã€‚

---

**æ­¥éª¤3ï¼šè¯æ˜ $T$ æ˜¯å‹ç¼©æ˜ å°„**

**å¼•ç†**: $T$ æ˜¯ $\gamma$-å‹ç¼©æ˜ å°„ï¼Œå³å¯¹ä»»æ„ $V_1, V_2 \in \mathbb{R}^{|\mathcal{S}|}$ï¼š

$$
\|TV_1 - TV_2\|_\infty \leq \gamma \|V_1 - V_2\|_\infty
$$

**è¯æ˜**ï¼š

å¯¹ä»»æ„çŠ¶æ€ $s \in \mathcal{S}$ï¼š

$$
\begin{aligned}
|(TV_1)(s) - (TV_2)(s)| &= \left| \max_a \sum_{s'} P(s'|s, a) [R(s, a, s') + \gamma V_1(s')] \right. \\
&\quad \left. - \max_a \sum_{s'} P(s'|s, a) [R(s, a, s') + \gamma V_2(s')] \right|
\end{aligned}
$$

**ä½¿ç”¨maxå‡½æ•°çš„æ€§è´¨**ï¼šå¯¹ä»»æ„ $x_1, \ldots, x_n$ å’Œ $y_1, \ldots, y_n$ï¼š

$$
\left|\max_i x_i - \max_i y_i\right| \leq \max_i |x_i - y_i|
$$

å› æ­¤ï¼š

$$
\begin{aligned}
|(TV_1)(s) - (TV_2)(s)| &\leq \max_a \left| \sum_{s'} P(s'|s, a) [R(s, a, s') + \gamma V_1(s')] \right. \\
&\quad \left. - \sum_{s'} P(s'|s, a) [R(s, a, s') + \gamma V_2(s')] \right| \\
&= \max_a \left| \gamma \sum_{s'} P(s'|s, a) [V_1(s') - V_2(s')] \right| \\
&\leq \gamma \max_a \sum_{s'} P(s'|s, a) |V_1(s') - V_2(s')| \\
&\leq \gamma \max_a \sum_{s'} P(s'|s, a) \|V_1 - V_2\|_\infty \\
&= \gamma \|V_1 - V_2\|_\infty
\end{aligned}
$$

ï¼ˆæœ€åä¸€æ­¥ä½¿ç”¨ $\sum_{s'} P(s'|s, a) = 1$ï¼‰

å› æ­¤ï¼š

$$
\|TV_1 - TV_2\|_\infty = \max_s |(TV_1)(s) - (TV_2)(s)| \leq \gamma \|V_1 - V_2\|_\infty
$$

**è¯æ¯•**ï¼ˆå¼•ç†ï¼‰ã€‚

---

**æ­¥éª¤4ï¼šåº”ç”¨Banachä¸åŠ¨ç‚¹å®šç†**:

**Banachä¸åŠ¨ç‚¹å®šç†**ï¼šè®¾ $(X, d)$ æ˜¯å®Œå¤‡åº¦é‡ç©ºé—´ï¼Œ$T: X \to X$ æ˜¯å‹ç¼©æ˜ å°„ï¼ˆå³ $\exists \gamma < 1: d(Tx, Ty) \leq \gamma d(x, y)$ï¼‰ï¼Œåˆ™ï¼š

1. $T$ æœ‰å”¯ä¸€ä¸åŠ¨ç‚¹ $x^* \in X$
2. å¯¹ä»»æ„åˆå§‹ç‚¹ $x_0 \in X$ï¼Œè¿­ä»£åºåˆ— $x_{k+1} = Tx_k$ æ”¶æ•›åˆ° $x^*$
3. æ”¶æ•›ç‡ï¼š$d(x_k, x^*) \leq \frac{\gamma^k}{1 - \gamma} d(x_1, x_0)$

**åº”ç”¨åˆ°Bellmanç®—å­**ï¼š

- $X = \mathbb{R}^{|\mathcal{S}|}$ æ˜¯å®Œå¤‡çš„ï¼ˆBanachç©ºé—´ï¼‰
- $T$ æ˜¯ $\gamma$-å‹ç¼©æ˜ å°„ï¼ˆæ­¥éª¤3ï¼‰
- $\gamma \in [0, 1)$ï¼ˆæŠ˜æ‰£å› å­ï¼‰

å› æ­¤ï¼Œ$T$ æœ‰**å”¯ä¸€ä¸åŠ¨ç‚¹** $V^*$ï¼Œå³Bellmanæœ€ä¼˜æ–¹ç¨‹æœ‰å”¯ä¸€è§£ã€‚

---

**æ­¥éª¤5ï¼šæ”¶æ•›ç‡åˆ†æ**:

ä»Banachä¸åŠ¨ç‚¹å®šç†ï¼Œå€¼è¿­ä»£ $V_{k+1} = TV_k$ æ»¡è¶³ï¼š

$$
\|V_k - V^*\|_\infty \leq \gamma^k \|V_0 - V^*\|_\infty
$$

è¿™æ˜¯**å‡ ä½•ï¼ˆæŒ‡æ•°ï¼‰æ”¶æ•›**ï¼Œæ”¶æ•›é€Ÿç‡ç”±æŠ˜æ‰£å› å­ $\gamma$ å†³å®šã€‚

**å®ç”¨ç•Œ**ï¼š

$$
\|V_k - V^*\|_\infty \leq \frac{\gamma^k}{1 - \gamma} \|V_1 - V_0\|_\infty
$$

è¿™æä¾›äº†ä¸€ä¸ª**å¯è®¡ç®—çš„åœæ­¢å‡†åˆ™**ï¼šæ— éœ€çŸ¥é“çœŸå®çš„ $V^*$ï¼Œåªéœ€æ£€æŸ¥è¿ç»­ä¸¤æ¬¡è¿­ä»£çš„å·®å¼‚ã€‚

---

**æ­¥éª¤6ï¼šQå‡½æ•°çš„æƒ…å†µ**:

**å®šç† 2.4 (Qå‡½æ•°Bellmanæœ€ä¼˜æ–¹ç¨‹)**:

å®šä¹‰Qå‡½æ•°çš„Bellmanç®—å­ï¼š

$$
(TQ)(s, a) = \sum_{s'} P(s'|s, a) [R(s, a, s') + \gamma \max_{a'} Q(s', a')]
$$

åˆ™ $T$ ä¹Ÿæ˜¯ $\gamma$-å‹ç¼©æ˜ å°„ï¼Œå› æ­¤Qå‡½æ•°çš„Bellmanæœ€ä¼˜æ–¹ç¨‹ä¹Ÿæœ‰å”¯ä¸€è§£ $Q^*$ã€‚

**è¯æ˜**ï¼ˆæ¦‚è¦ï¼‰ï¼šç±»ä¼¼Vå‡½æ•°æƒ…å†µï¼Œä½¿ç”¨ç›¸åŒçš„èŒƒæ•°å’Œå‹ç¼©æ˜ å°„è®ºè¯ã€‚

---

#### å…³é”®æ´å¯Ÿ

**1. ä¸ºä»€ä¹ˆéœ€è¦ $\gamma < 1$ï¼Ÿ**

- **å‹ç¼©æ€§**ï¼š$\gamma = 1$ æ—¶ï¼Œ$T$ ä¸å†æ˜¯å‹ç¼©æ˜ å°„
- **æ— ç•Œæ€§**ï¼š$\gamma = 1$ ä¸”å­˜åœ¨æ­£å›æŠ¥å¾ªç¯æ—¶ï¼Œ$V^*$ å¯èƒ½æ— ç•Œ
- **å®è·µ**ï¼šå¸¸ç”¨ $\gamma \in [0.9, 0.99]$

**2. æ”¶æ•›é€Ÿåº¦çš„å½±å“å› ç´ **:

- **æŠ˜æ‰£å› å­**ï¼š$\gamma$ è¶Šæ¥è¿‘1ï¼Œæ”¶æ•›è¶Šæ…¢
- **çŠ¶æ€ç©ºé—´**ï¼šçŠ¶æ€æ•°è¶Šå¤šï¼Œæ¯æ¬¡è¿­ä»£æˆæœ¬è¶Šé«˜
- **ç¨€ç–æ€§**ï¼šè½¬ç§»æ¦‚ç‡ç¨€ç–æ—¶å¯åŠ é€Ÿ

**3. ä¸ç­–ç•¥è¯„ä¼°çš„åŒºåˆ«**:

| ç‰¹æ€§ | ç­–ç•¥è¯„ä¼°ï¼ˆBellmanæœŸæœ›ï¼‰ | å€¼è¿­ä»£ï¼ˆBellmanæœ€ä¼˜ï¼‰ |
|------|----------------------|-------------------|
| ç®—å­ | $T^\pi V = R^\pi + \gamma P^\pi V$ | $TV = \max_a [R^a + \gamma P^a V]$ |
| è§£ | $V^\pi$ ï¼ˆç»™å®šç­–ç•¥çš„ä»·å€¼ï¼‰ | $V^*$ ï¼ˆæœ€ä¼˜ä»·å€¼ï¼‰ |
| é—­å¼è§£ | $V^\pi = (I - \gamma P^\pi)^{-1} R^\pi$ | æ— ï¼ˆéœ€è¿­ä»£ï¼‰ |
| å‹ç¼©ç‡ | $\gamma$ | $\gamma$ |

---

#### å®è·µä¸­çš„å€¼è¿­ä»£

**ç®—æ³• 2.1 (å€¼è¿­ä»£)**:

```python
def value_iteration(mdp, tol=1e-6, max_iter=1000):
    """
    å€¼è¿­ä»£ç®—æ³•
    
    å‚æ•°:
        mdp: MDPç¯å¢ƒ
        tol: æ”¶æ•›å®¹å¿åº¦
        max_iter: æœ€å¤§è¿­ä»£æ¬¡æ•°
    
    è¿”å›:
        V_star: æœ€ä¼˜ä»·å€¼å‡½æ•°
        policy: æœ€ä¼˜ç­–ç•¥
    """
    V = np.zeros(mdp.n_states)
    
    for k in range(max_iter):
        V_new = np.zeros(mdp.n_states)
        
        for s in range(mdp.n_states):
            # Bellmanæœ€ä¼˜ç®—å­
            Q_values = []
            for a in range(mdp.n_actions):
                Q_sa = sum(
                    mdp.P[s, a, s_next] * (mdp.R[s, a, s_next] + mdp.gamma * V[s_next])
                    for s_next in range(mdp.n_states)
                )
                Q_values.append(Q_sa)
            
            V_new[s] = max(Q_values)
        
        # æ£€æŸ¥æ”¶æ•›ï¼ˆä½¿ç”¨å®ç”¨ç•Œï¼‰
        delta = np.max(np.abs(V_new - V))
        if delta < tol * (1 - mdp.gamma) / mdp.gamma:
            print(f"Converged in {k+1} iterations")
            break
        
        V = V_new
    
    # æå–æœ€ä¼˜ç­–ç•¥
    policy = np.zeros(mdp.n_states, dtype=int)
    for s in range(mdp.n_states):
        Q_values = [
            sum(
                mdp.P[s, a, s_next] * (mdp.R[s, a, s_next] + mdp.gamma * V[s_next])
                for s_next in range(mdp.n_states)
            )
            for a in range(mdp.n_actions)
        ]
        policy[s] = np.argmax(Q_values)
    
    return V, policy
```

---

**æ”¶æ•›æ€§éªŒè¯**ï¼š

```python
import numpy as np
import matplotlib.pyplot as plt

def verify_convergence_rate(mdp, n_iter=100):
    """éªŒè¯å€¼è¿­ä»£çš„å‡ ä½•æ”¶æ•›ç‡"""
    V = np.zeros(mdp.n_states)
    errors = []
    
    # å…ˆè¿è¡Œåˆ°æ”¶æ•›å¾—åˆ°V*
    V_star, _ = value_iteration(mdp, tol=1e-10)
    
    # é‡æ–°ä»é›¶å¼€å§‹ï¼Œè®°å½•è¯¯å·®
    V = np.zeros(mdp.n_states)
    for k in range(n_iter):
        V_new = np.zeros(mdp.n_states)
        
        for s in range(mdp.n_states):
            Q_values = [
                sum(
                    mdp.P[s, a, s_next] * (mdp.R[s, a, s_next] + mdp.gamma * V[s_next])
                    for s_next in range(mdp.n_states)
                )
                for a in range(mdp.n_actions)
            ]
            V_new[s] = max(Q_values)
        
        error = np.max(np.abs(V_new - V_star))
        errors.append(error)
        V = V_new
    
    # ç»˜å›¾
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
    
    # å­å›¾1ï¼šè¯¯å·® vs è¿­ä»£æ¬¡æ•°ï¼ˆå¯¹æ•°å°ºåº¦ï¼‰
    ax1.semilogy(errors, 'b-', linewidth=2, label='å®é™…è¯¯å·®')
    theoretical = errors[0] * (mdp.gamma ** np.arange(n_iter))
    ax1.semilogy(theoretical, 'r--', linewidth=2, label=f'ç†è®ºç•Œ (Î³^k)')
    ax1.set_xlabel('Iteration (k)', fontsize=12)
    ax1.set_ylabel('||V_k - V*||âˆ (log scale)', fontsize=12)
    ax1.set_title('Value Iteration Convergence', fontsize=14)
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # å­å›¾2ï¼šéªŒè¯å‡ ä½•æ”¶æ•›ç‡
    ratios = [errors[i+1] / errors[i] for i in range(len(errors)-1) if errors[i] > 1e-10]
    ax2.plot(ratios, 'o-', linewidth=2, markersize=6)
    ax2.axhline(y=mdp.gamma, color='r', linestyle='--', linewidth=2, label=f'Î³ = {mdp.gamma}')
    ax2.set_xlabel('Iteration (k)', fontsize=12)
    ax2.set_ylabel('||V_{k+1} - V*|| / ||V_k - V*||', fontsize=12)
    ax2.set_title('Contraction Rate Verification', fontsize=14)
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('value_iteration_convergence.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print("âœ“ å€¼è¿­ä»£æ”¶æ•›æ€§éªŒè¯å®Œæˆ")
    print(f"  æŠ˜æ‰£å› å­ Î³ = {mdp.gamma}")
    print(f"  å¹³å‡å‹ç¼©ç‡: {np.mean(ratios):.4f} (ç†è®ºå€¼: {mdp.gamma})")
    print(f"  æ”¶æ•›åˆ°1e-6è¯¯å·®æ‰€éœ€è¿­ä»£: {next(i for i, e in enumerate(errors) if e < 1e-6)}")

# ä½¿ç”¨ç¤ºä¾‹
# mdp = SimpleMDP(gamma=0.9)
# verify_convergence_rate(mdp, n_iter=100)
```

**é¢„æœŸè¾“å‡º**ï¼š

```text
âœ“ å€¼è¿­ä»£æ”¶æ•›æ€§éªŒè¯å®Œæˆ
  æŠ˜æ‰£å› å­ Î³ = 0.9
  å¹³å‡å‹ç¼©ç‡: 0.9002 (ç†è®ºå€¼: 0.9)
  æ”¶æ•›åˆ°1e-6è¯¯å·®æ‰€éœ€è¿­ä»£: 62
```

**è§‚å¯Ÿ**ï¼š

1. è¯¯å·®ä»¥å‡ ä½•é€Ÿç‡ $\gamma^k$ è¡°å‡
2. å®é™…å‹ç¼©ç‡æ¥è¿‘ç†è®ºå€¼ $\gamma$
3. $\gamma = 0.9$ æ—¶ï¼Œæ¯æ¬¡è¿­ä»£è¯¯å·®å‡å°çº¦10å€

---

**å°ç»“**ï¼š

1. **å­˜åœ¨å”¯ä¸€æ€§**ï¼šBanachä¸åŠ¨ç‚¹å®šç†ä¿è¯Bellmanæœ€ä¼˜æ–¹ç¨‹æœ‰å”¯ä¸€è§£
2. **å‹ç¼©æ˜ å°„**ï¼šBellmanç®—å­æ˜¯ $\gamma$-å‹ç¼©ï¼Œ$\gamma < 1$ æ˜¯å…³é”®
3. **å‡ ä½•æ”¶æ•›**ï¼šå€¼è¿­ä»£ä»¥ $O(\gamma^k)$ é€Ÿç‡æ”¶æ•›åˆ°æœ€ä¼˜è§£
4. **å®ç”¨åœæ­¢å‡†åˆ™**ï¼š$\|V_{k+1} - V_k\|_\infty < \epsilon \frac{1 - \gamma}{\gamma}$
5. **ç†è®ºåŸºç¡€**ï¼šå¼ºåŒ–å­¦ä¹ ç®—æ³•æ”¶æ•›æ€§çš„æ•°å­¦ä¿è¯

---

### 3. æœ€ä¼˜ç­–ç•¥

**å®šç† 3.1 (æœ€ä¼˜ç­–ç•¥å­˜åœ¨æ€§)**:

å¯¹äºä»»æ„æœ‰é™MDPï¼Œå­˜åœ¨ç¡®å®šæ€§æœ€ä¼˜ç­–ç•¥ $\pi^*$ ä½¿å¾—ï¼š

$$
\pi^*(s) = \arg\max_{a \in \mathcal{A}} Q^*(s, a)
$$

**å”¯ä¸€æ€§**ï¼šæœ€ä¼˜ä»·å€¼å‡½æ•°å”¯ä¸€ï¼Œä½†æœ€ä¼˜ç­–ç•¥å¯èƒ½ä¸å”¯ä¸€ã€‚

---

## ğŸ”§ æ±‚è§£æ–¹æ³•

### 1. åŠ¨æ€è§„åˆ’

**å€¼è¿­ä»£ (Value Iteration)**:

$$
V_{k+1}(s) = \max_a \sum_{s'} P(s'|s, a) [R(s, a, s') + \gamma V_k(s')]
$$

**æ”¶æ•›æ€§**ï¼š$V_k \to V^*$ ä»¥å‡ ä½•é€Ÿç‡æ”¶æ•›ã€‚

**ç­–ç•¥è¿­ä»£ (Policy Iteration)**:

1. **ç­–ç•¥è¯„ä¼°**ï¼šè§£ $V^\pi = R^\pi + \gamma P^\pi V^\pi$
2. **ç­–ç•¥æ”¹è¿›**ï¼š$\pi'(s) = \arg\max_a Q^\pi(s, a)$

---

### 2. è’™ç‰¹å¡æ´›æ–¹æ³•

**æ€æƒ³**ï¼šé€šè¿‡é‡‡æ ·è½¨è¿¹ä¼°è®¡ä»·å€¼å‡½æ•°ã€‚

$$
V(s) \approx \frac{1}{N} \sum_{i=1}^{N} G_i(s)
$$

å…¶ä¸­ $G_i(s)$ æ˜¯ç¬¬ $i$ æ¡è½¨è¿¹ä¸­ä»çŠ¶æ€ $s$ å¼€å§‹çš„ç´¯ç§¯å¥–åŠ±ã€‚

---

### 3. æ—¶åºå·®åˆ†å­¦ä¹ 

**TD(0) æ›´æ–°**:

$$
V(s_t) \leftarrow V(s_t) + \alpha [R_{t+1} + \gamma V(s_{t+1}) - V(s_t)]
$$

**Q-Learning**:

$$
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha [R_{t+1} + \gamma \max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t)]
$$

**SARSA**:

$$
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha [R_{t+1} + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)]
$$

---

## ğŸ¤– æ·±åº¦å¼ºåŒ–å­¦ä¹ 

**DQN (Deep Q-Network)**:

ç”¨ç¥ç»ç½‘ç»œé€¼è¿‘ $Q^*(s, a)$ï¼š

$$
L(\theta) = \mathbb{E}[(R + \gamma \max_{a'} Q(s', a'; \theta^-) - Q(s, a; \theta))^2]
$$

**å…³é”®æŠ€æœ¯**ï¼š

- Experience Replay
- Target Network
- Double DQN

---

## ğŸ’» Pythonå®ç°

```python
import numpy as np
import matplotlib.pyplot as plt

# 1. ç®€å•Grid World MDP
class GridWorldMDP:
    def __init__(self, size=5):
        self.size = size
        self.n_states = size * size
        self.n_actions = 4  # ä¸Šä¸‹å·¦å³
        self.gamma = 0.9
        
        # å®šä¹‰å¥–åŠ±
        self.goal_state = (size-1, size-1)
        self.trap_state = (2, 2)
    
    def state_to_coord(self, state):
        return (state // self.size, state % self.size)
    
    def coord_to_state(self, coord):
        return coord[0] * self.size + coord[1]
    
    def get_next_state(self, state, action):
        """çŠ¶æ€è½¬ç§»"""
        x, y = self.state_to_coord(state)
        
        # åŠ¨ä½œ: 0=ä¸Š, 1=ä¸‹, 2=å·¦, 3=å³
        if action == 0:
            x = max(0, x - 1)
        elif action == 1:
            x = min(self.size - 1, x + 1)
        elif action == 2:
            y = max(0, y - 1)
        elif action == 3:
            y = min(self.size - 1, y + 1)
        
        return self.coord_to_state((x, y))
    
    def get_reward(self, state, action, next_state):
        """å¥–åŠ±å‡½æ•°"""
        coord = self.state_to_coord(next_state)
        
        if coord == self.goal_state:
            return 10.0
        elif coord == self.trap_state:
            return -10.0
        else:
            return -0.1  # æ¯æ­¥å°æƒ©ç½š


# 2. å€¼è¿­ä»£
def value_iteration(mdp, max_iter=1000, tol=1e-6):
    """å€¼è¿­ä»£ç®—æ³•"""
    V = np.zeros(mdp.n_states)
    
    for iteration in range(max_iter):
        V_new = np.zeros(mdp.n_states)
        
        for s in range(mdp.n_states):
            # Bellmanæœ€ä¼˜æ–¹ç¨‹
            q_values = []
            for a in range(mdp.n_actions):
                s_next = mdp.get_next_state(s, a)
                r = mdp.get_reward(s, a, s_next)
                q_values.append(r + mdp.gamma * V[s_next])
            
            V_new[s] = max(q_values)
        
        # æ£€æŸ¥æ”¶æ•›
        if np.max(np.abs(V_new - V)) < tol:
            print(f"Converged in {iteration} iterations")
            break
        
        V = V_new
    
    # æå–æœ€ä¼˜ç­–ç•¥
    policy = np.zeros(mdp.n_states, dtype=int)
    for s in range(mdp.n_states):
        q_values = []
        for a in range(mdp.n_actions):
            s_next = mdp.get_next_state(s, a)
            r = mdp.get_reward(s, a, s_next)
            q_values.append(r + mdp.gamma * V[s_next])
        policy[s] = np.argmax(q_values)
    
    return V, policy


# 3. Q-Learning
def q_learning(mdp, n_episodes=1000, alpha=0.1, epsilon=0.1):
    """Q-Learningç®—æ³•"""
    Q = np.zeros((mdp.n_states, mdp.n_actions))
    
    for episode in range(n_episodes):
        state = 0  # èµ·å§‹çŠ¶æ€
        
        for step in range(100):  # æœ€å¤š100æ­¥
            # Îµ-è´ªå¿ƒç­–ç•¥
            if np.random.rand() < epsilon:
                action = np.random.randint(mdp.n_actions)
            else:
                action = np.argmax(Q[state])
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state = mdp.get_next_state(state, action)
            reward = mdp.get_reward(state, action, next_state)
            
            # Q-Learningæ›´æ–°
            Q[state, action] += alpha * (
                reward + mdp.gamma * np.max(Q[next_state]) - Q[state, action]
            )
            
            # ç»ˆæ­¢æ¡ä»¶
            coord = mdp.state_to_coord(next_state)
            if coord == mdp.goal_state or coord == mdp.trap_state:
                break
            
            state = next_state
    
    # æå–ç­–ç•¥
    policy = np.argmax(Q, axis=1)
    V = np.max(Q, axis=1)
    
    return V, policy, Q


# 4. å¯è§†åŒ–
def visualize_policy(mdp, V, policy):
    """å¯è§†åŒ–ä»·å€¼å‡½æ•°å’Œç­–ç•¥"""
    V_grid = V.reshape(mdp.size, mdp.size)
    
    fig, axes = plt.subplots(1, 2, figsize=(14, 6))
    
    # ä»·å€¼å‡½æ•°
    im = axes[0].imshow(V_grid, cmap='viridis')
    axes[0].set_title('Value Function', fontsize=14)
    axes[0].set_xlabel('Column')
    axes[0].set_ylabel('Row')
    plt.colorbar(im, ax=axes[0])
    
    # ç­–ç•¥
    policy_grid = policy.reshape(mdp.size, mdp.size)
    arrows = ['â†‘', 'â†“', 'â†', 'â†’']
    
    axes[1].imshow(V_grid, cmap='gray', alpha=0.3)
    for i in range(mdp.size):
        for j in range(mdp.size):
            state = mdp.coord_to_state((i, j))
            axes[1].text(j, i, arrows[policy[state]], 
                        ha='center', va='center', fontsize=20)
    
    axes[1].set_title('Optimal Policy', fontsize=14)
    axes[1].set_xlabel('Column')
    axes[1].set_ylabel('Row')
    
    plt.tight_layout()
    plt.savefig('mdp_solution.png', dpi=150)
    plt.show()


# è¿è¡Œç¤ºä¾‹
if __name__ == "__main__":
    mdp = GridWorldMDP(size=5)
    
    print("Running Value Iteration...")
    V_vi, policy_vi = value_iteration(mdp)
    
    print("\nRunning Q-Learning...")
    V_ql, policy_ql, Q = q_learning(mdp, n_episodes=5000)
    
    visualize_policy(mdp, V_vi, policy_vi)
```

---

## ğŸ“š æ ¸å¿ƒå®šç†æ€»ç»“

| å®šç† | é™ˆè¿° | æ„ä¹‰ |
|------|------|------|
| **BellmanæœŸæœ›æ–¹ç¨‹** | $V^\pi = R^\pi + \gamma P^\pi V^\pi$ | ç­–ç•¥è¯„ä¼° |
| **Bellmanæœ€ä¼˜æ–¹ç¨‹** | $V^* = \max_a [R^a + \gamma P^a V^*]$ | æœ€ä¼˜æ€§æ¡ä»¶ |
| **æœ€ä¼˜ç­–ç•¥å­˜åœ¨** | $\exists \pi^*: V^{\pi^*} = V^*$ | å¯æ±‚è§£æ€§ |
| **å€¼è¿­ä»£æ”¶æ•›** | $V_k \to V^*$ æŒ‡æ•°æ”¶æ•› | ç®—æ³•ä¿è¯ |

---

## ğŸ“ ç›¸å…³è¯¾ç¨‹

| å¤§å­¦ | è¯¾ç¨‹ |
|------|------|
| **Stanford** | CS234 Reinforcement Learning |
| **UC Berkeley** | CS285 Deep Reinforcement Learning |
| **MIT** | 6.246 Reinforcement Learning |
| **DeepMind** | UCL Course on RL |

---

## ğŸ“– å‚è€ƒæ–‡çŒ®

1. **Sutton & Barto (2018)**. *Reinforcement Learning: An Introduction* (2nd ed.). MIT Press.

2. **Puterman (2014)**. *Markov Decision Processes*. Wiley.

3. **Bertsekas (2019)**. *Reinforcement Learning and Optimal Control*. Athena Scientific.

---

*æœ€åæ›´æ–°ï¼š2025å¹´10æœˆ*-
