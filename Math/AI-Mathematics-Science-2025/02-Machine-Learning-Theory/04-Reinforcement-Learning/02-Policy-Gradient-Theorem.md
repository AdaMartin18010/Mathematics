# ç­–ç•¥æ¢¯åº¦å®šç†

> **Policy Gradient Theorem**
>
> ç›´æ¥ä¼˜åŒ–ç­–ç•¥çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•

---

## ç›®å½•

- [ç­–ç•¥æ¢¯åº¦å®šç†](#ç­–ç•¥æ¢¯åº¦å®šç†)
  - [ç›®å½•](#ç›®å½•)
  - [ğŸ“‹ æ ¸å¿ƒæ€æƒ³](#-æ ¸å¿ƒæ€æƒ³)
  - [ğŸ¯ ç­–ç•¥æ¢¯åº¦å®šç†](#-ç­–ç•¥æ¢¯åº¦å®šç†)
    - [1. é—®é¢˜è®¾å®š](#1-é—®é¢˜è®¾å®š)
    - [2. å®šç†é™ˆè¿°](#2-å®šç†é™ˆè¿°)
    - [3. è¯æ˜æ€è·¯](#3-è¯æ˜æ€è·¯)
  - [ğŸ“Š ç»å…¸ç®—æ³•](#-ç»å…¸ç®—æ³•)
    - [1. REINFORCE](#1-reinforce)
    - [2. Actor-Critic](#2-actor-critic)
    - [3. ä¼˜åŠ¿å‡½æ•°](#3-ä¼˜åŠ¿å‡½æ•°)
  - [ğŸ”§ ç°ä»£å˜ä½“](#-ç°ä»£å˜ä½“)
    - [1. PPO (Proximal Policy Optimization)](#1-ppo-proximal-policy-optimization)
    - [2. TRPO (Trust Region Policy Optimization)](#2-trpo-trust-region-policy-optimization)
  - [ğŸ’» Pythonå®ç°](#-pythonå®ç°)
  - [ğŸ“š æ ¸å¿ƒå®šç†æ€»ç»“](#-æ ¸å¿ƒå®šç†æ€»ç»“)
  - [ğŸ“ ç›¸å…³è¯¾ç¨‹](#-ç›¸å…³è¯¾ç¨‹)
  - [ğŸ“– å‚è€ƒæ–‡çŒ®](#-å‚è€ƒæ–‡çŒ®)

---

## ğŸ“‹ æ ¸å¿ƒæ€æƒ³

**ç­–ç•¥æ¢¯åº¦æ–¹æ³•**ç›´æ¥ä¼˜åŒ–å‚æ•°åŒ–ç­–ç•¥ $\pi_\theta(a|s)$ã€‚

**æ ¸å¿ƒä¼˜åŠ¿**ï¼š

- å¯å¤„ç†è¿ç»­åŠ¨ä½œç©ºé—´
- å¯å­¦ä¹ éšæœºç­–ç•¥
- æ›´å¥½çš„æ”¶æ•›æ€§è´¨

**æŒ‘æˆ˜**ï¼š

- é«˜æ–¹å·®
- æ ·æœ¬æ•ˆç‡ä½

---

## ğŸ¯ ç­–ç•¥æ¢¯åº¦å®šç†

### 1. é—®é¢˜è®¾å®š

**ç›®æ ‡**ï¼šæœ€å¤§åŒ–æœŸæœ›ç´¯ç§¯å¥–åŠ±

$$
J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}[R(\tau)] = \mathbb{E}_{\tau \sim \pi_\theta}\left[\sum_{t=0}^{T} \gamma^t r_t\right]
$$

å…¶ä¸­ $\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \ldots)$ æ˜¯è½¨è¿¹ã€‚

---

### 2. å®šç†é™ˆè¿°

**å®šç† 2.1 (ç­–ç•¥æ¢¯åº¦å®šç†)**:

$$
\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}\left[\sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot G_t\right]
$$

å…¶ä¸­ $G_t = \sum_{k=t}^{T} \gamma^{k-t} r_k$ æ˜¯ä»æ—¶åˆ» $t$ å¼€å§‹çš„ç´¯ç§¯å¥–åŠ±ã€‚

**ç­‰ä»·å½¢å¼**ï¼ˆä½¿ç”¨Qå‡½æ•°ï¼‰ï¼š

$$
\nabla_\theta J(\theta) = \mathbb{E}_{s \sim d^\pi, a \sim \pi_\theta}\left[\nabla_\theta \log \pi_\theta(a|s) \cdot Q^\pi(s, a)\right]
$$

å…¶ä¸­ $d^\pi(s)$ æ˜¯åœ¨ç­–ç•¥ $\pi$ ä¸‹çš„çŠ¶æ€åˆ†å¸ƒã€‚

---

### 3. è¯æ˜æ€è·¯

**å…³é”®æ­¥éª¤**ï¼š

1. **è½¨è¿¹æ¦‚ç‡**ï¼š
   $$
   P(\tau|\theta) = \rho_0(s_0) \prod_{t=0}^{T} \pi_\theta(a_t|s_t) P(s_{t+1}|s_t, a_t)
   $$

2. **å¯¹æ•°æŠ€å·§**ï¼š
   $$
   \nabla_\theta P(\tau|\theta) = P(\tau|\theta) \nabla_\theta \log P(\tau|\theta)
   $$

3. **çŠ¶æ€è½¬ç§»ç‹¬ç«‹äº $\theta$**ï¼š
   $$
   \nabla_\theta \log P(\tau|\theta) = \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t|s_t)
   $$

4. **æœŸæœ›æ¢¯åº¦**ï¼š
   $$
   \nabla_\theta J(\theta) = \mathbb{E}_\tau\left[R(\tau) \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t|s_t)\right]
   $$

---

## ğŸ“Š ç»å…¸ç®—æ³•

### 1. REINFORCE

**ç®—æ³• 1.1 (REINFORCE / Monte Carlo Policy Gradient)**:

```text
å¯¹äºæ¯ä¸ªepisode:
  1. é‡‡æ ·è½¨è¿¹ Ï„ ~ Ï€_Î¸
  2. è®¡ç®—ç´¯ç§¯å¥–åŠ± G_t
  3. æ›´æ–°: Î¸ â† Î¸ + Î± âˆ‡_Î¸ log Ï€_Î¸(a_t|s_t) G_t
```

**ç‰¹ç‚¹**ï¼š

- æ— åä¼°è®¡
- é«˜æ–¹å·®

**åŸºçº¿æŠ€å·§**ï¼šå‡å»åŸºçº¿ $b(s_t)$ é™ä½æ–¹å·®ï¼š

$$
\nabla_\theta J(\theta) \approx \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t|s_t) (G_t - b(s_t))
$$

å¸¸ç”¨åŸºçº¿ï¼š$b(s_t) = V(s_t)$

---

### 2. Actor-Critic

**æ ¸å¿ƒæ€æƒ³**ï¼šç»“åˆç­–ç•¥æ¢¯åº¦å’Œä»·å€¼å‡½æ•°ã€‚

- **Actor**ï¼šç­–ç•¥ $\pi_\theta(a|s)$
- **Critic**ï¼šä»·å€¼å‡½æ•° $V_w(s)$ æˆ– $Q_w(s, a)$

**æ›´æ–°è§„åˆ™**ï¼š

```text
Critic: w â† w + Î±_w Î´_t âˆ‡_w V_w(s_t)
Actor:  Î¸ â† Î¸ + Î±_Î¸ Î´_t âˆ‡_Î¸ log Ï€_Î¸(a_t|s_t)
```

å…¶ä¸­ TDè¯¯å·®ï¼š$\delta_t = r_t + \gamma V_w(s_{t+1}) - V_w(s_t)$

---

### 3. ä¼˜åŠ¿å‡½æ•°

**å®šä¹‰ 3.1 (ä¼˜åŠ¿å‡½æ•°)**:

$$
A^\pi(s, a) = Q^\pi(s, a) - V^\pi(s)
$$

**æ„ä¹‰**ï¼šåŠ¨ä½œ $a$ ç›¸å¯¹äºå¹³å‡çš„ä¼˜åŠ¿ã€‚

**ä¼˜åŠ¿Actor-Critic (A2C)**ï¼š

$$
\nabla_\theta J(\theta) = \mathbb{E}\left[\nabla_\theta \log \pi_\theta(a|s) \cdot A^\pi(s, a)\right]
$$

**å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ (GAE)**ï¼š

$$
\hat{A}_t^{\text{GAE}} = \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l}
$$

---

## ğŸ”§ ç°ä»£å˜ä½“

### 1. PPO (Proximal Policy Optimization)

**æ ¸å¿ƒæ€æƒ³**ï¼šé™åˆ¶ç­–ç•¥æ›´æ–°å¹…åº¦ã€‚

**ç›®æ ‡å‡½æ•°**ï¼š

$$
L^{\text{CLIP}}(\theta) = \mathbb{E}\left[\min\left(r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t\right)\right]
$$

å…¶ä¸­ï¼š

- $r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}$ æ˜¯é‡è¦æ€§é‡‡æ ·æ¯”ç‡
- $\epsilon$ æ˜¯è£å‰ªå‚æ•°ï¼ˆé€šå¸¸0.2ï¼‰

**ä¼˜åŠ¿**ï¼š

- ç®€å•å®ç°
- ç¨³å®šè®­ç»ƒ
- æ ·æœ¬æ•ˆç‡é«˜

---

### 2. TRPO (Trust Region Policy Optimization)

**æ ¸å¿ƒæ€æƒ³**ï¼šçº¦æŸKLæ•£åº¦ã€‚

**ä¼˜åŒ–é—®é¢˜**ï¼š

$$
\max_\theta \mathbb{E}\left[r_t(\theta) \hat{A}_t\right] \quad \text{s.t.} \quad \mathbb{E}[D_{KL}(\pi_{\theta_{\text{old}}} \| \pi_\theta)] \leq \delta
$$

**å®ç°**ï¼šä½¿ç”¨å…±è½­æ¢¯åº¦æ³•æ±‚è§£ã€‚

**ä¼˜åŠ¿**ï¼š

- ç†è®ºä¿è¯
- å•è°ƒæ”¹è¿›

---

## ğŸ’» Pythonå®ç°

```python
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim

class PolicyNetwork(nn.Module):
    """ç­–ç•¥ç½‘ç»œ"""
    def __init__(self, state_dim, action_dim, hidden_dim=64):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim),
            nn.Softmax(dim=-1)
        )
    
    def forward(self, state):
        return self.net(state)


class REINFORCE:
    """REINFORCEç®—æ³•"""
    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99):
        self.policy = PolicyNetwork(state_dim, action_dim)
        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)
        self.gamma = gamma
    
    def select_action(self, state):
        """é€‰æ‹©åŠ¨ä½œ"""
        state = torch.FloatTensor(state)
        probs = self.policy(state)
        action = torch.multinomial(probs, 1).item()
        return action, probs[action]
    
    def update(self, states, actions, rewards):
        """æ›´æ–°ç­–ç•¥"""
        # è®¡ç®—ç´¯ç§¯å¥–åŠ±
        returns = []
        G = 0
        for r in reversed(rewards):
            G = r + self.gamma * G
            returns.insert(0, G)
        
        returns = torch.FloatTensor(returns)
        returns = (returns - returns.mean()) / (returns.std() + 1e-8)  # æ ‡å‡†åŒ–
        
        # è®¡ç®—æŸå¤±
        loss = 0
        for state, action, G in zip(states, actions, returns):
            state = torch.FloatTensor(state)
            probs = self.policy(state)
            log_prob = torch.log(probs[action])
            loss -= log_prob * G
        
        # æ›´æ–°
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        return loss.item()


class ActorCritic:
    """Actor-Criticç®—æ³•"""
    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99):
        self.actor = PolicyNetwork(state_dim, action_dim)
        self.critic = nn.Sequential(
            nn.Linear(state_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )
        
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr)
        self.gamma = gamma
    
    def select_action(self, state):
        """é€‰æ‹©åŠ¨ä½œ"""
        state = torch.FloatTensor(state)
        probs = self.actor(state)
        action = torch.multinomial(probs, 1).item()
        return action
    
    def update(self, state, action, reward, next_state, done):
        """æ›´æ–°Actorå’ŒCritic"""
        state = torch.FloatTensor(state)
        next_state = torch.FloatTensor(next_state)
        
        # Criticæ›´æ–°
        value = self.critic(state)
        next_value = self.critic(next_state)
        
        td_target = reward + self.gamma * next_value * (1 - done)
        td_error = td_target - value
        
        critic_loss = td_error.pow(2)
        
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        self.critic_optimizer.step()
        
        # Actoræ›´æ–°
        probs = self.actor(state)
        log_prob = torch.log(probs[action])
        actor_loss = -log_prob * td_error.detach()
        
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()


# ç¤ºä¾‹ï¼šCartPoleç¯å¢ƒ
import gym

env = gym.make('CartPole-v1')
agent = REINFORCE(state_dim=4, action_dim=2)

for episode in range(1000):
    state = env.reset()
    states, actions, rewards = [], [], []
    
    for t in range(500):
        action, _ = agent.select_action(state)
        next_state, reward, done, _ = env.step(action)
        
        states.append(state)
        actions.append(action)
        rewards.append(reward)
        
        state = next_state
        
        if done:
            break
    
    loss = agent.update(states, actions, rewards)
    
    if episode % 100 == 0:
        print(f"Episode {episode}, Total Reward: {sum(rewards)}, Loss: {loss:.4f}")
```

---

## ğŸ“š æ ¸å¿ƒå®šç†æ€»ç»“

| å®šç†/ç®—æ³• | æ ¸å¿ƒæ€æƒ³ | ä¼˜ç¼ºç‚¹ |
|-----------|----------|--------|
| **ç­–ç•¥æ¢¯åº¦å®šç†** | $\nabla J = \mathbb{E}[\nabla \log \pi \cdot Q]$ | ç†è®ºåŸºç¡€ |
| **REINFORCE** | Monte Carloé‡‡æ · | æ— åä½†é«˜æ–¹å·® |
| **Actor-Critic** | ç»“åˆç­–ç•¥å’Œä»·å€¼ | é™ä½æ–¹å·® |
| **PPO** | è£å‰ªé‡è¦æ€§é‡‡æ · | ç®€å•ç¨³å®š |
| **TRPO** | KLæ•£åº¦çº¦æŸ | ç†è®ºä¿è¯ |

---

## ğŸ“ ç›¸å…³è¯¾ç¨‹

| å¤§å­¦ | è¯¾ç¨‹ |
|------|------|
| **UC Berkeley** | CS285 Deep Reinforcement Learning |
| **Stanford** | CS234 Reinforcement Learning |
| **DeepMind** | UCL Course on RL |

---

## ğŸ“– å‚è€ƒæ–‡çŒ®

1. **Sutton et al. (2000)**. "Policy Gradient Methods for Reinforcement Learning with Function Approximation". *NeurIPS*.

2. **Schulman et al. (2017)**. "Proximal Policy Optimization Algorithms". *arXiv*.

3. **Schulman et al. (2015)**. "Trust Region Policy Optimization". *ICML*.

---

*æœ€åæ›´æ–°ï¼š2025å¹´10æœˆ*-
