# Transformerçš„æ•°å­¦åŸç† (Mathematics of Transformers)

> **Attention Is All You Need**  
> å¤§è¯­è¨€æ¨¡å‹èƒŒåçš„æ•°å­¦åŸºç¡€

---

## ğŸ“‹ ç›®å½•

- [Transformerçš„æ•°å­¦åŸç† (Mathematics of Transformers)](#transformerçš„æ•°å­¦åŸç†-mathematics-of-transformers)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [ğŸ¯ Transformeræ¶æ„æ¦‚è§ˆ](#-transformeræ¶æ„æ¦‚è§ˆ)
  - [ğŸ“ æ ¸å¿ƒæœºåˆ¶æ•°å­¦åˆ†æ](#-æ ¸å¿ƒæœºåˆ¶æ•°å­¦åˆ†æ)
    - [1. Self-Attentionæœºåˆ¶](#1-self-attentionæœºåˆ¶)
    - [2. Multi-Head Attention](#2-multi-head-attention)
    - [3. Position Encoding](#3-position-encoding)
  - [ğŸ” ç†è®ºåˆ†æ](#-ç†è®ºåˆ†æ)
    - [1. Attentionçš„è¡¨è¾¾èƒ½åŠ›](#1-attentionçš„è¡¨è¾¾èƒ½åŠ›)
    - [2. Transformerçš„é€šç”¨é€¼è¿‘æ€§è´¨](#2-transformerçš„é€šç”¨é€¼è¿‘æ€§è´¨)
    - [3. è®¡ç®—å¤æ‚åº¦åˆ†æ](#3-è®¡ç®—å¤æ‚åº¦åˆ†æ)
  - [ğŸ§® ä¼˜åŒ–ä¸è®­ç»ƒåŠ¨åŠ›å­¦](#-ä¼˜åŒ–ä¸è®­ç»ƒåŠ¨åŠ›å­¦)
    - [1. Layer Normalizationçš„ä½œç”¨](#1-layer-normalizationçš„ä½œç”¨)
    - [2. æ¢¯åº¦æµåˆ†æ](#2-æ¢¯åº¦æµåˆ†æ)
    - [3. Warmupä¸å­¦ä¹ ç‡è°ƒåº¦](#3-warmupä¸å­¦ä¹ ç‡è°ƒåº¦)
  - [ğŸ’» PyTorchå®ç°](#-pytorchå®ç°)
    - [Self-Attention From Scratch](#self-attention-from-scratch)
    - [å®Œæ•´Transformerå—](#å®Œæ•´transformerå—)
  - [ğŸ”¬ å‰æ²¿ç ”ç©¶ (2025)](#-å‰æ²¿ç ”ç©¶-2025)
    - [1. Sparse Attentionå˜ä½“](#1-sparse-attentionå˜ä½“)
    - [2. çº¿æ€§Attentionè¿‘ä¼¼](#2-çº¿æ€§attentionè¿‘ä¼¼)
    - [3. State Space Models (Mamba)](#3-state-space-models-mamba)
  - [ğŸ¤– åœ¨LLMä¸­çš„åº”ç”¨](#-åœ¨llmä¸­çš„åº”ç”¨)
    - [1. GPTç³»åˆ—](#1-gptç³»åˆ—)
    - [2. In-Context Learningçš„æ•°å­¦è§£é‡Š](#2-in-context-learningçš„æ•°å­¦è§£é‡Š)
    - [3. Scaling Laws](#3-scaling-laws)
  - [ğŸ“š ç›¸å…³èµ„æº](#-ç›¸å…³èµ„æº)
    - [å¼€åˆ›æ€§è®ºæ–‡](#å¼€åˆ›æ€§è®ºæ–‡)
    - [ç†è®ºåˆ†æè®ºæ–‡](#ç†è®ºåˆ†æè®ºæ–‡)
    - [2025å¹´æœ€æ–°ç ”ç©¶](#2025å¹´æœ€æ–°ç ”ç©¶)
  - [ğŸ“ å¯¹æ ‡è¯¾ç¨‹](#-å¯¹æ ‡è¯¾ç¨‹)
  - [ğŸ’¡ ç»ƒä¹ é¢˜](#-ç»ƒä¹ é¢˜)
    - [åŸºç¡€é¢˜](#åŸºç¡€é¢˜)
    - [è¿›é˜¶é¢˜](#è¿›é˜¶é¢˜)
    - [æŒ‘æˆ˜é¢˜](#æŒ‘æˆ˜é¢˜)

---

## ğŸ¯ Transformeræ¶æ„æ¦‚è§ˆ

**Transformer** (Vaswani et al., 2017) æ˜¯ç°ä»£å¤§è¯­è¨€æ¨¡å‹çš„åŸºç¡€æ¶æ„ã€‚

**æ ¸å¿ƒç»„ä»¶**:

```text
Input Embedding + Positional Encoding
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Transformerå—  â”‚ Ã— Nå±‚
â”‚  â”œâ”€ Multi-Head   â”‚
â”‚  â”‚  Self-Attentionâ”‚
â”‚  â”œâ”€ Layer Norm   â”‚
â”‚  â”œâ”€ Feed-Forward â”‚
â”‚  â””â”€ Layer Norm   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
  Output Layer
```

**æ•°å­¦æµç¨‹**:

$$
\begin{align}
\text{Attention}(Q,K,V) &= \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V \\
\text{MultiHead}(Q,K,V) &= \text{Concat}(\text{head}_1, \ldots, \text{head}_h) W^O \\
\text{FFN}(x) &= \max(0, xW_1 + b_1)W_2 + b_2
\end{align}
$$

---

## ğŸ“ æ ¸å¿ƒæœºåˆ¶æ•°å­¦åˆ†æ

### 1. Self-Attentionæœºåˆ¶

**å®šä¹‰**:

ç»™å®šè¾“å…¥åºåˆ— $X = [x_1, \ldots, x_n] \in \mathbb{R}^{n \times d}$:

$$
\begin{align}
Q &= XW^Q, \quad K = XW^K, \quad V = XW^V \\
\text{Attention}(Q,K,V) &= \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V
\end{align}
$$

å…¶ä¸­:

- $W^Q, W^K \in \mathbb{R}^{d \times d_k}$, $W^V \in \mathbb{R}^{d \times d_v}$
- $\sqrt{d_k}$ æ˜¯**ç¼©æ”¾å› å­** (é˜²æ­¢softmaxé¥±å’Œ)

---

**é€æ­¥æ¨å¯¼**:

**æ­¥éª¤1**: è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°

$$
S = QK^T = (XW^Q)(XW^K)^T \in \mathbb{R}^{n \times n}
$$

çŸ©é˜µ $S$ çš„å…ƒç´ :

$$
S_{ij} = q_i^T k_j = \langle W^Q x_i, W^K x_j \rangle
$$

è¡¡é‡ token $i$ å’Œ token $j$ çš„ç›¸å…³æ€§ã€‚

---

**æ­¥éª¤2**: ç¼©æ”¾

$$
S' = \frac{S}{\sqrt{d_k}}
$$

**ä¸ºä»€ä¹ˆè¦ç¼©æ”¾?**

å‡è®¾ $q_i, k_j$ çš„åˆ†é‡ç‹¬ç«‹åŒåˆ†å¸ƒ,å‡å€¼0,æ–¹å·®1:

$$
\mathbb{E}[S_{ij}] = 0, \quad \text{Var}(S_{ij}) = d_k
$$

ç¼©æ”¾åæ–¹å·®å˜ä¸º1,é˜²æ­¢softmaxæ¢¯åº¦æ¶ˆå¤±ã€‚

---

**æ­¥éª¤3**: Softmaxå½’ä¸€åŒ–

$$
A_{ij} = \frac{\exp(S'_{ij})}{\sum_{k=1}^n \exp(S'_{ik})}
$$

æ»¡è¶³: $\sum_{j=1}^n A_{ij} = 1$ (æ¯è¡Œæ˜¯æ¦‚ç‡åˆ†å¸ƒ)

---

**æ­¥éª¤4**: åŠ æƒæ±‚å’Œ

$$
\text{Output}_i = \sum_{j=1}^n A_{ij} v_j
$$

æ¯ä¸ªè¾“å‡ºæ˜¯**å€¼å‘é‡çš„åŠ æƒå¹³å‡**,æƒé‡ç”±æ³¨æ„åŠ›åˆ†æ•°å†³å®šã€‚

---

**çŸ©é˜µå½¢å¼**:

$$
\text{Output} = A V = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V
$$

---

### 2. Multi-Head Attention

**åŠ¨æœº**: å•ä¸ªæ³¨æ„åŠ›å¤´åªèƒ½æ•æ‰ä¸€ç§ç›¸å…³æ€§æ¨¡å¼,å¤šå¤´å¯ä»¥å¹¶è¡Œå­¦ä¹ å¤šç§æ¨¡å¼ã€‚

**å®šä¹‰**:

$$
\begin{align}
\text{head}_i &= \text{Attention}(QW_i^Q, KW_i^K, VW_i^V) \\
\text{MultiHead}(Q,K,V) &= \text{Concat}(\text{head}_1, \ldots, \text{head}_h) W^O
\end{align}
$$

å…¶ä¸­:

- $W_i^Q, W_i^K \in \mathbb{R}^{d \times d_k}$, $W_i^V \in \mathbb{R}^{d \times d_v}$
- $W^O \in \mathbb{R}^{hd_v \times d}$ (è¾“å‡ºæŠ•å½±)
- é€šå¸¸: $d_k = d_v = d/h$

**å‚æ•°é‡**:

$$
\text{Params} = 4d^2 + 4d \quad \text{(å¿½ç•¥bias)}
$$

- $W^Q, W^K, W^V$: $3 \times hd_k \times d = 3d^2$
- $W^O$: $hd_v \times d = d^2$

---

### 3. Position Encoding

**é—®é¢˜**: Self-attentionæ˜¯**ç½®æ¢ä¸å˜**çš„,å³:

$$
\text{Attention}(\pi(X)) = \pi(\text{Attention}(X))
$$

éœ€è¦**ä½ç½®ç¼–ç **æ³¨å…¥åºåˆ—ä¿¡æ¯ã€‚

---

**æ­£å¼¦ä½ç½®ç¼–ç ** (åŸå§‹Transformer):

$$
\begin{align}
PE_{(pos, 2i)} &= \sin\left(\frac{pos}{10000^{2i/d}}\right) \\
PE_{(pos, 2i+1)} &= \cos\left(\frac{pos}{10000^{2i/d}}\right)
\end{align}
$$

**æ€§è´¨**:

1. æ¯ä¸ªç»´åº¦æ˜¯ä¸åŒé¢‘ç‡çš„æ­£å¼¦æ³¢
2. ç›¸å¯¹ä½ç½®ä¿¡æ¯: $PE_{pos+k}$ å¯ç”± $PE_{pos}$ çº¿æ€§è¡¨ç¤º

$$
PE_{pos+k} = A_k PE_{pos} + B_k
$$

---

**å¯å­¦ä¹ ä½ç½®ç¼–ç ** (GPT):

$$
PE_{pos} \in \mathbb{R}^d \quad \text{(å¯è®­ç»ƒå‚æ•°)}
$$

**ä¼˜ç‚¹**: æ›´çµæ´»  
**ç¼ºç‚¹**: æ— æ³•æ³›åŒ–åˆ°è®­ç»ƒæ—¶æœªè§è¿‡çš„é•¿åº¦

---

**æ—‹è½¬ä½ç½®ç¼–ç  (RoPE, 2021)**:

å°†æŸ¥è¯¢å’Œé”®æ—‹è½¬ä¸€ä¸ªè§’åº¦,è§’åº¦ä¾èµ–ä½ç½®:

$$
q_m' = e^{im\theta} q_m, \quad k_n' = e^{in\theta} k_n
$$

æ»¡è¶³:

$$
\langle q_m', k_n' \rangle = \text{Re}(e^{i(m-n)\theta} \langle q_m, k_n \rangle)
$$

**ä»…ä¾èµ–ç›¸å¯¹ä½ç½®** $m - n$!

---

## ğŸ” ç†è®ºåˆ†æ

### 1. Attentionçš„è¡¨è¾¾èƒ½åŠ›

**å®šç†** (Attention as Dictionary Lookup):

Attentionå±‚å¯ä»¥ç²¾ç¡®å®ç°**è½¯å­—å…¸æŸ¥æ‰¾**:

ç»™å®šé”®å€¼å¯¹ $(k_1, v_1), \ldots, (k_n, v_n)$, æŸ¥è¯¢ $q$:

$$
\text{Attention}(q, K, V) \approx v_{i^*} \quad \text{å…¶ä¸­} \quad i^* = \arg\max_i \langle q, k_i \rangle
$$

å½“æ¸©åº¦ $T \to 0$ (å³ $\frac{1}{\sqrt{d_k}} \to \infty$):

$$
\text{softmax}\left(\frac{qK^T}{T}\right) \to \text{one-hot}(i^*)
$$

---

**å®šç†** (Contextual Representation):

Attentionå±‚è®¡ç®—çš„æ˜¯**ä¸Šä¸‹æ–‡åŒ–è¡¨ç¤º**:

$$
h_i = \sum_{j=1}^n w_{ij} v_j, \quad w_{ij} \propto \exp(\text{similarity}(x_i, x_j))
$$

æ¯ä¸ªtokençš„è¡¨ç¤ºæ˜¯**æ‰€æœ‰ç›¸å…³tokençš„ä¿¡æ¯èšåˆ**ã€‚

---

### 2. Transformerçš„é€šç”¨é€¼è¿‘æ€§è´¨

**å®šç†** (Yun et al., 2020):

Transformerå¯ä»¥é€¼è¿‘**ä»»æ„åºåˆ—åˆ°åºåˆ—æ˜ å°„** (åœ¨é€‚å½“å‡è®¾ä¸‹)ã€‚

**è¯æ˜æ€è·¯**:

1. Attentionå±‚å¯ä»¥å®ç°**ä»»æ„ç¨€ç–è¿æ¥**
2. FFNå±‚æ˜¯é€šç”¨å‡½æ•°é€¼è¿‘å™¨ (ReLUç½‘ç»œ)
3. ç»„åˆèµ·æ¥å¯ä»¥é€¼è¿‘ä»»æ„è®¡ç®—å›¾

---

**å®šç†** (Turing Completeness):

å…·æœ‰è¶³å¤Ÿæ·±åº¦å’Œå®½åº¦çš„Transformeræ˜¯**å›¾çµå®Œå¤‡**çš„ã€‚

**æ„é€ **:

- ç”¨Attentionæ¨¡æ‹ŸæŒ‡é’ˆæ“ä½œ
- ç”¨FFNå®ç°ç®—æœ¯å’Œé€»è¾‘è¿ç®—
- å¯ä»¥æ¨¡æ‹Ÿé€šç”¨å›¾çµæœº

---

### 3. è®¡ç®—å¤æ‚åº¦åˆ†æ

**æ—¶é—´å¤æ‚åº¦**:

| æ“ä½œ | å¤æ‚åº¦ |
|------|--------|
| Self-Attention | $O(n^2 d)$ |
| FFN | $O(nd^2)$ |
| **æ€»è®¡** | $O(n^2 d + nd^2)$ |

**ç“¶é¢ˆ**: åºåˆ—é•¿åº¦ $n$ è¾ƒå¤§æ—¶, $O(n^2)$ æˆä¸ºä¸»è¦ç“¶é¢ˆã€‚

---

**ç©ºé—´å¤æ‚åº¦**:

- å­˜å‚¨æ³¨æ„åŠ›çŸ©é˜µ: $O(n^2)$
- æ¿€æ´»å€¼: $O(nLd)$ (Læ˜¯å±‚æ•°)

**KV Cache** (æ¨ç†ä¼˜åŒ–):

ç¼“å­˜è¿‡å»çš„é”®å€¼å¯¹,é¿å…é‡å¤è®¡ç®—:

$$
\text{Cache size} = O(Lnd)
$$

---

## ğŸ§® ä¼˜åŒ–ä¸è®­ç»ƒåŠ¨åŠ›å­¦

### 1. Layer Normalizationçš„ä½œç”¨

**å®šä¹‰**:

$$
\text{LayerNorm}(x) = \gamma \odot \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta
$$

å…¶ä¸­:

- $\mu = \frac{1}{d}\sum_{i=1}^d x_i$ (å‡å€¼)
- $\sigma^2 = \frac{1}{d}\sum_{i=1}^d (x_i - \mu)^2$ (æ–¹å·®)

---

**Pre-LN vs Post-LN**:

```text
Post-LN (åŸå§‹):                Pre-LN (ç°ä»£):
x â†’ Attention â†’ LN â†’ FFN â†’ LN   x â†’ LN â†’ Attention â†’ LN â†’ FFN
```

**Pre-LNçš„ä¼˜åŠ¿**:

- æ›´ç¨³å®šçš„æ¢¯åº¦æµ
- å¯ä»¥ä¸ç”¨Warmup
- æ›´æ·±çš„ç½‘ç»œè®­ç»ƒæ›´ç¨³å®š

---

### 2. æ¢¯åº¦æµåˆ†æ

**æ®‹å·®è¿æ¥çš„é‡è¦æ€§**:

$$
x_{l+1} = x_l + F_l(x_l)
$$

**åå‘ä¼ æ’­**:

$$
\frac{\partial L}{\partial x_l} = \frac{\partial L}{\partial x_{l+1}} \left(I + \frac{\partial F_l}{\partial x_l}\right)
$$

**æ¢¯åº¦å§‹ç»ˆåŒ…å«æ’ç­‰é¡¹** $I$, é¿å…æ¢¯åº¦æ¶ˆå¤±!

---

**å®šç†** (Gradient Flow in Transformers):

åœ¨Pre-LNæ¶æ„ä¸­,æ¢¯åº¦å¯ä»¥**æ— é˜»ç¢åœ°**ä»è¾“å‡ºæµå‘è¾“å…¥:

$$
\frac{\partial L}{\partial x_0} = \frac{\partial L}{\partial x_L} + \sum_{l=0}^{L-1} \text{correction terms}
$$

---

### 3. Warmupä¸å­¦ä¹ ç‡è°ƒåº¦

**Transformerå­¦ä¹ ç‡è°ƒåº¦** (åŸå§‹è®ºæ–‡):

$$
\text{lr}(t) = d^{-0.5} \cdot \min(t^{-0.5}, t \cdot \text{warmup}^{-1.5})
$$

**ä¸¤ä¸ªé˜¶æ®µ**:

1. **Warmup** ($t < \text{warmup}$): çº¿æ€§å¢é•¿
2. **Decay**: æŒ‰ $t^{-0.5}$ è¡°å‡

---

**ä¸ºä»€ä¹ˆéœ€è¦Warmup?**

**å‡è¯´1**: é˜²æ­¢Adamçš„äºŒé˜¶çŸ©ä¼°è®¡ä¸å‡†ç¡®  
**å‡è¯´2**: åˆæœŸæ¢¯åº¦æ–¹å·®å¤§,å°å­¦ä¹ ç‡æ›´ç¨³å®š  
**å‡è¯´3**: å¸®åŠ©æ‰¾åˆ°æ›´å¹³å¦çš„æœ€å°å€¼

---

## ğŸ’» PyTorchå®ç°

### Self-Attention From Scratch

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class SelfAttention(nn.Module):
    """ä»é›¶å®ç°Self-Attention"""
    
    def __init__(self, d_model: int, d_k: int, d_v: int):
        super().__init__()
        self.d_k = d_k
        
        # Query, Key, ValueæŠ•å½±
        self.W_q = nn.Linear(d_model, d_k, bias=False)
        self.W_k = nn.Linear(d_model, d_k, bias=False)
        self.W_v = nn.Linear(d_model, d_v, bias=False)
        
    def forward(self, X, mask=None):
        """
        Args:
            X: (batch, seq_len, d_model)
            mask: (batch, seq_len, seq_len) æˆ– None
        
        Returns:
            output: (batch, seq_len, d_v)
            attention_weights: (batch, seq_len, seq_len)
        """
        # æ­¥éª¤1: è®¡ç®—Q, K, V
        Q = self.W_q(X)  # (batch, seq_len, d_k)
        K = self.W_k(X)  # (batch, seq_len, d_k)
        V = self.W_v(X)  # (batch, seq_len, d_v)
        
        # æ­¥éª¤2: è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        scores = torch.matmul(Q, K.transpose(-2, -1))  # (batch, seq_len, seq_len)
        scores = scores / math.sqrt(self.d_k)  # ç¼©æ”¾
        
        # æ­¥éª¤3: åº”ç”¨mask (å¯é€‰)
        if mask is not None:
            scores = scores.masked_fill(mask == 0, float('-inf'))
        
        # æ­¥éª¤4: Softmax
        attention_weights = F.softmax(scores, dim=-1)  # (batch, seq_len, seq_len)
        
        # æ­¥éª¤5: åŠ æƒæ±‚å’Œ
        output = torch.matmul(attention_weights, V)  # (batch, seq_len, d_v)
        
        return output, attention_weights


class MultiHeadAttention(nn.Module):
    """å¤šå¤´æ³¨æ„åŠ›"""
    
    def __init__(self, d_model: int, num_heads: int):
        super().__init__()
        assert d_model % num_heads == 0, "d_model must be divisible by num_heads"
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        # æ‰€æœ‰å¤´çš„Q,K,VæŠ•å½± (åˆå¹¶æˆä¸€ä¸ªå¤§çŸ©é˜µ)
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        
        # è¾“å‡ºæŠ•å½±
        self.W_o = nn.Linear(d_model, d_model)
        
    def forward(self, X, mask=None):
        batch_size, seq_len, d_model = X.shape
        
        # è®¡ç®—Q,K,Vå¹¶åˆ†å‰²æˆå¤šä¸ªå¤´
        Q = self.W_q(X).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        K = self.W_k(X).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        V = self.W_v(X).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        # Shape: (batch, num_heads, seq_len, d_k)
        
        # è®¡ç®—æ³¨æ„åŠ›
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        
        if mask is not None:
            mask = mask.unsqueeze(1)  # (batch, 1, seq_len, seq_len)
            scores = scores.masked_fill(mask == 0, float('-inf'))
        
        attention_weights = F.softmax(scores, dim=-1)
        attention_output = torch.matmul(attention_weights, V)
        # Shape: (batch, num_heads, seq_len, d_k)
        
        # æ‹¼æ¥æ‰€æœ‰å¤´
        attention_output = attention_output.transpose(1, 2).contiguous()
        attention_output = attention_output.view(batch_size, seq_len, d_model)
        
        # è¾“å‡ºæŠ•å½±
        output = self.W_o(attention_output)
        
        return output, attention_weights


# æµ‹è¯•
if __name__ == "__main__":
    batch_size, seq_len, d_model = 2, 10, 512
    num_heads = 8
    
    X = torch.randn(batch_size, seq_len, d_model)
    
    mha = MultiHeadAttention(d_model, num_heads)
    output, attn_weights = mha(X)
    
    print(f"Input shape: {X.shape}")
    print(f"Output shape: {output.shape}")
    print(f"Attention weights shape: {attn_weights.shape}")
```

---

### å®Œæ•´Transformerå—

```python
class TransformerBlock(nn.Module):
    """å®Œæ•´çš„Transformerå— (Pre-LNæ¶æ„)"""
    
    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):
        super().__init__()
        
        # Multi-Head Attention
        self.attention = MultiHeadAttention(d_model, num_heads)
        self.norm1 = nn.LayerNorm(d_model)
        self.dropout1 = nn.Dropout(dropout)
        
        # Feed-Forward Network
        self.ffn = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model)
        )
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout2 = nn.Dropout(dropout)
        
    def forward(self, x, mask=None):
        # Pre-LNæ¶æ„
        
        # Multi-Head Attentionå­å±‚
        x_norm = self.norm1(x)
        attn_output, _ = self.attention(x_norm, mask)
        x = x + self.dropout1(attn_output)  # æ®‹å·®è¿æ¥
        
        # FFNå­å±‚
        x_norm = self.norm2(x)
        ffn_output = self.ffn(x_norm)
        x = x + self.dropout2(ffn_output)  # æ®‹å·®è¿æ¥
        
        return x


class PositionalEncoding(nn.Module):
    """æ­£å¼¦ä½ç½®ç¼–ç """
    
    def __init__(self, d_model: int, max_len: int = 5000):
        super().__init__()
        
        # è®¡ç®—ä½ç½®ç¼–ç 
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1).float()
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                             -(math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        pe = pe.unsqueeze(0)  # (1, max_len, d_model)
        self.register_buffer('pe', pe)
        
    def forward(self, x):
        """
        Args:
            x: (batch, seq_len, d_model)
        """
        seq_len = x.size(1)
        return x + self.pe[:, :seq_len, :]
```

---

## ğŸ”¬ å‰æ²¿ç ”ç©¶ (2025)

### 1. Sparse Attentionå˜ä½“

**åŠ¨æœº**: é™ä½ $O(n^2)$ å¤æ‚åº¦

**Longformer** (Beltagy et al., 2020):

- **å±€éƒ¨çª—å£æ³¨æ„åŠ›**: æ¯ä¸ªtokenåªå…³æ³¨å‘¨å›´ $w$ ä¸ªtoken
- **å…¨å±€æ³¨æ„åŠ›**: å°‘æ•°ç‰¹æ®Štokenå…³æ³¨æ‰€æœ‰ä½ç½®
- å¤æ‚åº¦: $O(nw)$

---

**BigBird** (Zaheer et al., 2020):

ç»“åˆä¸‰ç§æ³¨æ„åŠ›æ¨¡å¼:

1. éšæœºæ³¨æ„åŠ›
2. çª—å£æ³¨æ„åŠ›
3. å…¨å±€æ³¨æ„åŠ›

**å®šç†**: è¿™ä¸‰ç§æ¨¡å¼çš„ç»„åˆä¿æŒå›¾çµå®Œå¤‡æ€§ã€‚

---

### 2. çº¿æ€§Attentionè¿‘ä¼¼

**Linformer** (Wang et al., 2020):

å°† $K, V$ æŠ•å½±åˆ°ä½ç»´:

$$
K' = EK, \quad V' = FV
$$

å…¶ä¸­ $E, F \in \mathbb{R}^{k \times n}$, $k \ll n$

å¤æ‚åº¦: $O(nk)$

---

**Performer** (Choromanski et al., 2021):

ä½¿ç”¨éšæœºç‰¹å¾è¿‘ä¼¼softmax:

$$
\text{softmax}(qk^T) \approx \phi(q) \phi(k)^T
$$

å…¶ä¸­ $\phi$ æ˜¯éšæœºç‰¹å¾æ˜ å°„ã€‚

å¤æ‚åº¦: $O(n)$ (çº¿æ€§!)

---

### 3. State Space Models (Mamba)

**Mamba** (Gu & Dao, 2023):

ç”¨çŠ¶æ€ç©ºé—´æ¨¡å‹æ›¿ä»£Attention:

$$
\begin{align}
h_t &= A h_{t-1} + B x_t \\
y_t &= C h_t + D x_t
\end{align}
$$

**ä¼˜åŠ¿**:

- $O(n)$ å¤æ‚åº¦
- æ›´é•¿çš„ä¸Šä¸‹æ–‡
- æ›´å¿«çš„æ¨ç†

**æŒ‘æˆ˜**: å¯è§£é‡Šæ€§ä¸å¦‚Attention

---

## ğŸ¤– åœ¨LLMä¸­çš„åº”ç”¨

### 1. GPTç³»åˆ—

**GPT-1** (2018): 12å±‚, 117Må‚æ•°  
**GPT-2** (2019): 48å±‚, 1.5Bå‚æ•°  
**GPT-3** (2020): 96å±‚, 175Bå‚æ•°  
**GPT-4** (2023): ~1.76Tå‚æ•° (æ··åˆä¸“å®¶)

**æ¶æ„æ¼”åŒ–**:

- æ›´æ·±çš„ç½‘ç»œ (96å±‚ â†’ 120å±‚)
- æ›´å¤§çš„æ¨¡å‹ (175B â†’ 1.76T)
- æ··åˆä¸“å®¶ (MoE)
- æ›´é•¿çš„ä¸Šä¸‹æ–‡ (2k â†’ 32k â†’ 128k)

---

### 2. In-Context Learningçš„æ•°å­¦è§£é‡Š

**ç°è±¡**: LLMå¯ä»¥ä»few-shotç¤ºä¾‹ä¸­å­¦ä¹ ,æ— éœ€æ¢¯åº¦æ›´æ–°ã€‚

**ç†è®ºè§£é‡Š1** (Xie et al., 2022):

Transformeråœ¨éšå¼åœ°å®ç°**æ¢¯åº¦ä¸‹é™**:

$$
W_{t+1} = W_t - \eta \nabla_W L(x_t, y_t; W_t)
$$

æ¯ä¸ªAttentionå±‚æ›´æ–°"éšå¼æƒé‡"ã€‚

---

**ç†è®ºè§£é‡Š2** (Von Oswald et al., 2023):

Transformerå¯ä»¥æ¨¡æ‹Ÿ**å²­å›å½’**ç­‰ç®—æ³•:

ç»™å®šç¤ºä¾‹ $(x_1, y_1), \ldots, (x_k, y_k)$, æŸ¥è¯¢ $x_{test}$:

$$
\hat{y}_{test} = (X^TX + \lambda I)^{-1} X^T y \cdot x_{test}
$$

å¯ä»¥ç”¨å¤šå±‚Attentionç²¾ç¡®å®ç°!

---

### 3. Scaling Laws

**Kaplan et al. (2020)** å‘ç°:

$$
L(N) = \left(\frac{N_c}{N}\right)^\alpha
$$

å…¶ä¸­:

- $L$: æµ‹è¯•æŸå¤±
- $N$: æ¨¡å‹å‚æ•°é‡
- $N_c, \alpha$: å¸¸æ•° ($\alpha \approx 0.076$)

**Chinchilla Scaling** (Hoffmann et al., 2022):

æœ€ä¼˜è®­ç»ƒåº”è¯¥å¹³è¡¡å‚æ•°é‡å’Œæ•°æ®é‡:

$$
N_{\text{optimal}} \propto D_{\text{optimal}}
$$

---

## ğŸ“š ç›¸å…³èµ„æº

### å¼€åˆ›æ€§è®ºæ–‡

1. **Vaswani et al. (2017)**  
   "Attention Is All You Need"  
   *NeurIPS 2017*  
   â†’ Transformerçš„å¼€åˆ›æ€§è®ºæ–‡

2. **Devlin et al. (2019)**  
   "BERT: Pre-training of Deep Bidirectional Transformers"  
   â†’ åŒå‘Transformer

3. **Radford et al. (2019)**  
   "Language Models are Unsupervised Multitask Learners" (GPT-2)  
   â†’ è‡ªå›å½’è¯­è¨€æ¨¡å‹

---

### ç†è®ºåˆ†æè®ºæ–‡

1. **Yun et al. (2020)**  
   "Are Transformers Universal Approximators of Sequence-to-Sequence Functions?"  
   â†’ é€šç”¨é€¼è¿‘æ€§è´¨

2. **PÃ©rez et al. (2021)**  
   "Attention is Turing Complete"  
   â†’ å›¾çµå®Œå¤‡æ€§

3. **Xie et al. (2022)**  
   "An Explanation of In-context Learning as Implicit Bayesian Inference"  
   â†’ In-context learningç†è®º

---

### 2025å¹´æœ€æ–°ç ”ç©¶

1. **Gu & Dao (2023)**  
   "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"  
   â†’ çº¿æ€§å¤æ‚åº¦æ›¿ä»£

2. **Chen et al. (2024)**  
   "Training-Free Long-Context Scaling of Large Language Models"  
   â†’ é•¿ä¸Šä¸‹æ–‡æ‰©å±•

3. **Liu et al. (2025)**  
   "Mathematical Foundations of Transformer Scaling Laws" (arXiv)  
   â†’ Scaling lawsçš„ç†è®ºåŸºç¡€

---

## ğŸ“ å¯¹æ ‡è¯¾ç¨‹

| å¤§å­¦ | è¯¾ç¨‹ | ç›¸å…³å†…å®¹ |
|------|------|---------|
| Stanford | CS224N | Transformeræ¶æ„ (Week 5-6) |
| Stanford | CS324 | LLMç†è®º (å…¨è¯¾ç¨‹) |
| MIT | 6.S898 | æ·±åº¦å­¦ä¹  (Attentionæœºåˆ¶) |
| CMU | 11-747 | Neural NLP (Transformer) |

---

## ğŸ’¡ ç»ƒä¹ é¢˜

### åŸºç¡€é¢˜

**1. æ³¨æ„åŠ›çŸ©é˜µåˆ†æ**:

ç»™å®šåºåˆ— "I love AI", ç”»å‡ºå¯èƒ½çš„æ³¨æ„åŠ›çŸ©é˜µçƒ­å›¾,å¹¶è§£é‡Š:

- "love" åº”è¯¥å…³æ³¨å“ªäº›è¯?
- ä¸ºä»€ä¹ˆéœ€è¦å¤šå¤´æ³¨æ„åŠ›?

---

**2. è®¡ç®—é‡åˆ†æ**:

è®¡ç®—ä¸€ä¸ªTransformerå—çš„FLOPs:

- è¾“å…¥: $(n, d) = (512, 768)$
- 8ä¸ªæ³¨æ„åŠ›å¤´
- FFNéšè—å±‚: $4d$

---

### è¿›é˜¶é¢˜

**3. å®ç°å› æœMask**:

å®ç°GPTé£æ ¼çš„å› æœattention mask:

```python
def create_causal_mask(seq_len: int) -> torch.Tensor:
    """
    åˆ›å»ºå› æœmask,ä½¿å¾—ä½ç½®iåªèƒ½çœ‹åˆ° iä¹‹å‰çš„ä½ç½®
    
    è¿”å›: (seq_len, seq_len) çš„å¸ƒå°”çŸ©é˜µ
    """
    # TODO: å®ç°
    pass
```

---

**4. ä½ç½®ç¼–ç å¯è§†åŒ–**:

ç»˜åˆ¶æ­£å¼¦ä½ç½®ç¼–ç çš„å‰8ä¸ªç»´åº¦,åˆ†æå…¶æ€§è´¨:

- ä¸åŒç»´åº¦çš„é¢‘ç‡å¦‚ä½•å˜åŒ–?
- å¦‚ä½•ç¼–ç ç›¸å¯¹ä½ç½®ä¿¡æ¯?

---

### æŒ‘æˆ˜é¢˜

**5. è¯æ˜Attentionçš„é€šç”¨æ€§**:

è¯æ˜: å•å±‚Attention + FFNå¯ä»¥å®ç°ä»»æ„**ç¨€ç–è¿æ¥**çš„å‡½æ•°ã€‚

æç¤º: æ„é€ æ€§è¯æ˜,å±•ç¤ºå¦‚ä½•è®¾ç½®æƒé‡çŸ©é˜µã€‚

---

**6. Scaling Lawæ¨å¯¼**:

æ¨å¯¼ä¸ºä»€ä¹ˆæµ‹è¯•æŸå¤± $L(N) \propto N^{-\alpha}$ã€‚

è€ƒè™‘:

- å‚æ•°é‡ $N$ ä¸æœ‰æ•ˆå‡è®¾ç±»å¤§å°çš„å…³ç³»
- ç»Ÿè®¡å­¦ä¹ ç†è®ºçš„æ³›åŒ–ç•Œ

---

**ğŸ“Œ ä¸‹ä¸€ä¸»é¢˜**: [In-Context Learningç†è®º](./02-In-Context-Learning-Theory.md)

**ğŸ”™ è¿”å›**: [LLMç†è®º](../README.md) | [å‰æ²¿ç ”ç©¶](../../README.md)
