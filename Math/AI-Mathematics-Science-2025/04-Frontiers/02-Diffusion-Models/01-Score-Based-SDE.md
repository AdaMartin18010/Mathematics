# æ‰©æ•£æ¨¡å‹ä¸Score-Based SDE

> **Diffusion Models and Score-Based Stochastic Differential Equations**
>
> 2025å¹´ç”Ÿæˆæ¨¡å‹çš„å‰æ²¿ï¼šä»DDPMåˆ°è¿ç»­æ—¶é—´æ‰©æ•£

---

## ç›®å½•

- [æ‰©æ•£æ¨¡å‹ä¸Score-Based SDE](#æ‰©æ•£æ¨¡å‹ä¸score-based-sde)
  - [ç›®å½•](#ç›®å½•)
  - [ğŸ“‹ æ ¸å¿ƒæ€æƒ³](#-æ ¸å¿ƒæ€æƒ³)
  - [ğŸ¯ æ‰©æ•£è¿‡ç¨‹](#-æ‰©æ•£è¿‡ç¨‹)
    - [1. å‰å‘æ‰©æ•£](#1-å‰å‘æ‰©æ•£)
    - [2. åå‘æ‰©æ•£](#2-åå‘æ‰©æ•£)
  - [ğŸ“Š Score-Basedæ¨¡å‹](#-score-basedæ¨¡å‹)
    - [1. Scoreå‡½æ•°](#1-scoreå‡½æ•°)
    - [2. Score Matching](#2-score-matching)
  - [ğŸ”¬ æ•°å­¦ç†è®º](#-æ•°å­¦ç†è®º)
    - [1. éšæœºå¾®åˆ†æ–¹ç¨‹ (SDE)](#1-éšæœºå¾®åˆ†æ–¹ç¨‹-sde)
    - [2. æ¦‚ç‡æµODE](#2-æ¦‚ç‡æµode)
  - [ğŸ’» Pythonå®ç°](#-pythonå®ç°)
  - [ğŸ“š æ ¸å¿ƒæ¨¡å‹](#-æ ¸å¿ƒæ¨¡å‹)
  - [ğŸ“ ç›¸å…³è¯¾ç¨‹](#-ç›¸å…³è¯¾ç¨‹)
  - [ğŸ“– å‚è€ƒæ–‡çŒ®](#-å‚è€ƒæ–‡çŒ®)

---

## ğŸ“‹ æ ¸å¿ƒæ€æƒ³

**æ‰©æ•£æ¨¡å‹**é€šè¿‡é€æ­¥æ·»åŠ å™ªå£°ç„¶åå­¦ä¹ å»å™ªæ¥ç”Ÿæˆæ•°æ®ã€‚

**æ ¸å¿ƒæµç¨‹**ï¼š

```text
æ•°æ® xâ‚€ â†’ åŠ å™ª â†’ ... â†’ çº¯å™ªå£° x_T
         â†“ å­¦ä¹ åå‘è¿‡ç¨‹
ç”Ÿæˆ xâ‚€ â† å»å™ª â† ... â† é‡‡æ · x_T
```

**ä¼˜åŠ¿**ï¼š

- é«˜è´¨é‡ç”Ÿæˆ
- ç¨³å®šè®­ç»ƒ
- ç†è®ºä¿è¯

---

## ğŸ¯ æ‰©æ•£è¿‡ç¨‹

### 1. å‰å‘æ‰©æ•£

**ç¦»æ•£æ—¶é—´ (DDPM)**:

$$
q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{1 - \beta_t} x_{t-1}, \beta_t I)
$$

**å°é—­å½¢å¼**ï¼š

$$
q(x_t | x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t} x_0, (1 - \bar{\alpha}_t) I)
$$

å…¶ä¸­ $\bar{\alpha}_t = \prod_{s=1}^{t} (1 - \beta_s)$ã€‚

---

**è¿ç»­æ—¶é—´ (SDE)**:

$$
dx = f(x, t) dt + g(t) dw
$$

**æ–¹å·®ä¿æŒ (VP) SDE**ï¼š

$$
dx = -\frac{1}{2} \beta(t) x dt + \sqrt{\beta(t)} dw
$$

---

### 2. åå‘æ‰©æ•£

**å®šç† 2.1 (åå‘SDE, Anderson 1982)**:

å‰å‘SDEçš„åå‘è¿‡ç¨‹ä¸ºï¼š

$$
dx = [f(x, t) - g(t)^2 \nabla_x \log p_t(x)] dt + g(t) d\bar{w}
$$

å…¶ä¸­ $\nabla_x \log p_t(x)$ æ˜¯**scoreå‡½æ•°**ã€‚

---

**è®­ç»ƒç›®æ ‡**ï¼šå­¦ä¹ scoreå‡½æ•° $s_\theta(x, t) \approx \nabla_x \log p_t(x)$ã€‚

**é‡‡æ ·**ï¼šä» $x_T \sim \mathcal{N}(0, I)$ å¼€å§‹ï¼Œæ²¿åå‘SDEç§¯åˆ†ã€‚

---

## ğŸ“Š Score-Basedæ¨¡å‹

### 1. Scoreå‡½æ•°

**å®šä¹‰ 1.1 (Scoreå‡½æ•°)**:

$$
s(x) = \nabla_x \log p(x)
$$

**ç›´è§‰**ï¼šæŒ‡å‘æ•°æ®å¯†åº¦å¢åŠ æœ€å¿«çš„æ–¹å‘ã€‚

---

### 2. Score Matching

**ç›®æ ‡**ï¼šæœ€å°åŒ–Fisheræ•£åº¦ã€‚

$$
\mathbb{E}_{p(x)}[\|s_\theta(x) - \nabla_x \log p(x)\|^2]
$$

**å»å™ªScore Matching (DSM)**ï¼š

$$
\mathbb{E}_{p(x_0)} \mathbb{E}_{q(x_t|x_0)}[\|s_\theta(x_t, t) - \nabla_{x_t} \log q(x_t|x_0)\|^2]
$$

**ç®€åŒ–å½¢å¼ (DDPMç›®æ ‡)**ï¼š

$$
\mathbb{E}_{t, x_0, \epsilon}[\|\epsilon_\theta(x_t, t) - \epsilon\|^2]
$$

å…¶ä¸­ $x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon$ï¼Œ$\epsilon \sim \mathcal{N}(0, I)$ã€‚

---

## ğŸ”¬ æ•°å­¦ç†è®º

### 1. éšæœºå¾®åˆ†æ–¹ç¨‹ (SDE)

**å‰å‘VP-SDE**ï¼š

$$
dx = -\frac{1}{2} \beta(t) x dt + \sqrt{\beta(t)} dw, \quad t \in [0, T]
$$

**è¾¹é™…åˆ†å¸ƒ**ï¼š

$$
p_t(x | x_0) = \mathcal{N}(x; \sqrt{\bar{\alpha}_t} x_0, (1 - \bar{\alpha}_t) I)
$$

å…¶ä¸­ $\bar{\alpha}_t = \exp\left(-\frac{1}{2} \int_0^t \beta(s) ds\right)$ã€‚

---

**åå‘SDE**ï¼š

$$
dx = \left[-\frac{1}{2} \beta(t) x - \beta(t) \nabla_x \log p_t(x)\right] dt + \sqrt{\beta(t)} d\bar{w}
$$

---

### 2. æ¦‚ç‡æµODE

**å®šç† 2.1 (æ¦‚ç‡æµODE, Song et al. 2021)**:

ä¸SDEå…·æœ‰ç›¸åŒè¾¹é™…åˆ†å¸ƒçš„ODEï¼š

$$
\frac{dx}{dt} = f(x, t) - \frac{1}{2} g(t)^2 \nabla_x \log p_t(x)
$$

**VP-SDEçš„æ¦‚ç‡æµODE**ï¼š

$$
\frac{dx}{dt} = -\frac{1}{2} \beta(t) [x + \nabla_x \log p_t(x)]
$$

**ä¼˜åŠ¿**ï¼š

- ç¡®å®šæ€§é‡‡æ ·
- ç²¾ç¡®ä¼¼ç„¶è®¡ç®—
- å¯é€†æ€§

---

## ğŸ’» Pythonå®ç°

```python
import torch
import torch.nn as nn
import numpy as np

class DiffusionModel(nn.Module):
    """ç®€åŒ–çš„æ‰©æ•£æ¨¡å‹"""
    def __init__(self, data_dim=2, hidden_dim=128, T=1000):
        super().__init__()
        self.T = T
        
        # å™ªå£°è°ƒåº¦
        self.beta = torch.linspace(1e-4, 0.02, T)
        self.alpha = 1 - self.beta
        self.alpha_bar = torch.cumprod(self.alpha, dim=0)
        
        # Scoreç½‘ç»œ (ç®€åŒ–ä¸ºMLP)
        self.net = nn.Sequential(
            nn.Linear(data_dim + 1, hidden_dim),  # +1 for time
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, data_dim)
        )
    
    def forward(self, x, t):
        """é¢„æµ‹å™ªå£°"""
        # æ—¶é—´åµŒå…¥ (ç®€åŒ–)
        t_emb = t.float() / self.T
        x_t = torch.cat([x, t_emb.unsqueeze(-1)], dim=-1)
        return self.net(x_t)
    
    def q_sample(self, x0, t, noise=None):
        """å‰å‘æ‰©æ•£é‡‡æ · q(x_t | x_0)"""
        if noise is None:
            noise = torch.randn_like(x0)
        
        alpha_bar_t = self.alpha_bar[t].view(-1, 1)
        return torch.sqrt(alpha_bar_t) * x0 + torch.sqrt(1 - alpha_bar_t) * noise
    
    def p_sample(self, xt, t):
        """åå‘æ‰©æ•£é‡‡æ · p(x_{t-1} | x_t)"""
        # é¢„æµ‹å™ªå£°
        eps_pred = self.forward(xt, t)
        
        # è®¡ç®—å‡å€¼
        alpha_t = self.alpha[t].view(-1, 1)
        alpha_bar_t = self.alpha_bar[t].view(-1, 1)
        
        mean = (xt - (1 - alpha_t) / torch.sqrt(1 - alpha_bar_t) * eps_pred) / torch.sqrt(alpha_t)
        
        # æ·»åŠ å™ªå£°
        if t[0] > 0:
            noise = torch.randn_like(xt)
            sigma = torch.sqrt(self.beta[t].view(-1, 1))
            return mean + sigma * noise
        else:
            return mean
    
    @torch.no_grad()
    def sample(self, batch_size, data_dim):
        """ä»å™ªå£°ç”Ÿæˆæ ·æœ¬"""
        # ä»çº¯å™ªå£°å¼€å§‹
        x = torch.randn(batch_size, data_dim)
        
        # åå‘æ‰©æ•£
        for t in reversed(range(self.T)):
            t_batch = torch.full((batch_size,), t, dtype=torch.long)
            x = self.p_sample(x, t_batch)
        
        return x


def train_diffusion(model, data_loader, epochs=100, lr=1e-3):
    """è®­ç»ƒæ‰©æ•£æ¨¡å‹"""
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    
    for epoch in range(epochs):
        total_loss = 0
        
        for x0 in data_loader:
            # éšæœºæ—¶é—´æ­¥
            t = torch.randint(0, model.T, (x0.shape[0],))
            
            # é‡‡æ ·å™ªå£°
            noise = torch.randn_like(x0)
            
            # å‰å‘æ‰©æ•£
            xt = model.q_sample(x0, t, noise)
            
            # é¢„æµ‹å™ªå£°
            noise_pred = model.forward(xt, t)
            
            # æŸå¤± (ç®€å•MSE)
            loss = nn.MSELoss()(noise_pred, noise)
            
            # æ›´æ–°
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
        
        if (epoch + 1) % 10 == 0:
            print(f"Epoch {epoch+1}, Loss: {total_loss/len(data_loader):.6f}")


# ç¤ºä¾‹ï¼š2Dé«˜æ–¯æ··åˆ
def generate_2d_data(n_samples=1000):
    """ç”Ÿæˆ2Dé«˜æ–¯æ··åˆæ•°æ®"""
    centers = torch.tensor([[2, 2], [-2, -2], [2, -2], [-2, 2]])
    n_per_center = n_samples // 4
    
    data = []
    for center in centers:
        samples = torch.randn(n_per_center, 2) * 0.5 + center
        data.append(samples)
    
    return torch.cat(data, dim=0)


# è®­ç»ƒ
data = generate_2d_data(1000)
data_loader = torch.utils.data.DataLoader(data, batch_size=64, shuffle=True)

model = DiffusionModel(data_dim=2, T=100)
train_diffusion(model, data_loader, epochs=100)

# ç”Ÿæˆ
samples = model.sample(batch_size=100, data_dim=2)
print(f"Generated samples shape: {samples.shape}")
```

---

## ğŸ“š æ ¸å¿ƒæ¨¡å‹

| æ¨¡å‹ | æ ¸å¿ƒæ€æƒ³ | ç‰¹ç‚¹ |
|------|----------|------|
| **DDPM** | ç¦»æ•£æ—¶é—´æ‰©æ•£ | ç®€å•ç¨³å®š |
| **Score-Based** | è¿ç»­æ—¶é—´SDE | ç†è®ºä¼˜é›… |
| **DDIM** | ç¡®å®šæ€§é‡‡æ · | å¿«é€Ÿç”Ÿæˆ |
| **Latent Diffusion** | æ½œç©ºé—´æ‰©æ•£ | é«˜æ•ˆè®­ç»ƒ |

---

## ğŸ“ ç›¸å…³è¯¾ç¨‹

| å¤§å­¦ | è¯¾ç¨‹ |
|------|------|
| **Stanford** | CS236 Deep Generative Models |
| **MIT** | 6.S191 Introduction to Deep Learning |
| **UC Berkeley** | CS294 Deep Unsupervised Learning |

---

## ğŸ“– å‚è€ƒæ–‡çŒ®

1. **Ho et al. (2020)**. "Denoising Diffusion Probabilistic Models". *NeurIPS*.

2. **Song et al. (2021)**. "Score-Based Generative Modeling through Stochastic Differential Equations". *ICLR*.

3. **Rombach et al. (2022)**. "High-Resolution Image Synthesis with Latent Diffusion Models". *CVPR*.

---

*æœ€åæ›´æ–°ï¼š2025å¹´10æœˆ*-
