# å› æœæ¨æ–­ç†è®º (Causal Inference Theory)

> **From Correlation to Causation: The Mathematics of Causal Reasoning**
>
> ä»ç›¸å…³åˆ°å› æœï¼šå› æœæ¨ç†çš„æ•°å­¦åŸºç¡€

---

## ç›®å½•

- [å› æœæ¨æ–­ç†è®º (Causal Inference Theory)](#å› æœæ¨æ–­ç†è®º-causal-inference-theory)
  - [ç›®å½•](#ç›®å½•)
  - [ğŸ“‹ æ¦‚è¿°](#-æ¦‚è¿°)
  - [ğŸ¯ æ ¸å¿ƒæ¦‚å¿µ](#-æ ¸å¿ƒæ¦‚å¿µ)
    - [1. å› æœå…³ç³» vs ç›¸å…³å…³ç³»](#1-å› æœå…³ç³»-vs-ç›¸å…³å…³ç³»)
    - [2. åäº‹å®æ¨ç†](#2-åäº‹å®æ¨ç†)
    - [3. å› æœæ•ˆåº”](#3-å› æœæ•ˆåº”)
  - [ğŸ“ å› æœå›¾æ¨¡å‹](#-å› æœå›¾æ¨¡å‹)
    - [1. ç»“æ„å› æœæ¨¡å‹ (SCM)](#1-ç»“æ„å› æœæ¨¡å‹-scm)
    - [2. æœ‰å‘æ— ç¯å›¾ (DAG)](#2-æœ‰å‘æ— ç¯å›¾-dag)
    - [3. d-åˆ†ç¦»](#3-d-åˆ†ç¦»)
  - [ğŸ”¬ å› æœæ¨æ–­æ¡†æ¶](#-å› æœæ¨æ–­æ¡†æ¶)
    - [1. Rubinå› æœæ¨¡å‹ (æ½œåœ¨ç»“æœæ¡†æ¶)](#1-rubinå› æœæ¨¡å‹-æ½œåœ¨ç»“æœæ¡†æ¶)
    - [2. Pearlå› æœæ¨¡å‹ (ç»“æ„æ–¹ç¨‹æ¡†æ¶)](#2-pearlå› æœæ¨¡å‹-ç»“æ„æ–¹ç¨‹æ¡†æ¶)
    - [3. ä¸¤ç§æ¡†æ¶çš„ç»Ÿä¸€](#3-ä¸¤ç§æ¡†æ¶çš„ç»Ÿä¸€)
  - [ğŸ“Š å› æœè¯†åˆ«](#-å› æœè¯†åˆ«)
    - [1. åé—¨å‡†åˆ™ (Backdoor Criterion)](#1-åé—¨å‡†åˆ™-backdoor-criterion)
    - [2. å‰é—¨å‡†åˆ™ (Frontdoor Criterion)](#2-å‰é—¨å‡†åˆ™-frontdoor-criterion)
    - [3. do-æ¼”ç®— (do-Calculus)](#3-do-æ¼”ç®—-do-calculus)
  - [ğŸ’¡ å› æœæ•ˆåº”ä¼°è®¡](#-å› æœæ•ˆåº”ä¼°è®¡)
    - [1. éšæœºå¯¹ç…§è¯•éªŒ (RCT)](#1-éšæœºå¯¹ç…§è¯•éªŒ-rct)
    - [2. å€¾å‘å¾—åˆ†åŒ¹é… (PSM)](#2-å€¾å‘å¾—åˆ†åŒ¹é…-psm)
    - [3. å·¥å…·å˜é‡ (IV)](#3-å·¥å…·å˜é‡-iv)
    - [4. åŒé‡å·®åˆ† (DID)](#4-åŒé‡å·®åˆ†-did)
    - [5. å›å½’ä¸è¿ç»­ (RD)](#5-å›å½’ä¸è¿ç»­-rd)
  - [ğŸ§  æœºå™¨å­¦ä¹ ä¸­çš„å› æœæ¨æ–­](#-æœºå™¨å­¦ä¹ ä¸­çš„å› æœæ¨æ–­)
    - [1. å› æœè¡¨ç¤ºå­¦ä¹ ](#1-å› æœè¡¨ç¤ºå­¦ä¹ )
    - [2. åäº‹å®æ¨ç†ä¸è§£é‡Šæ€§](#2-åäº‹å®æ¨ç†ä¸è§£é‡Šæ€§)
    - [3. å› æœå¼ºåŒ–å­¦ä¹ ](#3-å› æœå¼ºåŒ–å­¦ä¹ )
    - [4. è¿ç§»å­¦ä¹ ä¸åŸŸé€‚åº”](#4-è¿ç§»å­¦ä¹ ä¸åŸŸé€‚åº”)
  - [ğŸ’» Pythonå®ç°](#-pythonå®ç°)
    - [ç¤ºä¾‹1: å› æœå›¾ä¸d-åˆ†ç¦»](#ç¤ºä¾‹1-å› æœå›¾ä¸d-åˆ†ç¦»)
    - [ç¤ºä¾‹2: å€¾å‘å¾—åˆ†åŒ¹é…](#ç¤ºä¾‹2-å€¾å‘å¾—åˆ†åŒ¹é…)
    - [ç¤ºä¾‹3: å·¥å…·å˜é‡ä¼°è®¡](#ç¤ºä¾‹3-å·¥å…·å˜é‡ä¼°è®¡)
    - [ç¤ºä¾‹4: å› æœå‘ç°](#ç¤ºä¾‹4-å› æœå‘ç°)
  - [ğŸ“ å¯¹æ ‡è¯¾ç¨‹](#-å¯¹æ ‡è¯¾ç¨‹)
  - [ğŸ“– æ ¸å¿ƒæ•™æä¸è®ºæ–‡](#-æ ¸å¿ƒæ•™æä¸è®ºæ–‡)
    - [æ•™æ](#æ•™æ)
    - [ç»å…¸è®ºæ–‡](#ç»å…¸è®ºæ–‡)
    - [æœ€æ–°è¿›å±• (2024-2025)](#æœ€æ–°è¿›å±•-2024-2025)
  - [ğŸ”— ç›¸å…³ä¸»é¢˜](#-ç›¸å…³ä¸»é¢˜)
  - [ğŸ“ æ€»ç»“](#-æ€»ç»“)
    - [æ ¸å¿ƒæ¦‚å¿µ](#æ ¸å¿ƒæ¦‚å¿µ)
    - [ç†è®ºæ¡†æ¶](#ç†è®ºæ¡†æ¶)
    - [è¯†åˆ«æ–¹æ³•](#è¯†åˆ«æ–¹æ³•)
    - [ä¼°è®¡æ–¹æ³•](#ä¼°è®¡æ–¹æ³•)
    - [AIåº”ç”¨](#aiåº”ç”¨)

---

## ğŸ“‹ æ¦‚è¿°

**å› æœæ¨æ–­**ç ”ç©¶å¦‚ä½•ä»è§‚å¯Ÿæ•°æ®ä¸­æ¨æ–­å› æœå…³ç³»ï¼Œæ˜¯ç»Ÿè®¡å­¦ã€æœºå™¨å­¦ä¹ ã€ç»æµå­¦ã€æµè¡Œç—…å­¦ç­‰é¢†åŸŸçš„æ ¸å¿ƒé—®é¢˜ã€‚

**æ ¸å¿ƒé—®é¢˜**:

1. **å› æœè¯†åˆ«**: ç»™å®šè§‚å¯Ÿæ•°æ®ï¼Œèƒ½å¦è¯†åˆ«å› æœæ•ˆåº”ï¼Ÿ
2. **å› æœä¼°è®¡**: å¦‚ä½•ä»æ•°æ®ä¸­ä¼°è®¡å› æœæ•ˆåº”ï¼Ÿ
3. **å› æœå‘ç°**: å¦‚ä½•ä»æ•°æ®ä¸­å‘ç°å› æœç»“æ„ï¼Ÿ

**ä¸ºä»€ä¹ˆé‡è¦**:

- **é¢„æµ‹ vs å¹²é¢„**: ç›¸å…³æ€§é¢„æµ‹æœªæ¥ï¼Œå› æœæ€§æŒ‡å¯¼å¹²é¢„
- **å¯è§£é‡Šæ€§**: ç†è§£"ä¸ºä»€ä¹ˆ"è€Œéä»…ä»…"æ˜¯ä»€ä¹ˆ"
- **æ³›åŒ–èƒ½åŠ›**: å› æœæ¨¡å‹åœ¨åˆ†å¸ƒå˜åŒ–ä¸‹æ›´é²æ£’
- **å†³ç­–æ”¯æŒ**: è¯„ä¼°æ”¿ç­–ã€æ²»ç–—ã€å¹²é¢„çš„æ•ˆæœ

---

## ğŸ¯ æ ¸å¿ƒæ¦‚å¿µ

### 1. å› æœå…³ç³» vs ç›¸å…³å…³ç³»

**ç›¸å…³å…³ç³»** (Correlation):

$$
\text{Corr}(X, Y) = \frac{\text{Cov}(X, Y)}{\sigma_X \sigma_Y}
$$

**å› æœå…³ç³»** (Causation):

$$
X \to Y: \text{æ”¹å˜ } X \text{ ä¼šå¯¼è‡´ } Y \text{ çš„æ”¹å˜}
$$

**Simpsonæ‚–è®º**: ç›¸å…³æ€§å¯èƒ½è¯¯å¯¼å› æœæ¨æ–­

**ç¤ºä¾‹**:

```text
æ€»ä½“: Corr(æ²»ç–—, åº·å¤) < 0  (è´Ÿç›¸å…³)
ç”·æ€§: Corr(æ²»ç–—, åº·å¤) > 0  (æ­£ç›¸å…³)
å¥³æ€§: Corr(æ²»ç–—, åº·å¤) > 0  (æ­£ç›¸å…³)
```

åŸå› : æ€§åˆ«æ˜¯æ··æ·†å› å­ (confounder)

### 2. åäº‹å®æ¨ç†

**åäº‹å®** (Counterfactual): "å¦‚æœå½“æ—¶...ä¼šæ€æ ·?"

**å®šä¹‰**: ä¸ªä½“ $i$ åœ¨æ¥å—æ²»ç–— $T=1$ æ—¶çš„ç»“æœ $Y_i(1)$ å’Œæœªæ¥å—æ²»ç–— $T=0$ æ—¶çš„ç»“æœ $Y_i(0)$

**æ ¹æœ¬é—®é¢˜**: æˆ‘ä»¬åªèƒ½è§‚å¯Ÿåˆ° $Y_i(T_i)$ï¼Œæ— æ³•åŒæ—¶è§‚å¯Ÿ $Y_i(1)$ å’Œ $Y_i(0)$

### 3. å› æœæ•ˆåº”

**å¹³å‡å› æœæ•ˆåº”** (ATE):

$$
\text{ATE} = \mathbb{E}[Y(1) - Y(0)]
$$

**æ¡ä»¶å¹³å‡å› æœæ•ˆåº”** (CATE):

$$
\text{CATE}(x) = \mathbb{E}[Y(1) - Y(0) | X = x]
$$

**å¤„ç†ç»„å¹³å‡å› æœæ•ˆåº”** (ATT):

$$
\text{ATT} = \mathbb{E}[Y(1) - Y(0) | T = 1]
$$

---

## ğŸ“ å› æœå›¾æ¨¡å‹

### 1. ç»“æ„å› æœæ¨¡å‹ (SCM)

**å®šä¹‰**: SCMæ˜¯ä¸€ä¸ªä¸‰å…ƒç»„ $(U, V, F)$

- $U$: å¤–ç”Ÿå˜é‡ (unobserved)
- $V$: å†…ç”Ÿå˜é‡ (observed)
- $F$: ç»“æ„æ–¹ç¨‹ $V_i = f_i(\text{PA}_i, U_i)$

**ç¤ºä¾‹**:

$$
\begin{align}
X &= U_X \\
Y &= \beta X + U_Y
\end{align}
$$

### 2. æœ‰å‘æ— ç¯å›¾ (DAG)

**å®šä¹‰**: DAG $G = (V, E)$ è¡¨ç¤ºå˜é‡é—´çš„å› æœå…³ç³»

- èŠ‚ç‚¹ $V$: å˜é‡
- æœ‰å‘è¾¹ $E$: å› æœå…³ç³» $X \to Y$

**ç¤ºä¾‹**:

```text
    Z
   â†™ â†˜
  X â†’ Y
```

- $X$: æ²»ç–—
- $Y$: ç»“æœ
- $Z$: æ··æ·†å› å­

### 3. d-åˆ†ç¦»

**å®šä¹‰**: ç»™å®šé›†åˆ $Z$ï¼Œè·¯å¾„ $p$ è¢« $Z$ é˜»æ–­ (blocked) å¦‚æœ:

1. **é“¾** (Chain): $X \to Z \to Y$ï¼Œ$Z \in \mathbf{Z}$
2. **å‰** (Fork): $X \leftarrow Z \to Y$ï¼Œ$Z \in \mathbf{Z}$
3. **å¯¹æ’** (Collider): $X \to Z \leftarrow Y$ï¼Œ$Z \notin \mathbf{Z}$ ä¸” $Z$ çš„åä»£ $\notin \mathbf{Z}$

**å®šç†** (d-åˆ†ç¦»å‡†åˆ™):

$$
X \perp_G Y | Z \iff X \perp Y | Z \text{ (åœ¨ } G \text{ å¯¹åº”çš„åˆ†å¸ƒä¸­)}
$$

---

## ğŸ”¬ å› æœæ¨æ–­æ¡†æ¶

### 1. Rubinå› æœæ¨¡å‹ (æ½œåœ¨ç»“æœæ¡†æ¶)

**æ ¸å¿ƒæ€æƒ³**: æ¯ä¸ªä¸ªä½“éƒ½æœ‰æ½œåœ¨ç»“æœ $Y_i(0), Y_i(1)$

**è§‚å¯Ÿæ•°æ®**:

$$
Y_i = T_i Y_i(1) + (1 - T_i) Y_i(0)
$$

**è¯†åˆ«å‡è®¾**:

1. **SUTVA** (Stable Unit Treatment Value Assumption)
   - æ— å¹²æ‰°: ä¸ªä½“ $i$ çš„ç»“æœä¸å—å…¶ä»–ä¸ªä½“æ²»ç–—çš„å½±å“
   - æ²»ç–—ä¸€è‡´æ€§: æ²»ç–—åªæœ‰ä¸€ä¸ªç‰ˆæœ¬

2. **å¯å¿½ç•¥æ€§** (Ignorability):

    $$
    (Y(0), Y(1)) \perp T | X
    $$

    å³ç»™å®šåå˜é‡ $X$ï¼Œæ²»ç–—åˆ†é…ä¸æ½œåœ¨ç»“æœç‹¬ç«‹ã€‚

3. **æ­£æ€§** (Positivity):

    $$
    0 < P(T = 1 | X = x) < 1, \quad \forall x
    $$

### 2. Pearlå› æœæ¨¡å‹ (ç»“æ„æ–¹ç¨‹æ¡†æ¶)

**æ ¸å¿ƒæ€æƒ³**: å› æœå…³ç³»ç”±ç»“æ„æ–¹ç¨‹è¡¨ç¤º

**do-ç®—å­**: $P(Y | do(X = x))$ è¡¨ç¤ºå¹²é¢„ $X$ ä¸º $x$ å $Y$ çš„åˆ†å¸ƒ

**do-æ¼”ç®—è§„åˆ™**:

1. **æ’å…¥/åˆ é™¤è§‚å¯Ÿ**:

    $$
    P(y | do(x), z, w) = P(y | do(x), w) \quad \text{if } (Y \perp Z | X, W)_{\overline{G_X}}
    $$

2. **åŠ¨ä½œ/è§‚å¯Ÿäº¤æ¢**:

    $$
    P(y | do(x), do(z), w) = P(y | do(x), z, w) \quad \text{if } (Y \perp Z | X, W)_{\overline{G_{XZ}}}
    $$

3. **æ’å…¥/åˆ é™¤åŠ¨ä½œ**:

    $$
    P(y | do(x), do(z), w) = P(y | do(x), w) \quad \text{if } (Y \perp Z | X, W)_{\overline{G_X}, \underline{G_{Z(W)}}}
    $$

### 3. ä¸¤ç§æ¡†æ¶çš„ç»Ÿä¸€

**å®šç†**: åœ¨å¯å¿½ç•¥æ€§å‡è®¾ä¸‹ï¼Œ

$$
\mathbb{E}[Y(1) - Y(0)] = \mathbb{E}_X[\mathbb{E}[Y | T=1, X] - \mathbb{E}[Y | T=0, X]]
$$

è¿™è¿æ¥äº†Rubinçš„æ½œåœ¨ç»“æœå’ŒPearlçš„æ¡ä»¶æœŸæœ›ã€‚

---

## ğŸ“Š å› æœè¯†åˆ«

### 1. åé—¨å‡†åˆ™ (Backdoor Criterion)

**å®šä¹‰**: é›†åˆ $Z$ æ»¡è¶³åé—¨å‡†åˆ™å¦‚æœ:

1. $Z$ é˜»æ–­æ‰€æœ‰ä» $X$ åˆ° $Y$ çš„åé—¨è·¯å¾„ (åŒ…å«æŒ‡å‘ $X$ çš„ç®­å¤´çš„è·¯å¾„)
2. $Z$ ä¸åŒ…å« $X$ çš„åä»£

**å®šç†**: å¦‚æœ $Z$ æ»¡è¶³åé—¨å‡†åˆ™ï¼Œåˆ™

$$
P(y | do(x)) = \sum_z P(y | x, z) P(z)
$$

**ç¤ºä¾‹**:

```text
    Z
   â†™ â†˜
  X â†’ Y
```

$Z$ æ»¡è¶³åé—¨å‡†åˆ™ï¼Œå› æ­¤:

$$
P(y | do(x)) = \sum_z P(y | x, z) P(z)
$$

### 2. å‰é—¨å‡†åˆ™ (Frontdoor Criterion)

**å®šä¹‰**: é›†åˆ $M$ æ»¡è¶³å‰é—¨å‡†åˆ™å¦‚æœ:

1. $M$ é˜»æ–­æ‰€æœ‰ä» $X$ åˆ° $Y$ çš„æœ‰å‘è·¯å¾„
2. $X$ é˜»æ–­æ‰€æœ‰ä» $M$ åˆ° $Y$ çš„åé—¨è·¯å¾„
3. æ²¡æœ‰ä» $X$ åˆ° $M$ çš„åé—¨è·¯å¾„

**å®šç†**: å¦‚æœ $M$ æ»¡è¶³å‰é—¨å‡†åˆ™ï¼Œåˆ™

$$
P(y | do(x)) = \sum_m P(m | x) \sum_{x'} P(y | m, x') P(x')
$$

**ç¤ºä¾‹**:

```text
  U
 â†™ â†˜
X â†’ M â†’ Y
```

$U$ æ˜¯æœªè§‚å¯Ÿçš„æ··æ·†å› å­ï¼Œä½† $M$ æ»¡è¶³å‰é—¨å‡†åˆ™ã€‚

### 3. do-æ¼”ç®— (do-Calculus)

**ç›®æ ‡**: å°† $P(y | do(x))$ åŒ–ç®€ä¸ºè§‚å¯Ÿåˆ†å¸ƒçš„å‡½æ•°

**ä¸‰æ¡è§„åˆ™** (è§ä¸Šæ–‡)

**å®Œå¤‡æ€§å®šç†** (Shpitser & Pearl, 2006):

å¦‚æœ $P(y | do(x))$ å¯è¯†åˆ«ï¼Œåˆ™å¯é€šè¿‡do-æ¼”ç®—æ¨å¯¼ã€‚

---

## ğŸ’¡ å› æœæ•ˆåº”ä¼°è®¡

### 1. éšæœºå¯¹ç…§è¯•éªŒ (RCT)

**é»„é‡‘æ ‡å‡†**: éšæœºåˆ†é…æ²»ç–—

$$
T \perp (Y(0), Y(1))
$$

**ä¼°è®¡**:

$$
\widehat{\text{ATE}} = \frac{1}{n_1} \sum_{i: T_i=1} Y_i - \frac{1}{n_0} \sum_{i: T_i=0} Y_i
$$

**ä¼˜ç‚¹**: æ— åä¼°è®¡
**ç¼ºç‚¹**: æ˜‚è´µã€ä¸å¯è¡Œã€ä¼¦ç†é—®é¢˜

### 2. å€¾å‘å¾—åˆ†åŒ¹é… (PSM)

**å€¾å‘å¾—åˆ†** (Propensity Score):

$$
e(x) = P(T = 1 | X = x)
$$

**å®šç†** (Rosenbaum & Rubin, 1983):

å¦‚æœ $(Y(0), Y(1)) \perp T | X$ï¼Œåˆ™ $(Y(0), Y(1)) \perp T | e(X)$

**ä¼°è®¡æ­¥éª¤**:

1. ä¼°è®¡å€¾å‘å¾—åˆ† $\hat{e}(x)$ (å¦‚é€»è¾‘å›å½’)
2. åŒ¹é…: ä¸ºæ¯ä¸ªæ²»ç–—ç»„ä¸ªä½“æ‰¾åˆ°å€¾å‘å¾—åˆ†ç›¸è¿‘çš„å¯¹ç…§ç»„ä¸ªä½“
3. è®¡ç®—åŒ¹é…åçš„å¹³å‡å·®å¼‚

**ATTä¼°è®¡**:

$$
\widehat{\text{ATT}} = \frac{1}{n_1} \sum_{i: T_i=1} \left[ Y_i - \sum_{j: T_j=0} w_{ij} Y_j \right]
$$

å…¶ä¸­ $w_{ij}$ æ˜¯åŒ¹é…æƒé‡ã€‚

### 3. å·¥å…·å˜é‡ (IV)

**å®šä¹‰**: $Z$ æ˜¯ $X$ å¯¹ $Y$ çš„å·¥å…·å˜é‡å¦‚æœ:

1. **ç›¸å…³æ€§**: $Z$ ä¸ $X$ ç›¸å…³
2. **æ’ä»–æ€§**: $Z$ åªé€šè¿‡ $X$ å½±å“ $Y$
3. **ç‹¬ç«‹æ€§**: $Z$ ä¸æœªè§‚å¯Ÿæ··æ·†å› å­ç‹¬ç«‹

**ä¸¤é˜¶æ®µæœ€å°äºŒä¹˜ (2SLS)**:

**ç¬¬ä¸€é˜¶æ®µ**: å›å½’ $X$ å¯¹ $Z$

$$
X = \alpha_0 + \alpha_1 Z + \nu
$$

**ç¬¬äºŒé˜¶æ®µ**: å›å½’ $Y$ å¯¹ $\hat{X}$

$$
Y = \beta_0 + \beta_1 \hat{X} + \epsilon
$$

**å› æœæ•ˆåº”**: $\beta_1$

**Waldä¼°è®¡**:

$$
\beta = \frac{\mathbb{E}[Y | Z=1] - \mathbb{E}[Y | Z=0]}{\mathbb{E}[X | Z=1] - \mathbb{E}[X | Z=0]}
$$

### 4. åŒé‡å·®åˆ† (DID)

**åœºæ™¯**: é¢æ¿æ•°æ®ï¼Œæ²»ç–—ç»„å’Œå¯¹ç…§ç»„ï¼Œå‰åå¯¹æ¯”

**æ¨¡å‹**:

$$
Y_{it} = \alpha + \beta \cdot \text{Treat}_i + \gamma \cdot \text{Post}_t + \delta \cdot (\text{Treat}_i \times \text{Post}_t) + \epsilon_{it}
$$

**å› æœæ•ˆåº”**: $\delta$

**å¹³è¡Œè¶‹åŠ¿å‡è®¾**: åœ¨æ— å¹²é¢„æƒ…å†µä¸‹ï¼Œæ²»ç–—ç»„å’Œå¯¹ç…§ç»„çš„è¶‹åŠ¿ç›¸åŒ

### 5. å›å½’ä¸è¿ç»­ (RD)

**åœºæ™¯**: æ²»ç–—åˆ†é…åŸºäºæŸä¸ªè¿ç»­å˜é‡çš„é˜ˆå€¼

**æ¨¡å‹**:

$$
Y_i = \alpha + \tau D_i + \beta (X_i - c) + \epsilon_i
$$

å…¶ä¸­ $D_i = \mathbb{1}(X_i \geq c)$

**å› æœæ•ˆåº”**: $\tau$ (åœ¨é˜ˆå€¼ $c$ å¤„çš„å±€éƒ¨æ•ˆåº”)

---

## ğŸ§  æœºå™¨å­¦ä¹ ä¸­çš„å› æœæ¨æ–­

### 1. å› æœè¡¨ç¤ºå­¦ä¹ 

**ç›®æ ‡**: å­¦ä¹ å› æœä¸å˜çš„è¡¨ç¤º

**ç‹¬ç«‹å› æœæœºåˆ¶ (ICM)** åŸåˆ™:

$$
P(X_1, \ldots, X_n) = \prod_i P(X_i | \text{PA}_i)
$$

æ¯ä¸ªæœºåˆ¶ $P(X_i | \text{PA}_i)$ ç‹¬ç«‹å˜åŒ–ã€‚

**å› æœVAE**:

$$
\begin{align}
z &\sim P(z) \\
x &\sim P(x | z, u)
\end{align}
$$

å…¶ä¸­ $z$ æ˜¯å› æœå› å­ï¼Œ$u$ æ˜¯éå› æœå› å­ã€‚

### 2. åäº‹å®æ¨ç†ä¸è§£é‡Šæ€§

**åäº‹å®è§£é‡Š**: "å¦‚æœç‰¹å¾ $X_i$ ä¸åŒï¼Œé¢„æµ‹ä¼šå¦‚ä½•å˜åŒ–?"

**ç¤ºä¾‹**: LIME, SHAP

**æ•°å­¦å½¢å¼**:

$$
\text{Explanation}_i = f(x) - f(x_{-i}, x'_i)
$$

å…¶ä¸­ $x'_i$ æ˜¯ $x_i$ çš„åäº‹å®å€¼ã€‚

### 3. å› æœå¼ºåŒ–å­¦ä¹ 

**ç›®æ ‡**: å­¦ä¹ å› æœç­–ç•¥ï¼Œæ³›åŒ–åˆ°æ–°ç¯å¢ƒ

**å› æœMDP**:

$$
\begin{align}
s_{t+1} &= f_s(s_t, a_t, u_t) \\
r_t &= f_r(s_t, a_t, u_t)
\end{align}
$$

å…¶ä¸­ $u_t$ æ˜¯æœªè§‚å¯Ÿçš„æ··æ·†å› å­ã€‚

**å› æœQ-learning**:

$$
Q(s, a) = \mathbb{E}[R | do(s, a)]
$$

### 4. è¿ç§»å­¦ä¹ ä¸åŸŸé€‚åº”

**å› æœè§†è§’**: å› æœå…³ç³»åœ¨åŸŸé—´ä¸å˜

**åå˜é‡åç§»** (Covariate Shift):

$$
P_{\text{source}}(X) \neq P_{\text{target}}(X), \quad P_{\text{source}}(Y | X) = P_{\text{target}}(Y | X)
$$

**å› æœä¸å˜æ€§**:

$$
P(Y | do(X)) \text{ åœ¨åŸŸé—´ä¸å˜}
$$

---

## ğŸ’» Pythonå®ç°

### ç¤ºä¾‹1: å› æœå›¾ä¸d-åˆ†ç¦»

```python
import networkx as nx
import matplotlib.pyplot as plt

class CausalGraph:
    """å› æœå›¾ç±»"""
    
    def __init__(self):
        self.graph = nx.DiGraph()
    
    def add_edge(self, cause, effect):
        """æ·»åŠ å› æœè¾¹"""
        self.graph.add_edge(cause, effect)
    
    def visualize(self):
        """å¯è§†åŒ–å› æœå›¾"""
        pos = nx.spring_layout(self.graph)
        nx.draw(self.graph, pos, with_labels=True, 
                node_color='lightblue', node_size=1500,
                font_size=12, font_weight='bold',
                arrows=True, arrowsize=20)
        plt.title("Causal Graph")
        plt.show()
    
    def is_d_separated(self, X, Y, Z):
        """
        æ£€æŸ¥Xå’ŒYæ˜¯å¦è¢«Z d-åˆ†ç¦»
        
        ç®€åŒ–å®ç°ï¼šæ£€æŸ¥æ˜¯å¦å­˜åœ¨æœªè¢«é˜»æ–­çš„è·¯å¾„
        """
        # ç§»é™¤ZåŠå…¶åä»£
        G_minus_Z = self.graph.copy()
        descendants_Z = set()
        for z in Z:
            descendants_Z.update(nx.descendants(G_minus_Z, z))
            descendants_Z.add(z)
        
        G_minus_Z.remove_nodes_from(descendants_Z)
        
        # æ£€æŸ¥Xåˆ°Yæ˜¯å¦æœ‰è·¯å¾„
        try:
            path = nx.shortest_path(G_minus_Z.to_undirected(), X, Y)
            return False  # å­˜åœ¨è·¯å¾„ï¼Œæœªd-åˆ†ç¦»
        except nx.NetworkXNoPath:
            return True  # æ— è·¯å¾„ï¼Œd-åˆ†ç¦»

# ç¤ºä¾‹ï¼šæ··æ·†å› å­
G = CausalGraph()
G.add_edge('Z', 'X')  # Z -> X
G.add_edge('Z', 'Y')  # Z -> Y
G.add_edge('X', 'Y')  # X -> Y

print("å› æœå›¾ç»“æ„:")
print("Z -> X, Z -> Y, X -> Y")

# æ£€æŸ¥d-åˆ†ç¦»
print(f"\nXå’ŒYæ˜¯å¦è¢«ç©ºé›†d-åˆ†ç¦»? {G.is_d_separated('X', 'Y', [])}")
print(f"Xå’ŒYæ˜¯å¦è¢«{{Z}}d-åˆ†ç¦»? {G.is_d_separated('X', 'Y', ['Z'])}")

G.visualize()
```

### ç¤ºä¾‹2: å€¾å‘å¾—åˆ†åŒ¹é…

```python
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import NearestNeighbors

def propensity_score_matching(X, T, Y, n_neighbors=1):
    """
    å€¾å‘å¾—åˆ†åŒ¹é…
    
    Args:
        X: åå˜é‡ (n_samples, n_features)
        T: æ²»ç–—æŒ‡ç¤º (n_samples,)
        Y: ç»“æœ (n_samples,)
        n_neighbors: åŒ¹é…çš„é‚»å±…æ•°
    
    Returns:
        ATT: å¤„ç†ç»„å¹³å‡å› æœæ•ˆåº”
    """
    # æ­¥éª¤1: ä¼°è®¡å€¾å‘å¾—åˆ†
    ps_model = LogisticRegression()
    ps_model.fit(X, T)
    propensity_scores = ps_model.predict_proba(X)[:, 1]
    
    # æ­¥éª¤2: åŒ¹é…
    treated_idx = np.where(T == 1)[0]
    control_idx = np.where(T == 0)[0]
    
    # ä¸ºæ¯ä¸ªæ²»ç–—ç»„ä¸ªä½“æ‰¾åˆ°æœ€è¿‘çš„å¯¹ç…§ç»„ä¸ªä½“
    nn = NearestNeighbors(n_neighbors=n_neighbors)
    nn.fit(propensity_scores[control_idx].reshape(-1, 1))
    
    distances, indices = nn.kneighbors(
        propensity_scores[treated_idx].reshape(-1, 1)
    )
    
    # æ­¥éª¤3: è®¡ç®—ATT
    treated_outcomes = Y[treated_idx]
    matched_control_outcomes = np.mean(
        Y[control_idx[indices]], axis=1
    )
    
    ATT = np.mean(treated_outcomes - matched_control_outcomes)
    
    return ATT, propensity_scores

# æ¨¡æ‹Ÿæ•°æ®
np.random.seed(42)
n = 1000

# åå˜é‡
X = np.random.randn(n, 3)

# å€¾å‘å¾—åˆ†ï¼ˆçœŸå®ï¼‰
true_ps = 1 / (1 + np.exp(-(X[:, 0] + 0.5 * X[:, 1])))

# æ²»ç–—åˆ†é…
T = (np.random.rand(n) < true_ps).astype(int)

# æ½œåœ¨ç»“æœ
Y0 = X[:, 0] + X[:, 1] + np.random.randn(n) * 0.5
Y1 = Y0 + 2  # çœŸå®ATE = 2

# è§‚å¯Ÿç»“æœ
Y = T * Y1 + (1 - T) * Y0

# çœŸå®ATT
true_ATT = np.mean(Y1[T == 1] - Y0[T == 1])

# PSMä¼°è®¡
estimated_ATT, ps = propensity_score_matching(X, T, Y)

print(f"çœŸå®ATT: {true_ATT:.4f}")
print(f"ä¼°è®¡ATT (PSM): {estimated_ATT:.4f}")
print(f"ä¼°è®¡è¯¯å·®: {abs(estimated_ATT - true_ATT):.4f}")

# å¯è§†åŒ–å€¾å‘å¾—åˆ†åˆ†å¸ƒ
plt.figure(figsize=(10, 5))
plt.hist(ps[T == 0], bins=30, alpha=0.5, label='Control', density=True)
plt.hist(ps[T == 1], bins=30, alpha=0.5, label='Treated', density=True)
plt.xlabel('Propensity Score')
plt.ylabel('Density')
plt.title('Propensity Score Distribution')
plt.legend()
plt.show()
```

### ç¤ºä¾‹3: å·¥å…·å˜é‡ä¼°è®¡

```python
from sklearn.linear_model import LinearRegression

def instrumental_variable_2sls(Z, X, Y):
    """
    ä¸¤é˜¶æ®µæœ€å°äºŒä¹˜ (2SLS) å·¥å…·å˜é‡ä¼°è®¡
    
    Args:
        Z: å·¥å…·å˜é‡ (n_samples, n_instruments)
        X: å†…ç”Ÿå˜é‡ (n_samples, n_endogenous)
        Y: ç»“æœå˜é‡ (n_samples,)
    
    Returns:
        beta: å› æœæ•ˆåº”ä¼°è®¡
    """
    # ç¬¬ä¸€é˜¶æ®µ: X ~ Z
    first_stage = LinearRegression()
    first_stage.fit(Z, X)
    X_hat = first_stage.predict(Z)
    
    # ç¬¬äºŒé˜¶æ®µ: Y ~ X_hat
    second_stage = LinearRegression()
    second_stage.fit(X_hat.reshape(-1, 1), Y)
    beta = second_stage.coef_[0]
    
    return beta

# æ¨¡æ‹Ÿæ•°æ®
np.random.seed(42)
n = 1000

# å·¥å…·å˜é‡
Z = np.random.randn(n)

# æœªè§‚å¯Ÿæ··æ·†å› å­
U = np.random.randn(n)

# å†…ç”Ÿå˜é‡ (å—Zå’ŒUå½±å“)
X = 0.5 * Z + 0.3 * U + np.random.randn(n) * 0.1

# ç»“æœå˜é‡ (çœŸå®å› æœæ•ˆåº” = 2)
Y = 2 * X + 0.5 * U + np.random.randn(n) * 0.5

# OLSä¼°è®¡ (æœ‰å)
ols = LinearRegression()
ols.fit(X.reshape(-1, 1), Y)
beta_ols = ols.coef_[0]

# IVä¼°è®¡ (æ— å)
beta_iv = instrumental_variable_2sls(Z.reshape(-1, 1), X, Y)

print(f"çœŸå®å› æœæ•ˆåº”: 2.0000")
print(f"OLSä¼°è®¡ (æœ‰å): {beta_ols:.4f}")
print(f"IVä¼°è®¡ (æ— å): {beta_iv:.4f}")
```

### ç¤ºä¾‹4: å› æœå‘ç°

```python
from itertools import permutations

def pc_algorithm_simple(data, alpha=0.05):
    """
    ç®€åŒ–çš„PCç®—æ³•ç”¨äºå› æœå‘ç°
    
    Args:
        data: æ•°æ®çŸ©é˜µ (n_samples, n_variables)
        alpha: æ˜¾è‘—æ€§æ°´å¹³
    
    Returns:
        adjacency_matrix: é‚»æ¥çŸ©é˜µ
    """
    from scipy.stats import pearsonr
    
    n_vars = data.shape[1]
    
    # åˆå§‹åŒ–å®Œå…¨å›¾
    adj_matrix = np.ones((n_vars, n_vars)) - np.eye(n_vars)
    
    # æ­¥éª¤1: ç§»é™¤æ¡ä»¶ç‹¬ç«‹çš„è¾¹
    for i in range(n_vars):
        for j in range(i + 1, n_vars):
            # æµ‹è¯•è¾¹ç¼˜ç‹¬ç«‹æ€§
            corr, p_value = pearsonr(data[:, i], data[:, j])
            
            if p_value > alpha:
                adj_matrix[i, j] = 0
                adj_matrix[j, i] = 0
                continue
            
            # æµ‹è¯•æ¡ä»¶ç‹¬ç«‹æ€§ (ç®€åŒ–: åªæµ‹è¯•ä¸€é˜¶)
            for k in range(n_vars):
                if k != i and k != j:
                    # éƒ¨åˆ†ç›¸å…³
                    corr_ik, _ = pearsonr(data[:, i], data[:, k])
                    corr_jk, _ = pearsonr(data[:, j], data[:, k])
                    corr_ij, _ = pearsonr(data[:, i], data[:, j])
                    
                    partial_corr = (corr_ij - corr_ik * corr_jk) / \
                                   np.sqrt((1 - corr_ik**2) * (1 - corr_jk**2))
                    
                    # Fisher's z-transformation
                    n = data.shape[0]
                    z = 0.5 * np.log((1 + partial_corr) / (1 - partial_corr))
                    p_value = 2 * (1 - norm.cdf(abs(z) * np.sqrt(n - 3)))
                    
                    if p_value > alpha:
                        adj_matrix[i, j] = 0
                        adj_matrix[j, i] = 0
                        break
    
    return adj_matrix

# æ¨¡æ‹Ÿå› æœæ•°æ®
from scipy.stats import norm

np.random.seed(42)
n = 500

# çœŸå®å› æœç»“æ„: X -> Y -> Z
X = np.random.randn(n)
Y = 0.8 * X + np.random.randn(n) * 0.3
Z = 0.7 * Y + np.random.randn(n) * 0.3

data = np.column_stack([X, Y, Z])

# å› æœå‘ç°
adj_matrix = pc_algorithm_simple(data, alpha=0.05)

print("å‘ç°çš„å› æœç»“æ„ (é‚»æ¥çŸ©é˜µ):")
print(adj_matrix)
print("\nçœŸå®å› æœç»“æ„: X -> Y -> Z")
```

---

## ğŸ“ å¯¹æ ‡è¯¾ç¨‹

| å¤§å­¦ | è¯¾ç¨‹ä»£ç  | è¯¾ç¨‹åç§° |
|------|----------|----------|
| **Stanford** | STATS361 | Causal Inference |
| **MIT** | 14.387 | Applied Econometrics: Mostly Harmless |
| **UC Berkeley** | PH252D | Causal Inference |
| **Harvard** | STAT186 | Causal Inference |
| **CMU** | 10-708 | Probabilistic Graphical Models |

---

## ğŸ“– æ ¸å¿ƒæ•™æä¸è®ºæ–‡

### æ•™æ

1. **Pearl, J.** *Causality: Models, Reasoning, and Inference*. Cambridge University Press, 2009.
   - å› æœæ¨æ–­çš„ç»å…¸æ•™æ

2. **Imbens, G. & Rubin, D.** *Causal Inference for Statistics, Social, and Biomedical Sciences*. Cambridge University Press, 2015.
   - æ½œåœ¨ç»“æœæ¡†æ¶

3. **HernÃ¡n, M. & Robins, J.** *Causal Inference: What If*. Chapman & Hall/CRC, 2020.
   - æµè¡Œç—…å­¦è§†è§’

4. **Peters, J., Janzing, D., & SchÃ¶lkopf, B.** *Elements of Causal Inference*. MIT Press, 2017.
   - æœºå™¨å­¦ä¹ è§†è§’

### ç»å…¸è®ºæ–‡

1. **Rubin, D. (1974)** - *Estimating Causal Effects of Treatments*
   - æ½œåœ¨ç»“æœæ¡†æ¶

2. **Pearl, J. (1995)** - *Causal Diagrams for Empirical Research*
   - å› æœå›¾æ¨¡å‹

3. **Rosenbaum, P. & Rubin, D. (1983)** - *The Central Role of the Propensity Score*
   - å€¾å‘å¾—åˆ†

4. **Angrist, J. et al. (1996)** - *Identification of Causal Effects Using Instrumental Variables*
   - å·¥å…·å˜é‡

### æœ€æ–°è¿›å±• (2024-2025)

1. **å› æœè¡¨ç¤ºå­¦ä¹ **
   - å­¦ä¹ å› æœä¸å˜çš„è¡¨ç¤º

2. **å› æœå¼ºåŒ–å­¦ä¹ **
   - æ³›åŒ–åˆ°æ–°ç¯å¢ƒçš„ç­–ç•¥

3. **å› æœLLM**
   - å¤§è¯­è¨€æ¨¡å‹çš„å› æœæ¨ç†èƒ½åŠ›

4. **å› æœå‘ç°çš„æ·±åº¦å­¦ä¹ æ–¹æ³•**
   - ç¥ç»ç½‘ç»œç”¨äºå› æœç»“æ„å­¦ä¹ 

---

## ğŸ”— ç›¸å…³ä¸»é¢˜

- [æ¦‚ç‡ç»Ÿè®¡](../../01-Mathematical-Foundations/02-Probability-Statistics/)
- [ç»Ÿè®¡å­¦ä¹ ç†è®º](../../02-Machine-Learning-Theory/01-Statistical-Learning/)
- [å¼ºåŒ–å­¦ä¹ ](../../02-Machine-Learning-Theory/04-Reinforcement-Learning/)

---

## ğŸ“ æ€»ç»“

**å› æœæ¨æ–­**æ˜¯ä»è§‚å¯Ÿæ•°æ®ä¸­æ¨æ–­å› æœå…³ç³»çš„ç§‘å­¦ï¼Œæ ¸å¿ƒåŒ…æ‹¬:

### æ ¸å¿ƒæ¦‚å¿µ

1. **å› æœå…³ç³» vs ç›¸å…³å…³ç³»**: ç›¸å…³ä¸ç­‰äºå› æœ
2. **åäº‹å®æ¨ç†**: "å¦‚æœ...ä¼šæ€æ ·?"
3. **å› æœæ•ˆåº”**: ATE, ATT, CATE

### ç†è®ºæ¡†æ¶

1. **Rubinå› æœæ¨¡å‹**: æ½œåœ¨ç»“æœæ¡†æ¶
2. **Pearlå› æœæ¨¡å‹**: ç»“æ„æ–¹ç¨‹ä¸å› æœå›¾
3. **ç»Ÿä¸€**: ä¸¤ç§æ¡†æ¶çš„ç­‰ä»·æ€§

### è¯†åˆ«æ–¹æ³•

1. **åé—¨å‡†åˆ™**: è°ƒæ•´æ··æ·†å› å­
2. **å‰é—¨å‡†åˆ™**: åˆ©ç”¨ä¸­ä»‹å˜é‡
3. **do-æ¼”ç®—**: ç³»ç»ŸåŒ–çš„è¯†åˆ«æ–¹æ³•

### ä¼°è®¡æ–¹æ³•

1. **RCT**: é»„é‡‘æ ‡å‡†
2. **PSM**: å€¾å‘å¾—åˆ†åŒ¹é…
3. **IV**: å·¥å…·å˜é‡
4. **DID**: åŒé‡å·®åˆ†
5. **RD**: å›å½’ä¸è¿ç»­

### AIåº”ç”¨

1. **å› æœè¡¨ç¤ºå­¦ä¹ **: å­¦ä¹ å› æœä¸å˜ç‰¹å¾
2. **åäº‹å®è§£é‡Š**: å¯è§£é‡ŠAI
3. **å› æœå¼ºåŒ–å­¦ä¹ **: æ³›åŒ–ç­–ç•¥
4. **åŸŸé€‚åº”**: åˆ©ç”¨å› æœä¸å˜æ€§

**æœªæ¥æ–¹å‘**:

- å› æœå‘ç°çš„æ·±åº¦å­¦ä¹ æ–¹æ³•
- å› æœLLM
- å¯éªŒè¯çš„å› æœæ¨æ–­
- å› æœä¸å…¬å¹³æ€§

å› æœæ¨æ–­ä¸ºAIæä¾›äº†ä»"é¢„æµ‹"åˆ°"ç†è§£"å’Œ"å¹²é¢„"çš„æ¡¥æ¢ï¼

---

**Â© 2025 AI Mathematics and Science Knowledge System**-

*Building the mathematical foundations for the AI era*-

*æœ€åæ›´æ–°ï¼š2025å¹´10æœˆ5æ—¥*-
