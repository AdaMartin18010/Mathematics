# å‰æ²¿ç ”ç©¶ä¸åº”ç”¨ (Frontiers and Applications)

> **2025å¹´AIæ•°å­¦çš„æœ€å‰æ²¿: ä»LLMç†è®ºåˆ°é‡å­æœºå™¨å­¦ä¹ **

---

## ç›®å½•

- [å‰æ²¿ç ”ç©¶ä¸åº”ç”¨ (Frontiers and Applications)](#å‰æ²¿ç ”ç©¶ä¸åº”ç”¨-frontiers-and-applications)
  - [ç›®å½•](#ç›®å½•)
  - [ğŸ“‹ æ¨¡å—æ¦‚è§ˆ](#-æ¨¡å—æ¦‚è§ˆ)
  - [ğŸ“š å­æ¨¡å—ç»“æ„](#-å­æ¨¡å—ç»“æ„)
    - [1. å¤§è¯­è¨€æ¨¡å‹ç†è®º (LLM Theory)](#1-å¤§è¯­è¨€æ¨¡å‹ç†è®º-llm-theory)
    - [2. æ‰©æ•£æ¨¡å‹ (Diffusion Models)](#2-æ‰©æ•£æ¨¡å‹-diffusion-models)
    - [3. ç¥ç»ç¬¦å·AI (Neuro-Symbolic AI)](#3-ç¥ç»ç¬¦å·ai-neuro-symbolic-ai)
    - [4. é‡å­æœºå™¨å­¦ä¹  (Quantum ML)](#4-é‡å­æœºå™¨å­¦ä¹ -quantum-ml)
    - [5. 2025æœ€æ–°ç ”ç©¶è®ºæ–‡æ±‡æ€» âœ…](#5-2025æœ€æ–°ç ”ç©¶è®ºæ–‡æ±‡æ€»-)
  - [ğŸ¯ å­¦ä¹ è·¯å¾„](#-å­¦ä¹ è·¯å¾„)
    - [å¿«é€Ÿæµè§ˆ (1å‘¨)](#å¿«é€Ÿæµè§ˆ-1å‘¨)
    - [æ·±å…¥ç ”ç©¶ (1-3ä¸ªæœˆ)](#æ·±å…¥ç ”ç©¶-1-3ä¸ªæœˆ)
  - [ğŸ”— ä¸å…¶ä»–æ¨¡å—çš„è”ç³»](#-ä¸å…¶ä»–æ¨¡å—çš„è”ç³»)
  - [ğŸ“– æ ¸å¿ƒèµ„æº](#-æ ¸å¿ƒèµ„æº)
    - [å¿…è¯»ç»¼è¿°](#å¿…è¯»ç»¼è¿°)
    - [2025é¡¶ä¼šè®ºæ–‡](#2025é¡¶ä¼šè®ºæ–‡)
    - [åœ¨çº¿è¯¾ç¨‹](#åœ¨çº¿è¯¾ç¨‹)
  - [ğŸ”¬ ç ”ç©¶æ–¹å‘æŒ‡å¼•](#-ç ”ç©¶æ–¹å‘æŒ‡å¼•)
    - [æ–¹å‘1: LLMç†è®ºæ·±åŒ–](#æ–¹å‘1-llmç†è®ºæ·±åŒ–)
    - [æ–¹å‘2: æ‰©æ•£æ¨¡å‹åŠ é€Ÿ](#æ–¹å‘2-æ‰©æ•£æ¨¡å‹åŠ é€Ÿ)
    - [æ–¹å‘3: ç¥ç»ç¬¦å·ç»“åˆ](#æ–¹å‘3-ç¥ç»ç¬¦å·ç»“åˆ)
    - [æ–¹å‘4: å¯éªŒè¯AI](#æ–¹å‘4-å¯éªŒè¯ai)
  - [ğŸ’» å®è·µé¡¹ç›®](#-å®è·µé¡¹ç›®)
    - [é¡¹ç›®1: ä»é›¶å®ç°Mini-GPT](#é¡¹ç›®1-ä»é›¶å®ç°mini-gpt)
    - [é¡¹ç›®2: å®ç°DDPMæ‰©æ•£æ¨¡å‹](#é¡¹ç›®2-å®ç°ddpmæ‰©æ•£æ¨¡å‹)
    - [é¡¹ç›®3: ç¥ç»ç¬¦å·VQA](#é¡¹ç›®3-ç¥ç»ç¬¦å·vqa)
  - [ğŸ“Š é‡è¦benchmarkä¸æ•°æ®é›†](#-é‡è¦benchmarkä¸æ•°æ®é›†)
  - [ğŸ† é‡Œç¨‹ç¢‘æ£€æŸ¥](#-é‡Œç¨‹ç¢‘æ£€æŸ¥)
    - [å…¥é—¨çº§](#å…¥é—¨çº§)
    - [ä¸­çº§](#ä¸­çº§)
    - [é«˜çº§](#é«˜çº§)
  - [ğŸ“ å¯¹æ ‡å¤§å­¦è¯¾ç¨‹](#-å¯¹æ ‡å¤§å­¦è¯¾ç¨‹)
  - [ğŸ” å‰æ²¿ä¼šè®®ä¸æœŸåˆŠ](#-å‰æ²¿ä¼šè®®ä¸æœŸåˆŠ)
    - [é¡¶çº§ä¼šè®®](#é¡¶çº§ä¼šè®®)
    - [é‡è¦æœŸåˆŠ](#é‡è¦æœŸåˆŠ)
    - [é¢„å°æœ¬è¿½è¸ª](#é¢„å°æœ¬è¿½è¸ª)
  - [ğŸš€ ä¸‹ä¸€æ­¥è¡ŒåŠ¨](#-ä¸‹ä¸€æ­¥è¡ŒåŠ¨)
  - [ğŸ”— ç¤¾åŒºèµ„æº](#-ç¤¾åŒºèµ„æº)

## ğŸ“‹ æ¨¡å—æ¦‚è§ˆ

æœ¬æ¨¡å—èšç„¦**2025å¹´æœ€æ–°ç ”ç©¶æˆæœ**,æ¶µç›–å¤§è¯­è¨€æ¨¡å‹ã€æ‰©æ•£æ¨¡å‹ã€ç¥ç»ç¬¦å·AIç­‰å‰æ²¿æ–¹å‘ã€‚

**æ ¸å¿ƒç‰¹è‰²**:

- âœ… æ•´åˆæœ€æ–°è®ºæ–‡ (2024-2025)
- âœ… æ•°å­¦ç†è®ºæ·±åº¦åˆ†æ
- âœ… å‰æ²¿æŠ€æœ¯å®è·µ
- âœ… ç ”ç©¶æ–¹å‘æŒ‡å¼•

---

## ğŸ“š å­æ¨¡å—ç»“æ„

### 1. [å¤§è¯­è¨€æ¨¡å‹ç†è®º (LLM Theory)](./01-LLM-Theory/)

**æ ¸å¿ƒä¸»é¢˜**:

- [Transformeræ•°å­¦åŸç†](./01-LLM-Theory/01-Transformer-Mathematics.md) âœ…
- In-Context Learningç†è®º
- æ¶Œç°èƒ½åŠ›çš„æ•°å­¦è§£é‡Š
- Scaling Lawsä¸ç›¸å˜ç°è±¡
- Mixture of Experts (MoE)æ¶æ„

**2025ç ”ç©¶çƒ­ç‚¹**:

- **è¶…é•¿ä¸Šä¸‹æ–‡**: ç™¾ä¸‡tokençº§åˆ«
- **å¤šæ¨¡æ€èåˆ**: è§†è§‰+è¯­è¨€+éŸ³é¢‘
- **é«˜æ•ˆè®­ç»ƒ**: MoE, Sparse Attention
- **å¯¹é½ç†è®º**: RLHFæ•°å­¦åŸºç¡€

**å…³é”®æ•°å­¦**:

$$
\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V
$$

$$
L(N, D) = \left(\frac{N_c}{N}\right)^{\alpha_N} + \left(\frac{D_c}{D}\right)^{\alpha_D}
$$

---

### 2. [æ‰©æ•£æ¨¡å‹ (Diffusion Models)](./02-Diffusion-Models/)

**æ ¸å¿ƒç†è®º**:

- Score-basedç”Ÿæˆæ¨¡å‹
- éšæœºå¾®åˆ†æ–¹ç¨‹ (SDE)ç†è®º
- æœ€ä¼˜ä¼ è¾“ä¸SchrÃ¶dingeræ¡¥
- ç¦»æ•£æ‰©æ•£æ¨¡å‹
- Flow Matching

**æ•°å­¦æ¡†æ¶**:

**å‰å‘è¿‡ç¨‹** (æ‰©æ•£):

$$
dX_t = -\frac{1}{2}\beta(t) X_t dt + \sqrt{\beta(t)} dW_t
$$

**åå‘è¿‡ç¨‹** (ç”Ÿæˆ):

$$
dX_t = \left[-\frac{1}{2}\beta(t)X_t - \beta(t) \nabla_x \log p_t(X_t)\right] dt + \sqrt{\beta(t)} d\bar{W}_t
$$

**ScoreåŒ¹é…**:

$$
\mathcal{L} = \mathbb{E}_{t, x_0, \epsilon} \left[ \| \epsilon - \epsilon_\theta(x_t, t) \|^2 \right]
$$

**2025å‰æ²¿**:

- **Consistency Models**: ä¸€æ­¥ç”Ÿæˆ
- **Rectified Flow**: ç›´çº¿è·¯å¾„
- **Latent Diffusion**: é«˜åˆ†è¾¨ç‡å›¾åƒ
- **Text-to-3D**: æ‰©æ•£æ¨¡å‹ç”Ÿæˆ3D

---

### 3. [ç¥ç»ç¬¦å·AI (Neuro-Symbolic AI)](./03-Neuro-Symbolic-AI/)

**æ ¸å¿ƒæ€æƒ³**: ç»“åˆç¥ç»ç½‘ç»œä¸ç¬¦å·æ¨ç†

**ä¸»é¢˜**:

- é€»è¾‘æ¨ç†ä¸ç¥ç»ç½‘ç»œç»“åˆ
- çŸ¥è¯†å›¾è°±åµŒå…¥
- å¯è§£é‡Šæ€§ä¸å› æœæ¨ç†
- ç¨‹åºç»¼åˆ
- ç¥ç»å®šç†è¯æ˜

**æ¶æ„æ¨¡å¼**:

1. **ç¥ç»å¼•å¯¼ç¬¦å·** (Neural-Guided Symbolic)
2. **ç¬¦å·çº¦æŸç¥ç»** (Symbolic-Constrained Neural)
3. **æ··åˆæ¨ç†** (Hybrid Reasoning)

**åº”ç”¨**:

- è§†è§‰é—®ç­” (VQA)
- çŸ¥è¯†æ¨ç†
- æ•°å­¦é—®é¢˜æ±‚è§£
- ä»£ç ç”Ÿæˆ

---

### 4. [é‡å­æœºå™¨å­¦ä¹  (Quantum ML)](./04-Quantum-Machine-Learning/)

**æ ¸å¿ƒæ¦‚å¿µ**:

- é‡å­ç”µè·¯ä¸é‡å­é—¨
- å˜åˆ†é‡å­ç®—æ³• (VQA)
- é‡å­æ ¸æ–¹æ³•
- é‡å­ä¼˜åŠ¿åˆ†æ

**æ•°å­¦å·¥å…·**:

- é‡å­æ€: $|\psi\rangle = \alpha|0\rangle + \beta|1\rangle$
- é‡å­é—¨: Hadamard, CNOT, Pauli
- é‡å­æµ‹é‡: æŠ•å½±ç®—ç¬¦
- é‡å­çº ç¼ : Bellæ€

**å‰æ²¿åº”ç”¨**:

- é‡å­ç”Ÿæˆæ¨¡å‹
- é‡å­å¼ºåŒ–å­¦ä¹ 
- é‡å­ä¼˜åŒ–
- NISQæ—¶ä»£ç®—æ³•

---

### 5. [2025æœ€æ–°ç ”ç©¶è®ºæ–‡æ±‡æ€»](./2025-Latest-Research-Papers.md) âœ…

**åˆ†ç±»æ±‡æ€»**:

- LLMç†è®ºçªç ´
- æ‰©æ•£æ¨¡å‹æ–°æ–¹æ³•
- å½¢å¼åŒ–AIè¿›å±•
- ç¥ç»ç¬¦å·ç»“åˆ
- ä¼˜åŒ–ç†è®ºåˆ›æ–°

**é‡ç‚¹è®ºæ–‡** (2024-2025):

1. "Scaling Laws for Large Language Models: A Phase Transition Perspective"
2. "Stochastic Interpolants: A Unifying Framework for Flows and Diffusions"
3. "DeepSeek-Prover-V1.5: Advancing Theorem Proving with LLMs"
4. "Flow Matching for Generative Modeling"
5. "Consistency Models: One-Step Generation"

---

## ğŸ¯ å­¦ä¹ è·¯å¾„

### å¿«é€Ÿæµè§ˆ (1å‘¨)

```text
Day 1-2: LLMç†è®º
  â”œâ”€ Transformeræ¶æ„å›é¡¾
  â”œâ”€ Attentionæœºåˆ¶æ•°å­¦
  â””â”€ Scaling Lawsæ¦‚è¿°

Day 3-4: æ‰©æ•£æ¨¡å‹
  â”œâ”€ DDPMåŸºç¡€
  â”œâ”€ Score-basedæ–¹æ³•
  â””â”€ åº”ç”¨æ¡ˆä¾‹

Day 5-6: ç¥ç»ç¬¦å·AI
  â”œâ”€ æ¶æ„æ¨¡å¼
  â”œâ”€ åº”ç”¨åœºæ™¯
  â””â”€ å‰æ²¿è¿›å±•

Day 7: ç»¼åˆ
  â”œâ”€ å‰æ²¿æ–¹å‘æ€»ç»“
  â”œâ”€ é€‰æ‹©ç ”ç©¶æ–¹å‘
  â””â”€ åˆ¶å®šæ·±å…¥è®¡åˆ’
```

---

### æ·±å…¥ç ”ç©¶ (1-3ä¸ªæœˆ)

```text
é˜¶æ®µ1: ç†è®ºåŸºç¡€ (2-3å‘¨)
  â”œâ”€ LLMæ•°å­¦åŸç†æ·±å…¥
  â”œâ”€ æ‰©æ•£æ¨¡å‹SDEç†è®º
  â”œâ”€ ä¼˜åŒ–ç†è®ºè¡¥å……
  â””â”€ é˜…è¯»ç»å…¸è®ºæ–‡

é˜¶æ®µ2: å®è·µå¤ç° (3-4å‘¨)
  â”œâ”€ å®ç°Mini-GPT
  â”œâ”€ è®­ç»ƒæ‰©æ•£æ¨¡å‹
  â”œâ”€ ç¥ç»ç¬¦å·æ¡ˆä¾‹
  â””â”€ è¶…å‚æ•°è°ƒä¼˜

é˜¶æ®µ3: å‰æ²¿è·Ÿè¸ª (æŒç»­)
  â”œâ”€ arXivæ¯æ—¥è¿½è¸ª
  â”œâ”€ é¡¶ä¼šè®ºæ–‡é˜…è¯»
  â”œâ”€ å¤ç°SOTAæ–¹æ³•
  â””â”€ å‚ä¸å¼€æºé¡¹ç›®

é˜¶æ®µ4: åŸåˆ›ç ”ç©¶ (2-3ä¸ªæœˆ)
  â”œâ”€ æå‡ºç ”ç©¶é—®é¢˜
  â”œâ”€ è®¾è®¡å®éªŒæ–¹æ¡ˆ
  â”œâ”€ æ’°å†™æŠ€æœ¯æŠ¥å‘Š
  â””â”€ æŠ•ç¨¿ä¼šè®®/æœŸåˆŠ
```

---

## ğŸ”— ä¸å…¶ä»–æ¨¡å—çš„è”ç³»

```text
å‰æ²¿ç ”ç©¶
â”œâ”€â†’ æ•°å­¦åŸºç¡€ (æ¦‚ç‡è®º, ä¼˜åŒ–, æ³›å‡½åˆ†æ)
â”œâ”€â†’ ç»Ÿè®¡å­¦ä¹  (æ³›åŒ–ç†è®º, PACå­¦ä¹ )
â”œâ”€â†’ æ·±åº¦å­¦ä¹  (ç¥ç»ç½‘ç»œæ•°å­¦)
â”œâ”€â†’ å½¢å¼åŒ–æ–¹æ³• (AIè¾…åŠ©è¯æ˜)
â””â”€â†’ ä¼˜åŒ–ç®—æ³• (è®­ç»ƒæ–¹æ³•)
```

---

## ğŸ“– æ ¸å¿ƒèµ„æº

### å¿…è¯»ç»¼è¿°

1. **"State of GPT"** - Andrej Karpathy (2023)  
   â†’ LLMè®­ç»ƒä¸åº”ç”¨å…¨æ™¯

2. **"Denoising Diffusion Probabilistic Models: A Survey"** (2024)  
   â†’ æ‰©æ•£æ¨¡å‹ç»¼è¿°

3. **"Neuro-Symbolic AI: The 3rd Wave"** - Garcez et al. (2023)  
   â†’ ç¥ç»ç¬¦å·AIç»¼è¿°

---

### 2025é¡¶ä¼šè®ºæ–‡

**NeurIPS 2024**:

- Best Paperå€™é€‰: "Benign Overfitting in Transformers"
- Spotlight: "Flow Matching with Optimal Transport"

**ICML 2024**:

- Outstanding Paper: "Understanding In-Context Learning via Function Approximation"
- Best Theory Paper: "Convergence of Diffusion Models in Total Variation"

**ICLR 2025** (é¢„è®¡):

- é‡ç‚¹å…³æ³¨: è¶…é•¿ä¸Šä¸‹æ–‡ã€Mixture of Expertsã€DiffusionåŠ é€Ÿ

---

### åœ¨çº¿è¯¾ç¨‹

| è¯¾ç¨‹ | æœºæ„ | å†…å®¹ |
|------|------|------|
| **CS324** | Stanford | Large Language Models (Tatsu Hashimoto) |
| **CS236** | Stanford | Deep Generative Models (Stefano Ermon) |
| **Diffusion Models** | Hugging Face | æ‰©æ•£æ¨¡å‹ä»ç†è®ºåˆ°å®è·µ |
| **Neural-Symbolic** | MIT | ç¥ç»ç¬¦å·å­¦ä¹  |

---

## ğŸ”¬ ç ”ç©¶æ–¹å‘æŒ‡å¼•

### æ–¹å‘1: LLMç†è®ºæ·±åŒ–

**å¼€æ”¾é—®é¢˜**:

- ä¸ºä»€ä¹ˆIn-Context Learningæœ‰æ•ˆ?
- å¦‚ä½•è§£é‡Šæ¶Œç°èƒ½åŠ›?
- Scaling Lawsçš„ç†è®ºåŸºç¡€?
- æœ€ä¼˜çš„æ¶æ„è®¾è®¡?

**æ¨èåˆ‡å…¥ç‚¹**:

- Neural Tangent Kernel for Transformers
- ä¿¡æ¯è®ºè§†è§’åˆ†æICL
- ç»Ÿè®¡ç‰©ç†ç›¸å˜ç†è®º

---

### æ–¹å‘2: æ‰©æ•£æ¨¡å‹åŠ é€Ÿ

**æ ¸å¿ƒæŒ‘æˆ˜**:

- é‡‡æ ·æ­¥æ•°å¤ªå¤š (50-1000æ­¥)
- è®¡ç®—æˆæœ¬é«˜
- æ¨ç†é€Ÿåº¦æ…¢

**å‰æ²¿æ–¹æ³•**:

- **Consistency Models**: ä¸€æ­¥ç”Ÿæˆ
- **Rectified Flow**: ç›´çº¿æœ€ä¼˜è·¯å¾„
- **Distillation**: çŸ¥è¯†è’¸é¦åŠ é€Ÿ
- **Latent Space**: æ½œç©ºé—´æ‰©æ•£

---

### æ–¹å‘3: ç¥ç»ç¬¦å·ç»“åˆ

**ç ”ç©¶ä¸»é¢˜**:

- å¦‚ä½•æ›´å¥½åœ°ç»“åˆç¥ç»ä¸ç¬¦å·?
- å¯å¾®åˆ†é€»è¾‘æ¨ç†
- ç¥ç»ç¨‹åºç»¼åˆ
- ç¬¦å·çŸ¥è¯†æ³¨å…¥ç¥ç»ç½‘ç»œ

**åº”ç”¨åœºæ™¯**:

- æ•°å­¦é—®é¢˜æ±‚è§£ (AlphaGeometry)
- ä»£ç ç”Ÿæˆä¸éªŒè¯
- çŸ¥è¯†å›¾è°±æ¨ç†
- å¯è§£é‡ŠAI

---

### æ–¹å‘4: å¯éªŒè¯AI

**ç›®æ ‡**: å½¢å¼åŒ–ä¿è¯AIç³»ç»Ÿçš„æ€§è´¨

**ä¸»é¢˜**:

- LLMå¯¹é½çš„æ•°å­¦ä¿è¯
- æ‰©æ•£æ¨¡å‹çš„é²æ£’æ€§
- ç¥ç»ç½‘ç»œéªŒè¯
- å®‰å…¨å…³é”®AI

---

## ğŸ’» å®è·µé¡¹ç›®

### é¡¹ç›®1: ä»é›¶å®ç°Mini-GPT

**ç›®æ ‡**: ç†è§£Transformerè®­ç»ƒå…¨æµç¨‹

```python
# æ¶æ„è¦ç‚¹
class MiniGPT(nn.Module):
    def __init__(self, vocab_size, d_model, n_layers, n_heads):
        self.embed = nn.Embedding(vocab_size, d_model)
        self.pos_enc = PositionalEncoding(d_model)
        self.blocks = nn.ModuleList([
            TransformerBlock(d_model, n_heads) 
            for _ in range(n_layers)
        ])
        self.ln_f = nn.LayerNorm(d_model)
        self.head = nn.Linear(d_model, vocab_size)
    
    def forward(self, x):
        x = self.embed(x) + self.pos_enc(x)
        for block in self.blocks:
            x = block(x)
        x = self.ln_f(x)
        return self.head(x)

# è®­ç»ƒ
model = MiniGPT(vocab_size=50000, d_model=512, n_layers=6, n_heads=8)
# ... è®­ç»ƒä»£ç 
```

---

### é¡¹ç›®2: å®ç°DDPMæ‰©æ•£æ¨¡å‹

**ç›®æ ‡**: åœ¨CIFAR-10ä¸Šè®­ç»ƒå›¾åƒç”Ÿæˆæ¨¡å‹

**å…³é”®æ­¥éª¤**:

1. å‰å‘æ‰©æ•£è¿‡ç¨‹
2. å™ªå£°é¢„æµ‹ç½‘ç»œ (U-Net)
3. åå‘é‡‡æ ·
4. FIDè¯„ä¼°

---

### é¡¹ç›®3: ç¥ç»ç¬¦å·VQA

**ç›®æ ‡**: ç»“åˆç¥ç»è§†è§‰æ¨¡å—ä¸ç¬¦å·æ¨ç†å¼•æ“

**æ¶æ„**:

```text
Image â†’ CNN â†’ Object Detection
                â†“
         Scene Graph
                â†“
Question â†’ NLP â†’ Symbolic Query
                â†“
         Logic Reasoning
                â†“
            Answer
```

---

## ğŸ“Š é‡è¦benchmarkä¸æ•°æ®é›†

| ä»»åŠ¡ | Benchmark | è¯´æ˜ |
|------|-----------|------|
| **LLM** | MMLU, BBH, HumanEval | é€šç”¨èƒ½åŠ›, æ¨ç†, ä»£ç  |
| **ç”Ÿæˆ** | FID, LPIPS, IS | å›¾åƒè´¨é‡è¯„ä¼° |
| **æ¨ç†** | ARC, StrategyQA | å¸¸è¯†æ¨ç† |
| **æ•°å­¦** | GSM8K, MATH | æ•°å­¦é—®é¢˜æ±‚è§£ |
| **ä»£ç ** | HumanEval, MBPP | ä»£ç ç”Ÿæˆ |

---

## ğŸ† é‡Œç¨‹ç¢‘æ£€æŸ¥

### å…¥é—¨çº§

- [ ] ç†è§£Transformeræ¶æ„ç»†èŠ‚
- [ ] å®ç°Self-Attention from scratch
- [ ] å¤ç°DDPMé‡‡æ ·è¿‡ç¨‹
- [ ] é˜…è¯»10ç¯‡å‰æ²¿è®ºæ–‡

---

### ä¸­çº§

- [ ] è®­ç»ƒå°å‹è¯­è¨€æ¨¡å‹ (100Må‚æ•°)
- [ ] å®ç°æ‰©æ•£æ¨¡å‹å¹¶ç”Ÿæˆå›¾åƒ
- [ ] å¤ç°ä¸€ç¯‡é¡¶ä¼šè®ºæ–‡ç»“æœ
- [ ] å‚ä¸å¼€æºé¡¹ç›®è´¡çŒ®

---

### é«˜çº§

- [ ] æå‡ºåŸåˆ›ç ”ç©¶æƒ³æ³•
- [ ] å®Œæˆå®Œæ•´ç ”ç©¶é¡¹ç›®
- [ ] æ’°å†™æŠ€æœ¯æŠ¥å‘Š/è®ºæ–‡
- [ ] æŠ•ç¨¿é¡¶ä¼š (NeurIPS/ICML/ICLR)

---

## ğŸ“ å¯¹æ ‡å¤§å­¦è¯¾ç¨‹

| å¤§å­¦ | è¯¾ç¨‹ | å†…å®¹ |
|------|------|------|
| **Stanford** | CS324 | Large Language Models (å…¨æ–¹ä½) |
| **Stanford** | CS236 | Deep Generative Models (VAE, GAN, Diffusion) |
| **MIT** | 6.S898 | Deep Learning (å‰æ²¿ä¸»é¢˜) |
| **CMU** | 11-747 | Neural NLP (Transformeræ·±å…¥) |
| **Berkeley** | CS285 | Deep RL (MDP, Policy Gradient) |

---

## ğŸ” å‰æ²¿ä¼šè®®ä¸æœŸåˆŠ

### é¡¶çº§ä¼šè®®

- **NeurIPS**: ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿ (12æœˆ)
- **ICML**: å›½é™…æœºå™¨å­¦ä¹ ä¼šè®® (7æœˆ)
- **ICLR**: å›½é™…å­¦ä¹ è¡¨å¾ä¼šè®® (5æœˆ)
- **AAAI**: äººå·¥æ™ºèƒ½åä¼šå¹´ä¼š (2æœˆ)

---

### é‡è¦æœŸåˆŠ

- **JMLR**: Journal of Machine Learning Research
- **PAMI**: IEEE Trans. on Pattern Analysis and Machine Intelligence
- **Nature Machine Intelligence**
- **TMLR**: Transactions on Machine Learning Research

---

### é¢„å°æœ¬è¿½è¸ª

- **arXiv**: cs.LG, cs.AI, stat.ML
- **OpenReview**: ICLR/NeurIPSå…¬å¼€è¯„å®¡
- **Papers with Code**: ä»£ç +è®ºæ–‡

---

## ğŸš€ ä¸‹ä¸€æ­¥è¡ŒåŠ¨

1. **é€‰æ‹©å­æ–¹å‘**: LLM / Diffusion / Neuro-Symbolic
2. **æ·±å…¥å­¦ä¹ **: é˜…è¯»è¯¥æ–¹å‘top10è®ºæ–‡
3. **å®è·µé¡¹ç›®**: å¤ç°ä¸€ä¸ªSOTAæ–¹æ³•
4. **æŒç»­è·Ÿè¸ª**: è®¢é˜…arXiv alert
5. **ç¤¾åŒºå‚ä¸**: åŠ å…¥Discord/Slackè®¨è®ºç»„

---

## ğŸ”— ç¤¾åŒºèµ„æº

- **Hugging Face**: æ¨¡å‹åº“ + ç¤¾åŒº
- **Papers with Code**: è®ºæ–‡ + ä»£ç 
- **Yannic Kilcher**: YouTubeè®ºæ–‡è§£è¯»
- **Two Minute Papers**: è§†è§‰åŒ–è®ºæ–‡ä»‹ç»

---

**ğŸ”™ è¿”å›**: [AIæ•°å­¦ä½“ç³»ä¸»é¡µ](../README.md)

**â–¶ï¸ å¼€å§‹æ¢ç´¢**: [Transformeræ•°å­¦åŸç†](./01-LLM-Theory/01-Transformer-Mathematics.md) | [æœ€æ–°è®ºæ–‡æ±‡æ€»](./2025-Latest-Research-Papers.md)
