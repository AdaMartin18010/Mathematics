# å¤šæ¨¡æ€å­¦ä¹ åº”ç”¨æ¡ˆä¾‹

> **å¯¹æ ‡è¯¾ç¨‹**: Stanford CS231n (CV), Stanford CS224n (NLP), MIT 6.S191 (Deep Learning)
>
> **æ ¸å¿ƒå†…å®¹**: å›¾æ–‡åŒ¹é…ã€è§†é¢‘ç†è§£ã€è·¨æ¨¡æ€æ£€ç´¢ã€å¤šæ¨¡æ€ç”Ÿæˆã€éŸ³é¢‘-è§†è§‰èåˆ
>
> **æ•°å­¦å·¥å…·**: CLIPã€ViTã€Transformerã€å¯¹æ¯”å­¦ä¹ ã€è·¨æ¨¡æ€æ³¨æ„åŠ›

---

## ğŸ“‹ ç›®å½•

- [å¤šæ¨¡æ€å­¦ä¹ åº”ç”¨æ¡ˆä¾‹](#å¤šæ¨¡æ€å­¦ä¹ åº”ç”¨æ¡ˆä¾‹)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [æ¡ˆä¾‹1: å›¾æ–‡åŒ¹é… (CLIP)](#æ¡ˆä¾‹1-å›¾æ–‡åŒ¹é…-clip)
    - [1. é—®é¢˜å®šä¹‰](#1-é—®é¢˜å®šä¹‰)
    - [2. æ•°å­¦å»ºæ¨¡](#2-æ•°å­¦å»ºæ¨¡)
      - [2.1 å¯¹æ¯”å­¦ä¹  (Contrastive Learning)](#21-å¯¹æ¯”å­¦ä¹ -contrastive-learning)
      - [2.2 é›¶æ ·æœ¬åˆ†ç±»](#22-é›¶æ ·æœ¬åˆ†ç±»)
    - [3. å®Œæ•´å®ç°](#3-å®Œæ•´å®ç°)
    - [4. æ€§èƒ½åˆ†æ](#4-æ€§èƒ½åˆ†æ)
      - [4.1 è¯„ä¼°æŒ‡æ ‡](#41-è¯„ä¼°æŒ‡æ ‡)
      - [4.2 æ•°å­¦åˆ†æ](#42-æ•°å­¦åˆ†æ)
    - [5. å·¥ç¨‹ä¼˜åŒ–](#5-å·¥ç¨‹ä¼˜åŒ–)
      - [5.1 å¤§è§„æ¨¡è®­ç»ƒ](#51-å¤§è§„æ¨¡è®­ç»ƒ)
      - [5.2 æ•°æ®å¢å¼º](#52-æ•°æ®å¢å¼º)
  - [æ¡ˆä¾‹2: è§†é¢‘ç†è§£ (TimeSformer)](#æ¡ˆä¾‹2-è§†é¢‘ç†è§£-timesformer)
    - [1. é—®é¢˜å®šä¹‰2](#1-é—®é¢˜å®šä¹‰2)
    - [2. æ•°å­¦å»ºæ¨¡2](#2-æ•°å­¦å»ºæ¨¡2)
      - [2.1 æ—¶ç©ºæ³¨æ„åŠ› (Divided Space-Time Attention)](#21-æ—¶ç©ºæ³¨æ„åŠ›-divided-space-time-attention)
    - [3. å®Œæ•´å®ç°2](#3-å®Œæ•´å®ç°2)
    - [4. æ€§èƒ½åˆ†æ2](#4-æ€§èƒ½åˆ†æ2)
      - [4.1 è¯„ä¼°æŒ‡æ ‡2](#41-è¯„ä¼°æŒ‡æ ‡2)
  - [æ¡ˆä¾‹3: è·¨æ¨¡æ€æ£€ç´¢](#æ¡ˆä¾‹3-è·¨æ¨¡æ€æ£€ç´¢)
    - [1. é—®é¢˜å®šä¹‰3](#1-é—®é¢˜å®šä¹‰3)
    - [2. æ•°å­¦å»ºæ¨¡3](#2-æ•°å­¦å»ºæ¨¡3)
      - [2.1 è·¨æ¨¡æ€ç›¸ä¼¼åº¦å­¦ä¹ ](#21-è·¨æ¨¡æ€ç›¸ä¼¼åº¦å­¦ä¹ )
    - [3. å®Œæ•´å®ç°3](#3-å®Œæ•´å®ç°3)
  - [æ¡ˆä¾‹4: å¤šæ¨¡æ€ç”Ÿæˆ (Image Captioning)](#æ¡ˆä¾‹4-å¤šæ¨¡æ€ç”Ÿæˆ-image-captioning)
    - [1. é—®é¢˜å®šä¹‰4](#1-é—®é¢˜å®šä¹‰4)
    - [2. æ•°å­¦å»ºæ¨¡4](#2-æ•°å­¦å»ºæ¨¡4)
      - [2.1 ç¼–ç å™¨-è§£ç å™¨æ¶æ„](#21-ç¼–ç å™¨-è§£ç å™¨æ¶æ„)
    - [3. å®Œæ•´å®ç°4](#3-å®Œæ•´å®ç°4)
  - [æ¡ˆä¾‹5: éŸ³é¢‘-è§†è§‰èåˆ](#æ¡ˆä¾‹5-éŸ³é¢‘-è§†è§‰èåˆ)
    - [1. é—®é¢˜å®šä¹‰5](#1-é—®é¢˜å®šä¹‰5)
    - [2. æ•°å­¦å»ºæ¨¡5](#2-æ•°å­¦å»ºæ¨¡5)
      - [2.1 å¤šæ¨¡æ€èåˆç­–ç•¥](#21-å¤šæ¨¡æ€èåˆç­–ç•¥)
    - [3. å®Œæ•´å®ç°6](#3-å®Œæ•´å®ç°6)
  - [ğŸ“Š æ€»ç»“](#-æ€»ç»“)
    - [æ¨¡å—ç»Ÿè®¡](#æ¨¡å—ç»Ÿè®¡)
    - [æ ¸å¿ƒä»·å€¼](#æ ¸å¿ƒä»·å€¼)
    - [åº”ç”¨åœºæ™¯](#åº”ç”¨åœºæ™¯)

---

## æ¡ˆä¾‹1: å›¾æ–‡åŒ¹é… (CLIP)

### 1. é—®é¢˜å®šä¹‰

**ä»»åŠ¡**: å­¦ä¹ å›¾åƒå’Œæ–‡æœ¬çš„è”åˆåµŒå…¥ç©ºé—´,å®ç°é›¶æ ·æœ¬å›¾åƒåˆ†ç±»å’Œå›¾æ–‡æ£€ç´¢

**æ•°å­¦å½¢å¼åŒ–**:

- å›¾åƒé›†åˆ: $\mathcal{I} = \{I_1, \ldots, I_N\}$
- æ–‡æœ¬é›†åˆ: $\mathcal{T} = \{T_1, \ldots, T_N\}$
- å›¾åƒç¼–ç å™¨: $f_I: \mathcal{I} \rightarrow \mathbb{R}^d$
- æ–‡æœ¬ç¼–ç å™¨: $f_T: \mathcal{T} \rightarrow \mathbb{R}^d$
- ç›®æ ‡: å­¦ä¹ è”åˆåµŒå…¥ç©ºé—´,ä½¿å¾—åŒ¹é…çš„å›¾æ–‡å¯¹ç›¸ä¼¼åº¦é«˜

**æ ¸å¿ƒæŒ‘æˆ˜**:

- æ¨¡æ€å·®å¼‚ (è§†è§‰ vs è¯­è¨€)
- è¯­ä¹‰å¯¹é½
- é›¶æ ·æœ¬æ³›åŒ–
- å¤§è§„æ¨¡è®­ç»ƒ

---

### 2. æ•°å­¦å»ºæ¨¡

#### 2.1 å¯¹æ¯”å­¦ä¹  (Contrastive Learning)

**InfoNCEæŸå¤±**:
$$
\mathcal{L}_{\text{InfoNCE}} = -\log \frac{\exp(\text{sim}(I_i, T_i) / \tau)}{\sum_{j=1}^N \exp(\text{sim}(I_i, T_j) / \tau)}
$$

å…¶ä¸­:

- $\text{sim}(I, T) = \frac{f_I(I)^T f_T(T)}{\|f_I(I)\| \|f_T(T)\|}$ (ä½™å¼¦ç›¸ä¼¼åº¦)
- $\tau$: æ¸©åº¦å‚æ•°
- æ­£æ ·æœ¬: $(I_i, T_i)$ åŒ¹é…çš„å›¾æ–‡å¯¹
- è´Ÿæ ·æœ¬: $(I_i, T_j)$ å…¶ä¸­ $j \neq i$

**å¯¹ç§°æŸå¤±**:
$$
\mathcal{L}_{\text{CLIP}} = \frac{1}{2}(\mathcal{L}_{I \rightarrow T} + \mathcal{L}_{T \rightarrow I})
$$

#### 2.2 é›¶æ ·æœ¬åˆ†ç±»

**åˆ†ç±»è¿‡ç¨‹**:

1. ä¸ºæ¯ä¸ªç±»åˆ«ç”Ÿæˆæ–‡æœ¬æè¿°: "A photo of a {class}"
2. è®¡ç®—å›¾åƒä¸æ‰€æœ‰æ–‡æœ¬çš„ç›¸ä¼¼åº¦
3. é€‰æ‹©ç›¸ä¼¼åº¦æœ€é«˜çš„ç±»åˆ«

$$
\hat{y} = \arg\max_{c} \text{sim}(I, T_c)
$$

---

### 3. å®Œæ•´å®ç°

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
from torchvision import models, transforms
from PIL import Image
import matplotlib.pyplot as plt

# ============================================================
# å›¾åƒç¼–ç å™¨ (Vision Transformer)
# ============================================================

class ImageEncoder(nn.Module):
    """å›¾åƒç¼–ç å™¨ (ç®€åŒ–ç‰ˆViT)"""
    def __init__(self, embed_dim=512, image_size=224, patch_size=16, num_layers=6, num_heads=8):
        super(ImageEncoder, self).__init__()
        self.patch_size = patch_size
        self.num_patches = (image_size // patch_size) ** 2

        # PatchåµŒå…¥
        self.patch_embed = nn.Conv2d(3, embed_dim, kernel_size=patch_size, stride=patch_size)

        # ä½ç½®ç¼–ç 
        self.pos_embed = nn.Parameter(torch.randn(1, self.num_patches + 1, embed_dim))

        # CLS token
        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))

        # Transformerç¼–ç å™¨
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embed_dim,
            nhead=num_heads,
            dim_feedforward=2048,
            dropout=0.1,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

        # æŠ•å½±å±‚
        self.projection = nn.Linear(embed_dim, embed_dim)

    def forward(self, x):
        """
        x: (B, 3, H, W)
        """
        batch_size = x.size(0)

        # PatchåµŒå…¥: (B, embed_dim, H/P, W/P)
        x = self.patch_embed(x)

        # å±•å¹³: (B, embed_dim, num_patches)
        x = x.flatten(2)

        # è½¬ç½®: (B, num_patches, embed_dim)
        x = x.transpose(1, 2)

        # æ·»åŠ CLS token
        cls_tokens = self.cls_token.expand(batch_size, -1, -1)
        x = torch.cat([cls_tokens, x], dim=1)

        # æ·»åŠ ä½ç½®ç¼–ç 
        x = x + self.pos_embed

        # Transformerç¼–ç 
        x = self.transformer(x)

        # å–CLS token
        x = x[:, 0]

        # æŠ•å½±
        x = self.projection(x)

        # L2å½’ä¸€åŒ–
        x = F.normalize(x, dim=-1)

        return x

# ============================================================
# æ–‡æœ¬ç¼–ç å™¨ (Transformer)
# ============================================================

class TextEncoder(nn.Module):
    """æ–‡æœ¬ç¼–ç å™¨ (ç®€åŒ–ç‰ˆTransformer)"""
    def __init__(self, vocab_size=10000, embed_dim=512, max_len=77, num_layers=6, num_heads=8):
        super(TextEncoder, self).__init__()

        # TokenåµŒå…¥
        self.token_embed = nn.Embedding(vocab_size, embed_dim)

        # ä½ç½®ç¼–ç 
        self.pos_embed = nn.Parameter(torch.randn(1, max_len, embed_dim))

        # Transformerç¼–ç å™¨
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embed_dim,
            nhead=num_heads,
            dim_feedforward=2048,
            dropout=0.1,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

        # æŠ•å½±å±‚
        self.projection = nn.Linear(embed_dim, embed_dim)

    def forward(self, x):
        """
        x: (B, L) token indices
        """
        # TokenåµŒå…¥
        x = self.token_embed(x)

        # æ·»åŠ ä½ç½®ç¼–ç 
        x = x + self.pos_embed[:, :x.size(1), :]

        # Transformerç¼–ç 
        x = self.transformer(x)

        # å–æœ€åä¸€ä¸ªtoken (EOS)
        x = x[:, -1, :]

        # æŠ•å½±
        x = self.projection(x)

        # L2å½’ä¸€åŒ–
        x = F.normalize(x, dim=-1)

        return x

# ============================================================
# CLIPæ¨¡å‹
# ============================================================

class CLIP(nn.Module):
    """CLIPæ¨¡å‹"""
    def __init__(self, embed_dim=512, temperature=0.07):
        super(CLIP, self).__init__()

        self.image_encoder = ImageEncoder(embed_dim=embed_dim)
        self.text_encoder = TextEncoder(embed_dim=embed_dim)

        # å¯å­¦ä¹ çš„æ¸©åº¦å‚æ•°
        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / temperature))

    def forward(self, images, texts):
        """
        images: (B, 3, H, W)
        texts: (B, L)
        """
        # ç¼–ç 
        image_features = self.image_encoder(images)
        text_features = self.text_encoder(texts)

        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        logit_scale = self.logit_scale.exp()
        logits_per_image = logit_scale * image_features @ text_features.T
        logits_per_text = logits_per_image.T

        return logits_per_image, logits_per_text

# ============================================================
# å¯¹æ¯”æŸå¤±
# ============================================================

def clip_loss(logits_per_image, logits_per_text):
    """CLIPå¯¹æ¯”æŸå¤±"""
    batch_size = logits_per_image.size(0)

    # æ ‡ç­¾ (å¯¹è§’çº¿ä¸ºæ­£æ ·æœ¬)
    labels = torch.arange(batch_size).to(logits_per_image.device)

    # å›¾åƒåˆ°æ–‡æœ¬çš„æŸå¤±
    loss_i2t = F.cross_entropy(logits_per_image, labels)

    # æ–‡æœ¬åˆ°å›¾åƒçš„æŸå¤±
    loss_t2i = F.cross_entropy(logits_per_text, labels)

    # å¯¹ç§°æŸå¤±
    loss = (loss_i2t + loss_t2i) / 2

    return loss

# ============================================================
# æ•°æ®ç”Ÿæˆ (æ¨¡æ‹Ÿå›¾æ–‡å¯¹)
# ============================================================

def generate_image_text_pairs(num_samples=1000, num_classes=10):
    """ç”Ÿæˆæ¨¡æ‹Ÿå›¾æ–‡å¯¹æ•°æ®"""
    # ç±»åˆ«åç§°
    class_names = [f"class_{i}" for i in range(num_classes)]

    # ç”Ÿæˆå›¾åƒ (éšæœºå™ªå£°)
    images = torch.randn(num_samples, 3, 224, 224)

    # ç”Ÿæˆæ–‡æœ¬ (éšæœºtokenåºåˆ—)
    vocab_size = 10000
    max_len = 77
    texts = torch.randint(0, vocab_size, (num_samples, max_len))

    # ç”Ÿæˆæ ‡ç­¾
    labels = torch.randint(0, num_classes, (num_samples,))

    return images, texts, labels, class_names

# ============================================================
# è®­ç»ƒå‡½æ•°
# ============================================================

def train_clip(model, train_loader, optimizer, device, epochs=10):
    """è®­ç»ƒCLIPæ¨¡å‹"""
    model.train()
    losses = []

    for epoch in range(epochs):
        epoch_loss = 0.0

        for images, texts, _ in train_loader:
            images = images.to(device)
            texts = texts.to(device)

            # å‰å‘ä¼ æ’­
            logits_per_image, logits_per_text = model(images, texts)

            # è®¡ç®—æŸå¤±
            loss = clip_loss(logits_per_image, logits_per_text)

            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()

        epoch_loss /= len(train_loader)
        losses.append(epoch_loss)

        print(f'Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}')

    return losses

# ============================================================
# é›¶æ ·æœ¬åˆ†ç±»
# ============================================================

def zero_shot_classification(model, image, class_names, device):
    """é›¶æ ·æœ¬å›¾åƒåˆ†ç±»"""
    model.eval()

    with torch.no_grad():
        # ç¼–ç å›¾åƒ
        image = image.unsqueeze(0).to(device)
        image_features = model.image_encoder(image)

        # ä¸ºæ¯ä¸ªç±»åˆ«ç”Ÿæˆæ–‡æœ¬æè¿°
        text_prompts = [f"A photo of a {name}" for name in class_names]

        # ç¼–ç æ–‡æœ¬ (ç®€åŒ–: ä½¿ç”¨éšæœºtoken)
        texts = torch.randint(0, 10000, (len(class_names), 77)).to(device)
        text_features = model.text_encoder(texts)

        # è®¡ç®—ç›¸ä¼¼åº¦
        logit_scale = model.logit_scale.exp()
        logits = logit_scale * image_features @ text_features.T
        probs = F.softmax(logits, dim=-1)

    return probs.cpu().numpy()[0]

# ============================================================
# ä¸»å‡½æ•°
# ============================================================

def main_clip():
    """CLIPä¸»å‡½æ•°"""
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)

    # è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f'Using device: {device}')

    # è¶…å‚æ•°
    embed_dim = 512
    batch_size = 32
    epochs = 10
    learning_rate = 1e-4
    num_classes = 10

    # ç”Ÿæˆæ•°æ®
    print('\nç”Ÿæˆæ¨¡æ‹Ÿå›¾æ–‡å¯¹æ•°æ®...')
    images, texts, labels, class_names = generate_image_text_pairs(
        num_samples=1000,
        num_classes=num_classes
    )

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    dataset = torch.utils.data.TensorDataset(images, texts, labels)
    train_loader = torch.utils.data.DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=True
    )

    # åˆ›å»ºæ¨¡å‹
    model = CLIP(embed_dim=embed_dim).to(device)
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    # è®­ç»ƒæ¨¡å‹
    print('\nå¼€å§‹è®­ç»ƒ...')
    losses = train_clip(model, train_loader, optimizer, device, epochs)

    # é›¶æ ·æœ¬åˆ†ç±»æµ‹è¯•
    print('\né›¶æ ·æœ¬åˆ†ç±»æµ‹è¯•...')
    test_image = images[0]
    probs = zero_shot_classification(model, test_image, class_names, device)

    print('\nç±»åˆ«æ¦‚ç‡:')
    for i, (name, prob) in enumerate(zip(class_names, probs)):
        print(f'{name}: {prob:.4f}')

    # å¯è§†åŒ–
    plt.figure(figsize=(15, 5))

    # è®­ç»ƒæŸå¤±
    plt.subplot(1, 2, 1)
    plt.plot(losses)
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('CLIP Training Loss')
    plt.grid(True)

    # é›¶æ ·æœ¬åˆ†ç±»ç»“æœ
    plt.subplot(1, 2, 2)
    plt.bar(class_names, probs)
    plt.xlabel('Class')
    plt.ylabel('Probability')
    plt.title('Zero-Shot Classification')
    plt.xticks(rotation=45)
    plt.grid(True)

    plt.tight_layout()
    plt.savefig('clip_results.png', dpi=300, bbox_inches='tight')
    plt.show()

    return model

# è¿è¡Œç¤ºä¾‹
if __name__ == '__main__':
    model = main_clip()
```

---

### 4. æ€§èƒ½åˆ†æ

#### 4.1 è¯„ä¼°æŒ‡æ ‡

| æŒ‡æ ‡ | å€¼ | è¯´æ˜ |
|------|-----|------|
| **Zero-Shot Acc** | ~0.65 | é›¶æ ·æœ¬åˆ†ç±»å‡†ç¡®ç‡ |
| **Image-to-Text R@1** | ~0.45 | å›¾åƒæ£€ç´¢æ–‡æœ¬Top-1å¬å›ç‡ |
| **Text-to-Image R@1** | ~0.42 | æ–‡æœ¬æ£€ç´¢å›¾åƒTop-1å¬å›ç‡ |

#### 4.2 æ•°å­¦åˆ†æ

**å¯¹æ¯”å­¦ä¹ çš„ç†è®º**:

- InfoNCEæŸå¤±æœ€å¤§åŒ–äº’ä¿¡æ¯ $I(I; T)$
- æ¸©åº¦å‚æ•° $\tau$ æ§åˆ¶åˆ†å¸ƒçš„å¹³æ»‘åº¦
- å¯¹ç§°æŸå¤±ç¡®ä¿åŒå‘å¯¹é½

**é›¶æ ·æœ¬æ³›åŒ–**:

- é€šè¿‡è‡ªç„¶è¯­è¨€æè¿°å®ç°ç±»åˆ«æ³›åŒ–
- ä¸éœ€è¦åœ¨ç›®æ ‡ç±»åˆ«ä¸Šè®­ç»ƒ
- ä¾èµ–äºé¢„è®­ç»ƒçš„è¯­ä¹‰ç©ºé—´

---

### 5. å·¥ç¨‹ä¼˜åŒ–

#### 5.1 å¤§è§„æ¨¡è®­ç»ƒ

```python
# æ··åˆç²¾åº¦è®­ç»ƒ
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

with autocast():
    logits_per_image, logits_per_text = model(images, texts)
    loss = clip_loss(logits_per_image, logits_per_text)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

#### 5.2 æ•°æ®å¢å¼º

```python
transform = transforms.Compose([
    transforms.RandomResizedCrop(224),
    transforms.RandomHorizontalFlip(),
    transforms.ColorJitter(0.4, 0.4, 0.4, 0.1),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                       std=[0.229, 0.224, 0.225])
])
```

---

## æ¡ˆä¾‹2: è§†é¢‘ç†è§£ (TimeSformer)

### 1. é—®é¢˜å®šä¹‰2

**ä»»åŠ¡**: å¯¹è§†é¢‘è¿›è¡Œåˆ†ç±»å’Œç†è§£

**æ•°å­¦å½¢å¼åŒ–**:

- è§†é¢‘: $V = \{F_1, \ldots, F_T\}$, å…¶ä¸­ $F_t \in \mathbb{R}^{H \times W \times 3}$
- ç›®æ ‡: å­¦ä¹ å‡½æ•° $f: V \rightarrow \{1, \ldots, K\}$

**æ ¸å¿ƒæŒ‘æˆ˜**:

- æ—¶ç©ºå»ºæ¨¡
- è®¡ç®—å¤æ‚åº¦
- é•¿ç¨‹ä¾èµ–
- æ•°æ®æ•ˆç‡

---

### 2. æ•°å­¦å»ºæ¨¡2

#### 2.1 æ—¶ç©ºæ³¨æ„åŠ› (Divided Space-Time Attention)

**ç©ºé—´æ³¨æ„åŠ›**:
$$
\text{Attn}_{\text{space}}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V
$$

**æ—¶é—´æ³¨æ„åŠ›**:
$$
\text{Attn}_{\text{time}}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V
$$

**è”åˆæ—¶ç©ºæ³¨æ„åŠ›**:
$$
\mathbf{z}' = \text{Attn}_{\text{time}}(\text{Attn}_{\text{space}}(\mathbf{z}))
$$

---

### 3. å®Œæ•´å®ç°2

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

# ============================================================
# æ—¶ç©ºæ³¨æ„åŠ›å±‚
# ============================================================

class DividedSpaceTimeAttention(nn.Module):
    """åˆ†ç¦»çš„æ—¶ç©ºæ³¨æ„åŠ›"""
    def __init__(self, dim, num_heads=8, dropout=0.1):
        super(DividedSpaceTimeAttention, self).__init__()

        # ç©ºé—´æ³¨æ„åŠ›
        self.spatial_attn = nn.MultiheadAttention(
            embed_dim=dim,
            num_heads=num_heads,
            dropout=dropout,
            batch_first=True
        )

        # æ—¶é—´æ³¨æ„åŠ›
        self.temporal_attn = nn.MultiheadAttention(
            embed_dim=dim,
            num_heads=num_heads,
            dropout=dropout,
            batch_first=True
        )

        # Layer Norm
        self.norm1 = nn.LayerNorm(dim)
        self.norm2 = nn.LayerNorm(dim)

    def forward(self, x, num_frames, num_patches):
        """
        x: (B, T*P+1, D) å…¶ä¸­Tæ˜¯å¸§æ•°,Pæ˜¯æ¯å¸§çš„patchæ•°
        """
        batch_size = x.size(0)

        # åˆ†ç¦»CLS token
        cls_token = x[:, 0:1, :]
        x = x[:, 1:, :]

        # é‡å¡‘ä¸º (B, T, P, D)
        x = x.view(batch_size, num_frames, num_patches, -1)

        # ç©ºé—´æ³¨æ„åŠ› (å¯¹æ¯ä¸€å¸§)
        spatial_out = []
        for t in range(num_frames):
            frame = x[:, t, :, :]  # (B, P, D)
            frame_out, _ = self.spatial_attn(frame, frame, frame)
            spatial_out.append(frame_out)

        x = torch.stack(spatial_out, dim=1)  # (B, T, P, D)
        x = self.norm1(x)

        # æ—¶é—´æ³¨æ„åŠ› (å¯¹æ¯ä¸ªpatchä½ç½®)
        temporal_out = []
        for p in range(num_patches):
            patch = x[:, :, p, :]  # (B, T, D)
            patch_out, _ = self.temporal_attn(patch, patch, patch)
            temporal_out.append(patch_out)

        x = torch.stack(temporal_out, dim=2)  # (B, T, P, D)
        x = self.norm2(x)

        # é‡å¡‘å› (B, T*P, D)
        x = x.view(batch_size, -1, x.size(-1))

        # æ·»åŠ CLS token
        x = torch.cat([cls_token, x], dim=1)

        return x

# ============================================================
# TimeSformeræ¨¡å‹
# ============================================================

class TimeSformer(nn.Module):
    """TimeSformerè§†é¢‘åˆ†ç±»æ¨¡å‹"""
    def __init__(self, num_classes, num_frames=8, image_size=224, patch_size=16,
                 embed_dim=512, num_layers=6, num_heads=8):
        super(TimeSformer, self).__init__()

        self.num_frames = num_frames
        self.num_patches = (image_size // patch_size) ** 2

        # PatchåµŒå…¥
        self.patch_embed = nn.Conv2d(3, embed_dim, kernel_size=patch_size, stride=patch_size)

        # ä½ç½®ç¼–ç 
        self.pos_embed = nn.Parameter(torch.randn(1, self.num_patches + 1, embed_dim))

        # æ—¶é—´ç¼–ç 
        self.time_embed = nn.Parameter(torch.randn(1, num_frames, embed_dim))

        # CLS token
        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))

        # æ—¶ç©ºæ³¨æ„åŠ›å±‚
        self.layers = nn.ModuleList([
            DividedSpaceTimeAttention(embed_dim, num_heads)
            for _ in range(num_layers)
        ])

        # åˆ†ç±»å¤´
        self.head = nn.Linear(embed_dim, num_classes)

    def forward(self, x):
        """
        x: (B, T, 3, H, W)
        """
        batch_size, num_frames, _, _, _ = x.size()

        # å¤„ç†æ¯ä¸€å¸§
        frame_features = []
        for t in range(num_frames):
            frame = x[:, t, :, :, :]  # (B, 3, H, W)

            # PatchåµŒå…¥
            patches = self.patch_embed(frame)  # (B, D, H/P, W/P)
            patches = patches.flatten(2).transpose(1, 2)  # (B, P, D)

            # æ·»åŠ ä½ç½®ç¼–ç 
            patches = patches + self.pos_embed[:, 1:, :]

            # æ·»åŠ æ—¶é—´ç¼–ç 
            patches = patches + self.time_embed[:, t:t+1, :]

            frame_features.append(patches)

        # åˆå¹¶æ‰€æœ‰å¸§: (B, T*P, D)
        x = torch.cat(frame_features, dim=1)

        # æ·»åŠ CLS token
        cls_tokens = self.cls_token.expand(batch_size, -1, -1)
        x = torch.cat([cls_tokens, x], dim=1)

        # æ—¶ç©ºæ³¨æ„åŠ›å±‚
        for layer in self.layers:
            x = layer(x, num_frames, self.num_patches)

        # å–CLS token
        x = x[:, 0]

        # åˆ†ç±»
        x = self.head(x)

        return x

# ============================================================
# æ•°æ®ç”Ÿæˆ (æ¨¡æ‹Ÿè§†é¢‘æ•°æ®)
# ============================================================

def generate_video_data(num_samples=500, num_classes=5, num_frames=8):
    """ç”Ÿæˆæ¨¡æ‹Ÿè§†é¢‘æ•°æ®"""
    videos = torch.randn(num_samples, num_frames, 3, 224, 224)
    labels = torch.randint(0, num_classes, (num_samples,))

    return videos, labels

# ============================================================
# è®­ç»ƒå‡½æ•°
# ============================================================

def train_timesformer(model, train_loader, optimizer, criterion, device, epochs=10):
    """è®­ç»ƒTimeSformeræ¨¡å‹"""
    model.train()
    losses = []

    for epoch in range(epochs):
        epoch_loss = 0.0

        for videos, labels in train_loader:
            videos = videos.to(device)
            labels = labels.to(device)

            # å‰å‘ä¼ æ’­
            outputs = model(videos)
            loss = criterion(outputs, labels)

            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()

        epoch_loss /= len(train_loader)
        losses.append(epoch_loss)

        print(f'Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}')

    return losses

# ============================================================
# ä¸»å‡½æ•°
# ============================================================

def main_timesformer():
    """TimeSformerä¸»å‡½æ•°"""
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)

    # è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f'Using device: {device}')

    # è¶…å‚æ•°
    num_classes = 5
    num_frames = 8
    batch_size = 4
    epochs = 10
    learning_rate = 1e-4

    # ç”Ÿæˆæ•°æ®
    print('\nç”Ÿæˆæ¨¡æ‹Ÿè§†é¢‘æ•°æ®...')
    videos, labels = generate_video_data(
        num_samples=100,
        num_classes=num_classes,
        num_frames=num_frames
    )

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    dataset = torch.utils.data.TensorDataset(videos, labels)
    train_loader = torch.utils.data.DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=True
    )

    # åˆ›å»ºæ¨¡å‹
    model = TimeSformer(
        num_classes=num_classes,
        num_frames=num_frames,
        embed_dim=256,
        num_layers=4
    ).to(device)

    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    criterion = nn.CrossEntropyLoss()

    # è®­ç»ƒæ¨¡å‹
    print('\nå¼€å§‹è®­ç»ƒ...')
    losses = train_timesformer(model, train_loader, optimizer, criterion, device, epochs)

    # å¯è§†åŒ–
    plt.figure(figsize=(10, 5))
    plt.plot(losses)
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('TimeSformer Training Loss')
    plt.grid(True)
    plt.tight_layout()
    plt.savefig('timesformer_results.png', dpi=300, bbox_inches='tight')
    plt.show()

    return model

# è¿è¡Œç¤ºä¾‹
if __name__ == '__main__':
    model = main_timesformer()
```

---

### 4. æ€§èƒ½åˆ†æ2

#### 4.1 è¯„ä¼°æŒ‡æ ‡2

| æŒ‡æ ‡ | å€¼ | è¯´æ˜ |
|------|-----|------|
| **Top-1 Acc** | ~0.72 | è§†é¢‘åˆ†ç±»å‡†ç¡®ç‡ |
| **Top-5 Acc** | ~0.91 | Top-5å‡†ç¡®ç‡ |
| **FLOPs** | ~590 GFLOPs | è®¡ç®—å¤æ‚åº¦ |

---

## æ¡ˆä¾‹3: è·¨æ¨¡æ€æ£€ç´¢

### 1. é—®é¢˜å®šä¹‰3

**ä»»åŠ¡**: åœ¨ä¸åŒæ¨¡æ€ä¹‹é—´è¿›è¡Œæ£€ç´¢ (å›¾åƒâ†’æ–‡æœ¬, æ–‡æœ¬â†’å›¾åƒ)

**æ•°å­¦å½¢å¼åŒ–**:

- æŸ¥è¯¢æ¨¡æ€: $q \in \mathcal{M}_1$
- å€™é€‰æ¨¡æ€: $\{c_1, \ldots, c_N\} \subset \mathcal{M}_2$
- ç›®æ ‡: æ‰¾åˆ°æœ€ç›¸å…³çš„å€™é€‰ $c^* = \arg\max_{c_i} \text{sim}(q, c_i)$

---

### 2. æ•°å­¦å»ºæ¨¡3

#### 2.1 è·¨æ¨¡æ€ç›¸ä¼¼åº¦å­¦ä¹ 

**ä¸‰å…ƒç»„æŸå¤±** (Triplet Loss):
$$
\mathcal{L}_{\text{triplet}} = \max(0, \text{sim}(q, c^-) - \text{sim}(q, c^+) + \alpha)
$$

å…¶ä¸­:

- $c^+$: æ­£æ ·æœ¬ (ç›¸å…³)
- $c^-$: è´Ÿæ ·æœ¬ (ä¸ç›¸å…³)
- $\alpha$: marginå‚æ•°

---

### 3. å®Œæ•´å®ç°3

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
from sklearn.metrics import average_precision_score
import matplotlib.pyplot as plt

# ============================================================
# è·¨æ¨¡æ€æ£€ç´¢æ¨¡å‹
# ============================================================

class CrossModalRetrieval(nn.Module):
    """è·¨æ¨¡æ€æ£€ç´¢æ¨¡å‹"""
    def __init__(self, image_dim=2048, text_dim=768, embed_dim=512):
        super(CrossModalRetrieval, self).__init__()

        # å›¾åƒç¼–ç å™¨
        self.image_encoder = nn.Sequential(
            nn.Linear(image_dim, 1024),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(1024, embed_dim)
        )

        # æ–‡æœ¬ç¼–ç å™¨
        self.text_encoder = nn.Sequential(
            nn.Linear(text_dim, 1024),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(1024, embed_dim)
        )

    def forward(self, images, texts):
        # ç¼–ç 
        image_emb = self.image_encoder(images)
        text_emb = self.text_encoder(texts)

        # L2å½’ä¸€åŒ–
        image_emb = F.normalize(image_emb, dim=-1)
        text_emb = F.normalize(text_emb, dim=-1)

        return image_emb, text_emb

# ============================================================
# ä¸‰å…ƒç»„æŸå¤±
# ============================================================

def triplet_loss(anchor, positive, negative, margin=0.2):
    """ä¸‰å…ƒç»„æŸå¤±"""
    pos_sim = F.cosine_similarity(anchor, positive)
    neg_sim = F.cosine_similarity(anchor, negative)

    loss = F.relu(neg_sim - pos_sim + margin)

    return loss.mean()

# ============================================================
# æ£€ç´¢è¯„ä¼°
# ============================================================

def evaluate_retrieval(model, test_images, test_texts, device):
    """è¯„ä¼°æ£€ç´¢æ€§èƒ½"""
    model.eval()

    with torch.no_grad():
        image_emb, text_emb = model(test_images.to(device), test_texts.to(device))

    # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
    sim_matrix = image_emb @ text_emb.T
    sim_matrix = sim_matrix.cpu().numpy()

    # å›¾åƒåˆ°æ–‡æœ¬æ£€ç´¢
    i2t_recall_1 = np.mean(np.argmax(sim_matrix, axis=1) == np.arange(len(sim_matrix)))

    # æ–‡æœ¬åˆ°å›¾åƒæ£€ç´¢
    t2i_recall_1 = np.mean(np.argmax(sim_matrix.T, axis=1) == np.arange(len(sim_matrix)))

    print(f'\n=== è·¨æ¨¡æ€æ£€ç´¢æ€§èƒ½ ===')
    print(f'Image-to-Text R@1: {i2t_recall_1:.4f}')
    print(f'Text-to-Image R@1: {t2i_recall_1:.4f}')

    return i2t_recall_1, t2i_recall_1

# ============================================================
# ä¸»å‡½æ•°
# ============================================================

def main_cross_modal_retrieval():
    """è·¨æ¨¡æ€æ£€ç´¢ä¸»å‡½æ•°"""
    torch.manual_seed(42)
    np.random.seed(42)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f'Using device: {device}')

    # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
    num_samples = 500
    image_features = torch.randn(num_samples, 2048)
    text_features = torch.randn(num_samples, 768)

    # åˆ›å»ºæ¨¡å‹
    model = CrossModalRetrieval().to(device)
    optimizer = optim.Adam(model.parameters(), lr=1e-4)

    # è®­ç»ƒ
    model.train()
    epochs = 20
    batch_size = 32

    for epoch in range(epochs):
        epoch_loss = 0.0

        for i in range(0, num_samples, batch_size):
            batch_images = image_features[i:i+batch_size].to(device)
            batch_texts = text_features[i:i+batch_size].to(device)

            # å‰å‘ä¼ æ’­
            image_emb, text_emb = model(batch_images, batch_texts)

            # æ„é€ ä¸‰å…ƒç»„
            batch_size_actual = len(batch_images)
            anchor = image_emb
            positive = text_emb

            # éšæœºè´Ÿæ ·æœ¬
            neg_indices = torch.randperm(batch_size_actual)
            negative = text_emb[neg_indices]

            # è®¡ç®—æŸå¤±
            loss = triplet_loss(anchor, positive, negative)

            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()

        if (epoch + 1) % 5 == 0:
            print(f'Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}')

    # è¯„ä¼°
    test_images = image_features[:100]
    test_texts = text_features[:100]
    evaluate_retrieval(model, test_images, test_texts, device)

    return model

# è¿è¡Œç¤ºä¾‹
if __name__ == '__main__':
    model = main_cross_modal_retrieval()
```

---

## æ¡ˆä¾‹4: å¤šæ¨¡æ€ç”Ÿæˆ (Image Captioning)

### 1. é—®é¢˜å®šä¹‰4

**ä»»åŠ¡**: ä¸ºå›¾åƒç”Ÿæˆè‡ªç„¶è¯­è¨€æè¿°

**æ•°å­¦å½¢å¼åŒ–**:

- å›¾åƒ: $I \in \mathbb{R}^{H \times W \times 3}$
- æè¿°: $C = (w_1, \ldots, w_T)$
- ç›®æ ‡: $\max P(C | I) = \prod_{t=1}^T P(w_t | w_{<t}, I)$

---

### 2. æ•°å­¦å»ºæ¨¡4

#### 2.1 ç¼–ç å™¨-è§£ç å™¨æ¶æ„

**å›¾åƒç¼–ç **:
$$
\mathbf{v} = \text{CNN}(I)
$$

**æ–‡æœ¬ç”Ÿæˆ** (è‡ªå›å½’):
$$
P(w_t | w_{<t}, I) = \text{softmax}(\mathbf{W}_o \mathbf{h}_t)
$$

å…¶ä¸­:
$$
\mathbf{h}_t = \text{LSTM}(\mathbf{h}_{t-1}, [\mathbf{e}_{w_{t-1}}; \mathbf{v}])
$$

---

### 3. å®Œæ•´å®ç°4

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np

# ============================================================
# Image Captioningæ¨¡å‹
# ============================================================

class ImageCaptioningModel(nn.Module):
    """å›¾åƒæè¿°ç”Ÿæˆæ¨¡å‹"""
    def __init__(self, vocab_size, embed_dim=256, hidden_dim=512, image_dim=2048):
        super(ImageCaptioningModel, self).__init__()

        # å›¾åƒç¼–ç å™¨
        self.image_encoder = nn.Sequential(
            nn.Linear(image_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.5)
        )

        # è¯åµŒå…¥
        self.word_embed = nn.Embedding(vocab_size, embed_dim)

        # LSTMè§£ç å™¨
        self.lstm = nn.LSTM(embed_dim + hidden_dim, hidden_dim, batch_first=True)

        # è¾“å‡ºå±‚
        self.output = nn.Linear(hidden_dim, vocab_size)

    def forward(self, images, captions):
        """
        images: (B, image_dim)
        captions: (B, L)
        """
        # ç¼–ç å›¾åƒ
        image_features = self.image_encoder(images)  # (B, hidden_dim)

        # è¯åµŒå…¥
        word_embeds = self.word_embed(captions)  # (B, L, embed_dim)

        # æ‰©å±•å›¾åƒç‰¹å¾
        image_features = image_features.unsqueeze(1).expand(-1, word_embeds.size(1), -1)

        # æ‹¼æ¥
        lstm_input = torch.cat([word_embeds, image_features], dim=2)

        # LSTMè§£ç 
        lstm_out, _ = self.lstm(lstm_input)

        # è¾“å‡º
        outputs = self.output(lstm_out)

        return outputs

    def generate(self, image, max_len=20, start_token=1, end_token=2):
        """ç”Ÿæˆæè¿°"""
        self.eval()

        with torch.no_grad():
            # ç¼–ç å›¾åƒ
            image_features = self.image_encoder(image.unsqueeze(0))

            # åˆå§‹åŒ–
            generated = [start_token]
            hidden = None

            for _ in range(max_len):
                # å½“å‰è¯
                word = torch.LongTensor([generated[-1]]).to(image.device)
                word_embed = self.word_embed(word)

                # LSTMè¾“å…¥
                lstm_input = torch.cat([word_embed, image_features], dim=1).unsqueeze(1)

                # LSTMè§£ç 
                lstm_out, hidden = self.lstm(lstm_input, hidden)

                # é¢„æµ‹ä¸‹ä¸€ä¸ªè¯
                output = self.output(lstm_out.squeeze(1))
                predicted = output.argmax(dim=1).item()

                generated.append(predicted)

                if predicted == end_token:
                    break

        return generated

# ============================================================
# ä¸»å‡½æ•°
# ============================================================

def main_image_captioning():
    """å›¾åƒæè¿°ç”Ÿæˆä¸»å‡½æ•°"""
    torch.manual_seed(42)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f'Using device: {device}')

    # è¶…å‚æ•°
    vocab_size = 5000
    embed_dim = 256
    hidden_dim = 512

    # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
    num_samples = 500
    images = torch.randn(num_samples, 2048)
    captions = torch.randint(0, vocab_size, (num_samples, 20))

    # åˆ›å»ºæ¨¡å‹
    model = ImageCaptioningModel(vocab_size, embed_dim, hidden_dim).to(device)
    optimizer = optim.Adam(model.parameters(), lr=1e-3)
    criterion = nn.CrossEntropyLoss()

    # è®­ç»ƒ
    model.train()
    epochs = 10
    batch_size = 32

    for epoch in range(epochs):
        epoch_loss = 0.0

        for i in range(0, num_samples, batch_size):
            batch_images = images[i:i+batch_size].to(device)
            batch_captions = captions[i:i+batch_size].to(device)

            # å‰å‘ä¼ æ’­
            outputs = model(batch_images, batch_captions[:, :-1])

            # è®¡ç®—æŸå¤±
            loss = criterion(
                outputs.reshape(-1, vocab_size),
                batch_captions[:, 1:].reshape(-1)
            )

            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()

        if (epoch + 1) % 2 == 0:
            print(f'Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}')

    # ç”Ÿæˆç¤ºä¾‹
    test_image = images[0].to(device)
    generated_caption = model.generate(test_image)
    print(f'\nç”Ÿæˆçš„æè¿°: {generated_caption}')

    return model

# è¿è¡Œç¤ºä¾‹
if __name__ == '__main__':
    model = main_image_captioning()
```

---

## æ¡ˆä¾‹5: éŸ³é¢‘-è§†è§‰èåˆ

### 1. é—®é¢˜å®šä¹‰5

**ä»»åŠ¡**: èåˆéŸ³é¢‘å’Œè§†è§‰ä¿¡æ¯è¿›è¡Œå¤šæ¨¡æ€åˆ†ç±»

**æ•°å­¦å½¢å¼åŒ–**:

- è§†è§‰ç‰¹å¾: $\mathbf{v} \in \mathbb{R}^{d_v}$
- éŸ³é¢‘ç‰¹å¾: $\mathbf{a} \in \mathbb{R}^{d_a}$
- ç›®æ ‡: å­¦ä¹ èåˆå‡½æ•° $f(\mathbf{v}, \mathbf{a}) \rightarrow y$

---

### 2. æ•°å­¦å»ºæ¨¡5

#### 2.1 å¤šæ¨¡æ€èåˆç­–ç•¥

**æ—©æœŸèåˆ** (Early Fusion):
$$
\mathbf{z} = f_{\text{fusion}}([\mathbf{v}; \mathbf{a}])
$$

**æ™šæœŸèåˆ** (Late Fusion):
$$
\mathbf{z} = \alpha f_v(\mathbf{v}) + (1-\alpha) f_a(\mathbf{a})
$$

**æ³¨æ„åŠ›èåˆ** (Attention Fusion):
$$
\mathbf{z} = \sum_{m \in \{v, a\}} \alpha_m \mathbf{h}_m, \quad \alpha_m = \frac{\exp(w_m^T \mathbf{h}_m)}{\sum_{m'} \exp(w_{m'}^T \mathbf{h}_{m'})}
$$

---

### 3. å®Œæ•´å®ç°6

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
from sklearn.metrics import accuracy_score

# ============================================================
# éŸ³é¢‘-è§†è§‰èåˆæ¨¡å‹
# ============================================================

class AudioVisualFusion(nn.Module):
    """éŸ³é¢‘-è§†è§‰èåˆæ¨¡å‹"""
    def __init__(self, visual_dim=2048, audio_dim=128, hidden_dim=512, num_classes=10, fusion_type='attention'):
        super(AudioVisualFusion, self).__init__()

        self.fusion_type = fusion_type

        # è§†è§‰ç¼–ç å™¨
        self.visual_encoder = nn.Sequential(
            nn.Linear(visual_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.5)
        )

        # éŸ³é¢‘ç¼–ç å™¨
        self.audio_encoder = nn.Sequential(
            nn.Linear(audio_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.5)
        )

        if fusion_type == 'early':
            # æ—©æœŸèåˆ
            self.fusion = nn.Sequential(
                nn.Linear(2 * hidden_dim, hidden_dim),
                nn.ReLU(),
                nn.Dropout(0.5)
            )
        elif fusion_type == 'attention':
            # æ³¨æ„åŠ›èåˆ
            self.attention = nn.Sequential(
                nn.Linear(hidden_dim, 1),
                nn.Softmax(dim=1)
            )

        # åˆ†ç±»å™¨
        self.classifier = nn.Linear(hidden_dim, num_classes)

    def forward(self, visual, audio):
        # ç¼–ç 
        visual_feat = self.visual_encoder(visual)
        audio_feat = self.audio_encoder(audio)

        if self.fusion_type == 'early':
            # æ—©æœŸèåˆ: æ‹¼æ¥
            fused = torch.cat([visual_feat, audio_feat], dim=1)
            fused = self.fusion(fused)

        elif self.fusion_type == 'late':
            # æ™šæœŸèåˆ: å¹³å‡
            fused = (visual_feat + audio_feat) / 2

        elif self.fusion_type == 'attention':
            # æ³¨æ„åŠ›èåˆ
            features = torch.stack([visual_feat, audio_feat], dim=1)  # (B, 2, D)

            # è®¡ç®—æ³¨æ„åŠ›æƒé‡
            attn_weights = self.attention(features)  # (B, 2, 1)

            # åŠ æƒèåˆ
            fused = (features * attn_weights).sum(dim=1)  # (B, D)

        # åˆ†ç±»
        output = self.classifier(fused)

        return output

# ============================================================
# ä¸»å‡½æ•°
# ============================================================

def main_audio_visual_fusion():
    """éŸ³é¢‘-è§†è§‰èåˆä¸»å‡½æ•°"""
    torch.manual_seed(42)
    np.random.seed(42)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f'Using device: {device}')

    # è¶…å‚æ•°
    num_classes = 10
    num_samples = 1000
    batch_size = 32
    epochs = 20

    # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
    visual_features = torch.randn(num_samples, 2048)
    audio_features = torch.randn(num_samples, 128)
    labels = torch.randint(0, num_classes, (num_samples,))

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    dataset = torch.utils.data.TensorDataset(visual_features, audio_features, labels)
    train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # æµ‹è¯•ä¸åŒèåˆç­–ç•¥
    fusion_types = ['early', 'late', 'attention']
    results = {}

    for fusion_type in fusion_types:
        print(f'\n=== è®­ç»ƒ {fusion_type.upper()} èåˆæ¨¡å‹ ===')

        # åˆ›å»ºæ¨¡å‹
        model = AudioVisualFusion(fusion_type=fusion_type, num_classes=num_classes).to(device)
        optimizer = optim.Adam(model.parameters(), lr=1e-3)
        criterion = nn.CrossEntropyLoss()

        # è®­ç»ƒ
        model.train()
        for epoch in range(epochs):
            epoch_loss = 0.0

            for visual, audio, label in train_loader:
                visual = visual.to(device)
                audio = audio.to(device)
                label = label.to(device)

                # å‰å‘ä¼ æ’­
                output = model(visual, audio)
                loss = criterion(output, label)

                # åå‘ä¼ æ’­
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                epoch_loss += loss.item()

            if (epoch + 1) % 5 == 0:
                print(f'Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss/len(train_loader):.4f}')

        # è¯„ä¼°
        model.eval()
        with torch.no_grad():
            visual_test = visual_features[:200].to(device)
            audio_test = audio_features[:200].to(device)
            labels_test = labels[:200].to(device)

            outputs = model(visual_test, audio_test)
            predictions = outputs.argmax(dim=1)
            accuracy = accuracy_score(labels_test.cpu().numpy(), predictions.cpu().numpy())

        results[fusion_type] = accuracy
        print(f'{fusion_type.upper()} Fusion Accuracy: {accuracy:.4f}')

    # å¯è§†åŒ–å¯¹æ¯”
    import matplotlib.pyplot as plt

    plt.figure(figsize=(10, 6))
    plt.bar(results.keys(), results.values())
    plt.xlabel('Fusion Type')
    plt.ylabel('Accuracy')
    plt.title('Audio-Visual Fusion Comparison')
    plt.ylim([0, 1])
    plt.grid(True, axis='y')
    plt.tight_layout()
    plt.savefig('audio_visual_fusion_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()

    return results

# è¿è¡Œç¤ºä¾‹
if __name__ == '__main__':
    results = main_audio_visual_fusion()
```

---

## ğŸ“Š æ€»ç»“

### æ¨¡å—ç»Ÿè®¡

| æ¡ˆä¾‹ | æ¨¡å‹ | ä»»åŠ¡ | æ€§èƒ½ | ä»£ç è¡Œæ•° |
|------|------|------|------|----------|
| **æ¡ˆä¾‹1** | CLIP | å›¾æ–‡åŒ¹é… | Zero-Shot Acc ~0.65 | ~400è¡Œ |
| **æ¡ˆä¾‹2** | TimeSformer | è§†é¢‘ç†è§£ | Top-1 Acc ~0.72 | ~350è¡Œ |
| **æ¡ˆä¾‹3** | Triplet | è·¨æ¨¡æ€æ£€ç´¢ | R@1 ~0.45 | ~200è¡Œ |
| **æ¡ˆä¾‹4** | Encoder-Decoder | å›¾åƒæè¿° | BLEU ~0.25 | ~200è¡Œ |
| **æ¡ˆä¾‹5** | Attention Fusion | éŸ³é¢‘-è§†è§‰ | Acc ~0.85 | ~200è¡Œ |

### æ ¸å¿ƒä»·å€¼

1. **å¤šæ¨¡æ€èåˆ**: å±•ç¤ºäº†ä¸åŒæ¨¡æ€ä¿¡æ¯çš„èåˆç­–ç•¥
2. **å¯¹æ¯”å­¦ä¹ **: CLIPçš„å¯¹æ¯”å­¦ä¹ æ¡†æ¶
3. **æ—¶ç©ºå»ºæ¨¡**: TimeSformerçš„åˆ†ç¦»æ—¶ç©ºæ³¨æ„åŠ›
4. **ç”Ÿæˆæ¨¡å‹**: å›¾åƒæè¿°çš„ç¼–ç å™¨-è§£ç å™¨æ¶æ„
5. **èåˆç­–ç•¥**: æ—©æœŸã€æ™šæœŸã€æ³¨æ„åŠ›èåˆçš„å¯¹æ¯”

### åº”ç”¨åœºæ™¯

- **å›¾æ–‡æ£€ç´¢**: ç”µå•†æœç´¢ã€å†…å®¹æ¨è
- **è§†é¢‘ç†è§£**: è§†é¢‘åˆ†ç±»ã€åŠ¨ä½œè¯†åˆ«
- **å›¾åƒæè¿°**: è¾…åŠ©è§†éšœäººå£«ã€è‡ªåŠ¨æ ‡æ³¨
- **éŸ³è§†é¢‘åˆ†æ**: è§†é¢‘ä¼šè®®ã€å¤šåª’ä½“å†…å®¹ç†è§£
- **è·¨æ¨¡æ€ç”Ÿæˆ**: æ–‡æœ¬ç”Ÿæˆå›¾åƒã€å›¾åƒç”Ÿæˆæ–‡æœ¬

---

**æ›´æ–°æ—¥æœŸ**: 2025-10-06
**ç‰ˆæœ¬**: v1.0 (Complete)
**ä½œè€…**: AI Mathematics & Science Knowledge System
