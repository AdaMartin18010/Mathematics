# 04-人工智能与机器学习 | Artificial Intelligence & Machine Learning

## 目录

- [04-人工智能与机器学习 | Artificial Intelligence \& Machine Learning](#04-人工智能与机器学习--artificial-intelligence--machine-learning)
  - [目录](#目录)
  - [1. 主题简介 | Topic Introduction](#1-主题简介--topic-introduction)
  - [2. 机器学习理论基础 | Theoretical Foundations of Machine Learning](#2-机器学习理论基础--theoretical-foundations-of-machine-learning)
    - [2.1 学习理论 | Learning Theory](#21-学习理论--learning-theory)
    - [2.2 统计学习理论 | Statistical Learning Theory](#22-统计学习理论--statistical-learning-theory)
    - [2.3 计算学习理论 | Computational Learning Theory](#23-计算学习理论--computational-learning-theory)
  - [3. 神经网络与深度学习 | Neural Networks \& Deep Learning](#3-神经网络与深度学习--neural-networks--deep-learning)
    - [3.1 人工神经网络 | Artificial Neural Networks](#31-人工神经网络--artificial-neural-networks)
    - [3.2 反向传播算法 | Backpropagation Algorithm](#32-反向传播算法--backpropagation-algorithm)
    - [3.3 深度学习架构 | Deep Learning Architectures](#33-深度学习架构--deep-learning-architectures)
  - [4. 优化理论与算法 | Optimization Theory \& Algorithms](#4-优化理论与算法--optimization-theory--algorithms)
    - [4.1 梯度下降方法 | Gradient Descent Methods](#41-梯度下降方法--gradient-descent-methods)
    - [4.2 随机优化 | Stochastic Optimization](#42-随机优化--stochastic-optimization)
    - [4.3 正则化技术 | Regularization Techniques](#43-正则化技术--regularization-techniques)
  - [5. 强化学习 | Reinforcement Learning](#5-强化学习--reinforcement-learning)
    - [5.1 马尔可夫决策过程 | Markov Decision Processes](#51-马尔可夫决策过程--markov-decision-processes)
    - [5.2 动态规划与值迭代 | Dynamic Programming \& Value Iteration](#52-动态规划与值迭代--dynamic-programming--value-iteration)
    - [5.3 策略梯度方法 | Policy Gradient Methods](#53-策略梯度方法--policy-gradient-methods)
  - [6. 相关性与本地跳转 | Relevance \& Local Navigation](#6-相关性与本地跳转--relevance--local-navigation)
  - [进度日志 | Progress Log](#进度日志--progress-log)

---

## 1. 主题简介 | Topic Introduction

人工智能与机器学习是计算机科学中最具革命性的领域之一，从统计学习理论到深度学习，从优化算法到强化学习，这一领域不仅推动了技术的突破性发展，也为数学理论提供了丰富的应用场景和新的研究方向。

Artificial intelligence and machine learning are among the most revolutionary fields in computer science, from statistical learning theory to deep learning, from optimization algorithms to reinforcement learning. This field has not only driven breakthrough technological developments but also provided rich application scenarios and new research directions for mathematical theory.

---

## 2. 机器学习理论基础 | Theoretical Foundations of Machine Learning

### 2.1 学习理论 | Learning Theory

**学习问题形式化：**

```lean
-- 学习问题
structure learning_problem :=
  (input_space : Type)
  (output_space : Type)
  (hypothesis_space : set (input_space → output_space))
  (loss_function : output_space → output_space → ℝ)
  (training_data : list (input_space × output_space))

-- 经验风险
def empirical_risk (h : hypothesis) (data : training_data) (loss : loss_function) : ℝ :=
  (1/n) * ∑ᵢ loss(h(xᵢ), yᵢ)

-- 真实风险
def true_risk (h : hypothesis) (distribution : probability_distribution) (loss : loss_function) : ℝ :=
  E_{(x,y)~D}[loss(h(x), y)]
```

**学习算法：**

```lean
-- 学习算法
def learning_algorithm (problem : learning_problem) (data : training_data) : hypothesis :=
  argmin_{h ∈ hypothesis_space} empirical_risk h data problem.loss_function

-- 一致性
def consistency (algorithm : learning_algorithm) : Prop :=
  ∀ ε > 0, ∀ δ > 0, ∃ n₀, ∀ n ≥ n₀,
    P[true_risk(algorithm(data)) - min_risk > ε] < δ
```

### 2.2 统计学习理论 | Statistical Learning Theory

**VC维理论：**

```lean
-- VC维
def vc_dimension (H : hypothesis_space) : ℕ :=
  max{n | ∃ shattered_set of size n}
  where shattered_set means H can realize all 2^n labelings

-- VC维上界
theorem vc_bound (H : hypothesis_space) (δ : ℝ) (n : ℕ) :
  with_probability ≥ 1 - δ,
  ∀ h ∈ H, true_risk h ≤ empirical_risk h + √((VC_dim(H) * log(n) + log(1/δ)) / n)
```

**Rademacher复杂度：**

```lean
-- Rademacher复杂度
def rademacher_complexity (H : hypothesis_space) (n : ℕ) : ℝ :=
  E_σ[sup_{h ∈ H} (1/n) * ∑ᵢ σᵢ * h(xᵢ)]
  where σᵢ are i.i.d. Rademacher random variables

-- Rademacher上界
theorem rademacher_bound (H : hypothesis_space) (δ : ℝ) (n : ℕ) :
  with_probability ≥ 1 - δ,
  ∀ h ∈ H, true_risk h ≤ empirical_risk h + 2 * R_n(H) + 3 * √(log(2/δ) / (2n))
```

### 2.3 计算学习理论 | Computational Learning Theory

**PAC学习：**

```lean
-- PAC学习
def pac_learnable (C : concept_class) : Prop :=
  ∃ algorithm, ∃ poly : ℕ → ℕ,
    ∀ ε δ > 0, ∀ n ≥ poly(1/ε, 1/δ, size(c)),
      algorithm learns c ∈ C with error ≤ ε and confidence ≥ 1 - δ

-- 样本复杂度
def sample_complexity (C : concept_class) (ε δ : ℝ) : ℕ :=
  O((VC_dim(C) + log(1/δ)) / ε²)
```

**计算复杂度：**

```python
def pac_learning_analysis():
    """PAC学习分析"""
    import numpy as np
    from sklearn.linear_model import Perceptron
    
    def learn_boolean_conjunction(n_variables, epsilon, delta):
        """学习布尔合取式"""
        # 样本复杂度
        m = int((n_variables + np.log(1/delta)) / epsilon)
        
        # 生成训练数据
        X = np.random.randint(0, 2, (m, n_variables))
        # 目标概念：前k个变量的合取
        k = n_variables // 2
        y = np.all(X[:, :k], axis=1).astype(int)
        
        # 学习
        model = Perceptron()
        model.fit(X, y)
        
        # 评估
        test_X = np.random.randint(0, 2, (1000, n_variables))
        test_y = np.all(test_X[:, :k], axis=1).astype(int)
        accuracy = model.score(test_X, test_y)
        
        return accuracy, m
    
    # 分析不同参数下的学习效果
    n_vars = [5, 10, 15, 20]
    epsilons = [0.1, 0.05, 0.01]
    delta = 0.05
    
    results = []
    for n in n_vars:
        for eps in epsilons:
            acc, samples = learn_boolean_conjunction(n, eps, delta)
            results.append({
                'n_variables': n,
                'epsilon': eps,
                'samples': samples,
                'accuracy': acc
            })
    
    return results
```

---

## 3. 神经网络与深度学习 | Neural Networks & Deep Learning

### 3.1 人工神经网络 | Artificial Neural Networks

**神经元模型：**

```lean
-- 神经元
structure neuron :=
  (weights : ℝⁿ)
  (bias : ℝ)
  (activation : ℝ → ℝ)

-- 前馈传播
def forward_propagation (x : ℝⁿ) (neuron : neuron) : ℝ :=
  neuron.activation(neuron.weights · x + neuron.bias)

-- 多层感知机
structure multilayer_perceptron :=
  (layers : list layer)
  (weights : list weight_matrix)
  (biases : list bias_vector)
```

**激活函数：**

```python
import numpy as np

class ActivationFunctions:
    @staticmethod
    def sigmoid(x):
        """Sigmoid激活函数"""
        return 1 / (1 + np.exp(-x))
    
    @staticmethod
    def sigmoid_derivative(x):
        """Sigmoid导数"""
        s = ActivationFunctions.sigmoid(x)
        return s * (1 - s)
    
    @staticmethod
    def relu(x):
        """ReLU激活函数"""
        return np.maximum(0, x)
    
    @staticmethod
    def relu_derivative(x):
        """ReLU导数"""
        return np.where(x > 0, 1, 0)
    
    @staticmethod
    def tanh(x):
        """双曲正切激活函数"""
        return np.tanh(x)
    
    @staticmethod
    def tanh_derivative(x):
        """双曲正切导数"""
        return 1 - np.tanh(x)**2

class NeuralNetwork:
    def __init__(self, layer_sizes, activation='relu'):
        self.layer_sizes = layer_sizes
        self.activation = activation
        self.weights = []
        self.biases = []
        
        # 初始化权重和偏置
        for i in range(len(layer_sizes) - 1):
            w = np.random.randn(layer_sizes[i+1], layer_sizes[i]) * 0.01
            b = np.zeros((layer_sizes[i+1], 1))
            self.weights.append(w)
            self.biases.append(b)
    
    def forward(self, X):
        """前向传播"""
        A = X
        activations = [A]
        
        for i in range(len(self.weights)):
            Z = np.dot(self.weights[i], A) + self.biases[i]
            if i == len(self.weights) - 1:
                # 输出层使用sigmoid
                A = ActivationFunctions.sigmoid(Z)
            else:
                # 隐藏层使用指定激活函数
                if self.activation == 'relu':
                    A = ActivationFunctions.relu(Z)
                elif self.activation == 'tanh':
                    A = ActivationFunctions.tanh(Z)
            activations.append(A)
        
        return A, activations
```

### 3.2 反向传播算法 | Backpropagation Algorithm

**反向传播：**

```lean
-- 反向传播
def backpropagation (network : neural_network) (x : input) (y : target) : gradients :=
  -- 前向传播
  let activations = forward_propagation network x
  
  -- 计算输出层误差
  let output_error = (activations[-1] - y) * activation_derivative(activations[-1])
  
  -- 反向传播误差
  let errors = [output_error]
  for i in range(len(network.layers)-2, -1, -1):
    let error = network.weights[i+1]ᵀ * errors[0] * activation_derivative(activations[i+1])
    errors.insert(0, error)
  
  -- 计算梯度
  let gradients = []
  for i in range(len(network.layers)-1):
    let weight_grad = errors[i] * activations[i]ᵀ
    let bias_grad = errors[i]
    gradients.append((weight_grad, bias_grad))
  
  return gradients
```

**梯度计算：**

```python
def backpropagation_implementation():
    """反向传播实现"""
    def compute_gradients(network, X, Y):
        m = X.shape[1]
        gradients = []
        
        # 前向传播
        A, activations = network.forward(X)
        
        # 计算输出层误差
        dA = A - Y
        
        # 反向传播
        for i in range(len(network.weights) - 1, -1, -1):
            # 计算dZ
            if i == len(network.weights) - 1:
                dZ = dA * ActivationFunctions.sigmoid_derivative(activations[i+1])
            else:
                dZ = dA * ActivationFunctions.relu_derivative(activations[i+1])
            
            # 计算权重和偏置梯度
            dW = np.dot(dZ, activations[i].T) / m
            db = np.sum(dZ, axis=1, keepdims=True) / m
            
            # 计算下一层的dA
            if i > 0:
                dA = np.dot(network.weights[i].T, dZ)
            
            gradients.insert(0, (dW, db))
        
        return gradients
    
    def update_parameters(network, gradients, learning_rate):
        """更新参数"""
        for i in range(len(network.weights)):
            network.weights[i] -= learning_rate * gradients[i][0]
            network.biases[i] -= learning_rate * gradients[i][1]
    
    return compute_gradients, update_parameters
```

### 3.3 深度学习架构 | Deep Learning Architectures

**卷积神经网络：**

```python
class ConvolutionalLayer:
    def __init__(self, num_filters, filter_size, stride=1, padding=0):
        self.num_filters = num_filters
        self.filter_size = filter_size
        self.stride = stride
        self.padding = padding
        self.filters = np.random.randn(num_filters, filter_size, filter_size) * 0.01
    
    def forward(self, input_data):
        """卷积前向传播"""
        batch_size, input_channels, input_height, input_width = input_data.shape
        
        # 计算输出尺寸
        output_height = (input_height - self.filter_size + 2 * self.padding) // self.stride + 1
        output_width = (input_width - self.filter_size + 2 * self.padding) // self.stride + 1
        
        # 初始化输出
        output = np.zeros((batch_size, self.num_filters, output_height, output_width))
        
        # 执行卷积
        for b in range(batch_size):
            for f in range(self.num_filters):
                for h in range(output_height):
                    for w in range(output_width):
                        h_start = h * self.stride
                        h_end = h_start + self.filter_size
                        w_start = w * self.stride
                        w_end = w_start + self.filter_size
                        
                        # 提取输入区域
                        input_region = input_data[b, :, h_start:h_end, w_start:w_end]
                        
                        # 计算卷积
                        output[b, f, h, w] = np.sum(input_region * self.filters[f])
        
        return output

class PoolingLayer:
    def __init__(self, pool_size, stride=None, mode='max'):
        self.pool_size = pool_size
        self.stride = stride if stride else pool_size
        self.mode = mode
    
    def forward(self, input_data):
        """池化前向传播"""
        batch_size, channels, input_height, input_width = input_data.shape
        
        # 计算输出尺寸
        output_height = (input_height - self.pool_size) // self.stride + 1
        output_width = (input_width - self.pool_size) // self.stride + 1
        
        # 初始化输出
        output = np.zeros((batch_size, channels, output_height, output_width))
        
        # 执行池化
        for b in range(batch_size):
            for c in range(channels):
                for h in range(output_height):
                    for w in range(output_width):
                        h_start = h * self.stride
                        h_end = h_start + self.pool_size
                        w_start = w * self.stride
                        w_end = w_start + self.pool_size
                        
                        # 提取区域
                        region = input_data[b, c, h_start:h_end, w_start:w_end]
                        
                        # 池化操作
                        if self.mode == 'max':
                            output[b, c, h, w] = np.max(region)
                        elif self.mode == 'average':
                            output[b, c, h, w] = np.mean(region)
        
        return output
```

**循环神经网络：**

```python
class RNNCell:
    def __init__(self, input_size, hidden_size):
        self.input_size = input_size
        self.hidden_size = hidden_size
        
        # 权重矩阵
        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01
        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01
        self.W_hy = np.random.randn(input_size, hidden_size) * 0.01
        
        # 偏置
        self.b_h = np.zeros((hidden_size, 1))
        self.b_y = np.zeros((input_size, 1))
    
    def forward(self, x, h_prev):
        """RNN单元前向传播"""
        # 隐藏状态更新
        h_next = np.tanh(np.dot(self.W_hh, h_prev) + np.dot(self.W_xh, x) + self.b_h)
        
        # 输出
        y = np.dot(self.W_hy, h_next) + self.b_y
        
        return h_next, y

class LSTM:
    def __init__(self, input_size, hidden_size):
        self.input_size = input_size
        self.hidden_size = hidden_size
        
        # 权重矩阵
        self.W_f = np.random.randn(hidden_size, hidden_size + input_size) * 0.01
        self.W_i = np.random.randn(hidden_size, hidden_size + input_size) * 0.01
        self.W_c = np.random.randn(hidden_size, hidden_size + input_size) * 0.01
        self.W_o = np.random.randn(hidden_size, hidden_size + input_size) * 0.01
        
        # 偏置
        self.b_f = np.zeros((hidden_size, 1))
        self.b_i = np.zeros((hidden_size, 1))
        self.b_c = np.zeros((hidden_size, 1))
        self.b_o = np.zeros((hidden_size, 1))
    
    def forward(self, x, h_prev, c_prev):
        """LSTM前向传播"""
        # 连接输入和隐藏状态
        concat = np.vstack((h_prev, x))
        
        # 门控机制
        f_t = ActivationFunctions.sigmoid(np.dot(self.W_f, concat) + self.b_f)  # 遗忘门
        i_t = ActivationFunctions.sigmoid(np.dot(self.W_i, concat) + self.b_i)  # 输入门
        c_tilde = np.tanh(np.dot(self.W_c, concat) + self.b_c)  # 候选值
        o_t = ActivationFunctions.sigmoid(np.dot(self.W_o, concat) + self.b_o)  # 输出门
        
        # 状态更新
        c_next = f_t * c_prev + i_t * c_tilde
        h_next = o_t * np.tanh(c_next)
        
        return h_next, c_next
```

---

## 4. 优化理论与算法 | Optimization Theory & Algorithms

### 4.1 梯度下降方法 | Gradient Descent Methods

**梯度下降：**

```lean
-- 梯度下降
def gradient_descent (f : ℝⁿ → ℝ) (x₀ : ℝⁿ) (α : ℝ) : ℝⁿ :=
  x₀ - α * ∇f(x₀)

-- 随机梯度下降
def stochastic_gradient_descent (f : ℝⁿ → ℝ) (x₀ : ℝⁿ) (α : ℝ) (batch : ℝⁿ) : ℝⁿ :=
  x₀ - α * ∇f_batch(x₀, batch)

-- 动量法
def momentum_method (f : ℝⁿ → ℝ) (x₀ : ℝⁿ) (α β : ℝ) : ℝⁿ :=
  let v = β * v_prev + α * ∇f(x₀)
  x₀ - v
```

**优化算法实现：**

```python
class Optimizers:
    @staticmethod
    def sgd(params, grads, learning_rate):
        """随机梯度下降"""
        for param, grad in zip(params, grads):
            param -= learning_rate * grad
    
    @staticmethod
    def momentum(params, grads, velocities, learning_rate, momentum_rate):
        """带动量的SGD"""
        for i, (param, grad, velocity) in enumerate(zip(params, grads, velocities)):
            velocities[i] = momentum_rate * velocity + learning_rate * grad
            param -= velocities[i]
    
    @staticmethod
    def adam(params, grads, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):
        """Adam优化器"""
        for i, (param, grad) in enumerate(zip(params, grads)):
            # 更新偏置修正的一阶矩估计
            m[i] = beta1 * m[i] + (1 - beta1) * grad
            
            # 更新偏置修正的二阶矩估计
            v[i] = beta2 * v[i] + (1 - beta2) * (grad ** 2)
            
            # 偏置修正
            m_hat = m[i] / (1 - beta1 ** t)
            v_hat = v[i] / (1 - beta2 ** t)
            
            # 更新参数
            param -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)
```

### 4.2 随机优化 | Stochastic Optimization

**随机优化理论：**

```lean
-- 随机优化问题
def stochastic_optimization_problem :=
  min_{x ∈ ℝⁿ} E_ξ[f(x, ξ)]
  where ξ is a random variable

-- 随机梯度估计
def stochastic_gradient (f : ℝⁿ × Ω → ℝ) (x : ℝⁿ) (ξ : Ω) : ℝⁿ :=
  ∇ₓ f(x, ξ)

-- 收敛性分析
theorem sgd_convergence (f : ℝⁿ → ℝ) (L : ℝ) (σ : ℝ) :
  if f is L-smooth and gradient_noise ≤ σ²,
  then E[||∇f(x_T)||²] ≤ O(1/√T)
```

**随机优化算法：**

```python
def stochastic_optimization_demo():
    """随机优化演示"""
    def objective_function(x, noise_level=0.1):
        """目标函数：f(x) = ||x||² + noise"""
        return np.sum(x**2) + noise_level * np.random.randn()
    
    def stochastic_gradient(x, noise_level=0.1):
        """随机梯度"""
        return 2 * x + noise_level * np.random.randn(x.shape)
    
    def sgd_optimization(initial_x, learning_rate, num_iterations):
        """SGD优化"""
        x = initial_x.copy()
        trajectory = [x.copy()]
        
        for i in range(num_iterations):
            grad = stochastic_gradient(x)
            x -= learning_rate * grad
            trajectory.append(x.copy())
        
        return x, trajectory
    
    def sgd_with_momentum(initial_x, learning_rate, momentum_rate, num_iterations):
        """带动量的SGD"""
        x = initial_x.copy()
        velocity = np.zeros_like(x)
        trajectory = [x.copy()]
        
        for i in range(num_iterations):
            grad = stochastic_gradient(x)
            velocity = momentum_rate * velocity + learning_rate * grad
            x -= velocity
            trajectory.append(x.copy())
        
        return x, trajectory
    
    # 比较不同优化方法
    initial_x = np.array([2.0, 2.0])
    learning_rate = 0.01
    num_iterations = 1000
    
    # SGD
    x_sgd, traj_sgd = sgd_optimization(initial_x, learning_rate, num_iterations)
    
    # SGD with momentum
    x_momentum, traj_momentum = sgd_with_momentum(
        initial_x, learning_rate, 0.9, num_iterations
    )
    
    return {
        'sgd_result': x_sgd,
        'momentum_result': x_momentum,
        'sgd_trajectory': traj_sgd,
        'momentum_trajectory': traj_momentum
    }
```

### 4.3 正则化技术 | Regularization Techniques

**正则化方法：**

```lean
-- L2正则化
def l2_regularization (θ : ℝⁿ) (λ : ℝ) : ℝ :=
  (λ/2) * ||θ||²

-- L1正则化
def l1_regularization (θ : ℝⁿ) (λ : ℝ) : ℝ :=
  λ * ||θ||₁

-- Dropout正则化
def dropout_regularization (x : ℝⁿ) (p : ℝ) : ℝⁿ :=
  x * mask / (1 - p)
  where mask ~ Bernoulli(1-p)
```

**正则化实现：**

```python
class Regularization:
    @staticmethod
    def l2_regularization_loss(weights, lambda_reg):
        """L2正则化损失"""
        l2_loss = 0
        for w in weights:
            l2_loss += np.sum(w**2)
        return (lambda_reg / 2) * l2_loss
    
    @staticmethod
    def l2_regularization_gradient(weights, lambda_reg):
        """L2正则化梯度"""
        gradients = []
        for w in weights:
            gradients.append(lambda_reg * w)
        return gradients
    
    @staticmethod
    def dropout_mask(shape, dropout_rate):
        """生成Dropout掩码"""
        mask = np.random.binomial(1, 1-dropout_rate, shape) / (1-dropout_rate)
        return mask
    
    @staticmethod
    def apply_dropout(activations, dropout_rate, training=True):
        """应用Dropout"""
        if training and dropout_rate > 0:
            mask = Regularization.dropout_mask(activations.shape, dropout_rate)
            return activations * mask
        return activations

def regularization_comparison():
    """正则化方法比较"""
    # 生成过拟合数据
    np.random.seed(42)
    X = np.random.randn(100, 20)
    y = np.random.randn(100, 1)
    
    # 创建高容量模型（容易过拟合）
    model_no_reg = NeuralNetwork([20, 50, 50, 1])
    model_l2 = NeuralNetwork([20, 50, 50, 1])
    model_dropout = NeuralNetwork([20, 50, 50, 1])
    
    # 训练参数
    learning_rate = 0.01
    num_epochs = 1000
    lambda_reg = 0.01
    dropout_rate = 0.5
    
    # 训练历史
    history_no_reg = []
    history_l2 = []
    history_dropout = []
    
    for epoch in range(num_epochs):
        # 前向传播
        y_pred_no_reg, _ = model_no_reg.forward(X.T)
        y_pred_l2, _ = model_l2.forward(X.T)
        y_pred_dropout, _ = model_dropout.forward(X.T)
        
        # 计算损失
        loss_no_reg = np.mean((y_pred_no_reg.T - y)**2)
        loss_l2 = loss_no_reg + Regularization.l2_regularization_loss(
            model_l2.weights, lambda_reg
        )
        loss_dropout = loss_no_reg
        
        # 记录历史
        history_no_reg.append(loss_no_reg)
        history_l2.append(loss_l2)
        history_dropout.append(loss_dropout)
    
    return {
        'no_regularization': history_no_reg,
        'l2_regularization': history_l2,
        'dropout': history_dropout
    }
```

---

## 5. 强化学习 | Reinforcement Learning

### 5.1 马尔可夫决策过程 | Markov Decision Processes

**MDP定义：**

```lean
-- 马尔可夫决策过程
structure markov_decision_process :=
  (states : set state)
  (actions : set action)
  (transitions : state → action → state → ℝ)  -- P(s'|s,a)
  (rewards : state → action → state → ℝ)      -- R(s,a,s')
  (discount_factor : ℝ)                       -- γ

-- 策略
def policy (π : state → action → ℝ) : Prop :=
  ∀ s, ∑ₐ π(s,a) = 1

-- 值函数
def value_function (π : policy) (s : state) : ℝ :=
  E_π[∑ₜ γᵗ * Rₜ | s₀ = s]
```

**MDP求解：**

```python
class MarkovDecisionProcess:
    def __init__(self, states, actions, transitions, rewards, gamma=0.9):
        self.states = states
        self.actions = actions
        self.transitions = transitions
        self.rewards = rewards
        self.gamma = gamma
    
    def value_iteration(self, epsilon=1e-6, max_iterations=1000):
        """值迭代算法"""
        V = {s: 0 for s in self.states}
        
        for iteration in range(max_iterations):
            V_new = {}
            delta = 0
            
            for s in self.states:
                # 计算所有动作的值
                action_values = []
                for a in self.actions:
                    value = 0
                    for s_next in self.states:
                        prob = self.transitions[s][a][s_next]
                        reward = self.rewards[s][a][s_next]
                        value += prob * (reward + self.gamma * V[s_next])
                    action_values.append(value)
                
                # 选择最大值
                V_new[s] = max(action_values)
                delta = max(delta, abs(V_new[s] - V[s]))
            
            V = V_new
            
            if delta < epsilon:
                break
        
        return V
    
    def policy_iteration(self, max_iterations=100):
        """策略迭代算法"""
        # 初始化随机策略
        policy = {s: np.random.choice(self.actions) for s in self.states}
        
        for iteration in range(max_iterations):
            # 策略评估
            V = self.policy_evaluation(policy)
            
            # 策略改进
            policy_stable = True
            for s in self.states:
                old_action = policy[s]
                
                # 找到最优动作
                action_values = []
                for a in self.actions:
                    value = 0
                    for s_next in self.states:
                        prob = self.transitions[s][a][s_next]
                        reward = self.rewards[s][a][s_next]
                        value += prob * (reward + self.gamma * V[s_next])
                    action_values.append(value)
                
                policy[s] = self.actions[np.argmax(action_values)]
                
                if old_action != policy[s]:
                    policy_stable = False
            
            if policy_stable:
                break
        
        return policy, V
```

### 5.2 动态规划与值迭代 | Dynamic Programming & Value Iteration

**贝尔曼方程：**

```lean
-- 贝尔曼方程
theorem bellman_equation (π : policy) (s : state) :
  V^π(s) = ∑ₐ π(s,a) * ∑ₛ' P(s'|s,a) * (R(s,a,s') + γ * V^π(s'))

-- 最优贝尔曼方程
theorem optimal_bellman_equation (s : state) :
  V*(s) = maxₐ ∑ₛ' P(s'|s,a) * (R(s,a,s') + γ * V*(s'))

-- 值迭代收敛
theorem value_iteration_convergence :
  V_{k+1}(s) = maxₐ ∑ₛ' P(s'|s,a) * (R(s,a,s') + γ * V_k(s'))
  converges to V*
```

**Q学习：**

```python
class QLearning:
    def __init__(self, states, actions, learning_rate=0.1, gamma=0.9, epsilon=0.1):
        self.states = states
        self.actions = actions
        self.learning_rate = learning_rate
        self.gamma = gamma
        self.epsilon = epsilon
        self.Q = {}
        
        # 初始化Q表
        for s in states:
            self.Q[s] = {}
            for a in actions:
                self.Q[s][a] = 0
    
    def choose_action(self, state):
        """ε-贪婪策略选择动作"""
        if np.random.random() < self.epsilon:
            return np.random.choice(self.actions)
        else:
            return max(self.Q[state], key=self.Q[state].get)
    
    def update(self, state, action, reward, next_state):
        """Q值更新"""
        best_next_action = max(self.Q[next_state], key=self.Q[next_state].get)
        td_target = reward + self.gamma * self.Q[next_state][best_next_action]
        td_error = td_target - self.Q[state][action]
        self.Q[state][action] += self.learning_rate * td_error
    
    def train(self, env, num_episodes=1000):
        """训练Q学习智能体"""
        episode_rewards = []
        
        for episode in range(num_episodes):
            state = env.reset()
            total_reward = 0
            
            while True:
                action = self.choose_action(state)
                next_state, reward, done, _ = env.step(action)
                
                self.update(state, action, reward, next_state)
                total_reward += reward
                state = next_state
                
                if done:
                    break
            
            episode_rewards.append(total_reward)
            
            # 衰减探索率
            if episode % 100 == 0:
                self.epsilon *= 0.9
        
        return episode_rewards
```

### 5.3 策略梯度方法 | Policy Gradient Methods

**策略梯度定理：**

```lean
-- 策略梯度定理
theorem policy_gradient_theorem (θ : ℝⁿ) (π_θ : policy) :
  ∇_θ J(θ) = E_π[∇_θ log π_θ(s,a) * Q^π(s,a)]

-- REINFORCE算法
def reinforce_update (θ : ℝⁿ) (τ : trajectory) (α : ℝ) : ℝⁿ :=
  θ + α * ∑ₜ ∇_θ log π_θ(sₜ,aₜ) * Gₜ
  where Gₜ is the return from time t
```

**策略梯度实现：**

```python
class PolicyGradient:
    def __init__(self, state_size, action_size, learning_rate=0.01):
        self.state_size = state_size
        self.action_size = action_size
        self.learning_rate = learning_rate
        
        # 策略网络参数
        self.weights = np.random.randn(state_size, action_size) * 0.01
        self.bias = np.zeros((action_size, 1))
    
    def policy(self, state):
        """计算策略概率"""
        logits = np.dot(self.weights.T, state) + self.bias
        probs = ActivationFunctions.sigmoid(logits)
        return probs / np.sum(probs)  # 归一化
    
    def choose_action(self, state):
        """根据策略选择动作"""
        probs = self.policy(state)
        return np.random.choice(self.action_size, p=probs.flatten())
    
    def policy_gradient(self, states, actions, rewards):
        """计算策略梯度"""
        gradients = []
        
        for state, action, reward in zip(states, actions, rewards):
            probs = self.policy(state)
            
            # 计算log概率的梯度
            log_prob_grad = np.zeros((self.action_size, 1))
            log_prob_grad[action] = 1 / (probs[action] + 1e-8)
            
            # 策略梯度
            grad = log_prob_grad * reward
            gradients.append(grad)
        
        return np.mean(gradients, axis=0)
    
    def update(self, states, actions, rewards):
        """更新策略参数"""
        gradient = self.policy_gradient(states, actions, rewards)
        self.weights += self.learning_rate * gradient.reshape(self.weights.shape)
        self.bias += self.learning_rate * gradient
    
    def train(self, env, num_episodes=1000):
        """训练策略梯度智能体"""
        episode_rewards = []
        
        for episode in range(num_episodes):
            state = env.reset()
            states, actions, rewards = [], [], []
            
            while True:
                action = self.choose_action(state)
                next_state, reward, done, _ = env.step(action)
                
                states.append(state)
                actions.append(action)
                rewards.append(reward)
                
                state = next_state
                
                if done:
                    break
            
            # 计算折扣回报
            discounted_rewards = []
            R = 0
            for r in reversed(rewards):
                R = r + 0.99 * R
                discounted_rewards.insert(0, R)
            
            # 标准化回报
            discounted_rewards = np.array(discounted_rewards)
            discounted_rewards = (discounted_rewards - np.mean(discounted_rewards)) / (np.std(discounted_rewards) + 1e-8)
            
            # 更新策略
            self.update(states, actions, discounted_rewards)
            
            episode_rewards.append(np.sum(rewards))
        
        return episode_rewards
```

---

## 6. 相关性与本地跳转 | Relevance & Local Navigation

- 参见 [01-总览.md](./01-总览.md)
- 参见 [02-算法与复杂性理论.md](./02-算法与复杂性理论.md)
- 参见 [03-形式化方法与程序验证.md](./03-形式化方法与程序验证.md)
- 参见 [../01-总览.md](../01-总览.md)
- 参见 [../../03-代数结构与理论/01-基本代数系统/01-总览.md](../../03-代数结构与理论/01-基本代数系统/01-总览.md)

---

## 进度日志 | Progress Log

```markdown
### 进度日志
- 日期：2024-12-19
- 当前主题：人工智能与机器学习
- 已完成内容：学习理论、神经网络、优化算法、强化学习
- 中断点：需要进一步细化深度学习架构和实际应用部分
- 待续内容：具体机器学习应用案例、前沿算法实现
- 责任人/AI协作：AI+人工
```

<!-- 中断点：深度学习架构/实际应用/前沿算法实现的递归扩展 -->