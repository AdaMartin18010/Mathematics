# 线性代数 | 谱分解 → PCA（条目与练习）

---

## 1. 学习导引 | Cognitive Primer

- 先修：内积空间、正定矩阵、特征值与特征向量、SVD
- 认知主线：
  - 协方差矩阵的几何意义（拉伸尺度/主方向）
  - 谱分解与主轴对齐（去相关/降维）

---

## 2. 核心条目 | Core Entries

### 2.1 协方差矩阵与谱分解

- 设数据矩阵 X ∈ R^{n×d}（行样本、列特征），中心化后协方差：
  C = (1/n) X^T X ∈ R^{d×d}（对称半正定）
- 谱分解：C = Q Λ Q^T，其中 Q 列向量为特征向量，Λ 为非负特征值对角阵
- 解释：
  - 特征向量 = 数据方差最大的正交方向
  - 特征值 = 对应方向上的方差大小

### 2.2 PCA 的两种等价形式

- 最大方差方向：
  - argmax_{‖u‖=1} Var(Xu) = u_1 为 C 的最大特征向量
  - 第 k 主成分由前 k 个最大特征向量张成
- 最小重构误差（秩-k近似）：
  - X ≈ X_k = U_k Σ_k V_k^T（SVD 取前 k 奇异值/向量）
  - 结论：SVD 的前 k 列 V_k 与 C 的前 k 特征向量一致

### 2.3 算法流程（标准版）

1) 数据中心化：X ← X − 1_n (均值)^T
2) 计算协方差：C = (1/n) X^T X
3) 求谱分解：C = Q Λ Q^T
4) 选择前 k 主方向：Q_k（对应最大 k 个特征值）
5) 投影得到低维表示：Y = X Q_k

认知要点：最大方差 ↔ 主方向；谱分解/奇异值分解在几何上等价为“对齐-拉伸-旋转”。

---

## 3. 典例 | Worked Examples

1) 二维数据的 PCA：给出 X（中心化），手推 C、特征对，画出主轴；取 k=1 进行投影
2) 对高维数据用 SVD 直接得到前 k 主方向，验证与 C 的谱分解一致
3) 对异常值敏感性：对同一数据加入离群点，观察主方向变化（鲁棒性讨论）

---

## 4. 练习（8题）| Exercises (8)

1) 证明：对称矩阵的不同特征值对应的特征向量正交
2) 设 C = Q Λ Q^T，证明最大化 u^T C u, ‖u‖=1 的解为最大特征向量
3) 给定 2×2 协方差矩阵 C=[ [4,2],[2,3] ]，求特征分解并画主轴
4) 证明：min_{rank(B)≤k} ‖X−B‖_F 的解为 SVD 取前 k 项
5) 设数据 X，比较“先标准化各特征”vs“不标准化”对 PCA 结果的影响
6) 证明：PCA 去相关（新坐标下协方差对角）
7) 讨论：当特征值相等时，主方向不唯一；给出一例
8) 连接：PCA 与最小二乘拟合直线在二维数据上的几何关系

---

## 5. 认知提示与误区 | Tips & Pitfalls

- PCA 是线性正交变换，不等同于一般的“特征选择”
- 最大方差不一定等于最有判别力（与监督信息无关）
- 数据需中心化；尺度差异大时应标准化

---

## 6. 外部参考 | References

- `https://en.wikipedia.org/wiki/Principal_component_analysis`
- `https://en.wikipedia.org/wiki/Singular_value_decomposition`
