# 信息论 | 熵·互信息·编码定理（条目与练习）

---

## 1. 学习导引 | Cognitive Primer

- 先修：概率论、对数函数
- 主线：信息量→熵→互信息→信道容量→编码定理

---

## 2. 信息量与熵 | Information & Entropy

- 信息量：I(x) = -log₂ P(x)（比特）
- 熵：H(X) = E[-log₂ P(X)] = -Σ p_i log₂ p_i
- 联合熵：H(X,Y) = -Σ p_{ij} log₂ p_{ij}
- 条件熵：H(X|Y) = H(X,Y) - H(Y)

---

## 3. 互信息与信道 | Mutual Information & Channels

- 互信息：I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)
- 信道容量：C = max_{P(X)} I(X;Y)
- 对称性：I(X;Y) = I(Y;X) ≥ 0

---

## 4. 编码定理 | Coding Theorems

- 无噪声编码定理：平均码长 ≥ H(X)
- 信道编码定理：R < C 时存在可靠编码
- 数据压缩：熵是压缩的理论下界

---

## 5. 典例 | Worked Examples

1) 计算：伯努利分布 B(p) 的熵 H(p) = -p log p - (1-p) log(1-p)
2) 二元对称信道：P(Y=0|X=1) = P(Y=1|X=0) = ε
3) 霍夫曼编码：构造最优前缀码

---

## 6. 练习（8题） | Exercises (8)

1) 证明：H(X) ≥ 0，等号当且仅当 X 退化
2) 计算：均匀分布 U{1,...,n} 的熵
3) 证明：H(X,Y) ≤ H(X) + H(Y)，等号当且仅当独立
4) 计算：二元对称信道的容量 C = 1 - H(ε)
5) 证明：I(X;Y) = 0 当且仅当 X,Y 独立
6) 构造：霍夫曼编码树（给定概率分布）
7) 讨论：香农极限与信道编码的关系
8) 连接：熵与热力学熵的类比（概念性）

---

## 7. 认知提示 | Tips

- 熵度量不确定性，互信息度量相关性
- 编码定理提供信息传输的理论极限

---

## 8. 参考 | References

- `https://en.wikipedia.org/wiki/Information_theory`
- `https://en.wikipedia.org/wiki/Entropy_(information_theory)`
