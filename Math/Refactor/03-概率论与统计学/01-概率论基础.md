# 01-概率论基础

## 目录

1. [概率论基础概述](#1-概率论基础概述)
2. [概率空间](#2-概率空间)
3. [随机变量](#3-随机变量)
4. [分布函数](#4-分布函数)
5. [期望与方差](#5-期望与方差)
6. [独立性](#6-独立性)
7. [条件概率](#7-条件概率)
8. [形式化实现](#8-形式化实现)
9. [习题与练习](#9-习题与练习)
10. [参考文献](#10-参考文献)

## 1. 概率论基础概述

### 1.1 历史背景

概率论的起源可以追溯到17世纪的赌博问题：

**帕斯卡-费马问题**（1654）：
两个赌徒在游戏中断时如何公平分配赌注？

**雅各布·伯努利**（1713）：
《猜度术》建立了概率论的基础概念。

**拉普拉斯**（1812）：
《概率分析理论》建立了古典概率论体系。

**科尔莫戈罗夫**（1933）：
建立了现代概率论的公理体系。

### 1.2 基本思想

**不确定性建模**：
将随机现象转化为可计算的数学对象。

**频率稳定性**：
长期重复试验中相对频率的稳定性。

**主观信念**：
概率作为个人信念的量化表示。

### 1.3 概率解释

**古典解释**：
等可能事件中有利事件数与总事件数的比值。

**频率解释**：
长期重复试验中事件发生的相对频率。

**主观解释**：
个人对事件发生可能性的信念程度。

## 2. 概率空间

### 2.1 样本空间

**定义 2.1.1**（样本空间）
样本空间 $\Omega$ 是随机试验所有可能结果的集合。

**例子**：

- 掷硬币：$\Omega = \{H, T\}$
- 掷骰子：$\Omega = \{1, 2, 3, 4, 5, 6\}$
- 测量温度：$\Omega = \mathbb{R}$

### 2.2 事件域

**定义 2.2.1**（事件域）
事件域 $\mathcal{F}$ 是 $\Omega$ 的子集族，满足：

1. $\Omega \in \mathcal{F}$
2. 如果 $A \in \mathcal{F}$，则 $A^c \in \mathcal{F}$
3. 如果 $A_i \in \mathcal{F}$，则 $\bigcup_{i=1}^{\infty} A_i \in \mathcal{F}$

**定义 2.2.2**（事件）
事件域中的元素称为事件。

**定理 2.2.1**（事件域的性质）

1. $\emptyset \in \mathcal{F}$
2. 如果 $A_i \in \mathcal{F}$，则 $\bigcap_{i=1}^{\infty} A_i \in \mathcal{F}$
3. 如果 $A, B \in \mathcal{F}$，则 $A \setminus B \in \mathcal{F}$

### 2.3 概率测度

**定义 2.3.1**（概率测度）
概率测度 $P$ 是 $\mathcal{F} \to [0,1]$ 的函数，满足：

1. $P(\Omega) = 1$
2. 对于互不相交的事件 $A_i$，$P(\bigcup_{i=1}^{\infty} A_i) = \sum_{i=1}^{\infty} P(A_i)$

**定理 2.3.1**（概率测度的性质）

1. $P(\emptyset) = 0$
2. $P(A^c) = 1 - P(A)$
3. 如果 $A \subset B$，则 $P(A) \leq P(B)$
4. $P(A \cup B) = P(A) + P(B) - P(A \cap B)$

**证明**：

1. 由可列可加性，$P(\emptyset) = 0$
2. $P(A^c) = P(\Omega \setminus A) = P(\Omega) - P(A) = 1 - P(A)$
3. $B = A \cup (B \setminus A)$，所以 $P(B) = P(A) + P(B \setminus A) \geq P(A)$
4. $A \cup B = A \cup (B \setminus A)$，所以 $P(A \cup B) = P(A) + P(B \setminus A) = P(A) + P(B) - P(A \cap B)$

## 3. 随机变量

### 3.1 随机变量的定义

**定义 3.1.1**（随机变量）
随机变量 $X$ 是从概率空间 $(\Omega, \mathcal{F}, P)$ 到实数集的可测函数，即对于任意 $a \in \mathbb{R}$，$\{\omega : X(\omega) \leq a\} \in \mathcal{F}$。

**定义 3.1.2**（离散随机变量）
取有限或可列个值的随机变量称为离散随机变量。

**定义 3.1.3**（连续随机变量）
分布函数连续的随机变量称为连续随机变量。

### 3.2 随机变量的分布

**定义 3.2.1**（概率质量函数）
离散随机变量 $X$ 的概率质量函数定义为：
$$p_X(x) = P(X = x)$$

**定义 3.2.2**（概率密度函数）
连续随机变量 $X$ 的概率密度函数 $f_X(x)$ 满足：
$$P(a \leq X \leq b) = \int_a^b f_X(x) \, dx$$

**定理 3.2.1**（概率质量函数的性质）

1. $p_X(x) \geq 0$ 对所有 $x$
2. $\sum_x p_X(x) = 1$

**定理 3.2.2**（概率密度函数的性质）

1. $f_X(x) \geq 0$ 对所有 $x$
2. $\int_{-\infty}^{\infty} f_X(x) \, dx = 1$

## 4. 分布函数

### 4.1 分布函数的定义

**定义 4.1.1**（分布函数）
随机变量 $X$ 的分布函数定义为：
$$F_X(x) = P(X \leq x)$$

**定理 4.1.1**（分布函数的性质）

1. $F_X$ 是非减函数
2. $\lim_{x \to -\infty} F_X(x) = 0$
3. $\lim_{x \to \infty} F_X(x) = 1$
4. $F_X$ 是右连续的

**证明**：

1. 如果 $x_1 < x_2$，则 $\{X \leq x_1\} \subset \{X \leq x_2\}$，所以 $F_X(x_1) \leq F_X(x_2)$
2. 当 $x \to -\infty$ 时，$\{X \leq x\} \to \emptyset$，所以 $F_X(x) \to 0$
3. 当 $x \to \infty$ 时，$\{X \leq x\} \to \Omega$，所以 $F_X(x) \to 1$
4. 对于任意 $x$，$\lim_{h \to 0^+} F_X(x + h) = F_X(x)$

### 4.2 分布函数与密度函数的关系

**定理 4.2.1**（连续随机变量）
对于连续随机变量 $X$：
$$F_X(x) = \int_{-\infty}^x f_X(t) \, dt$$

**定理 4.2.2**（密度函数的计算）
如果 $F_X$ 在 $x$ 处可导，则：
$$f_X(x) = \frac{d}{dx} F_X(x)$$

### 4.3 常见分布

**定义 4.3.1**（均匀分布）
随机变量 $X$ 服从区间 $[a,b]$ 上的均匀分布，如果：
$$
f_X(x) = \begin{cases}
\frac{1}{b-a} & \text{如果 } a \leq x \leq b \\
0 & \text{其他}
\end{cases}
$$

**定义 4.3.2**（正态分布）
随机变量 $X$ 服从参数为 $\mu, \sigma^2$ 的正态分布，如果：
$$f_X(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}$$

**定义 4.3.3**（指数分布）
随机变量 $X$ 服从参数为 $\lambda$ 的指数分布，如果：
$$
f_X(x) = \begin{cases}
\lambda e^{-\lambda x} & \text{如果 } x \geq 0 \\
0 & \text{其他}
\end{cases}
$$

## 5. 期望与方差

### 5.1 期望的定义

**定义 5.1.1**（离散随机变量的期望）
离散随机变量 $X$ 的期望定义为：
$$E[X] = \sum_x x p_X(x)$$

**定义 5.1.2**（连续随机变量的期望）
连续随机变量 $X$ 的期望定义为：
$$E[X] = \int_{-\infty}^{\infty} x f_X(x) \, dx$$

**定理 5.1.1**（期望的线性性）
对于任意常数 $a, b$ 和随机变量 $X, Y$：
$$E[aX + bY] = aE[X] + bE[Y]$$

**证明**：
对于连续情况：
$$
\begin{align}
E[aX + bY] &= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} (ax + by) f_{X,Y}(x,y) \, dx \, dy \\
&= a \int_{-\infty}^{\infty} x f_X(x) \, dx + b \int_{-\infty}^{\infty} y f_Y(y) \, dy \\
&= aE[X] + bE[Y]
\end{align}
$$

### 5.2 方差与标准差

**定义 5.2.1**（方差）
随机变量 $X$ 的方差定义为：
$$\text{Var}(X) = E[(X - E[X])^2]$$

**定义 5.2.2**（标准差）
随机变量 $X$ 的标准差定义为：
$$\sigma_X = \sqrt{\text{Var}(X)}$$

**定理 5.2.1**（方差的计算公式）
$$\text{Var}(X) = E[X^2] - (E[X])^2$$

**证明**：
$$
\begin{align}
\text{Var}(X) &= E[(X - E[X])^2] \\
&= E[X^2 - 2XE[X] + (E[X])^2] \\
&= E[X^2] - 2E[X]E[X] + (E[X])^2 \\
&= E[X^2] - (E[X])^2
\end{align}
$$

**定理 5.2.2**（方差的性质）

1. $\text{Var}(X) \geq 0$
2. $\text{Var}(aX + b) = a^2\text{Var}(X)$
3. 如果 $X, Y$ 独立，则 $\text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y)$

### 5.3 矩与矩母函数

**定义 5.3.1**（矩）
随机变量 $X$ 的 $k$ 阶矩定义为：
$$\mu_k = E[X^k]$$

**定义 5.3.2**（中心矩）
随机变量 $X$ 的 $k$ 阶中心矩定义为：
$$\mu_k' = E[(X - E[X])^k]$$

**定义 5.3.3**（矩母函数）
随机变量 $X$ 的矩母函数定义为：
$$M_X(t) = E[e^{tX}]$$

**定理 5.3.1**（矩母函数的性质）
$$E[X^k] = \frac{d^k}{dt^k} M_X(t) \bigg|_{t=0}$$

## 6. 独立性

### 6.1 事件的独立性

**定义 6.1.1**（独立事件）
事件 $A, B$ 独立，如果：
$$P(A \cap B) = P(A)P(B)$$

**定义 6.1.2**（条件独立）
事件 $A, B$ 在给定事件 $C$ 的条件下独立，如果：
$$P(A \cap B|C) = P(A|C)P(B|C)$$

**定理 6.1.1**（独立性的等价条件）
事件 $A, B$ 独立的充分必要条件是：
$$P(A|B) = P(A) \text{ 或 } P(B|A) = P(B)$$

### 6.2 随机变量的独立性

**定义 6.2.1**（独立随机变量）
随机变量 $X, Y$ 独立，如果对于任意 $A, B \subset \mathbb{R}$：
$$P(X \in A, Y \in B) = P(X \in A)P(Y \in B)$$

**定理 6.2.1**（独立性的等价条件）
随机变量 $X, Y$ 独立的充分必要条件是：
$$F_{X,Y}(x,y) = F_X(x)F_Y(y)$$

**定理 6.2.2**（独立随机变量的期望）
如果 $X, Y$ 独立，则：
$$E[XY] = E[X]E[Y]$$

**证明**：
$$
\begin{align}
E[XY] &= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} xy f_{X,Y}(x,y) \, dx \, dy \\
&= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} xy f_X(x) f_Y(y) \, dx \, dy \\
&= \int_{-\infty}^{\infty} x f_X(x) \, dx \int_{-\infty}^{\infty} y f_Y(y) \, dy \\
&= E[X]E[Y]
\end{align}
$$

### 6.3 协方差与相关系数

**定义 6.3.1**（协方差）
随机变量 $X, Y$ 的协方差定义为：
$$\text{Cov}(X,Y) = E[(X - E[X])(Y - E[Y])]$$

**定义 6.3.2**（相关系数）
随机变量 $X, Y$ 的相关系数定义为：
$$\rho_{X,Y} = \frac{\text{Cov}(X,Y)}{\sigma_X \sigma_Y}$$

**定理 6.3.1**（协方差的性质）

1. $\text{Cov}(X,Y) = E[XY] - E[X]E[Y]$
2. $\text{Cov}(X,X) = \text{Var}(X)$
3. $\text{Cov}(aX + b, cY + d) = ac\text{Cov}(X,Y)$

**定理 6.3.2**（相关系数的性质）

1. $-1 \leq \rho_{X,Y} \leq 1$
2. $|\rho_{X,Y}| = 1$ 当且仅当 $Y = aX + b$（线性关系）

## 7. 条件概率

### 7.1 条件概率的定义

**定义 7.1.1**（条件概率）
设 $A, B \in \mathcal{F}$，$P(B) > 0$，则条件概率定义为：
$$P(A|B) = \frac{P(A \cap B)}{P(B)}$$

**定理 7.1.1**（条件概率的性质）

1. $0 \leq P(A|B) \leq 1$
2. $P(\Omega|B) = 1$
3. 如果 $A_1, A_2, \ldots$ 互不相交，则：
   $$P(\bigcup_{i=1}^{\infty} A_i|B) = \sum_{i=1}^{\infty} P(A_i|B)$$

### 7.2 贝叶斯定理

**定理 7.2.1**（贝叶斯定理）
设 $A_1, A_2, \ldots, A_n$ 是互不相交的事件，且 $\bigcup_{i=1}^n A_i = \Omega$，$P(A_i) > 0$，$P(B) > 0$，则：
$$P(A_i|B) = \frac{P(B|A_i)P(A_i)}{\sum_{j=1}^n P(B|A_j)P(A_j)}$$

**证明**：
由条件概率定义和全概率公式：
$$
\begin{align}
P(A_i|B) &= \frac{P(A_i \cap B)}{P(B)} \\
&= \frac{P(B|A_i)P(A_i)}{P(B)} \\
&= \frac{P(B|A_i)P(A_i)}{\sum_{j=1}^n P(B|A_j)P(A_j)}
\end{align}
$$

### 7.3 条件期望

**定义 7.3.1**（条件期望）
随机变量 $X$ 在给定事件 $B$ 的条件期望定义为：
$$E[X|B] = \frac{E[X \cdot \mathbf{1}_B]}{P(B)}$$

**定义 7.3.2**（条件期望函数）
随机变量 $X$ 在给定随机变量 $Y$ 的条件期望函数定义为：
$$E[X|Y] = \int_{-\infty}^{\infty} x f_{X|Y}(x|y) \, dx$$

**定理 7.3.1**（全期望公式）
$$E[X] = E[E[X|Y]]$$

## 8. 形式化实现

### 8.1 Haskell实现

```haskell
-- 概率论基础模块
module Probability where

import Data.List (nub, sort)
import qualified Data.Map as Map
import System.Random (Random, randomR, mkStdGen)

-- 概率空间
data ProbabilitySpace a = ProbabilitySpace {
    sampleSpace :: [a],
    events :: Map.Map String [a],
    probabilities :: Map.Map String Double
} deriving (Show)

-- 随机变量
data RandomVariable a b = RandomVariable {
    domain :: ProbabilitySpace a,
    mapping :: a -> b
} deriving Show

-- 分布函数
data Distribution a = Distribution {
    values :: [a],
    probabilities :: [Double]
} deriving (Show, Eq)

-- 连续分布
data ContinuousDistribution = ContinuousDistribution {
    density :: Double -> Double,
    support :: (Double, Double)
} deriving Show

-- 创建均匀分布
uniformDistribution :: [a] -> Distribution a
uniformDistribution xs = Distribution {
    values = xs,
    probabilities = replicate (length xs) (1.0 / fromIntegral (length xs))
}

-- 创建伯努利分布
bernoulliDistribution :: Double -> Distribution Bool
bernoulliDistribution p = Distribution {
    values = [True, False],
    probabilities = [p, 1 - p]
}

-- 期望计算
expectation :: (Num b, Fractional b) => Distribution b -> b
expectation dist = sum $ zipWith (*) (values dist) (map fromRational $ map toRational $ probabilities dist)

-- 方差计算
variance :: (Num b, Fractional b) => Distribution b -> b
variance dist = let
    mu = expectation dist
    in expectation $ Distribution {
        values = map (\x -> (x - mu)^2) (values dist),
        probabilities = probabilities dist
    }

-- 标准差
standardDeviation :: (Num b, Fractional b, Floating b) => Distribution b -> b
standardDeviation = sqrt . variance

-- 协方差计算
covariance :: (Num b, Fractional b) => Distribution b -> Distribution b -> b
covariance dist1 dist2 = let
    mu1 = expectation dist1
    mu2 = expectation dist2
    jointProbs = zipWith (*) (probabilities dist1) (probabilities dist2)
    jointValues = zipWith (\x y -> (x - mu1) * (y - mu2)) (values dist1) (values dist2)
    in sum $ zipWith (*) jointValues jointProbs

-- 相关系数
correlation :: (Num b, Fractional b, Floating b) => Distribution b -> Distribution b -> b
correlation dist1 dist2 = let
    cov = covariance dist1 dist2
    std1 = standardDeviation dist1
    std2 = standardDeviation dist2
    in cov / (std1 * std2)

-- 条件概率
conditionalProbability :: Eq a => Distribution a -> a -> a -> Double
conditionalProbability dist eventA eventB = let
    pA = probability dist eventA
    pB = probability dist eventB
    pAB = probability dist eventA  -- 简化版本，假设独立
    in if pB > 0 then pAB / pB else 0
  where
    probability dist x = case lookup x (zip (values dist) (probabilities dist)) of
        Just p -> p
        Nothing -> 0

-- 贝叶斯定理
bayesTheorem :: Double -> Double -> Double -> Double
bayesTheorem pA pB pBA = let
    pAB = pBA * pA
    pBTotal = pAB + (1 - pBA) * (1 - pA)  -- 简化版本
    in pAB / pBTotal

-- 随机数生成
generateRandom :: Distribution Double -> IO Double
generateRandom dist = do
    let cumulative = scanl1 (+) (probabilities dist)
    r <- randomR (0, 1) <$> mkStdGen <$> randomIO
    return $ selectValue r (zip (values dist) cumulative)
  where
    selectValue r ((v, _):[]) = v
    selectValue r ((v, c):rest) = if r <= c then v else selectValue r rest

-- 蒙特卡洛模拟
monteCarlo :: Int -> IO Double -> IO Double
monteCarlo n simulation = do
    results <- replicateM n simulation
    return $ sum results / fromIntegral n

-- 大数定律验证
lawOfLargeNumbers :: Distribution Double -> Int -> IO Double
lawOfLargeNumbers dist n = do
    samples <- replicateM n (generateRandom dist)
    return $ sum samples / fromIntegral n

-- 中心极限定理验证
centralLimitTheorem :: Distribution Double -> Int -> Int -> IO [Double]
centralLimitTheorem dist sampleSize numSamples = do
    sampleMeans <- replicateM numSamples (lawOfLargeNumbers dist sampleSize)
    return sampleMeans

-- 测试函数
testProbability :: IO ()
testProbability = do
    putStrLn "概率论基础测试："

    -- 测试均匀分布
    let uniform = uniformDistribution [1, 2, 3, 4, 5, 6]
    putStrLn $ "均匀分布期望: " ++ show (expectation uniform)
    putStrLn $ "均匀分布方差: " ++ show (variance uniform)

    -- 测试伯努利分布
    let bernoulli = bernoulliDistribution 0.3
    putStrLn $ "伯努利分布期望: " ++ show (expectation bernoulli)
    putStrLn $ "伯努利分布方差: " ++ show (variance bernoulli)

    -- 测试协方差
    let dist1 = uniformDistribution [1, 2, 3]
    let dist2 = uniformDistribution [4, 5, 6]
    putStrLn $ "协方差: " ++ show (covariance dist1 dist2)
    putStrLn $ "相关系数: " ++ show (correlation dist1 dist2)

    -- 测试贝叶斯定理
    let posterior = bayesTheorem 0.1 0.05 0.8
    putStrLn $ "后验概率: " ++ show posterior

    -- 测试大数定律
    mean <- lawOfLargeNumbers uniform 1000
    putStrLn $ "大数定律验证 (n=1000): " ++ show mean
```

### 8.2 Rust实现

```rust
use std::collections::HashMap;
use rand::Rng;

// 概率空间
# [derive(Clone, Debug)]
struct ProbabilitySpace<T> {
    sample_space: Vec<T>,
    events: HashMap<String, Vec<T>>,
    probabilities: HashMap<String, f64>,
}

// 随机变量
# [derive(Clone, Debug)]
struct RandomVariable<T, U> {
    domain: ProbabilitySpace<T>,
    mapping: Box<dyn Fn(&T) -> U>,
}

// 分布
# [derive(Clone, Debug)]
struct Distribution<T> {
    values: Vec<T>,
    probabilities: Vec<f64>,
}

// 连续分布
# [derive(Clone, Debug)]
struct ContinuousDistribution {
    density: Box<dyn Fn(f64) -> f64>,
    support: (f64, f64),
}

impl<T: Clone> ProbabilitySpace<T> {
    fn new(sample_space: Vec<T>) -> Self {
        Self {
            sample_space,
            events: HashMap::new(),
            probabilities: HashMap::new(),
        }
    }

    fn add_event(&mut self, name: String, event: Vec<T>, probability: f64) {
        self.events.insert(name.clone(), event);
        self.probabilities.insert(name, probability);
    }
}

impl<T: Clone + std::fmt::Debug> Distribution<T> {
    fn new(values: Vec<T>, probabilities: Vec<f64>) -> Self {
        assert_eq!(values.len(), probabilities.len());
        assert!((probabilities.iter().sum::<f64>() - 1.0).abs() < 1e-10);
        Self { values, probabilities }
    }

    fn uniform(values: Vec<T>) -> Self {
        let n = values.len() as f64;
        let probabilities = vec![1.0 / n; values.len()];
        Self { values, probabilities }
    }

    fn bernoulli(p: f64) -> Distribution<bool> {
        Distribution {
            values: vec![true, false],
            probabilities: vec![p, 1.0 - p],
        }
    }
}

// 数值计算特征
trait Numeric {
    fn zero() -> Self;
    fn one() -> Self;
    fn add(&self, other: &Self) -> Self;
    fn mult(&self, other: &Self) -> Self;
    fn sub(&self, other: &Self) -> Self;
    fn div(&self, other: &Self) -> Self;
    fn to_f64(&self) -> f64;
}

impl Numeric for f64 {
    fn zero() -> Self { 0.0 }
    fn one() -> Self { 1.0 }
    fn add(&self, other: &Self) -> Self { self + other }
    fn mult(&self, other: &Self) -> Self { self * other }
    fn sub(&self, other: &Self) -> Self { self - other }
    fn div(&self, other: &Self) -> Self { self / other }
    fn to_f64(&self) -> f64 { *self }
}

impl Numeric for i32 {
    fn zero() -> Self { 0 }
    fn one() -> Self { 1 }
    fn add(&self, other: &Self) -> Self { self + other }
    fn mult(&self, other: &Self) -> Self { self * other }
    fn sub(&self, other: &Self) -> Self { self - other }
    fn div(&self, other: &Self) -> Self { self / other }
    fn to_f64(&self) -> f64 { *self as f64 }
}

impl<T: Numeric + Clone> Distribution<T> {
    fn expectation(&self) -> f64 {
        self.values.iter()
            .zip(&self.probabilities)
            .map(|(x, p)| x.to_f64() * p)
            .sum()
    }

    fn variance(&self) -> f64 {
        let mu = self.expectation();
        self.values.iter()
            .zip(&self.probabilities)
            .map(|(x, p)| (x.to_f64() - mu).powi(2) * p)
            .sum()
    }

    fn standard_deviation(&self) -> f64 {
        self.variance().sqrt()
    }
}

// 协方差和相关系数
fn covariance<T: Numeric + Clone>(dist1: &Distribution<T>, dist2: &Distribution<T>) -> f64 {
    let mu1 = dist1.expectation();
    let mu2 = dist2.expectation();

    dist1.values.iter()
        .zip(&dist1.probabilities)
        .zip(dist2.values.iter().zip(&dist2.probabilities))
        .map(|((x1, p1), (x2, p2))| {
            (x1.to_f64() - mu1) * (x2.to_f64() - mu2) * p1 * p2
        })
        .sum()
}

fn correlation<T: Numeric + Clone>(dist1: &Distribution<T>, dist2: &Distribution<T>) -> f64 {
    let cov = covariance(dist1, dist2);
    let std1 = dist1.standard_deviation();
    let std2 = dist2.standard_deviation();

    if std1 > 0.0 && std2 > 0.0 {
        cov / (std1 * std2)
    } else {
        0.0
    }
}

// 条件概率
fn conditional_probability<T: PartialEq + Clone>(
    dist: &Distribution<T>,
    event_a: &T,
    event_b: &T
) -> f64 {
    let p_a = dist.probabilities.iter()
        .zip(&dist.values)
        .find(|(_, x)| x == event_a)
        .map(|(p, _)| *p)
        .unwrap_or(0.0);

    let p_b = dist.probabilities.iter()
        .zip(&dist.values)
        .find(|(_, x)| x == event_b)
        .map(|(p, _)| *p)
        .unwrap_or(0.0);

    if p_b > 0.0 {
        // 简化版本，假设独立
        p_a
    } else {
        0.0
    }
}

// 贝叶斯定理
fn bayes_theorem(p_a: f64, p_b: f64, p_ba: f64) -> f64 {
    let p_ab = p_ba * p_a;
    let p_b_total = p_ab + (1.0 - p_ba) * (1.0 - p_a); // 简化版本
    p_ab / p_b_total
}

// 随机数生成
fn generate_random<T: Clone>(dist: &Distribution<T>) -> T {
    let mut rng = rand::thread_rng();
    let r: f64 = rng.gen();

    let mut cumulative = 0.0;
    for (value, prob) in dist.values.iter().zip(&dist.probabilities) {
        cumulative += prob;
        if r <= cumulative {
            return value.clone();
        }
    }

    dist.values.last().unwrap().clone()
}

// 大数定律
fn law_of_large_numbers<T: Numeric + Clone>(dist: &Distribution<T>, n: usize) -> f64 {
    let samples: Vec<f64> = (0..n)
        .map(|_| generate_random(dist).to_f64())
        .collect();

    samples.iter().sum::<f64>() / n as f64
}

// 中心极限定理
fn central_limit_theorem<T: Numeric + Clone>(
    dist: &Distribution<T>,
    sample_size: usize,
    num_samples: usize
) -> Vec<f64> {
    (0..num_samples)
        .map(|_| law_of_large_numbers(dist, sample_size))
        .collect()
}

// 测试函数
fn test_probability() {
    println!("概率论基础测试：");

    // 测试均匀分布
    let uniform = Distribution::uniform(vec![1, 2, 3, 4, 5, 6]);
    println!("均匀分布期望: {}", uniform.expectation());
    println!("均匀分布方差: {}", uniform.variance());

    // 测试伯努利分布
    let bernoulli = Distribution::bernoulli(0.3);
    println!("伯努利分布期望: {}", bernoulli.expectation());
    println!("伯努利分布方差: {}", bernoulli.variance());

    // 测试协方差
    let dist1 = Distribution::uniform(vec![1, 2, 3]);
    let dist2 = Distribution::uniform(vec![4, 5, 6]);
    println!("协方差: {}", covariance(&dist1, &dist2));
    println!("相关系数: {}", correlation(&dist1, &dist2));

    // 测试贝叶斯定理
    let posterior = bayes_theorem(0.1, 0.05, 0.8);
    println!("后验概率: {}", posterior);

    // 测试大数定律
    let mean = law_of_large_numbers(&uniform, 1000);
    println!("大数定律验证 (n=1000): {}", mean);

    // 测试中心极限定理
    let sample_means = central_limit_theorem(&uniform, 100, 1000);
    let clt_mean = sample_means.iter().sum::<f64>() / sample_means.len() as f64;
    println!("中心极限定理验证: {}", clt_mean);
}

fn main() {
    test_probability();
}
```

## 9. 习题与练习

### 9.1 基础练习

**练习 9.1.1**
计算以下概率：

1. 掷两个骰子，和为7的概率
2. 从52张牌中随机抽取5张，得到同花顺的概率
3. 抛硬币10次，恰好出现5次正面的概率

**练习 9.1.2**
验证以下分布的性质：

1. 均匀分布 $U(a,b)$ 的期望和方差
2. 伯努利分布 $B(p)$ 的期望和方差
3. 正态分布 $N(\mu,\sigma^2)$ 的期望和方差

**练习 9.1.3**
证明以下定理：

1. 期望的线性性
2. 方差的计算公式
3. 独立随机变量的期望性质

### 9.2 中级练习

**练习 9.2.1**
研究以下问题：

1. 证明切比雪夫不等式
2. 证明马尔可夫不等式
3. 证明詹森不等式

**练习 9.2.2**
计算以下条件概率：

1. 在已知第一个孩子是男孩的条件下，第二个孩子也是男孩的概率
2. 在已知至少有一个男孩的条件下，两个孩子都是男孩的概率

**练习 9.2.3**
研究以下随机变量：

1. 几何分布的期望和方差
2. 泊松分布的期望和方差
3. 指数分布的期望和方差

### 9.3 高级练习

**练习 9.3.1**
研究以下理论问题：

1. 证明大数定律
2. 证明中心极限定理
3. 研究随机变量的收敛性

**练习 9.3.2**
实现以下算法：

1. 随机数生成算法
2. 蒙特卡洛积分算法
3. 马尔可夫链蒙特卡洛算法

**练习 9.3.3**
研究以下应用问题：

1. 概率在金融风险管理中的应用
2. 概率在信息论中的应用
3. 概率在机器学习中的应用

## 10. 参考文献

1. **Kolmogorov, A. N.** (1933). *Foundations of the Theory of Probability*. Chelsea.

2. **Feller, W.** (1968). *An Introduction to Probability Theory and Its Applications*. Wiley.

3. **Billingsley, P.** (1995). *Probability and Measure*. Wiley.

4. **Durrett, R.** (2019). *Probability: Theory and Examples*. Cambridge University Press.

5. **Grimmett, G., Stirzaker, D.** (2001). *Probability and Random Processes*. Oxford University Press.

6. **Ross, S. M.** (2014). *A First Course in Probability*. Pearson.

7. **Sheldon, M. R.** (2014). *Introduction to Probability Models*. Academic Press.

8. **Williams, D.** (1991). *Probability with Martingales*. Cambridge University Press.

---

> **文档信息**
>
> - **创建时间**：2024年12月19日
> - **最后更新**：2024年12月19日
> - **版本**：1.0
> - **状态**：已完成
> - **下一步**：创建 02-随机变量与分布.md
