# 05-极值与凸性

## 目录

1. [引言](#引言)
2. [函数的极值](#函数的极值)
3. [凸函数理论](#凸函数理论)
4. [极值判别法](#极值判别法)
5. [凸性判别法](#凸性判别法)
6. [应用与实例](#应用与实例)
7. [代码实现](#代码实现)
8. [习题与练习](#习题与练习)

## 引言

极值与凸性是微分学中的重要概念，它们在优化理论、经济学、物理学等领域有广泛应用。极值理论帮助我们找到函数的最大值和最小值，而凸性理论则为函数的几何性质提供了重要的分析工具。

### 基本概念

**定义 1.1** (局部极值)
设函数 $f$ 在点 $x_0$ 的邻域内有定义。如果存在 $\delta > 0$，使得对任意 $x \in (x_0 - \delta, x_0 + \delta)$，都有：
- $f(x) \leq f(x_0)$，则称 $f$ 在 $x_0$ 处取得局部最大值
- $f(x) \geq f(x_0)$，则称 $f$ 在 $x_0$ 处取得局部最小值

**定义 1.2** (全局极值)
设函数 $f$ 在区间 $I$ 上有定义。如果对任意 $x \in I$，都有：
- $f(x) \leq f(x_0)$，则称 $f$ 在 $x_0$ 处取得全局最大值
- $f(x) \geq f(x_0)$，则称 $f$ 在 $x_0$ 处取得全局最小值

## 函数的极值

### 费马定理

**定理 2.1** (费马定理)
设函数 $f$ 在点 $x_0$ 处可导，且在 $x_0$ 处取得局部极值，则 $f'(x_0) = 0$。

**证明**：
假设 $f$ 在 $x_0$ 处取得局部最大值（局部最小值的情况类似）。

由于 $f$ 在 $x_0$ 处可导，我们有：
$$f'(x_0) = \lim_{h \to 0} \frac{f(x_0 + h) - f(x_0)}{h}$$

当 $h > 0$ 时，$f(x_0 + h) \leq f(x_0)$，因此 $\frac{f(x_0 + h) - f(x_0)}{h} \leq 0$。

当 $h < 0$ 时，$f(x_0 + h) \leq f(x_0)$，因此 $\frac{f(x_0 + h) - f(x_0)}{h} \geq 0$。

由于极限存在，必有 $f'(x_0) = 0$。

### 临界点

**定义 2.1** (临界点)
如果 $f'(x_0) = 0$ 或 $f'(x_0)$ 不存在，则称 $x_0$ 为函数 $f$ 的临界点。

**注意**：费马定理表明，如果函数在一点处取得局部极值且在该点可导，则该点必为临界点。但临界点不一定是极值点。

### 极值点的几何意义

极值点的几何意义是：函数图像在该点的切线是水平的（如果函数在该点可导）。

## 凸函数理论

### 凸函数的定义

**定义 3.1** (凸函数)
设函数 $f$ 在区间 $I$ 上有定义。如果对任意 $x_1, x_2 \in I$ 和任意 $\lambda \in [0, 1]$，都有：
$$f(\lambda x_1 + (1 - \lambda) x_2) \leq \lambda f(x_1) + (1 - \lambda) f(x_2)$$

则称 $f$ 在 $I$ 上是凸函数。

**定义 3.2** (严格凸函数)
如果上述不等式对 $\lambda \in (0, 1)$ 严格成立，则称 $f$ 在 $I$ 上是严格凸函数。

**定义 3.3** (凹函数)
如果 $-f$ 是凸函数，则称 $f$ 是凹函数。

### 凸函数的几何意义

凸函数的几何意义是：函数图像上任意两点之间的线段都在函数图像的上方或重合。

### 凸函数的性质

**定理 3.1**
如果 $f$ 在区间 $I$ 上是凸函数，则 $f$ 在 $I$ 的内部连续。

**定理 3.2**
如果 $f$ 在区间 $I$ 上是凸函数，则 $f$ 在 $I$ 的内部几乎处处可导。

**定理 3.3**
如果 $f$ 在区间 $I$ 上二阶可导，则 $f$ 在 $I$ 上是凸函数的充要条件是 $f''(x) \geq 0$ 对所有 $x \in I$ 成立。

## 极值判别法

### 一阶导数判别法

**定理 4.1** (一阶导数判别法)
设函数 $f$ 在点 $x_0$ 的邻域内连续，在 $x_0$ 的去心邻域内可导。

1. 如果 $f'(x) > 0$ 对 $x \in (x_0 - \delta, x_0)$ 成立，且 $f'(x) < 0$ 对 $x \in (x_0, x_0 + \delta)$ 成立，则 $f$ 在 $x_0$ 处取得局部最大值。

2. 如果 $f'(x) < 0$ 对 $x \in (x_0 - \delta, x_0)$ 成立，且 $f'(x) > 0$ 对 $x \in (x_0, x_0 + \delta)$ 成立，则 $f$ 在 $x_0$ 处取得局部最小值。

### 二阶导数判别法

**定理 4.2** (二阶导数判别法)
设函数 $f$ 在点 $x_0$ 处二阶可导，且 $f'(x_0) = 0$。

1. 如果 $f''(x_0) < 0$，则 $f$ 在 $x_0$ 处取得局部最大值。

2. 如果 $f''(x_0) > 0$，则 $f$ 在 $x_0$ 处取得局部最小值。

3. 如果 $f''(x_0) = 0$，则无法确定（需要进一步分析）。

**证明**：
根据泰勒公式，在 $x_0$ 附近有：
$$f(x) = f(x_0) + f'(x_0)(x - x_0) + \frac{f''(\xi)}{2}(x - x_0)^2$$

由于 $f'(x_0) = 0$，我们有：
$$f(x) - f(x_0) = \frac{f''(\xi)}{2}(x - x_0)^2$$

如果 $f''(x_0) < 0$，由于 $f''$ 连续，在 $x_0$ 附近 $f''(\xi) < 0$，因此 $f(x) - f(x_0) < 0$，即 $f$ 在 $x_0$ 处取得局部最大值。

类似地可以证明其他情况。

### 高阶导数判别法

**定理 4.3** (高阶导数判别法)
设函数 $f$ 在点 $x_0$ 处 $n$ 阶可导，且 $f'(x_0) = f''(x_0) = \cdots = f^{(n-1)}(x_0) = 0$，$f^{(n)}(x_0) \neq 0$。

1. 如果 $n$ 为偶数且 $f^{(n)}(x_0) < 0$，则 $f$ 在 $x_0$ 处取得局部最大值。

2. 如果 $n$ 为偶数且 $f^{(n)}(x_0) > 0$，则 $f$ 在 $x_0$ 处取得局部最小值。

3. 如果 $n$ 为奇数，则 $x_0$ 不是极值点。

## 凸性判别法

### 一阶导数判别法

**定理 5.1**
设函数 $f$ 在区间 $I$ 上可导，则 $f$ 在 $I$ 上是凸函数的充要条件是 $f'$ 在 $I$ 上单调递增。

**证明**：
**必要性**：设 $f$ 是凸函数，$x_1 < x_2$。对任意 $\lambda \in (0, 1)$，有：
$$f(\lambda x_1 + (1 - \lambda) x_2) \leq \lambda f(x_1) + (1 - \lambda) f(x_2)$$

整理得：
$$\frac{f(x_2) - f(\lambda x_1 + (1 - \lambda) x_2)}{(1 - \lambda)(x_2 - x_1)} \geq \frac{f(\lambda x_1 + (1 - \lambda) x_2) - f(x_1)}{\lambda(x_2 - x_1)}$$

令 $\lambda \to 0$，得到 $f'(x_2) \geq f'(x_1)$。

**充分性**：设 $f'$ 单调递增，$x_1 < x_2$，$\lambda \in (0, 1)$。设 $c = \lambda x_1 + (1 - \lambda) x_2$。

根据拉格朗日中值定理：
$$f(x_1) = f(c) + f'(\xi_1)(x_1 - c)$$
$$f(x_2) = f(c) + f'(\xi_2)(x_2 - c)$$

其中 $\xi_1 \in (x_1, c)$，$\xi_2 \in (c, x_2)$。

由于 $f'$ 单调递增，$f'(\xi_1) \leq f'(\xi_2)$。

计算 $\lambda f(x_1) + (1 - \lambda) f(x_2)$ 并整理，可以得到凸函数的定义。

### 二阶导数判别法

**定理 5.2**
设函数 $f$ 在区间 $I$ 上二阶可导，则：
1. $f$ 在 $I$ 上是凸函数的充要条件是 $f''(x) \geq 0$ 对所有 $x \in I$ 成立。
2. $f$ 在 $I$ 上是严格凸函数的充要条件是 $f''(x) > 0$ 对所有 $x \in I$ 成立。

### 凸函数的性质

**定理 5.3**
如果 $f$ 是凸函数，则：
1. $f$ 的局部最小值也是全局最小值。
2. 如果 $f$ 是严格凸函数，则全局最小值是唯一的。

**定理 5.4** (Jensen不等式)
如果 $f$ 是凸函数，$x_1, x_2, \ldots, x_n$ 是任意实数，$\lambda_1, \lambda_2, \ldots, \lambda_n$ 是非负实数且 $\sum_{i=1}^n \lambda_i = 1$，则：
$$f\left(\sum_{i=1}^n \lambda_i x_i\right) \leq \sum_{i=1}^n \lambda_i f(x_i)$$

## 应用与实例

### 实例 1：函数极值分析

**问题**：求函数 $f(x) = x^3 - 3x^2 + 2$ 的极值点。

**解**：
1. 计算导数：$f'(x) = 3x^2 - 6x = 3x(x - 2)$
2. 临界点：$x = 0$ 和 $x = 2$
3. 计算二阶导数：$f''(x) = 6x - 6$
4. 判别：
   - $f''(0) = -6 < 0$，因此 $x = 0$ 是局部最大值点
   - $f''(2) = 6 > 0$，因此 $x = 2$ 是局部最小值点

### 实例 2：凸性分析

**问题**：分析函数 $f(x) = e^x$ 的凸性。

**解**：
1. 计算导数：$f'(x) = e^x$，$f''(x) = e^x > 0$
2. 由于 $f''(x) > 0$ 对所有 $x \in \mathbb{R}$ 成立，$f$ 是严格凸函数

### 实例 3：优化问题

**问题**：在给定周长 $P$ 的矩形中，求面积最大的矩形。

**解**：
设矩形的长为 $x$，宽为 $y$，则 $2(x + y) = P$，即 $y = \frac{P}{2} - x$。

面积函数为：$A(x) = x\left(\frac{P}{2} - x\right) = \frac{P}{2}x - x^2$

计算导数：$A'(x) = \frac{P}{2} - 2x$

临界点：$x = \frac{P}{4}$

计算二阶导数：$A''(x) = -2 < 0$

因此 $x = \frac{P}{4}$ 是最大值点，此时 $y = \frac{P}{4}$，即正方形面积最大。

## 代码实现

### Rust实现

```rust
use std::f64;

/// 极值与凸性分析
pub struct ExtremumAnalysis;

impl ExtremumAnalysis {
    /// 使用数值方法计算导数
    pub fn derivative<F>(f: F, x: f64, h: f64) -> f64 
    where 
        F: Fn(f64) -> f64 
    {
        (f(x + h) - f(x - h)) / (2.0 * h)
    }
    
    /// 计算二阶导数
    pub fn second_derivative<F>(f: F, x: f64, h: f64) -> f64 
    where 
        F: Fn(f64) -> f64 + Copy 
    {
        (f(x + h) - 2.0 * f(x) + f(x - h)) / (h * h)
    }
    
    /// 寻找临界点
    pub fn find_critical_points<F>(
        f: F, 
        a: f64, 
        b: f64, 
        step: f64,
        tolerance: f64
    ) -> Vec<f64> 
    where 
        F: Fn(f64) -> f64 + Copy 
    {
        let mut critical_points = Vec::new();
        let mut x = a;
        
        while x <= b {
            let derivative = Self::derivative(f, x, 1e-6);
            if derivative.abs() < tolerance {
                critical_points.push(x);
            }
            x += step;
        }
        
        critical_points
    }
    
    /// 判别极值类型
    pub fn classify_extremum<F>(
        f: F, 
        x: f64, 
        tolerance: f64
    ) -> ExtremumType 
    where 
        F: Fn(f64) -> f64 + Copy 
    {
        let second_deriv = Self::second_derivative(f, x, 1e-6);
        
        if second_deriv.abs() < tolerance {
            // 需要进一步分析
            ExtremumType::Undetermined
        } else if second_deriv > 0.0 {
            ExtremumType::LocalMinimum
        } else {
            ExtremumType::LocalMaximum
        }
    }
    
    /// 检查函数的凸性
    pub fn check_convexity<F>(
        f: F, 
        a: f64, 
        b: f64, 
        step: f64,
        tolerance: f64
    ) -> ConvexityType 
    where 
        F: Fn(f64) -> f64 + Copy 
    {
        let mut x = a;
        let mut all_positive = true;
        let mut all_negative = true;
        
        while x <= b {
            let second_deriv = Self::second_derivative(f, x, 1e-6);
            
            if second_deriv > tolerance {
                all_negative = false;
            } else if second_deriv < -tolerance {
                all_positive = false;
            } else {
                all_positive = false;
                all_negative = false;
            }
            
            x += step;
        }
        
        if all_positive {
            ConvexityType::Convex
        } else if all_negative {
            ConvexityType::Concave
        } else {
            ConvexityType::Neither
        }
    }
    
    /// 寻找全局最小值（凸函数）
    pub fn find_global_minimum_convex<F>(
        f: F, 
        a: f64, 
        b: f64, 
        tolerance: f64
    ) -> Option<(f64, f64)> 
    where 
        F: Fn(f64) -> f64 + Copy 
    {
        // 使用黄金分割搜索
        let phi = (1.0 + 5.0_f64.sqrt()) / 2.0;
        let mut left = a;
        let mut right = b;
        
        for _ in 0..1000 {
            let h = right - left;
            let x1 = right - h / phi;
            let x2 = left + h / phi;
            
            if f(x1) < f(x2) {
                right = x2;
            } else {
                left = x1;
            }
            
            if h < tolerance {
                let min_x = (left + right) / 2.0;
                return Some((min_x, f(min_x)));
            }
        }
        
        None
    }
    
    /// 使用梯度下降寻找局部最小值
    pub fn gradient_descent<F>(
        f: F, 
        mut x: f64, 
        learning_rate: f64, 
        max_iterations: usize,
        tolerance: f64
    ) -> Option<(f64, f64)> 
    where 
        F: Fn(f64) -> f64 + Copy 
    {
        for _ in 0..max_iterations {
            let derivative = Self::derivative(f, x, 1e-6);
            let new_x = x - learning_rate * derivative;
            
            if (new_x - x).abs() < tolerance {
                return Some((new_x, f(new_x)));
            }
            
            x = new_x;
        }
        
        None
    }
}

/// 极值类型
#[derive(Debug, Clone, Copy, PartialEq)]
pub enum ExtremumType {
    LocalMaximum,
    LocalMinimum,
    Undetermined,
}

/// 凸性类型
#[derive(Debug, Clone, Copy, PartialEq)]
pub enum ConvexityType {
    Convex,
    Concave,
    Neither,
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_critical_points() {
        let f = |x: f64| x.powi(3) - 3.0 * x.powi(2) + 2.0;
        let critical_points = ExtremumAnalysis::find_critical_points(f, -1.0, 3.0, 0.1, 1e-4);
        
        // 应该找到 x = 0 和 x = 2
        assert!(critical_points.len() >= 2);
        assert!(critical_points.iter().any(|&x| (x - 0.0).abs() < 0.1));
        assert!(critical_points.iter().any(|&x| (x - 2.0).abs() < 0.1));
    }
    
    #[test]
    fn test_extremum_classification() {
        let f = |x: f64| x.powi(3) - 3.0 * x.powi(2) + 2.0;
        
        let extremum_type_0 = ExtremumAnalysis::classify_extremum(f, 0.0, 1e-4);
        assert_eq!(extremum_type_0, ExtremumType::LocalMaximum);
        
        let extremum_type_2 = ExtremumAnalysis::classify_extremum(f, 2.0, 1e-4);
        assert_eq!(extremum_type_2, ExtremumType::LocalMinimum);
    }
    
    #[test]
    fn test_convexity() {
        let f = |x: f64| x.exp();
        let convexity = ExtremumAnalysis::check_convexity(f, -2.0, 2.0, 0.1, 1e-4);
        assert_eq!(convexity, ConvexityType::Convex);
        
        let g = |x: f64| -x.powi(2);
        let convexity_g = ExtremumAnalysis::check_convexity(g, -2.0, 2.0, 0.1, 1e-4);
        assert_eq!(convexity_g, ConvexityType::Concave);
    }
    
    #[test]
    fn test_global_minimum() {
        let f = |x: f64| x.powi(2) + 2.0 * x + 1.0;
        let result = ExtremumAnalysis::find_global_minimum_convex(f, -5.0, 5.0, 1e-6);
        
        assert!(result.is_some());
        if let Some((x, y)) = result {
            assert!((x - (-1.0)).abs() < 0.01);
            assert!((y - 0.0).abs() < 0.01);
        }
    }
    
    #[test]
    fn test_gradient_descent() {
        let f = |x: f64| x.powi(2) + 2.0 * x + 1.0;
        let result = ExtremumAnalysis::gradient_descent(f, 1.0, 0.1, 1000, 1e-6);
        
        assert!(result.is_some());
        if let Some((x, y)) = result {
            assert!((x - (-1.0)).abs() < 0.01);
            assert!((y - 0.0).abs() < 0.01);
        }
    }
}
```

### Haskell实现

```haskell
module ExtremumAnalysis where

import Data.List (minimumBy, maximumBy)
import Data.Ord (comparing)

-- 极值类型
data ExtremumType = LocalMaximum | LocalMinimum | Undetermined deriving (Eq, Show)

-- 凸性类型
data ConvexityType = Convex | Concave | Neither deriving (Eq, Show)

-- 数值微分
derivative :: (Double -> Double) -> Double -> Double -> Double
derivative f x h = (f (x + h) - f (x - h)) / (2 * h)

secondDerivative :: (Double -> Double) -> Double -> Double -> Double
secondDerivative f x h = (f (x + h) - 2 * f x + f (x - h)) / (h * h)

-- 寻找临界点
findCriticalPoints :: (Double -> Double) -> Double -> Double -> Double -> Double -> [Double]
findCriticalPoints f a b step tolerance = 
    [x | x <- [a, a + step..b], abs (derivative f x 1e-6) < tolerance]

-- 判别极值类型
classifyExtremum :: (Double -> Double) -> Double -> Double -> ExtremumType
classifyExtremum f x tolerance = 
    let secondDeriv = secondDerivative f x 1e-6
    in if abs secondDeriv < tolerance
       then Undetermined
       else if secondDeriv > 0
            then LocalMinimum
            else LocalMaximum

-- 检查凸性
checkConvexity :: (Double -> Double) -> Double -> Double -> Double -> Double -> ConvexityType
checkConvexity f a b step tolerance = 
    let points = [a, a + step..b]
        secondDerivs = map (\x -> secondDerivative f x 1e-6) points
        allPositive = all (> tolerance) secondDerivs
        allNegative = all (< -tolerance) secondDerivs
    in if allPositive
       then Convex
       else if allNegative
            then Concave
            else Neither

-- 黄金分割搜索（用于凸函数优化）
goldenSectionSearch :: (Double -> Double) -> Double -> Double -> Double -> (Double, Double)
goldenSectionSearch f a b tolerance = 
    let phi = (1 + sqrt 5) / 2
        search left right = 
            let h = right - left
                x1 = right - h / phi
                x2 = left + h / phi
            in if h < tolerance
               then ((left + right) / 2, f ((left + right) / 2)
               else if f x1 < f x2
                    then search left x2
                    else search x1 right
    in search a b

-- 梯度下降
gradientDescent :: (Double -> Double) -> Double -> Double -> Int -> Double -> Maybe (Double, Double)
gradientDescent f x0 learningRate maxIterations tolerance = 
    let step x iteration = 
            if iteration >= maxIterations
            then Nothing
            else let deriv = derivative f x 1e-6
                     newX = x - learningRate * deriv
                 in if abs (newX - x) < tolerance
                    then Just (newX, f newX)
                    else step newX (iteration + 1)
    in step x0 0

-- 寻找所有极值点
findAllExtrema :: (Double -> Double) -> Double -> Double -> Double -> Double -> [(Double, ExtremumType)]
findAllExtrema f a b step tolerance = 
    let criticalPoints = findCriticalPoints f a b step tolerance
    in [(x, classifyExtremum f x tolerance) | x <- criticalPoints]

-- 凸函数优化
optimizeConvexFunction :: (Double -> Double) -> Double -> Double -> Double -> (Double, Double)
optimizeConvexFunction f a b tolerance = goldenSectionSearch f a b tolerance

-- 多变量函数的梯度（简化版本）
gradient2D :: (Double -> Double -> Double) -> Double -> Double -> Double -> (Double, Double)
gradient2D f x y h = 
    let dx = (f (x + h) y - f (x - h) y) / (2 * h)
        dy = (f x (y + h) - f x (y - h)) / (2 * h)
    in (dx, dy)

-- 测试函数
testFunctions :: [(String, Double -> Double)]
testFunctions = 
    [ ("x^3 - 3x^2 + 2", \x -> x^3 - 3*x^2 + 2)
    , ("x^2 + 2x + 1", \x -> x^2 + 2*x + 1)
    , ("exp(x)", exp)
    , ("-x^2", \x -> -x^2)
    ]

-- 运行测试
runTests :: IO ()
runTests = do
    putStrLn "Testing Extremum Analysis..."
    
    -- 测试临界点
    let f x = x^3 - 3*x^2 + 2
    let criticalPoints = findCriticalPoints f (-1) 3 0.1 1e-4
    putStrLn $ "Critical points: " ++ show criticalPoints
    
    -- 测试极值分类
    let extremumTypes = [(x, classifyExtremum f x 1e-4) | x <- criticalPoints]
    putStrLn $ "Extremum types: " ++ show extremumTypes
    
    -- 测试凸性
    let convexity = checkConvexity exp (-2) 2 0.1 1e-4
    putStrLn $ "Convexity of exp(x): " ++ show convexity
    
    -- 测试优化
    let g x = x^2 + 2*x + 1
    let (minX, minY) = optimizeConvexFunction g (-5) 5 1e-6
    putStrLn $ "Minimum of x^2 + 2x + 1: (" ++ show minX ++ ", " ++ show minY ++ ")"
    
    -- 测试梯度下降
    case gradientDescent g 1 0.1 1000 1e-6 of
        Just (x, y) -> putStrLn $ "Gradient descent result: (" ++ show x ++ ", " ++ show y ++ ")"
        Nothing -> putStrLn "Gradient descent failed to converge"

-- 可视化函数
visualizeFunction :: (Double -> Double) -> Double -> Double -> Double -> IO ()
visualizeFunction f a b step = do
    putStrLn $ "Function values from " ++ show a ++ " to " ++ show b
    putStrLn "x\tf(x)"
    putStrLn "----------------"
    mapM_ (\x -> putStrLn $ show x ++ "\t" ++ show (f x)) [a, a + step..b]
```

## 习题与练习

### 基础练习

1. **练习 1**：求函数 $f(x) = x^4 - 4x^3 + 6x^2 - 4x + 1$ 的极值点，并判别其类型。

2. **练习 2**：分析函数 $f(x) = \frac{1}{1 + x^2}$ 的凸性。

3. **练习 3**：证明函数 $f(x) = x^2$ 在 $\mathbb{R}$ 上是严格凸函数。

### 进阶练习

4. **练习 4**：设函数 $f$ 在区间 $[a, b]$ 上连续，在 $(a, b)$ 上可导，且 $f(a) = f(b)$。证明存在 $c \in (a, b)$ 使得 $f'(c) = 0$。

5. **练习 5**：使用凸函数理论证明算术-几何平均值不等式：对任意正数 $a_1, a_2, \ldots, a_n$，有：
   $$\frac{a_1 + a_2 + \cdots + a_n}{n} \geq \sqrt[n]{a_1 a_2 \cdots a_n}$$

6. **练习 6**：求函数 $f(x) = x^3 - 3x$ 在区间 $[-2, 2]$ 上的全局最大值和最小值。

### 编程练习

7. **练习 7**：实现一个函数，自动寻找给定函数的所有极值点并分类。

8. **练习 8**：编写程序可视化函数的凸性，绘制函数图像和其二阶导数。

9. **练习 9**：实现多变量函数的极值分析算法。

### 应用练习

10. **练习 10**：在经济学中，成本函数通常假设为凸函数。分析函数 $C(x) = x^2 + 2x + 10$ 的凸性，并解释其经济学意义。

11. **练习 11**：在物理学中，势能函数的最小值对应稳定平衡点。分析函数 $V(x) = x^4 - 2x^2$ 的极值，并确定稳定平衡点。

12. **练习 12**：在机器学习中，损失函数通常是凸函数。分析函数 $L(x) = (x - 1)^2$ 的凸性，并说明为什么这种函数适合作为损失函数。

---

**相关链接**：
- [03-中值定理](../03-中值定理.md)
- [04-泰勒展开](../04-泰勒展开.md)
- [返回微分学总览](../00-微分学总览.md)
- [返回微积分与分析总览](../../00-微积分与分析总览.md) 