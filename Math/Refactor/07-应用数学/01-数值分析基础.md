# 01-数值分析基础

## 目录

1. [数值分析概述](#1-数值分析概述)
2. [误差分析](#2-误差分析)
3. [插值与逼近](#3-插值与逼近)
4. [数值积分](#4-数值积分)
5. [数值微分](#5-数值微分)
6. [线性方程组数值解](#6-线性方程组数值解)
7. [非线性方程求根](#7-非线性方程求根)
8. [常微分方程数值解](#8-常微分方程数值解)
9. [代码实现](#9-代码实现)
10. [习题与练习](#10-习题与练习)

## 1. 数值分析概述

### 1.1 数值分析的定义

**定义 1.1** (数值分析)
数值分析是研究用计算机求解数学问题的算法及其理论，包括误差分析、收敛性、稳定性等。

### 1.2 数值分析的重要性

**重要性**：

1. **解析解不存在**：许多数学问题无法获得解析解
2. **计算复杂性**：解析解过于复杂，难以计算
3. **实际问题需求**：工程和科学计算需要数值解

### 1.3 数值分析的基本问题

**基本问题**：

1. **误差控制**：如何控制计算误差
2. **算法稳定性**：如何保证算法稳定
3. **收敛性**：如何保证算法收敛
4. **计算效率**：如何提高计算效率

## 2. 误差分析

### 2.1 误差类型

#### 2.1.1 截断误差

**定义 2.1** (截断误差)
由于使用近似公式而产生的误差称为截断误差。

**例子**：
泰勒级数截断：
$$e^x = 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \cdots$$
如果只取前3项，截断误差为：
$$R_3(x) = \frac{x^4}{4!} + \frac{x^5}{5!} + \cdots$$

#### 2.1.2 舍入误差

**定义 2.2** (舍入误差)
由于计算机浮点数精度限制而产生的误差称为舍入误差。

**例子**：
$$\pi = 3.141592653589793...$$
如果只保留6位小数，舍入误差为：
$$|\pi - 3.141593| \approx 2.65 \times 10^{-7}$$

### 2.2 误差传播

#### 2.2.1 绝对误差与相对误差

**定义 2.3** (绝对误差)
设 $x$ 是精确值，$\tilde{x}$ 是近似值，则绝对误差为：
$$E_a = |x - \tilde{x}|$$

**定义 2.4** (相对误差)
相对误差为：
$$E_r = \frac{|x - \tilde{x}|}{|x|}$$

#### 2.2.2 误差传播公式

**定理 2.1** (误差传播)
设 $f(x_1, x_2, \ldots, x_n)$ 是可微函数，$x_i$ 的绝对误差为 $\Delta x_i$，则：
$$\Delta f \approx \sum_{i=1}^n \left|\frac{\partial f}{\partial x_i}\right| \Delta x_i$$

**证明**：
由泰勒展开：
$$f(x_1 + \Delta x_1, \ldots, x_n + \Delta x_n) \approx f(x_1, \ldots, x_n) + \sum_{i=1}^n \frac{\partial f}{\partial x_i} \Delta x_i$$

因此：
$$\Delta f = |f(x_1 + \Delta x_1, \ldots, x_n + \Delta x_n) - f(x_1, \ldots, x_n)| \approx \left|\sum_{i=1}^n \frac{\partial f}{\partial x_i} \Delta x_i\right|$$

由三角不等式：
$$\left|\sum_{i=1}^n \frac{\partial f}{\partial x_i} \Delta x_i\right| \leq \sum_{i=1}^n \left|\frac{\partial f}{\partial x_i}\right| |\Delta x_i|$$

### 2.3 条件数

**定义 2.5** (条件数)
函数 $f$ 在点 $x$ 处的条件数为：
$$\kappa_f(x) = \left|\frac{x f'(x)}{f(x)}\right|$$

**意义**：

- $\kappa_f(x) \ll 1$：问题良态
- $\kappa_f(x) \gg 1$：问题病态

## 3. 插值与逼近

### 3.1 拉格朗日插值

#### 3.1.1 基本思想

**基本思想**：
通过 $n+1$ 个已知点 $(x_i, y_i)$ 构造 $n$ 次多项式 $P_n(x)$，使得 $P_n(x_i) = y_i$。

#### 3.1.2 拉格朗日基函数

**定义 3.1** (拉格朗日基函数)
$$L_i(x) = \prod_{j=0, j \neq i}^n \frac{x - x_j}{x_i - x_j}$$

**性质**：

1. $L_i(x_j) = \delta_{ij}$
2. $\sum_{i=0}^n L_i(x) = 1$

#### 3.1.3 拉格朗日插值多项式

**定理 3.1** (拉格朗日插值)
$$P_n(x) = \sum_{i=0}^n y_i L_i(x)$$

**误差估计**：
$$f(x) - P_n(x) = \frac{f^{(n+1)}(\xi)}{(n+1)!} \prod_{i=0}^n (x - x_i)$$
其中 $\xi \in [a, b]$，$a = \min\{x_i\}$，$b = \max\{x_i\}$。

### 3.2 牛顿插值

#### 3.2.1 差商

**定义 3.2** (差商)
一阶差商：
$$f[x_i, x_j] = \frac{f(x_j) - f(x_i)}{x_j - x_i}$$

$k$ 阶差商：
$$f[x_0, x_1, \ldots, x_k] = \frac{f[x_1, \ldots, x_k] - f[x_0, \ldots, x_{k-1}]}{x_k - x_0}$$

#### 3.2.2 牛顿插值多项式

**定理 3.2** (牛顿插值)
$$P_n(x) = f[x_0] + f[x_0, x_1](x - x_0) + f[x_0, x_1, x_2](x - x_0)(x - x_1) + \cdots + f[x_0, x_1, \ldots, x_n] \prod_{i=0}^{n-1} (x - x_i)$$

### 3.3 样条插值

#### 3.3.1 三次样条

**定义 3.3** (三次样条)
在区间 $[a, b]$ 上，给定节点 $a = x_0 < x_1 < \cdots < x_n = b$，三次样条函数 $S(x)$ 满足：

1. $S(x)$ 在每个子区间 $[x_i, x_{i+1}]$ 上是三次多项式
2. $S(x_i) = y_i$，$i = 0, 1, \ldots, n$
3. $S(x)$ 在 $[a, b]$ 上连续，且一阶、二阶导数连续

#### 3.3.2 样条方程

**方程**：
对于自然样条（$S''(x_0) = S''(x_n) = 0$）：
$$h_{i-1} M_{i-1} + 2(h_{i-1} + h_i) M_i + h_i M_{i+1} = 6 \left(\frac{y_{i+1} - y_i}{h_i} - \frac{y_i - y_{i-1}}{h_{i-1}}\right)$$

其中 $h_i = x_{i+1} - x_i$，$M_i = S''(x_i)$。

## 4. 数值积分

### 4.1 牛顿-科茨公式

#### 4.1.1 梯形法则

**公式**：
$$\int_a^b f(x) dx \approx \frac{h}{2} [f(a) + f(b)]$$

**误差**：
$$E = -\frac{(b-a)h^2}{12} f''(\xi)$$

#### 4.1.2 辛普森法则

**公式**：
$$\int_a^b f(x) dx \approx \frac{h}{3} [f(a) + 4f(c) + f(b)]$$

其中 $c = \frac{a+b}{2}$，$h = \frac{b-a}{2}$。

**误差**：
$$E = -\frac{(b-a)h^4}{180} f^{(4)}(\xi)$$

### 4.2 高斯求积

#### 4.2.1 基本思想

**基本思想**：
选择求积点和权重，使得对于尽可能高次的多项式，求积公式都能给出精确结果。

#### 4.2.2 高斯-勒让德求积

**公式**：
$$\int_{-1}^1 f(x) dx \approx \sum_{i=1}^n w_i f(x_i)$$

其中 $x_i$ 是勒让德多项式 $P_n(x)$ 的零点，$w_i$ 是对应的权重。

**权重公式**：
$$w_i = \frac{2}{[1-x_i^2](P_n'(x_i))^2}$$

## 5. 数值微分

### 5.1 有限差分

#### 5.1.1 前向差分

**公式**：
$$f'(x) \approx \frac{f(x+h) - f(x)}{h}$$

**误差**：
$$E = \frac{h}{2} f''(\xi)$$

#### 5.1.2 中心差分

**公式**：
$$f'(x) \approx \frac{f(x+h) - f(x-h)}{2h}$$

**误差**：
$$E = \frac{h^2}{6} f'''(\xi)$$

#### 5.1.3 二阶导数

**公式**：
$$f''(x) \approx \frac{f(x+h) - 2f(x) + f(x-h)}{h^2}$$

**误差**：
$$E = \frac{h^2}{12} f^{(4)}(\xi)$$

### 5.2 理查森外推

#### 5.2.1 基本思想

**基本思想**：
通过组合不同步长的近似值，消除低阶误差项。

#### 5.2.2 外推公式

**定理 5.1** (理查森外推)
设 $D(h)$ 是步长为 $h$ 的数值微分近似值，则：
$$D_1(h) = \frac{4D(h/2) - D(h)}{3}$$

**高阶外推**：
$$D_k(h) = \frac{4^k D_{k-1}(h/2) - D_{k-1}(h)}{4^k - 1}$$

## 6. 线性方程组数值解

### 6.1 直接方法

#### 6.1.1 高斯消元法

**算法步骤**：

1. 前向消元：将矩阵化为上三角形式
2. 后向代入：求解上三角方程组

**复杂度**：$O(n^3)$

#### 6.1.2 LU分解

**定理 6.1** (LU分解)
如果矩阵 $A$ 的所有主子式都不为零，则 $A$ 可以分解为：
$$A = LU$$
其中 $L$ 是下三角矩阵，$U$ 是上三角矩阵。

**求解过程**：

1. 分解：$A = LU$
2. 求解：$Ly = b$
3. 求解：$Ux = y$

### 6.2 迭代方法

#### 6.2.1 雅可比迭代

**迭代公式**：
$$x_i^{(k+1)} = \frac{1}{a_{ii}} \left(b_i - \sum_{j=1, j \neq i}^n a_{ij} x_j^{(k)}\right)$$

**收敛条件**：
矩阵 $A$ 严格对角占优。

#### 6.2.2 高斯-赛德尔迭代

**迭代公式**：
$$x_i^{(k+1)} = \frac{1}{a_{ii}} \left(b_i - \sum_{j=1}^{i-1} a_{ij} x_j^{(k+1)} - \sum_{j=i+1}^n a_{ij} x_j^{(k)}\right)$$

**收敛条件**：
矩阵 $A$ 严格对角占优或对称正定。

## 7. 非线性方程求根

### 7.1 二分法

#### 7.1.1 基本思想

**基本思想**：
如果 $f(a) \cdot f(b) < 0$，则在 $[a, b]$ 内必有根。

#### 7.1.2 算法步骤

**算法**：

1. 计算中点 $c = \frac{a+b}{2}$
2. 如果 $f(c) = 0$，则 $c$ 是根
3. 如果 $f(a) \cdot f(c) < 0$，则根在 $[a, c]$ 内
4. 否则根在 $[c, b]$ 内
5. 重复直到精度满足要求

**收敛性**：
每次迭代误差减半，线性收敛。

### 7.2 牛顿法

#### 7.2.1 基本思想

**基本思想**：
用切线近似函数，切线零点作为下一次迭代点。

#### 7.2.2 迭代公式

**公式**：
$$x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}$$

**几何意义**：
在点 $(x_n, f(x_n))$ 处的切线方程为：
$$y = f(x_n) + f'(x_n)(x - x_n)$$

切线与 $x$ 轴的交点就是 $x_{n+1}$。

#### 7.2.3 收敛性

**定理 7.1** (牛顿法收敛性)
如果 $f(x)$ 在根 $x^*$ 的邻域内二阶连续可微，且 $f'(x^*) \neq 0$，则牛顿法二次收敛。

**证明**：
由泰勒展开：
$$f(x_n) = f(x^*) + f'(x^*)(x_n - x^*) + \frac{f''(\xi_n)}{2}(x_n - x^*)^2$$

因此：
$$x_{n+1} - x^* = x_n - x^* - \frac{f(x_n)}{f'(x_n)} = \frac{f''(\xi_n)}{2f'(x_n)}(x_n - x^*)^2$$

### 7.3 割线法

#### 7.3.1 基本思想

**基本思想**：
用割线近似切线，避免计算导数。

#### 7.3.2 迭代公式

**公式**：
$$x_{n+1} = x_n - f(x_n) \frac{x_n - x_{n-1}}{f(x_n) - f(x_{n-1})}$$

**收敛性**：
超线性收敛，收敛阶约为 1.618。

## 8. 常微分方程数值解

### 8.1 欧拉方法

#### 8.1.1 显式欧拉法

**公式**：
$$y_{n+1} = y_n + h f(t_n, y_n)$$

**局部截断误差**：
$$T_{n+1} = \frac{h^2}{2} y''(\xi_n)$$

**全局截断误差**：
$$E_n = O(h)$$

#### 8.1.2 隐式欧拉法

**公式**：
$$y_{n+1} = y_n + h f(t_{n+1}, y_{n+1})$$

**特点**：

- 无条件稳定
- 需要求解非线性方程

### 8.2 龙格-库塔方法

#### 8.2.1 四阶龙格-库塔法

**公式**：
$$y_{n+1} = y_n + \frac{h}{6}(k_1 + 2k_2 + 2k_3 + k_4)$$

其中：
$$k_1 = f(t_n, y_n)$$
$$k_2 = f(t_n + \frac{h}{2}, y_n + \frac{h}{2}k_1)$$
$$k_3 = f(t_n + \frac{h}{2}, y_n + \frac{h}{2}k_2)$$
$$k_4 = f(t_n + h, y_n + hk_3)$$

**局部截断误差**：
$$T_{n+1} = O(h^5)$$

### 8.3 多步法

#### 8.3.1 Adams-Bashforth方法

**四阶Adams-Bashforth**：
$$y_{n+1} = y_n + \frac{h}{24}(55f_n - 59f_{n-1} + 37f_{n-2} - 9f_{n-3})$$

**特点**：

- 显式方法
- 需要多个初始值

#### 8.3.2 Adams-Moulton方法

**四阶Adams-Moulton**：
$$y_{n+1} = y_n + \frac{h}{24}(9f_{n+1} + 19f_n - 5f_{n-1} + f_{n-2})$$

**特点**：

- 隐式方法
- 精度更高

## 9. 代码实现

### 9.1 Rust实现

```rust
use std::f64;

/// 数值分析工具集
pub struct NumericalAnalysis {
    pub tolerance: f64,
    pub max_iterations: usize,
}

impl NumericalAnalysis {
    pub fn new() -> Self {
        Self {
            tolerance: 1e-10,
            max_iterations: 1000,
        }
    }

    /// 拉格朗日插值
    pub fn lagrange_interpolation(&self, x: f64, x_data: &[f64], y_data: &[f64]) -> f64 {
        let n = x_data.len();
        let mut result = 0.0;

        for i in 0..n {
            let mut term = y_data[i];
            for j in 0..n {
                if i != j {
                    term *= (x - x_data[j]) / (x_data[i] - x_data[j]);
                }
            }
            result += term;
        }

        result
    }

    /// 牛顿插值
    pub fn newton_interpolation(&self, x: f64, x_data: &[f64], y_data: &[f64]) -> f64 {
        let n = x_data.len();
        let mut divided_diff = vec![vec![0.0; n]; n];

        // 计算差商
        for i in 0..n {
            divided_diff[i][0] = y_data[i];
        }

        for j in 1..n {
            for i in 0..n-j {
                divided_diff[i][j] = (divided_diff[i+1][j-1] - divided_diff[i][j-1]) / 
                                   (x_data[i+j] - x_data[i]);
            }
        }

        // 计算插值多项式
        let mut result = divided_diff[0][0];
        let mut term = 1.0;

        for i in 1..n {
            term *= x - x_data[i-1];
            result += divided_diff[0][i] * term;
        }

        result
    }

    /// 梯形法则
    pub fn trapezoidal_rule<F>(&self, f: F, a: f64, b: f64, n: usize) -> f64
    where
        F: Fn(f64) -> f64,
    {
        let h = (b - a) / n as f64;
        let mut sum = (f(a) + f(b)) / 2.0;

        for i in 1..n {
            let x = a + i as f64 * h;
            sum += f(x);
        }

        h * sum
    }

    /// 辛普森法则
    pub fn simpson_rule<F>(&self, f: F, a: f64, b: f64, n: usize) -> f64
    where
        F: Fn(f64) -> f64,
    {
        if n % 2 != 0 {
            panic!("n must be even for Simpson's rule");
        }

        let h = (b - a) / n as f64;
        let mut sum = f(a) + f(b);

        for i in 1..n {
            let x = a + i as f64 * h;
            sum += if i % 2 == 0 { 2.0 * f(x) } else { 4.0 * f(x) };
        }

        h * sum / 3.0
    }

    /// 中心差分
    pub fn central_difference<F>(&self, f: F, x: f64, h: f64) -> f64
    where
        F: Fn(f64) -> f64,
    {
        (f(x + h) - f(x - h)) / (2.0 * h)
    }

    /// 牛顿法求根
    pub fn newton_method<F, DF>(&self, f: F, df: DF, x0: f64) -> Option<f64>
    where
        F: Fn(f64) -> f64,
        DF: Fn(f64) -> f64,
    {
        let mut x = x0;

        for _ in 0..self.max_iterations {
            let fx = f(x);
            let dfx = df(x);

            if dfx.abs() < self.tolerance {
                return None;
            }

            let x_new = x - fx / dfx;

            if (x_new - x).abs() < self.tolerance {
                return Some(x_new);
            }

            x = x_new;
        }

        None
    }

    /// 二分法求根
    pub fn bisection_method<F>(&self, f: F, a: f64, b: f64) -> Option<f64>
    where
        F: Fn(f64) -> f64,
    {
        let mut left = a;
        let mut right = b;

        if f(left) * f(right) > 0.0 {
            return None;
        }

        for _ in 0..self.max_iterations {
            let mid = (left + right) / 2.0;
            let f_mid = f(mid);

            if f_mid.abs() < self.tolerance {
                return Some(mid);
            }

            if f(left) * f_mid < 0.0 {
                right = mid;
            } else {
                left = mid;
            }

            if (right - left) < self.tolerance {
                return Some(mid);
            }
        }

        None
    }

    /// 四阶龙格-库塔法
    pub fn runge_kutta_4<F>(&self, f: F, t0: f64, y0: f64, h: f64, steps: usize) -> Vec<(f64, f64)>
    where
        F: Fn(f64, f64) -> f64,
    {
        let mut t = t0;
        let mut y = y0;
        let mut result = vec![(t, y)];

        for _ in 0..steps {
            let k1 = f(t, y);
            let k2 = f(t + h/2.0, y + h/2.0 * k1);
            let k3 = f(t + h/2.0, y + h/2.0 * k2);
            let k4 = f(t + h, y + h * k3);

            y += h/6.0 * (k1 + 2.0*k2 + 2.0*k3 + k4);
            t += h;

            result.push((t, y));
        }

        result
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_lagrange_interpolation() {
        let na = NumericalAnalysis::new();
        let x_data = vec![0.0, 1.0, 2.0];
        let y_data = vec![1.0, 2.0, 4.0];
        let result = na.lagrange_interpolation(1.5, &x_data, &y_data);
        assert!((result - 3.0).abs() < 1e-10);
    }

    #[test]
    fn test_trapezoidal_rule() {
        let na = NumericalAnalysis::new();
        let f = |x: f64| x * x;
        let result = na.trapezoidal_rule(f, 0.0, 1.0, 1000);
        assert!((result - 1.0/3.0).abs() < 1e-3);
    }

    #[test]
    fn test_newton_method() {
        let na = NumericalAnalysis::new();
        let f = |x: f64| x * x - 4.0;
        let df = |x: f64| 2.0 * x;
        let result = na.newton_method(f, df, 3.0);
        assert!(result.is_some());
        assert!((result.unwrap() - 2.0).abs() < 1e-6);
    }
}
```

### 9.2 Haskell实现

```haskell
module NumericalAnalysis where

import Data.Vector (Vector)
import qualified Data.Vector as V

-- 数值分析工具集
data NumericalAnalysis = NumericalAnalysis
    { tolerance :: Double
    , maxIterations :: Int
    }

defaultNumericalAnalysis :: NumericalAnalysis
defaultNumericalAnalysis = NumericalAnalysis
    { tolerance = 1e-10
    , maxIterations = 1000
    }

-- 拉格朗日插值
lagrangeInterpolation :: NumericalAnalysis -> Double -> Vector Double -> Vector Double -> Double
lagrangeInterpolation _ x xData yData = 
    sum [yData V.! i * lagrangeBase i x xData | i <- [0..V.length xData - 1]]
  where
    lagrangeBase i x xData = 
        product [(x - xData V.! j) / (xData V.! i - xData V.! j) | j <- [0..V.length xData - 1], i /= j]

-- 牛顿插值
newtonInterpolation :: NumericalAnalysis -> Double -> Vector Double -> Vector Double -> Double
newtonInterpolation _ x xData yData = 
    let n = V.length xData
        dividedDiff = computeDividedDiff xData yData
        result = newtonEval x xData dividedDiff
    in result

-- 计算差商
computeDividedDiff :: Vector Double -> Vector Double -> Vector (Vector Double)
computeDividedDiff xData yData = 
    let n = V.length xData
        initial = V.generate n (\i -> V.singleton (yData V.! i))
    in foldl computeColumn initial [1..n-1]
  where
    computeColumn diff j = 
        V.generate (n-j) (\i -> 
            let prev = diff V.! i V.! (j-1)
                next = diff V.! (i+1) V.! (j-1)
                xi = xData V.! i
                xij = xData V.! (i+j)
                newVal = (next - prev) / (xij - xi)
            in diff V.! i V.++ V.singleton newVal)

-- 牛顿插值求值
newtonEval :: Double -> Vector Double -> Vector (Vector Double) -> Double
newtonEval x xData dividedDiff = 
    let result = dividedDiff V.! 0 V.! 0
        term = foldl (\acc i -> 
            let factor = product [(x - xData V.! j) | j <- [0..i-1]]
                coeff = dividedDiff V.! 0 V.! i
            in acc + coeff * factor) result [1..V.length xData - 1]
    in term

-- 梯形法则
trapezoidalRule :: (Double -> Double) -> Double -> Double -> Int -> Double
trapezoidalRule f a b n = 
    let h = (b - a) / fromIntegral n
        sum = (f a + f b) / 2.0 + sum [f (a + fromIntegral i * h) | i <- [1..n-1]]
    in h * sum

-- 辛普森法则
simpsonRule :: (Double -> Double) -> Double -> Double -> Int -> Double
simpsonRule f a b n
    | odd n = error "n must be even for Simpson's rule"
    | otherwise = 
        let h = (b - a) / fromIntegral n
            sum = f a + f b + 2.0 * sum [f (a + fromIntegral i * h) | i <- [2,4..n-2]] + 
                  4.0 * sum [f (a + fromIntegral i * h) | i <- [1,3..n-1]]
        in h * sum / 3.0

-- 中心差分
centralDifference :: (Double -> Double) -> Double -> Double -> Double
centralDifference f x h = (f (x + h) - f (x - h)) / (2.0 * h)

-- 牛顿法求根
newtonMethod :: (Double -> Double) -> (Double -> Double) -> Double -> NumericalAnalysis -> Maybe Double
newtonMethod f df x0 na = 
    let go x iteration
            | iteration >= maxIterations na = Nothing
            | abs dfx < tolerance na = Nothing
            | abs (xNew - x) < tolerance na = Just xNew
            | otherwise = go xNew (iteration + 1)
          where
            fx = f x
            dfx = df x
            xNew = x - fx / dfx
    in go x0 0

-- 二分法求根
bisectionMethod :: (Double -> Double) -> Double -> Double -> NumericalAnalysis -> Maybe Double
bisectionMethod f a b na = 
    let go left right iteration
            | iteration >= maxIterations na = Nothing
            | f left * f right > 0 = Nothing
            | otherwise = 
                let mid = (left + right) / 2.0
                    fMid = f mid
                in if abs fMid < tolerance na
                   then Just mid
                   else if f left * fMid < 0
                        then go left mid (iteration + 1)
                        else go mid right (iteration + 1)
    in go a b 0

-- 四阶龙格-库塔法
rungeKutta4 :: (Double -> Double -> Double) -> Double -> Double -> Double -> Int -> [(Double, Double)]
rungeKutta4 f t0 y0 h steps = 
    let go t y n
            | n >= steps = [(t, y)]
            | otherwise = 
                let k1 = f t y
                    k2 = f (t + h/2) (y + h/2 * k1)
                    k3 = f (t + h/2) (y + h/2 * k2)
                    k4 = f (t + h) (y + h * k3)
                    yNew = y + h/6 * (k1 + 2*k2 + 2*k3 + k4)
                    tNew = t + h
                in (t, y) : go tNew yNew (n + 1)
    in go t0 y0 0

-- 测试函数
testLagrangeInterpolation :: Bool
testLagrangeInterpolation = 
    let na = defaultNumericalAnalysis
        xData = V.fromList [0.0, 1.0, 2.0]
        yData = V.fromList [1.0, 2.0, 4.0]
        result = lagrangeInterpolation na 1.5 xData yData
    in abs (result - 3.0) < 1e-10

testTrapezoidalRule :: Bool
testTrapezoidalRule = 
    let f x = x * x
        result = trapezoidalRule f 0.0 1.0 1000
    in abs (result - 1.0/3.0) < 1e-3

testNewtonMethod :: Bool
testNewtonMethod = 
    let na = defaultNumericalAnalysis
        f x = x * x - 4.0
        df x = 2.0 * x
        result = newtonMethod f df 3.0 na
    in case result of
         Just x -> abs (x - 2.0) < 1e-6
         Nothing -> False

-- 主函数用于测试
main :: IO ()
main = do
    putStrLn "Testing numerical analysis methods:"
    putStrLn $ "Lagrange interpolation: " ++ show testLagrangeInterpolation
    putStrLn $ "Trapezoidal rule: " ++ show testTrapezoidalRule
    putStrLn $ "Newton method: " ++ show testNewtonMethod
```

## 10. 习题与练习

### 10.1 基础练习

**练习 1**
计算函数 $f(x) = e^x$ 在 $x = 1$ 处的数值导数，使用中心差分法，步长 $h = 0.1$。

**解答**：
$$f'(1) \approx \frac{e^{1.1} - e^{0.9}}{0.2} \approx 2.7205$$

**练习 2**
使用梯形法则计算 $\int_0^1 x^2 dx$，取 $n = 4$。

**解答**：
$$h = \frac{1}{4} = 0.25$$
$$\int_0^1 x^2 dx \approx \frac{0.25}{2} [0 + 2(0.0625 + 0.25 + 0.5625) + 1] = 0.34375$$

### 10.2 进阶练习

**练习 3**
使用牛顿法求解方程 $x^3 - 2 = 0$，初始值 $x_0 = 1$。

**解答**：
迭代公式：$x_{n+1} = x_n - \frac{x_n^3 - 2}{3x_n^2}$

- $x_1 = 1 - \frac{-1}{3} = 1.3333$
- $x_2 = 1.3333 - \frac{0.3704}{5.3333} = 1.2639$
- $x_3 = 1.2639 - \frac{0.0189}{4.7963} = 1.2600$

**练习 4**
使用四阶龙格-库塔法求解初值问题：
$$\frac{dy}{dx} = -y, \quad y(0) = 1$$
在区间 $[0, 1]$ 上，步长 $h = 0.1$。

**解答**：
精确解为 $y(x) = e^{-x}$。
数值解与精确解的比较显示RK4方法具有很高的精度。

---

**创建时间**: 2024-12-19
**最后更新**: 2024-12-19
**状态**: 已完成
