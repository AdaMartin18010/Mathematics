# 02-优化理论

## 目录

1. [优化理论概述](#1-优化理论概述)
2. [线性规划](#2-线性规划)
3. [非线性规划](#3-非线性规划)
4. [凸优化](#4-凸优化)
5. [整数规划](#5-整数规划)
6. [动态规划](#6-动态规划)
7. [代码实现](#7-代码实现)
8. [习题与练习](#8-习题与练习)

## 1. 优化理论概述

### 1.1 优化问题的基本形式

**标准形式**：
$$\begin{align}
\min_{x \in \mathbb{R}^n} &\quad f(x) \\
\text{s.t.} &\quad g_i(x) \leq 0, \quad i = 1, 2, \ldots, m \\
&\quad h_j(x) = 0, \quad j = 1, 2, \ldots, p
\end{align}$$

其中：
- $f(x)$ 是目标函数
- $g_i(x) \leq 0$ 是不等式约束
- $h_j(x) = 0$ 是等式约束

### 1.2 优化问题的分类

**分类标准**：
1. **目标函数性质**：线性、非线性、凸、非凸
2. **约束类型**：无约束、有约束
3. **变量类型**：连续、离散、混合
4. **问题规模**：小规模、大规模

## 2. 线性规划

### 2.1 线性规划标准形式

**标准形式**：
$$\begin{align}
\min_{x \in \mathbb{R}^n} &\quad c^T x \\
\text{s.t.} &\quad Ax = b \\
&\quad x \geq 0
\end{align}$$

其中：
- $c \in \mathbb{R}^n$ 是目标函数系数向量
- $A \in \mathbb{R}^{m \times n}$ 是约束矩阵
- $b \in \mathbb{R}^m$ 是约束右端向量

### 2.2 几何意义

**可行域**：
- 可行域是凸多面体
- 最优解在可行域的顶点上

**最优性条件**：
- 如果可行域非空且有界，则存在最优解
- 最优解可能在多个顶点上

### 2.3 单纯形法

#### 2.3.1 基本思想

**基本思想**：
从一个顶点移动到相邻顶点，直到找到最优解。

#### 2.3.2 算法步骤

**步骤**：
1. 找到初始基可行解
2. 计算检验数
3. 选择进基变量
4. 选择出基变量
5. 进行基变换
6. 重复直到最优

#### 2.3.3 单纯形表

**表格形式**：
| 基变量 | $x_1$ | $x_2$ | $\cdots$ | $x_n$ | 右端 |
|--------|-------|-------|----------|-------|------|
| $x_{B1}$ | $a_{11}$ | $a_{12}$ | $\cdots$ | $a_{1n}$ | $b_1$ |
| $x_{B2}$ | $a_{21}$ | $a_{22}$ | $\cdots$ | $a_{2n}$ | $b_2$ |
| $\vdots$ | $\vdots$ | $\vdots$ | $\ddots$ | $\vdots$ | $\vdots$ |
| $x_{Bm}$ | $a_{m1}$ | $a_{m2}$ | $\cdots$ | $a_{mn}$ | $b_m$ |
| 检验数 | $\sigma_1$ | $\sigma_2$ | $\cdots$ | $\sigma_n$ | $z$ |

**检验数计算**：
$$\sigma_j = c_j - \sum_{i=1}^m c_{Bi} a_{ij}$$

### 2.4 对偶理论

#### 2.4.1 对偶问题

**原问题**：
$$\begin{align}
\min_{x} &\quad c^T x \\
\text{s.t.} &\quad Ax = b \\
&\quad x \geq 0
\end{align}$$

**对偶问题**：
$$\begin{align}
\max_{y} &\quad b^T y \\
\text{s.t.} &\quad A^T y \leq c
\end{align}$$

#### 2.4.2 对偶性质

**弱对偶性**：
如果 $x$ 是原问题的可行解，$y$ 是对偶问题的可行解，则：
$$c^T x \geq b^T y$$

**强对偶性**：
如果原问题有最优解，则对偶问题也有最优解，且最优值相等。

**互补松弛性**：
$$x_i y_i = 0, \quad i = 1, 2, \ldots, n$$

## 3. 非线性规划

### 3.1 最优性条件

#### 3.1.1 无约束优化

**一阶必要条件**：
如果 $x^*$ 是局部最优解，则：
$$\nabla f(x^*) = 0$$

**二阶充分条件**：
如果 $\nabla f(x^*) = 0$ 且 $\nabla^2 f(x^*)$ 正定，则 $x^*$ 是严格局部最优解。

#### 3.1.2 约束优化

**KKT条件**：
$$\begin{align}
\nabla f(x^*) + \sum_{i=1}^m \lambda_i \nabla g_i(x^*) + \sum_{j=1}^p \mu_j \nabla h_j(x^*) &= 0 \\
g_i(x^*) &\leq 0, \quad i = 1, 2, \ldots, m \\
h_j(x^*) &= 0, \quad j = 1, 2, \ldots, p \\
\lambda_i &\geq 0, \quad i = 1, 2, \ldots, m \\
\lambda_i g_i(x^*) &= 0, \quad i = 1, 2, \ldots, m
\end{align}$$

### 3.2 求解方法

#### 3.2.1 梯度法

**最速下降法**：
$$x_{k+1} = x_k - \alpha_k \nabla f(x_k)$$

**步长选择**：
1. **固定步长**：$\alpha_k = \alpha$
2. **线搜索**：$\alpha_k = \arg\min_{\alpha} f(x_k - \alpha \nabla f(x_k))$

#### 3.2.2 牛顿法

**迭代公式**：
$$x_{k+1} = x_k - [\nabla^2 f(x_k)]^{-1} \nabla f(x_k)$$

**优点**：
- 二次收敛
- 对凸函数收敛到全局最优

**缺点**：
- 需要计算二阶导数
- 计算成本高

#### 3.2.3 拟牛顿法

**BFGS方法**：
$$H_{k+1} = H_k + \frac{s_k s_k^T}{s_k^T y_k} - \frac{H_k y_k y_k^T H_k}{y_k^T H_k y_k}$$

其中：
- $s_k = x_{k+1} - x_k$
- $y_k = \nabla f(x_{k+1}) - \nabla f(x_k)$

## 4. 凸优化

### 4.1 凸集与凸函数

#### 4.1.1 凸集

**定义 4.1** (凸集)
集合 $C \subseteq \mathbb{R}^n$ 是凸集，如果对于任意 $x, y \in C$ 和 $\lambda \in [0, 1]$，有：
$$\lambda x + (1-\lambda) y \in C$$

**例子**：
- 超平面：$\{x : a^T x = b\}$
- 半空间：$\{x : a^T x \leq b\}$
- 多面体：$\{x : Ax \leq b\}$

#### 4.1.2 凸函数

**定义 4.2** (凸函数)
函数 $f: \mathbb{R}^n \to \mathbb{R}$ 是凸函数，如果对于任意 $x, y \in \mathbb{R}^n$ 和 $\lambda \in [0, 1]$，有：
$$f(\lambda x + (1-\lambda) y) \leq \lambda f(x) + (1-\lambda) f(y)$$

**判别条件**：
如果 $f$ 二阶连续可微，则 $f$ 是凸函数当且仅当 $\nabla^2 f(x) \succeq 0$ 对所有 $x$ 成立。

### 4.2 凸优化问题

#### 4.2.1 标准形式

**凸优化问题**：
$$\begin{align}
\min_{x} &\quad f(x) \\
\text{s.t.} &\quad g_i(x) \leq 0, \quad i = 1, 2, \ldots, m \\
&\quad h_j(x) = 0, \quad j = 1, 2, \ldots, p
\end{align}$$

其中 $f$ 和 $g_i$ 是凸函数，$h_j$ 是仿射函数。

#### 4.2.2 性质

**性质**：
1. 局部最优解是全局最优解
2. 最优解集合是凸集
3. KKT条件是充分必要条件

### 4.3 内点法

#### 4.3.1 障碍函数法

**障碍函数**：
$$B(x) = f(x) - \mu \sum_{i=1}^m \ln(-g_i(x))$$

**中心路径**：
$$\{x^*(\mu) : \mu > 0\}$$

其中 $x^*(\mu)$ 是障碍函数的最优解。

#### 4.3.2 原对偶内点法

**算法步骤**：
1. 选择初始点 $(x^0, \lambda^0, \nu^0)$
2. 计算牛顿方向
3. 选择步长
4. 更新迭代点
5. 重复直到收敛

## 5. 整数规划

### 5.1 整数规划模型

#### 5.1.1 纯整数规划

**标准形式**：
$$\begin{align}
\min_{x} &\quad c^T x \\
\text{s.t.} &\quad Ax \leq b \\
&\quad x \in \mathbb{Z}^n
\end{align}$$

#### 5.1.2 混合整数规划

**标准形式**：
$$\begin{align}
\min_{x, y} &\quad c^T x + d^T y \\
\text{s.t.} &\quad Ax + By \leq b \\
&\quad x \in \mathbb{Z}^n, \quad y \in \mathbb{R}^m
\end{align}$$

### 5.2 求解方法

#### 5.2.1 分支定界法

**基本思想**：
1. 求解松弛问题
2. 如果解不是整数，则分支
3. 对每个分支重复过程

**分支策略**：
- 选择分数变量 $x_j$
- 创建两个子问题：$x_j \leq \lfloor x_j^* \rfloor$ 和 $x_j \geq \lceil x_j^* \rceil$

#### 5.2.2 割平面法

**基本思想**：
1. 求解松弛问题
2. 如果解不是整数，则添加割平面
3. 重复直到得到整数解

**Gomory割**：
$$\sum_{j \in N} (a_{ij} - \lfloor a_{ij} \rfloor) x_j \geq a_{i0} - \lfloor a_{i0} \rfloor$$

## 6. 动态规划

### 6.1 基本概念

#### 6.1.1 多阶段决策

**阶段**：决策过程分为若干阶段
**状态**：每个阶段的系统状态
**决策**：在每个状态下的选择
**策略**：整个过程的决策序列

#### 6.1.2 最优性原理

**原理**：最优策略的子策略也是最优的。

**数学表达**：
$$f_n(s_n) = \min_{d_n} \{v_n(s_n, d_n) + f_{n+1}(s_{n+1})\}$$

### 6.2 经典问题

#### 6.2.1 最短路径问题

**问题描述**：
求从起点到终点的最短路径。

**状态定义**：
$f(i)$ 表示从 $i$ 到终点的最短距离。

**递推关系**：
$$f(i) = \min_{j} \{c_{ij} + f(j)\}$$

#### 6.2.2 背包问题

**0-1背包**：
$$\begin{align}
\max &\quad \sum_{i=1}^n v_i x_i \\
\text{s.t.} &\quad \sum_{i=1}^n w_i x_i \leq W \\
&\quad x_i \in \{0, 1\}
\end{align}$$

**状态定义**：
$f(i, j)$ 表示前 $i$ 件物品，容量 $j$ 的最大价值。

**递推关系**：
$$f(i, j) = \max \{f(i-1, j), f(i-1, j-w_i) + v_i\}$$

## 7. 代码实现

### 7.1 Rust实现

```rust
use std::f64;

/// 优化问题求解器
pub struct Optimizer {
    pub tolerance: f64,
    pub max_iterations: usize,
}

impl Optimizer {
    pub fn new() -> Self {
        Self {
            tolerance: 1e-6,
            max_iterations: 1000,
        }
    }

    /// 梯度下降法
    pub fn gradient_descent<F, G>(
        &self,
        f: F,
        grad_f: G,
        x0: Vec<f64>,
        learning_rate: f64,
    ) -> Option<Vec<f64>>
    where
        F: Fn(&[f64]) -> f64,
        G: Fn(&[f64]) -> Vec<f64>,
    {
        let mut x = x0;

        for iteration in 0..self.max_iterations {
            let gradient = grad_f(&x);
            let gradient_norm = gradient.iter().map(|&g| g * g).sum::<f64>().sqrt();

            if gradient_norm < self.tolerance {
                return Some(x);
            }

            for i in 0..x.len() {
                x[i] -= learning_rate * gradient[i];
            }
        }

        None
    }

    /// 牛顿法
    pub fn newton_method<F, G, H>(
        &self,
        f: F,
        grad_f: G,
        hessian_f: H,
        x0: Vec<f64>,
    ) -> Option<Vec<f64>>
    where
        F: Fn(&[f64]) -> f64,
        G: Fn(&[f64]) -> Vec<f64>,
        H: Fn(&[f64]) -> Vec<Vec<f64>>,
    {
        let mut x = x0;

        for iteration in 0..self.max_iterations {
            let gradient = grad_f(&x);
            let hessian = hessian_f(&x);

            // 求解线性方程组 H(x) * p = -gradient(x)
            let p = self.solve_linear_system(&hessian, &gradient.iter().map(|&g| -g).collect::<Vec<_>>());

            if p.is_none() {
                return None;
            }

            let p = p.unwrap();
            let p_norm = p.iter().map(|&pi| pi * pi).sum::<f64>().sqrt();

            if p_norm < self.tolerance {
                return Some(x);
            }

            for i in 0..x.len() {
                x[i] += p[i];
            }
        }

        None
    }

    /// 单纯形法（简化版）
    pub fn simplex_method(
        &self,
        c: &[f64],
        a: &[Vec<f64>],
        b: &[f64],
    ) -> Option<Vec<f64>> {
        let m = a.len();
        let n = c.len();

        // 构造单纯形表
        let mut tableau = vec![vec![0.0; n + m + 1]; m + 1];

        // 目标函数行
        for j in 0..n {
            tableau[0][j] = c[j];
        }

        // 约束行
        for i in 0..m {
            for j in 0..n {
                tableau[i + 1][j] = a[i][j];
            }
            tableau[i + 1][n + i] = 1.0; // 松弛变量
            tableau[i + 1][n + m] = b[i];
        }

        // 迭代求解
        for _ in 0..self.max_iterations {
            // 找到进基变量
            let mut entering = None;
            for j in 0..n + m {
                if tableau[0][j] > self.tolerance {
                    entering = Some(j);
                    break;
                }
            }

            if entering.is_none() {
                // 找到最优解
                let mut solution = vec![0.0; n];
                for i in 0..m {
                    let basic_var = n + i;
                    if basic_var < n {
                        solution[basic_var] = tableau[i + 1][n + m];
                    }
                }
                return Some(solution);
            }

            let entering = entering.unwrap();

            // 找到出基变量
            let mut leaving = None;
            let mut min_ratio = f64::INFINITY;

            for i in 0..m {
                if tableau[i + 1][entering] > self.tolerance {
                    let ratio = tableau[i + 1][n + m] / tableau[i + 1][entering];
                    if ratio < min_ratio {
                        min_ratio = ratio;
                        leaving = Some(i);
                    }
                }
            }

            if leaving.is_none() {
                return None; // 无界解
            }

            let leaving = leaving.unwrap();

            // 高斯消元
            let pivot = tableau[leaving + 1][entering];
            for j in 0..n + m + 1 {
                tableau[leaving + 1][j] /= pivot;
            }

            for i in 0..m + 1 {
                if i != leaving + 1 {
                    let factor = tableau[i][entering];
                    for j in 0..n + m + 1 {
                        tableau[i][j] -= factor * tableau[leaving + 1][j];
                    }
                }
            }
        }

        None
    }

    /// 动态规划：0-1背包问题
    pub fn knapsack_01(&self, weights: &[f64], values: &[f64], capacity: f64) -> (f64, Vec<usize>) {
        let n = weights.len();
        let capacity_int = (capacity * 1000.0) as usize; // 转换为整数

        let mut dp = vec![vec![0.0; capacity_int + 1]; n + 1];

        for i in 1..=n {
            for w in 0..=capacity_int {
                let weight_int = (weights[i - 1] * 1000.0) as usize;
                if weight_int <= w {
                    dp[i][w] = dp[i - 1][w].max(dp[i - 1][w - weight_int] + values[i - 1]);
                } else {
                    dp[i][w] = dp[i - 1][w];
                }
            }
        }

        // 回溯找到选择的物品
        let mut selected = Vec::new();
        let mut w = capacity_int;
        for i in (1..=n).rev() {
            let weight_int = (weights[i - 1] * 1000.0) as usize;
            if weight_int <= w && dp[i][w] == dp[i - 1][w - weight_int] + values[i - 1] {
                selected.push(i - 1);
                w -= weight_int;
            }
        }

        selected.reverse();
        (dp[n][capacity_int], selected)
    }

    /// 求解线性方程组（高斯消元）
    fn solve_linear_system(&self, a: &[Vec<f64>], b: &[f64]) -> Option<Vec<f64>> {
        let n = a.len();
        let mut augmented = vec![vec![0.0; n + 1]; n];

        // 构造增广矩阵
        for i in 0..n {
            for j in 0..n {
                augmented[i][j] = a[i][j];
            }
            augmented[i][n] = b[i];
        }

        // 前向消元
        for i in 0..n {
            // 寻找主元
            let mut max_row = i;
            for k in i + 1..n {
                if augmented[k][i].abs() > augmented[max_row][i].abs() {
                    max_row = k;
                }
            }

            if augmented[max_row][i].abs() < self.tolerance {
                return None; // 奇异矩阵
            }

            // 交换行
            if max_row != i {
                augmented.swap(i, max_row);
            }

            // 消元
            for k in i + 1..n {
                let factor = augmented[k][i] / augmented[i][i];
                for j in i..=n {
                    augmented[k][j] -= factor * augmented[i][j];
                }
            }
        }

        // 后向代入
        let mut x = vec![0.0; n];
        for i in (0..n).rev() {
            let mut sum = 0.0;
            for j in i + 1..n {
                sum += augmented[i][j] * x[j];
            }
            x[i] = (augmented[i][n] - sum) / augmented[i][i];
        }

        Some(x)
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_gradient_descent() {
        let optimizer = Optimizer::new();
        
        // 最小化 f(x) = x^2 + y^2
        let f = |x: &[f64]| x[0] * x[0] + x[1] * x[1];
        let grad_f = |x: &[f64]| vec![2.0 * x[0], 2.0 * x[1]];
        
        let result = optimizer.gradient_descent(f, grad_f, vec![1.0, 1.0], 0.1);
        assert!(result.is_some());
        
        let solution = result.unwrap();
        assert!(solution[0].abs() < 1e-3);
        assert!(solution[1].abs() < 1e-3);
    }

    #[test]
    fn test_knapsack() {
        let optimizer = Optimizer::new();
        let weights = vec![2.0, 1.0, 3.0, 2.0];
        let values = vec![12.0, 10.0, 20.0, 15.0];
        let capacity = 5.0;
        
        let (max_value, selected) = optimizer.knapsack_01(&weights, &values, capacity);
        
        assert!(max_value > 0.0);
        assert!(!selected.is_empty());
    }
}
```

### 7.2 Haskell实现

```haskell
module Optimization where

import Data.Vector (Vector)
import qualified Data.Vector as V
import Data.Matrix (Matrix)
import qualified Data.Matrix as M

-- 优化问题求解器
data Optimizer = Optimizer
    { tolerance :: Double
    , maxIterations :: Int
    }

defaultOptimizer :: Optimizer
defaultOptimizer = Optimizer
    { tolerance = 1e-6
    , maxIterations = 1000
    }

-- 梯度下降法
gradientDescent :: (Vector Double -> Double) -> (Vector Double -> Vector Double) -> Vector Double -> Double -> Optimizer -> Maybe (Vector Double)
gradientDescent f gradF x0 learningRate opt = 
    let go x iteration
            | iteration >= maxIterations opt = Nothing
            | otherwise = 
                let gradient = gradF x
                    gradientNorm = sqrt $ sum $ V.map (^2) gradient
                in if gradientNorm < tolerance opt
                   then Just x
                   else go (x - V.map (* learningRate) gradient) (iteration + 1)
    in go x0 0

-- 牛顿法
newtonMethod :: (Vector Double -> Double) -> (Vector Double -> Vector Double) -> (Vector Double -> Matrix Double) -> Vector Double -> Optimizer -> Maybe (Vector Double)
newtonMethod f gradF hessianF x0 opt = 
    let go x iteration
            | iteration >= maxIterations opt = Nothing
            | otherwise = 
                let gradient = gradF x
                    hessian = hessianF x
                    p = solveLinearSystem hessian (V.map negate gradient)
                in case p of
                     Nothing -> Nothing
                     Just pVec -> 
                         let pNorm = sqrt $ sum $ V.map (^2) pVec
                         in if pNorm < tolerance opt
                            then Just x
                            else go (x + pVec) (iteration + 1)
    in go x0 0

-- 0-1背包问题
knapsack01 :: Vector Double -> Vector Double -> Double -> Optimizer -> (Double, Vector Int)
knapsack01 weights values capacity opt = 
    let n = V.length weights
        capacityInt = round (capacity * 1000)
        
        -- 动态规划表
        dp = buildDP weights values capacityInt
        
        -- 回溯找到选择的物品
        selected = backtrack weights values capacityInt dp
    in (dp V.! n V.! capacityInt, selected)

-- 构建动态规划表
buildDP :: Vector Double -> Vector Double -> Int -> Vector (Vector Double)
buildDP weights values capacity = 
    let n = V.length weights
        dp = V.generate (n + 1) (\i -> 
            V.generate (capacity + 1) (\w -> 
                if i == 0 
                then 0.0
                else let weightInt = round (weights V.! (i - 1) * 1000)
                     in if weightInt <= w
                        then max (dp V.! (i - 1) V.! w) 
                                (dp V.! (i - 1) V.! (w - weightInt) + values V.! (i - 1))
                        else dp V.! (i - 1) V.! w))
    in dp

-- 回溯找到选择的物品
backtrack :: Vector Double -> Vector Double -> Int -> Vector (Vector Double) -> Vector Int
backtrack weights values capacity dp = 
    let n = V.length weights
        go w i acc
            | i == 0 = acc
            | otherwise = 
                let weightInt = round (weights V.! (i - 1) * 1000)
                in if weightInt <= w && dp V.! i V.! w == dp V.! (i - 1) V.! (w - weightInt) + values V.! (i - 1)
                   then go (w - weightInt) (i - 1) (V.cons (i - 1) acc)
                   else go w (i - 1) acc
    in go capacity n V.empty

-- 求解线性方程组（高斯消元）
solveLinearSystem :: Matrix Double -> Vector Double -> Maybe (Vector Double)
solveLinearSystem a b = 
    let n = M.nrows a
        augmented = M.fromLists $ zipWith (\i row -> row ++ [b V.! i]) [0..n-1] (M.toLists a)
        
        -- 前向消元
        reduced = forwardElimination augmented
        
        -- 后向代入
        solution = backSubstitution reduced
    in solution

-- 前向消元
forwardElimination :: Matrix Double -> Matrix Double
forwardElimination m = 
    let n = M.nrows m
        go i m'
            | i >= n = m'
            | otherwise = 
                let maxRow = findMaxRow m' i
                    m1 = if maxRow /= i then swapRows m' i maxRow else m'
                    m2 = eliminateColumn m1 i
                in go (i + 1) m2
    in go 0 m

-- 寻找主元行
findMaxRow :: Matrix Double -> Int -> Int
findMaxRow m i = 
    let n = M.nrows m
        maxVal = maximum [abs (m M.! (k, i)) | k <- [i..n-1]]
    in head [k | k <- [i..n-1], abs (m M.! (k, i)) == maxVal]

-- 交换行
swapRows :: Matrix Double -> Int -> Int -> Matrix Double
swapRows m i j = 
    let n = M.ncols m
        rowI = [m M.! (i, k) | k <- [0..n-1]]
        rowJ = [m M.! (j, k) | k <- [0..n-1]]
    in M.fromLists $ zipWith (\k row -> 
        if k == i then rowJ else if k == j then rowI else row) [0..M.nrows m-1] (M.toLists m)

-- 消元
eliminateColumn :: Matrix Double -> Int -> Matrix Double
eliminateColumn m i = 
    let n = M.nrows m
        pivot = m M.! (i, i)
        go k m'
            | k >= n = m'
            | k == i = go (k + 1) m'
            | otherwise = 
                let factor = (m' M.! (k, i)) / pivot
                    m1 = subtractRow m' k i factor
                in go (k + 1) m1
    in go 0 m

-- 减去行
subtractRow :: Matrix Double -> Int -> Int -> Double -> Matrix Double
subtractRow m row col factor = 
    let n = M.ncols m
        newRow = [m M.! (row, k) - factor * m M.! (col, k) | k <- [0..n-1]]
    in M.fromLists $ zipWith (\k rowList -> 
        if k == row then newRow else rowList) [0..M.nrows m-1] (M.toLists m)

-- 后向代入
backSubstitution :: Matrix Double -> Maybe (Vector Double)
backSubstitution m = 
    let n = M.nrows m
        go i x
            | i < 0 = Just (V.fromList x)
            | otherwise = 
                let sum = sum [m M.! (i, j) * (x !! (n - 1 - j)) | j <- [i+1..n-1]]
                    xi = (m M.! (i, n) - sum) / m M.! (i, i)
                in if abs (m M.! (i, i)) < 1e-10
                   then Nothing
                   else go (i - 1) (xi : x)
    in go (n - 1) []

-- 测试函数
testGradientDescent :: Bool
testGradientDescent = 
    let opt = defaultOptimizer
        f x = sum $ V.map (^2) x
        gradF x = V.map (* 2) x
        result = gradientDescent f gradF (V.fromList [1.0, 1.0]) 0.1 opt
    in case result of
         Just x -> all (< 1e-3) (V.toList x)
         Nothing -> False

testKnapsack :: Bool
testKnapsack = 
    let opt = defaultOptimizer
        weights = V.fromList [2.0, 1.0, 3.0, 2.0]
        values = V.fromList [12.0, 10.0, 20.0, 15.0]
        capacity = 5.0
        (maxValue, selected) = knapsack01 weights values capacity opt
    in maxValue > 0.0 && not (V.null selected)

-- 主函数用于测试
main :: IO ()
main = do
    putStrLn "Testing optimization methods:"
    putStrLn $ "Gradient descent: " ++ show testGradientDescent
    putStrLn $ "Knapsack 0-1: " ++ show testKnapsack
```

## 8. 习题与练习

### 8.1 基础练习

**练习 1**
求解线性规划问题：
$$\begin{align}
\max &\quad 3x_1 + 2x_2 \\
\text{s.t.} &\quad x_1 + x_2 \leq 4 \\
&\quad 2x_1 + x_2 \leq 6 \\
&\quad x_1, x_2 \geq 0
\end{align}$$

**解答**：
使用单纯形法求解，最优解为 $x_1 = 2, x_2 = 2$，最优值为 $10$。

**练习 2**
使用梯度下降法求解无约束优化问题：
$$\min f(x) = x^2 + 2x + 1$$

**解答**：
$f'(x) = 2x + 2$，迭代公式：$x_{k+1} = x_k - \alpha(2x_k + 2)$
最优解为 $x = -1$。

### 8.2 进阶练习

**练习 3**
求解0-1背包问题：
- 物品重量：$[2, 1, 3, 2]$
- 物品价值：$[12, 10, 20, 15]$
- 背包容量：$5$

**解答**：
使用动态规划求解，最优解为选择物品 $[1, 2, 3]$，总价值为 $42$。

**练习 4**
使用KKT条件求解约束优化问题：
$$\begin{align}
\min &\quad x_1^2 + x_2^2 \\
\text{s.t.} &\quad x_1 + x_2 \geq 1
\end{align}$$

**解答**：
KKT条件：
$$\begin{align}
2x_1 - \lambda &= 0 \\
2x_2 - \lambda &= 0 \\
x_1 + x_2 &\geq 1 \\
\lambda &\geq 0 \\
\lambda(x_1 + x_2 - 1) &= 0
\end{align}$$

解得：$x_1 = x_2 = 0.5$，$\lambda = 1$。

---

**创建时间**: 2024-12-19
**最后更新**: 2024-12-19
**状态**: 已完成 