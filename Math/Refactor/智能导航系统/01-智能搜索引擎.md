# 01-æ™ºèƒ½æœç´¢å¼•æ“

## ğŸ“Š å¼•æ“æ¦‚è¿°

- **å¼•æ“åç§°**: Refactoræ™ºèƒ½æœç´¢å¼•æ“
- **æ ¸å¿ƒåŠŸèƒ½**: åŸºäºAIçš„è¯­ä¹‰æœç´¢å’Œæ¦‚å¿µåŒ¹é…
- **æŠ€æœ¯æ ˆ**: Python + Transformers + Elasticsearch
- **æœç´¢å‡†ç¡®ç‡ç›®æ ‡**: 95%ä»¥ä¸Š

---

## ğŸ¯ æ ¸å¿ƒåŠŸèƒ½è®¾è®¡

### 1. è¯­ä¹‰æœç´¢åŠŸèƒ½

#### 1.1 è‡ªç„¶è¯­è¨€ç†è§£

```python
# è¯­ä¹‰æœç´¢æ ¸å¿ƒç®—æ³•
class SemanticSearchEngine:
    def __init__(self):
        self.model = SentenceTransformer('all-MiniLM-L6-v2')
        self.index = None
    
    def encode_query(self, query):
        """å°†æŸ¥è¯¢è½¬æ¢ä¸ºå‘é‡è¡¨ç¤º"""
        return self.model.encode(query)
    
    def semantic_search(self, query, top_k=10):
        """æ‰§è¡Œè¯­ä¹‰æœç´¢"""
        query_vector = self.encode_query(query)
        results = self.index.search(query_vector, top_k)
        return self.rank_results(results)
```

#### 1.2 æ•°å­¦æ¦‚å¿µè¯†åˆ«

```python
# æ•°å­¦æ¦‚å¿µè¯†åˆ«å™¨
class MathConceptRecognizer:
    def __init__(self):
        self.math_vocab = self.load_math_vocabulary()
        self.concept_patterns = self.load_concept_patterns()
    
    def extract_concepts(self, text):
        """æå–æ•°å­¦æ¦‚å¿µ"""
        concepts = []
        for pattern in self.concept_patterns:
            matches = re.findall(pattern, text)
            concepts.extend(matches)
        return list(set(concepts))
    
    def normalize_concept(self, concept):
        """æ ‡å‡†åŒ–æ•°å­¦æ¦‚å¿µ"""
        return self.math_vocab.get(concept.lower(), concept)
```

### 2. æ¦‚å¿µæœç´¢åŠŸèƒ½

#### 2.1 æ¦‚å¿µå›¾è°±æ„å»º

```python
# æ¦‚å¿µå›¾è°±æ„å»ºå™¨
class ConceptGraphBuilder:
    def __init__(self):
        self.graph = nx.DiGraph()
        self.concept_nodes = {}
    
    def add_concept(self, concept_id, concept_name, category):
        """æ·»åŠ æ¦‚å¿µèŠ‚ç‚¹"""
        self.graph.add_node(concept_id, 
                           name=concept_name, 
                           category=category)
        self.concept_nodes[concept_name] = concept_id
    
    def add_relationship(self, source, target, relationship_type):
        """æ·»åŠ æ¦‚å¿µå…³ç³»"""
        self.graph.add_edge(source, target, 
                           type=relationship_type)
    
    def find_related_concepts(self, concept, max_depth=2):
        """æŸ¥æ‰¾ç›¸å…³æ¦‚å¿µ"""
        related = []
        if concept in self.concept_nodes:
            node_id = self.concept_nodes[concept]
            for depth in range(1, max_depth + 1):
                neighbors = nx.single_source_shortest_path_length(
                    self.graph, node_id, cutoff=depth)
                related.extend(list(neighbors.keys()))
        return list(set(related))
```

#### 2.2 æ¦‚å¿µåŒ¹é…ç®—æ³•

```python
# æ¦‚å¿µåŒ¹é…å™¨
class ConceptMatcher:
    def __init__(self, concept_graph):
        self.graph = concept_graph
        self.similarity_model = self.load_similarity_model()
    
    def concept_search(self, query_concept, threshold=0.8):
        """æ¦‚å¿µæœç´¢"""
        matches = []
        for concept in self.graph.nodes():
            similarity = self.calculate_similarity(
                query_concept, concept)
            if similarity >= threshold:
                matches.append({
                    'concept': concept,
                    'similarity': similarity,
                    'related': self.graph.find_related_concepts(concept)
                })
        return sorted(matches, key=lambda x: x['similarity'], reverse=True)
```

### 3. æ¨¡ç³ŠåŒ¹é…åŠŸèƒ½

#### 3.1 æ‹¼å†™çº é”™

```python
# æ‹¼å†™çº é”™å™¨
class SpellChecker:
    def __init__(self):
        self.math_terms = self.load_math_terms()
        self.edit_distance_threshold = 2
    
    def correct_spelling(self, word):
        """æ‹¼å†™çº é”™"""
        if word in self.math_terms:
            return word
        
        corrections = []
        for term in self.math_terms:
            distance = self.levenshtein_distance(word, term)
            if distance <= self.edit_distance_threshold:
                corrections.append({
                    'term': term,
                    'distance': distance
                })
        
        if corrections:
            return min(corrections, key=lambda x: x['distance'])['term']
        return word
    
    def levenshtein_distance(self, s1, s2):
        """è®¡ç®—ç¼–è¾‘è·ç¦»"""
        if len(s1) < len(s2):
            return self.levenshtein_distance(s2, s1)
        
        if len(s2) == 0:
            return len(s1)
        
        previous_row = list(range(len(s2) + 1))
        for i, c1 in enumerate(s1):
            current_row = [i + 1]
            for j, c2 in enumerate(s2):
                insertions = previous_row[j + 1] + 1
                deletions = current_row[j] + 1
                substitutions = previous_row[j] + (c1 != c2)
                current_row.append(min(insertions, deletions, substitutions))
            previous_row = current_row
        
        return previous_row[-1]
```

#### 3.2 è¿‘ä¼¼åŒ¹é…

```python
# è¿‘ä¼¼åŒ¹é…å™¨
class FuzzyMatcher:
    def __init__(self):
        self.fuzzy_threshold = 0.7
    
    def fuzzy_search(self, query, candidates):
        """æ¨¡ç³Šæœç´¢"""
        matches = []
        for candidate in candidates:
            similarity = self.calculate_similarity(query, candidate)
            if similarity >= self.fuzzy_threshold:
                matches.append({
                    'candidate': candidate,
                    'similarity': similarity
                })
        return sorted(matches, key=lambda x: x['similarity'], reverse=True)
    
    def calculate_similarity(self, s1, s2):
        """è®¡ç®—å­—ç¬¦ä¸²ç›¸ä¼¼åº¦"""
        return difflib.SequenceMatcher(None, s1, s2).ratio()
```

### 4. æ™ºèƒ½æ¨èåŠŸèƒ½

#### 4.1 ç”¨æˆ·è¡Œä¸ºåˆ†æ

```python
# ç”¨æˆ·è¡Œä¸ºåˆ†æå™¨
class UserBehaviorAnalyzer:
    def __init__(self):
        self.user_profiles = {}
        self.search_history = {}
    
    def track_search(self, user_id, query, results, clicked_items):
        """è·Ÿè¸ªæœç´¢è¡Œä¸º"""
        if user_id not in self.search_history:
            self.search_history[user_id] = []
        
        self.search_history[user_id].append({
            'query': query,
            'results': results,
            'clicked': clicked_items,
            'timestamp': datetime.now()
        })
    
    def build_user_profile(self, user_id):
        """æ„å»ºç”¨æˆ·ç”»åƒ"""
        if user_id not in self.search_history:
            return {}
        
        profile = {
            'interests': self.extract_interests(user_id),
            'difficulty_preference': self.analyze_difficulty_preference(user_id),
            'search_patterns': self.analyze_search_patterns(user_id)
        }
        self.user_profiles[user_id] = profile
        return profile
```

#### 4.2 ä¸ªæ€§åŒ–æ¨è

```python
# ä¸ªæ€§åŒ–æ¨èå™¨
class PersonalizedRecommender:
    def __init__(self, behavior_analyzer):
        self.analyzer = behavior_analyzer
        self.collaborative_filter = self.load_collaborative_filter()
    
    def recommend_content(self, user_id, query, top_k=5):
        """ä¸ªæ€§åŒ–å†…å®¹æ¨è"""
        user_profile = self.analyzer.build_user_profile(user_id)
        
        # åŸºäºå†…å®¹çš„æ¨è
        content_based = self.content_based_recommendation(
            query, user_profile)
        
        # ååŒè¿‡æ»¤æ¨è
        collaborative = self.collaborative_filter.recommend(
            user_id, top_k)
        
        # æ··åˆæ¨è
        recommendations = self.hybrid_recommendation(
            content_based, collaborative)
        
        return recommendations[:top_k]
    
    def content_based_recommendation(self, query, user_profile):
        """åŸºäºå†…å®¹çš„æ¨è"""
        # å®ç°åŸºäºç”¨æˆ·å…´è¶£å’ŒæŸ¥è¯¢å†…å®¹çš„æ¨èé€»è¾‘
        pass
    
    def hybrid_recommendation(self, content_based, collaborative):
        """æ··åˆæ¨èç­–ç•¥"""
        # ç»“åˆå¤šç§æ¨èç­–ç•¥
        pass
```

---

## ğŸ—ï¸ æŠ€æœ¯æ¶æ„

### 1. æœç´¢å¼•æ“æ¶æ„

```text
æ™ºèƒ½æœç´¢å¼•æ“/
â”œâ”€â”€ core/                    # æ ¸å¿ƒå¼•æ“
â”‚   â”œâ”€â”€ semantic_engine.py   # è¯­ä¹‰æœç´¢å¼•æ“
â”‚   â”œâ”€â”€ concept_engine.py    # æ¦‚å¿µæœç´¢å¼•æ“
â”‚   â”œâ”€â”€ fuzzy_engine.py      # æ¨¡ç³ŠåŒ¹é…å¼•æ“
â”‚   â””â”€â”€ recommendation_engine.py # æ¨èå¼•æ“
â”œâ”€â”€ models/                  # AIæ¨¡å‹
â”‚   â”œâ”€â”€ sentence_transformer.py # å¥å­è½¬æ¢æ¨¡å‹
â”‚   â”œâ”€â”€ concept_recognizer.py   # æ¦‚å¿µè¯†åˆ«æ¨¡å‹
â”‚   â””â”€â”€ similarity_model.py     # ç›¸ä¼¼åº¦æ¨¡å‹
â”œâ”€â”€ data/                    # æ•°æ®å±‚
â”‚   â”œâ”€â”€ math_vocabulary.py   # æ•°å­¦è¯æ±‡åº“
â”‚   â”œâ”€â”€ concept_graph.py     # æ¦‚å¿µå›¾è°±
â”‚   â””â”€â”€ user_profiles.py     # ç”¨æˆ·ç”»åƒ
â”œâ”€â”€ utils/                   # å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ text_processor.py    # æ–‡æœ¬å¤„ç†å™¨
â”‚   â”œâ”€â”€ spell_checker.py     # æ‹¼å†™æ£€æŸ¥å™¨
â”‚   â””â”€â”€ similarity_calculator.py # ç›¸ä¼¼åº¦è®¡ç®—å™¨
â””â”€â”€ api/                     # APIæ¥å£
    â”œâ”€â”€ search_api.py        # æœç´¢API
    â”œâ”€â”€ concept_api.py       # æ¦‚å¿µAPI
    â””â”€â”€ recommendation_api.py # æ¨èAPI
```

### 2. æ•°æ®æµç¨‹

```text
ç”¨æˆ·æŸ¥è¯¢ â†’ æŸ¥è¯¢é¢„å¤„ç† â†’ å¤šå¼•æ“å¹¶è¡Œæœç´¢ â†’ ç»“æœèåˆ â†’ ä¸ªæ€§åŒ–æ’åº â†’ è¿”å›ç»“æœ
    â†“
æŸ¥è¯¢åˆ†æ â†’ æ¦‚å¿µæå– â†’ è¯­ä¹‰ç†è§£ â†’ æ¨¡ç³ŠåŒ¹é… â†’ æ¨èç”Ÿæˆ
```

### 3. æ€§èƒ½ä¼˜åŒ–

#### 3.1 ç´¢å¼•ä¼˜åŒ–

```python
# Elasticsearchç´¢å¼•é…ç½®
INDEX_CONFIG = {
    "settings": {
        "analysis": {
            "analyzer": {
                "math_analyzer": {
                    "type": "custom",
                    "tokenizer": "standard",
                    "filter": ["lowercase", "math_stemmer"]
                }
            },
            "filter": {
                "math_stemmer": {
                    "type": "stemmer",
                    "language": "english"
                }
            }
        }
    },
    "mappings": {
        "properties": {
            "title": {"type": "text", "analyzer": "math_analyzer"},
            "content": {"type": "text", "analyzer": "math_analyzer"},
            "concepts": {"type": "keyword"},
            "category": {"type": "keyword"},
            "difficulty": {"type": "integer"},
            "embedding": {"type": "dense_vector", "dims": 384}
        }
    }
}
```

#### 3.2 ç¼“å­˜ç­–ç•¥

```python
# Redisç¼“å­˜é…ç½®
CACHE_CONFIG = {
    "search_cache": {
        "ttl": 3600,  # 1å°æ—¶
        "max_size": 10000
    },
    "user_profile_cache": {
        "ttl": 86400,  # 24å°æ—¶
        "max_size": 1000
    },
    "concept_cache": {
        "ttl": 604800,  # 7å¤©
        "max_size": 5000
    }
}
```

---

## ğŸ“Š æ€§èƒ½æŒ‡æ ‡

### 1. æœç´¢æ€§èƒ½

- **å“åº”æ—¶é—´**: < 200ms
- **æœç´¢å‡†ç¡®ç‡**: > 95%
- **å¬å›ç‡**: > 90%
- **ç”¨æˆ·æ»¡æ„åº¦**: > 4.5/5

### 2. ç³»ç»Ÿæ€§èƒ½

- **å¹¶å‘å¤„ç†èƒ½åŠ›**: 1000+ QPS
- **ç´¢å¼•æ›´æ–°å»¶è¿Ÿ**: < 5åˆ†é’Ÿ
- **ç¼“å­˜å‘½ä¸­ç‡**: > 80%
- **ç³»ç»Ÿå¯ç”¨æ€§**: > 99.9%

### 3. è´¨é‡æŒ‡æ ‡

- **æ¦‚å¿µè¯†åˆ«å‡†ç¡®ç‡**: > 90%
- **æ‹¼å†™çº é”™å‡†ç¡®ç‡**: > 85%
- **æ¨èç›¸å…³æ€§**: > 80%
- **ç”¨æˆ·ç‚¹å‡»ç‡**: > 15%

---

## ğŸš€ éƒ¨ç½²æ–¹æ¡ˆ

### 1. å®¹å™¨åŒ–éƒ¨ç½²

```dockerfile
# Dockerfile
FROM python:3.9-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

EXPOSE 8000

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### 2. å¾®æœåŠ¡æ¶æ„

```yaml
# docker-compose.yml
version: '3.8'
services:
  search-engine:
    build: .
    ports:
      - "8000:8000"
    environment:
      - ELASTICSEARCH_URL=http://elasticsearch:9200
      - REDIS_URL=redis://redis:6379
    depends_on:
      - elasticsearch
      - redis
  
  elasticsearch:
    image: elasticsearch:7.17.0
    environment:
      - discovery.type=single-node
    ports:
      - "9200:9200"
  
  redis:
    image: redis:6.2-alpine
    ports:
      - "6379:6379"
```

---

## ğŸ“ˆ ç›‘æ§ä¸ä¼˜åŒ–

### 1. æ€§èƒ½ç›‘æ§

- **æœç´¢å»¶è¿Ÿç›‘æ§**: å®æ—¶ç›‘æ§æœç´¢å“åº”æ—¶é—´
- **å‡†ç¡®ç‡ç›‘æ§**: å®šæœŸè¯„ä¼°æœç´¢å‡†ç¡®ç‡
- **ç”¨æˆ·è¡Œä¸ºç›‘æ§**: è·Ÿè¸ªç”¨æˆ·æœç´¢å’Œç‚¹å‡»è¡Œä¸º
- **ç³»ç»Ÿèµ„æºç›‘æ§**: ç›‘æ§CPUã€å†…å­˜ã€ç£ç›˜ä½¿ç”¨æƒ…å†µ

### 2. æŒç»­ä¼˜åŒ–

- **æ¨¡å‹æ›´æ–°**: å®šæœŸæ›´æ–°AIæ¨¡å‹
- **ç´¢å¼•ä¼˜åŒ–**: ä¼˜åŒ–æœç´¢ç´¢å¼•ç»“æ„
- **ç¼“å­˜ä¼˜åŒ–**: ä¼˜åŒ–ç¼“å­˜ç­–ç•¥
- **ç®—æ³•è°ƒä¼˜**: æ ¹æ®ç”¨æˆ·åé¦ˆè°ƒä¼˜ç®—æ³•å‚æ•°

---

## ğŸ¯ åç»­å‘å±•

### 1. åŠŸèƒ½æ‰©å±•

- **å¤šè¯­è¨€æ”¯æŒ**: æ”¯æŒä¸­æ–‡ã€è‹±æ–‡ç­‰å¤šç§è¯­è¨€
- **è¯­éŸ³æœç´¢**: æ”¯æŒè¯­éŸ³è¾“å…¥æœç´¢
- **å›¾åƒæœç´¢**: æ”¯æŒæ•°å­¦å…¬å¼å›¾åƒæœç´¢
- **å®æ—¶æœç´¢**: æ”¯æŒå®æ—¶å†…å®¹æ›´æ–°æœç´¢

### 2. æ™ºèƒ½åŒ–æå‡

- **æ·±åº¦å­¦ä¹ **: å¼•å…¥æ›´å…ˆè¿›çš„æ·±åº¦å­¦ä¹ æ¨¡å‹
- **çŸ¥è¯†å›¾è°±**: æ„å»ºæ›´å®Œæ•´çš„æ•°å­¦çŸ¥è¯†å›¾è°±
- **ä¸ªæ€§åŒ–**: æä¾›æ›´ç²¾å‡†çš„ä¸ªæ€§åŒ–æ¨è
- **è‡ªé€‚åº”**: å®ç°è‡ªé€‚åº”çš„æœç´¢ä¼˜åŒ–

---

*æœ¬æ–‡æ¡£ä¸ºæ™ºèƒ½æœç´¢å¼•æ“çš„è¯¦ç»†è®¾è®¡ï¼Œä¸ºRefactoré¡¹ç›®çš„æ™ºèƒ½åŒ–å¯¼èˆªç³»ç»Ÿæä¾›æ ¸å¿ƒæŠ€æœ¯æ”¯æ’‘ã€‚*
