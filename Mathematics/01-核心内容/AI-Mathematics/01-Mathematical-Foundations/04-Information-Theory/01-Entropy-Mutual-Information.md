# ç†µä¸äº’ä¿¡æ¯

> **Entropy and Mutual Information**
>
> ä¿¡æ¯è®ºåŸºç¡€ï¼šé‡åŒ–ä¸ç¡®å®šæ€§ä¸ä¿¡æ¯é‡

---

## ç›®å½•

- [ç†µä¸äº’ä¿¡æ¯](#ç†µä¸äº’ä¿¡æ¯)
  - [ç›®å½•](#ç›®å½•)
  - [ğŸ“‹ æ ¸å¿ƒæ€æƒ³](#-æ ¸å¿ƒæ€æƒ³)
  - [ğŸ¯ ç†µçš„å®šä¹‰](#-ç†µçš„å®šä¹‰)
    - [1. Shannonç†µ](#1-shannonç†µ)
    - [2. è”åˆç†µä¸æ¡ä»¶ç†µ](#2-è”åˆç†µä¸æ¡ä»¶ç†µ)
    - [3. ç›¸å¯¹ç†µ (KLæ•£åº¦)](#3-ç›¸å¯¹ç†µ-klæ•£åº¦)
  - [ğŸ“Š äº’ä¿¡æ¯](#-äº’ä¿¡æ¯)
    - [1. å®šä¹‰ä¸æ€§è´¨](#1-å®šä¹‰ä¸æ€§è´¨)
    - [2. æ•°æ®å¤„ç†ä¸ç­‰å¼](#2-æ•°æ®å¤„ç†ä¸ç­‰å¼)
  - [ğŸ¤– AIåº”ç”¨](#-aiåº”ç”¨)
  - [ğŸ’» Pythonå®ç°](#-pythonå®ç°)
  - [ğŸ“š æ ¸å¿ƒå®šç†æ€»ç»“](#-æ ¸å¿ƒå®šç†æ€»ç»“)
  - [ğŸ“ ç›¸å…³è¯¾ç¨‹](#-ç›¸å…³è¯¾ç¨‹)
  - [ğŸ“– å‚è€ƒæ–‡çŒ®](#-å‚è€ƒæ–‡çŒ®)

---

## ğŸ“‹ æ ¸å¿ƒæ€æƒ³

**ä¿¡æ¯è®º**é‡åŒ–ä¿¡æ¯çš„ä¸ç¡®å®šæ€§å’Œä¿¡æ¯é‡ã€‚

**æ ¸å¿ƒæ¦‚å¿µ**ï¼š

- **ç†µ (Entropy)**ï¼šä¸ç¡®å®šæ€§çš„åº¦é‡
- **äº’ä¿¡æ¯ (Mutual Information)**ï¼šå…±äº«ä¿¡æ¯çš„åº¦é‡
- **KLæ•£åº¦**ï¼šåˆ†å¸ƒä¹‹é—´çš„"è·ç¦»"

---

## ğŸ¯ ç†µçš„å®šä¹‰

### 1. Shannonç†µ

**å®šä¹‰ 1.1 (Shannonç†µ)**:

å¯¹äºç¦»æ•£éšæœºå˜é‡ $X \sim p(x)$ï¼š

$$
H(X) = -\sum_{x} p(x) \log p(x)
$$

**å•ä½**ï¼šæ¯”ç‰¹ (bits) å½“ä½¿ç”¨ $\log_2$ï¼Œå¥ˆç‰¹ (nats) å½“ä½¿ç”¨ $\ln$ã€‚

**ç›´è§‰**ï¼šå¹³å‡ç¼–ç é•¿åº¦çš„ä¸‹ç•Œã€‚

---

**æ€§è´¨**ï¼š

1. **éè´Ÿæ€§**ï¼š$H(X) \geq 0$
2. **æœ€å¤§ç†µ**ï¼šå‡åŒ€åˆ†å¸ƒæ—¶ç†µæœ€å¤§
   $$
   H(X) \leq \log |\mathcal{X}|
   $$
3. **ç¡®å®šæ€§**ï¼š$H(X) = 0$ å½“ä¸”ä»…å½“ $X$ ç¡®å®š

---

### 2. è”åˆç†µä¸æ¡ä»¶ç†µ

**å®šä¹‰ 2.1 (è”åˆç†µ)**:

$$
H(X, Y) = -\sum_{x,y} p(x, y) \log p(x, y)
$$

**å®šä¹‰ 2.2 (æ¡ä»¶ç†µ)**:

$$
H(Y|X) = \sum_{x} p(x) H(Y|X=x) = -\sum_{x,y} p(x, y) \log p(y|x)
$$

**é“¾å¼æ³•åˆ™**ï¼š

$$
H(X, Y) = H(X) + H(Y|X)
$$

---

### 3. ç›¸å¯¹ç†µ (KLæ•£åº¦)

**å®šä¹‰ 3.1 (KLæ•£åº¦)**:

$$
D_{KL}(P \| Q) = \sum_{x} p(x) \log \frac{p(x)}{q(x)}
$$

**æ€§è´¨**ï¼š

1. **éè´Ÿæ€§**ï¼š$D_{KL}(P \| Q) \geq 0$ï¼Œç­‰å·æˆç«‹å½“ä¸”ä»…å½“ $P = Q$
2. **éå¯¹ç§°æ€§**ï¼š$D_{KL}(P \| Q) \neq D_{KL}(Q \| P)$

---

## ğŸ“Š äº’ä¿¡æ¯

### 1. å®šä¹‰ä¸æ€§è´¨

**å®šä¹‰ 1.1 (äº’ä¿¡æ¯)**:

$$
I(X; Y) = \sum_{x,y} p(x, y) \log \frac{p(x, y)}{p(x)p(y)}
$$

**ç­‰ä»·å½¢å¼**ï¼š

$$
I(X; Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)
$$

$$
I(X; Y) = D_{KL}(p(x,y) \| p(x)p(y))
$$

**æ€§è´¨**ï¼š

1. **å¯¹ç§°æ€§**ï¼š$I(X; Y) = I(Y; X)$
2. **éè´Ÿæ€§**ï¼š$I(X; Y) \geq 0$
3. **ç‹¬ç«‹æ€§**ï¼š$I(X; Y) = 0$ å½“ä¸”ä»…å½“ $X \perp Y$

---

### 2. æ•°æ®å¤„ç†ä¸ç­‰å¼

**å®šç† 2.1 (æ•°æ®å¤„ç†ä¸ç­‰å¼)**:

è‹¥ $X \to Y \to Z$ æ„æˆé©¬å°”å¯å¤«é“¾ï¼Œåˆ™ï¼š

$$
I(X; Z) \leq I(X; Y)
$$

**æ„ä¹‰**ï¼šä¿¡æ¯å¤„ç†ä¸èƒ½å¢åŠ ä¿¡æ¯é‡ã€‚

---

## ğŸ¤– AIåº”ç”¨

**1. æœºå™¨å­¦ä¹ **ï¼š

- ç‰¹å¾é€‰æ‹©ï¼ˆæœ€å¤§åŒ–äº’ä¿¡æ¯ï¼‰
- æ¨¡å‹å‹ç¼©ï¼ˆæœ€å°åŒ–KLæ•£åº¦ï¼‰

**2. æ·±åº¦å­¦ä¹ **ï¼š

- VAEæŸå¤±å‡½æ•°ï¼ˆELBOï¼‰
- ä¿¡æ¯ç“¶é¢ˆç†è®º

**3. å¼ºåŒ–å­¦ä¹ **ï¼š

- æœ€å¤§ç†µRL
- æ¢ç´¢-åˆ©ç”¨å¹³è¡¡

---

## ğŸ’» Pythonå®ç°

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import entropy

def shannon_entropy(p):
    """è®¡ç®—Shannonç†µ"""
    p = np.array(p)
    p = p[p > 0]  # ç§»é™¤é›¶æ¦‚ç‡
    return -np.sum(p * np.log2(p))

def kl_divergence(p, q):
    """è®¡ç®—KLæ•£åº¦"""
    p = np.array(p)
    q = np.array(q)
    return np.sum(p * np.log2(p / q))

def mutual_information(joint_prob):
    """è®¡ç®—äº’ä¿¡æ¯"""
    p_x = joint_prob.sum(axis=1)
    p_y = joint_prob.sum(axis=0)

    mi = 0
    for i in range(len(p_x)):
        for j in range(len(p_y)):
            if joint_prob[i, j] > 0:
                mi += joint_prob[i, j] * np.log2(
                    joint_prob[i, j] / (p_x[i] * p_y[j])
                )
    return mi

# ç¤ºä¾‹
p = [0.5, 0.3, 0.2]
print(f"Entropy: {shannon_entropy(p):.4f} bits")

joint = np.array([[0.2, 0.1], [0.1, 0.6]])
print(f"Mutual Information: {mutual_information(joint):.4f} bits")
```

---

## ğŸ“š æ ¸å¿ƒå®šç†æ€»ç»“

| å®šç† | é™ˆè¿° | æ„ä¹‰ |
|------|------|------|
| **æœ€å¤§ç†µ** | $H(X) \leq \log \|\mathcal{X}\|$ | å‡åŒ€åˆ†å¸ƒæœ€å¤§ |
| **é“¾å¼æ³•åˆ™** | $H(X,Y) = H(X) + H(Y\|X)$ | ç†µçš„åˆ†è§£ |
| **KLéè´Ÿæ€§** | $D_{KL}(P \| Q) \geq 0$ | åˆ†å¸ƒå·®å¼‚ |
| **æ•°æ®å¤„ç†** | $I(X;Z) \leq I(X;Y)$ | ä¿¡æ¯ä¸å¢ |

---

## ğŸ“ ç›¸å…³è¯¾ç¨‹

| å¤§å­¦ | è¯¾ç¨‹ |
|------|------|
| **MIT** | 6.441 Information Theory |
| **Stanford** | EE376A Information Theory |
| **Cambridge** | Information Theory |

---

## ğŸ“– å‚è€ƒæ–‡çŒ®

1. **Cover & Thomas (2006)**. *Elements of Information Theory* (2nd ed.). Wiley.

2. **MacKay (2003)**. *Information Theory, Inference, and Learning Algorithms*. Cambridge University Press.

---

*æœ€åæ›´æ–°ï¼š2025å¹´10æœˆ*-
