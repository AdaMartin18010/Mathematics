# ç¥ç»æ­£åˆ‡æ ¸ç†è®º

> **Neural Tangent Kernel (NTK)**
>
> ç†è§£è¿‡å‚æ•°åŒ–ç¥ç»ç½‘ç»œè®­ç»ƒåŠ¨åŠ›å­¦çš„ç†è®ºæ¡†æ¶

---

## ç›®å½•

- [ç¥ç»æ­£åˆ‡æ ¸ç†è®º](#ç¥ç»æ­£åˆ‡æ ¸ç†è®º)
  - [ç›®å½•](#ç›®å½•)
  - [ğŸ“‹ æ ¸å¿ƒæ€æƒ³](#-æ ¸å¿ƒæ€æƒ³)
  - [ğŸ¯ NTKçš„å®šä¹‰](#-ntkçš„å®šä¹‰)
    - [1. æœ‰é™å®½åº¦ç½‘ç»œ](#1-æœ‰é™å®½åº¦ç½‘ç»œ)
    - [2. æ— é™å®½åº¦æé™](#2-æ— é™å®½åº¦æé™)
  - [ğŸ“Š è®­ç»ƒåŠ¨åŠ›å­¦](#-è®­ç»ƒåŠ¨åŠ›å­¦)
    - [1. æ¢¯åº¦æµæ–¹ç¨‹](#1-æ¢¯åº¦æµæ–¹ç¨‹)
    - [2. Lazy Training](#2-lazy-training)
    - [3. çº¿æ€§åŒ–è¿‘ä¼¼](#3-çº¿æ€§åŒ–è¿‘ä¼¼)
  - [ğŸ”¬ ç†è®ºæ€§è´¨](#-ç†è®ºæ€§è´¨)
    - [1. NTKçš„ç¡®å®šæ€§æé™](#1-ntkçš„ç¡®å®šæ€§æé™)
    - [2. æ”¶æ•›æ€§åˆ†æ](#2-æ”¶æ•›æ€§åˆ†æ)
    - [3. æ³›åŒ–ç•Œ](#3-æ³›åŒ–ç•Œ)
  - [ğŸ¤– å®é™…æ„ä¹‰](#-å®é™…æ„ä¹‰)
  - [ğŸ’» Pythonå®ç°](#-pythonå®ç°)
  - [ğŸ“š æ ¸å¿ƒå®šç†æ€»ç»“](#-æ ¸å¿ƒå®šç†æ€»ç»“)
  - [ğŸ“ ç›¸å…³è¯¾ç¨‹](#-ç›¸å…³è¯¾ç¨‹)
  - [ğŸ“– å‚è€ƒæ–‡çŒ®](#-å‚è€ƒæ–‡çŒ®)
  - [ğŸ”— ç›¸å…³æ–‡æ¡£](#-ç›¸å…³æ–‡æ¡£)
  - [âœï¸ ç»ƒä¹ ](#ï¸-ç»ƒä¹ )

---

## ğŸ“‹ æ ¸å¿ƒæ€æƒ³

**ç¥ç»æ­£åˆ‡æ ¸ (NTK)** æ˜¯ç†è§£**è¿‡å‚æ•°åŒ–ç¥ç»ç½‘ç»œ**è®­ç»ƒåŠ¨åŠ›å­¦çš„å…³é”®å·¥å…·ã€‚

**æ ¸å¿ƒå‘ç°**ï¼š

- åœ¨**æ— é™å®½åº¦**æé™ä¸‹ï¼Œç¥ç»ç½‘ç»œçš„è®­ç»ƒç­‰ä»·äº**æ ¸å›å½’**
- ç½‘ç»œå‚æ•°åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å‡ ä¹ä¸å˜ï¼ˆ**Lazy Training**ï¼‰
- å¯ä»¥ç”¨**çº¿æ€§ç†è®º**åˆ†æéçº¿æ€§ç¥ç»ç½‘ç»œ

---

## ğŸ¯ NTKçš„å®šä¹‰

### 1. æœ‰é™å®½åº¦ç½‘ç»œ

è€ƒè™‘å‚æ•°ä¸º $\theta \in \mathbb{R}^P$ çš„ç¥ç»ç½‘ç»œ $f(x; \theta)$ã€‚

**å®šä¹‰ 1.1 (ç¥ç»æ­£åˆ‡æ ¸)**:

$$
\Theta(x, x'; \theta) = \nabla_\theta f(x; \theta)^\top \nabla_\theta f(x'; \theta)
$$

**ç›´è§‰**ï¼š

- è¡¡é‡è¾“å…¥ $x$ å’Œ $x'$ åœ¨**å‚æ•°ç©ºé—´**ä¸­çš„ç›¸ä¼¼æ€§
- é€šè¿‡å‚æ•°æ¢¯åº¦çš„å†…ç§¯å®šä¹‰

---

**è®­ç»ƒåŠ¨åŠ›å­¦**ï¼š

åœ¨æ¢¯åº¦ä¸‹é™ä¸‹ï¼š

$$
\frac{d\theta}{dt} = -\eta \nabla_\theta L(\theta)
$$

ç½‘ç»œè¾“å‡ºçš„å˜åŒ–ç‡ä¸ºï¼š

$$
\frac{df(x; \theta)}{dt} = \nabla_\theta f(x; \theta)^\top \frac{d\theta}{dt}
$$

å¯¹äºMSEæŸå¤± $L = \frac{1}{2n}\sum_{i=1}^n (f(x_i; \theta) - y_i)^2$ï¼š

$$
\frac{df(x; \theta)}{dt} = -\frac{\eta}{n} \sum_{i=1}^n \Theta(x, x_i; \theta) (f(x_i; \theta) - y_i)
$$

---

### 2. æ— é™å®½åº¦æé™

**å®šç† 2.1 (Jacot et al., 2018)**:

å¯¹äºå…¨è¿æ¥ç½‘ç»œï¼Œå½“æ¯å±‚å®½åº¦ $n \to \infty$ æ—¶ï¼Œåœ¨**éšæœºåˆå§‹åŒ–**ä¸‹ï¼ŒNTKæ”¶æ•›åˆ°ç¡®å®šæ€§æé™ï¼š

$$
\Theta(x, x'; \theta_0) \xrightarrow{n \to \infty} \Theta^{\infty}(x, x')
$$

ä¸”åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œ$\Theta(x, x'; \theta(t)) \approx \Theta^{\infty}(x, x')$ ä¿æŒä¸å˜ã€‚

---

**æ˜¾å¼å…¬å¼ï¼ˆä¸¤å±‚ç½‘ç»œï¼‰**ï¼š

å¯¹äºä¸¤å±‚ç½‘ç»œ $f(x; W) = \frac{1}{\sqrt{m}} \sum_{j=1}^m a_j \sigma(w_j^\top x)$ï¼š

$$
\Theta^{\infty}(x, x') = \mathbb{E}_{w \sim \mathcal{N}(0, I)}[\sigma'(w^\top x) \sigma'(w^\top x') x^\top x']
$$

å¯¹äºReLUæ¿€æ´»ï¼š

$$
\Theta^{\infty}_{\text{ReLU}}(x, x') = \frac{\|x\| \|x'\|}{2\pi} \left(\sin\theta + (\pi - \theta)\cos\theta\right)
$$

å…¶ä¸­ $\theta = \arccos\left(\frac{x^\top x'}{\|x\| \|x'\|}\right)$ã€‚

---

## ğŸ“Š è®­ç»ƒåŠ¨åŠ›å­¦

### 1. æ¢¯åº¦æµæ–¹ç¨‹

åœ¨æ— é™å®½åº¦æé™ä¸‹ï¼Œè®­ç»ƒåŠ¨åŠ›å­¦ç®€åŒ–ä¸º**çº¿æ€§å¾®åˆ†æ–¹ç¨‹**ï¼š

$$
\frac{du(t)}{dt} = -\eta K (u(t) - y)
$$

å…¶ä¸­ï¼š

- $u(t) = [f(x_1; \theta(t)), \ldots, f(x_n; \theta(t))]^\top$ æ˜¯é¢„æµ‹å‘é‡
- $K_{ij} = \Theta^{\infty}(x_i, x_j)$ æ˜¯æ ¸çŸ©é˜µ
- $y$ æ˜¯æ ‡ç­¾å‘é‡

**è§£æè§£**ï¼š

$$
u(t) = (I - e^{-\eta K t})(y - u(0)) + u(0)
$$

---

### 2. Lazy Training

**å®šä¹‰ 2.1 (Lazy Training)**:

åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå‚æ•° $\theta(t)$ ç›¸å¯¹äºåˆå§‹åŒ– $\theta_0$ çš„å˜åŒ–å¾ˆå°ï¼š

$$
\|\theta(t) - \theta_0\| = O\left(\frac{1}{\sqrt{m}}\right)
$$

**åŸå› **ï¼š

- è¿‡å‚æ•°åŒ–ï¼ˆ$P \gg n$ï¼‰
- æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦è¢« $1/\sqrt{m}$ ç¼©æ”¾
- ç½‘ç»œåœ¨åˆå§‹åŒ–é™„è¿‘çš„çº¿æ€§regimeä¸­è¿è¡Œ

---

### 3. çº¿æ€§åŒ–è¿‘ä¼¼

**ä¸€é˜¶Taylorå±•å¼€**ï¼š

$$
f(x; \theta(t)) \approx f(x; \theta_0) + \nabla_\theta f(x; \theta_0)^\top (\theta(t) - \theta_0)
$$

è¿™ä½¿å¾—éçº¿æ€§ç½‘ç»œçš„è®­ç»ƒç­‰ä»·äº**çº¿æ€§æ¨¡å‹**çš„è®­ç»ƒï¼

---

## ğŸ”¬ ç†è®ºæ€§è´¨

### 1. NTKçš„ç¡®å®šæ€§æé™

**å®šç† 1.1 (NTKæ”¶æ•›)**:

å¯¹äº $L$ å±‚å…¨è¿æ¥ç½‘ç»œï¼Œæ¯å±‚å®½åº¦ $n_\ell \to \infty$ï¼ˆæŒ‰é¡ºåºï¼‰ï¼ŒNTKæ”¶æ•›åˆ°ç¡®å®šæ€§å‡½æ•° $\Theta^{\infty}$ï¼Œä¸”ï¼š

$$
\mathbb{P}\left(\sup_{x,x'} |\Theta(x, x'; \theta_0) - \Theta^{\infty}(x, x')| > \epsilon\right) \to 0
$$

---

### 2. æ”¶æ•›æ€§åˆ†æ

**å®šç† 2.1 (å…¨å±€æ”¶æ•›)**:

è‹¥æ ¸çŸ©é˜µ $K$ çš„æœ€å°ç‰¹å¾å€¼ $\lambda_{\min}(K) > 0$ï¼Œåˆ™æ¢¯åº¦ä¸‹é™ä»¥æŒ‡æ•°é€Ÿç‡æ”¶æ•›ï¼š

$$
\|u(t) - y\|^2 \leq e^{-2\eta \lambda_{\min}(K) t} \|u(0) - y\|^2
$$

**æ”¶æ•›é€Ÿç‡**ï¼šç”±æ ¸çŸ©é˜µçš„è°±å†³å®šã€‚

---

### 3. æ³›åŒ–ç•Œ

**å®šç† 3.1 (NTKæ³›åŒ–ç•Œ)**:

åœ¨NTK regimeä¸‹ï¼Œæµ‹è¯•è¯¯å·®æ»¡è¶³ï¼š

$$
\mathbb{E}_{(x,y) \sim D}[(f(x; \theta(t)) - y)^2] \leq \text{è®­ç»ƒè¯¯å·®} + O\left(\frac{\text{Tr}(K)}{n}\right)
$$

å…¶ä¸­ $\text{Tr}(K)$ æ˜¯æ ¸çŸ©é˜µçš„è¿¹ã€‚

**æ„ä¹‰**ï¼šæ³›åŒ–ç”±æ ¸çš„å¤æ‚åº¦æ§åˆ¶ï¼Œè€Œéå‚æ•°æ•°é‡ã€‚

---

## ğŸ¤– å®é™…æ„ä¹‰

**NTKç†è®ºçš„è´¡çŒ®**ï¼š

âœ… **ç†è®ºç†è§£**ï¼š

- è§£é‡Šäº†ä¸ºä»€ä¹ˆè¿‡å‚æ•°åŒ–ç½‘ç»œèƒ½è®­ç»ƒæˆåŠŸ
- æä¾›äº†æ”¶æ•›æ€§ä¿è¯

âœ… **è®¾è®¡æŒ‡å¯¼**ï¼š

- åˆå§‹åŒ–æ–¹æ¡ˆï¼ˆä¿æŒNTKç¨³å®šï¼‰
- æ¶æ„é€‰æ‹©ï¼ˆä¼˜åŒ–æ ¸çš„æ€§è´¨ï¼‰

âŒ **å±€é™æ€§**ï¼š

- å®é™…ç½‘ç»œå®½åº¦æœ‰é™ï¼ŒNTKä¼šå˜åŒ–
- æ— æ³•è§£é‡Š**ç‰¹å¾å­¦ä¹ **ï¼ˆfeature learningï¼‰
- æ— æ³•è§£é‡Šå®é™…ç½‘ç»œçš„å¼ºæ³›åŒ–èƒ½åŠ›

---

**NTK vs ç‰¹å¾å­¦ä¹ **ï¼š

| Regime | å‚æ•°å˜åŒ– | è¡¨ç¤ºèƒ½åŠ› | æ³›åŒ– |
|--------|----------|----------|------|
| **NTK (Lazy)** | å° | å›ºå®šï¼ˆçº¿æ€§ï¼‰ | æ ¸æ–¹æ³•çº§åˆ« |
| **Feature Learning** | å¤§ | åŠ¨æ€æ¼”åŒ– | æ›´å¼º |

å®é™…æ·±åº¦å­¦ä¹ æ›´æ¥è¿‘**ç‰¹å¾å­¦ä¹ ** regimeï¼

---

## ğŸ’» Pythonå®ç°

```python
import numpy as np
import torch
import torch.nn as nn
import matplotlib.pyplot as plt

# 1. è®¡ç®—NTK (æœ‰é™å®½åº¦)
def compute_ntk(model, x1, x2):
    """
    è®¡ç®—ç¥ç»æ­£åˆ‡æ ¸ Î˜(x1, x2)
    
    Args:
        model: PyTorchæ¨¡å‹
        x1, x2: è¾“å…¥ç‚¹
    
    Returns:
        NTKå€¼
    """
    # è®¡ç®— f(x1) å’Œ f(x2)
    f1 = model(x1)
    f2 = model(x2)
    
    # è®¡ç®—æ¢¯åº¦
    grad1 = torch.autograd.grad(f1.sum(), model.parameters(), create_graph=True)
    grad2 = torch.autograd.grad(f2.sum(), model.parameters(), create_graph=True)
    
    # å†…ç§¯
    ntk = sum((g1 * g2).sum() for g1, g2 in zip(grad1, grad2))
    
    return ntk.item()


# 2. æ„å»ºNTKçŸ©é˜µ
def build_ntk_matrix(model, X):
    """
    æ„å»ºå®Œæ•´çš„NTKçŸ©é˜µ
    
    Args:
        model: PyTorchæ¨¡å‹
        X: æ•°æ®é›† (n, d)
    
    Returns:
        K: NTKçŸ©é˜µ (n, n)
    """
    n = len(X)
    K = np.zeros((n, n))
    
    for i in range(n):
        for j in range(i, n):
            x_i = X[i:i+1]
            x_j = X[j:j+1]
            K[i, j] = compute_ntk(model, x_i, x_j)
            K[j, i] = K[i, j]  # å¯¹ç§°
    
    return K


# 3. NTKé¢„æµ‹
def ntk_predict(K_train, K_test_train, y_train, eta, t):
    """
    ä½¿ç”¨NTKç†è®ºé¢„æµ‹
    
    Args:
        K_train: è®­ç»ƒé›†NTKçŸ©é˜µ (n, n)
        K_test_train: æµ‹è¯•-è®­ç»ƒNTKçŸ©é˜µ (m, n)
        y_train: è®­ç»ƒæ ‡ç­¾ (n,)
        eta: å­¦ä¹ ç‡
        t: è®­ç»ƒæ—¶é—´
    
    Returns:
        é¢„æµ‹å€¼ (m,)
    """
    n = len(y_train)
    
    # è§£æè§£: u(t) = (I - exp(-Î· K t)) y
    exp_term = np.linalg.matrix_power(
        np.eye(n) - eta * K_train, 
        int(t)
    )
    u_train = y_train - exp_term @ y_train
    
    # æµ‹è¯•é›†é¢„æµ‹
    u_test = K_test_train @ np.linalg.pinv(K_train) @ u_train
    
    return u_test


# 4. å¯è§†åŒ–è®­ç»ƒåŠ¨åŠ›å­¦
def visualize_ntk_dynamics():
    """å¯è§†åŒ–NTK regimeä¸‹çš„è®­ç»ƒåŠ¨åŠ›å­¦"""
    
    # ç”Ÿæˆæ•°æ®
    np.random.seed(42)
    X_train = np.random.randn(50, 2)
    y_train = np.sin(X_train[:, 0]) + 0.5 * X_train[:, 1]
    
    # ä¸¤å±‚ç½‘ç»œ
    class TwoLayerNet(nn.Module):
        def __init__(self, width):
            super().__init__()
            self.fc1 = nn.Linear(2, width)
            self.fc2 = nn.Linear(width, 1)
        
        def forward(self, x):
            return self.fc2(torch.relu(self.fc1(x)))
    
    # ä¸åŒå®½åº¦
    widths = [10, 50, 200, 1000]
    
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    axes = axes.flatten()
    
    for idx, width in enumerate(widths):
        model = TwoLayerNet(width)
        
        # è®¡ç®—åˆå§‹NTK
        X_tensor = torch.FloatTensor(X_train).requires_grad_(True)
        K_init = build_ntk_matrix(model, X_tensor)
        
        # è®­ç»ƒ
        optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
        criterion = nn.MSELoss()
        
        losses = []
        ntk_changes = []
        
        for epoch in range(100):
            optimizer.zero_grad()
            y_pred = model(X_tensor).squeeze()
            loss = criterion(y_pred, torch.FloatTensor(y_train))
            loss.backward()
            optimizer.step()
            
            losses.append(loss.item())
            
            # è®¡ç®—NTKå˜åŒ–
            if epoch % 10 == 0:
                K_current = build_ntk_matrix(model, X_tensor)
                ntk_change = np.linalg.norm(K_current - K_init, 'fro') / np.linalg.norm(K_init, 'fro')
                ntk_changes.append(ntk_change)
        
        # ç»˜å›¾
        ax = axes[idx]
        ax.plot(losses, label='Training Loss')
        ax.set_title(f'Width = {width}', fontsize=12)
        ax.set_xlabel('Epoch')
        ax.set_ylabel('Loss')
        ax.set_yscale('log')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # æ ‡æ³¨NTKå˜åŒ–
        ax.text(0.6, 0.9, f'NTK change: {ntk_changes[-1]:.3f}', 
                transform=ax.transAxes, fontsize=10,
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
    
    plt.tight_layout()
    plt.savefig('ntk_dynamics.png', dpi=150)
    plt.show()

# visualize_ntk_dynamics()
```

---

## ğŸ“š æ ¸å¿ƒå®šç†æ€»ç»“

| å®šç† | é™ˆè¿° | æ„ä¹‰ |
|------|------|------|
| **NTKæ”¶æ•›** | $\Theta(\theta_0) \to \Theta^{\infty}$ | æ— é™å®½åº¦ä¸‹ç¡®å®šæ€§ |
| **Lazy Training** | $\|\theta(t) - \theta_0\| = O(1/\sqrt{m})$ | å‚æ•°å‡ ä¹ä¸å˜ |
| **å…¨å±€æ”¶æ•›** | $\|u(t) - y\| \leq e^{-\lambda t} \|u(0) - y\|$ | æŒ‡æ•°æ”¶æ•› |
| **æ³›åŒ–ç•Œ** | æµ‹è¯•è¯¯å·® $\leq$ è®­ç»ƒè¯¯å·® $+ O(\text{Tr}(K)/n)$ | æ ¸å¤æ‚åº¦æ§åˆ¶ |

---

## ğŸ“ ç›¸å…³è¯¾ç¨‹

| å¤§å­¦ | è¯¾ç¨‹ | è¦†ç›–å†…å®¹ |
|------|------|----------|
| **MIT** | 9.520 Statistical Learning Theory | NTKã€æ ¸æ–¹æ³•ã€æ³›åŒ–ç†è®º |
| **Stanford** | CS229 Machine Learning | æ ¸æ–¹æ³•åŸºç¡€ |
| **Princeton** | COS 597E Deep Learning Theory | NTKã€ç‰¹å¾å­¦ä¹  |
| **Cambridge** | Advanced Topics in ML | ç¥ç»ç½‘ç»œç†è®º |

---

## ğŸ“– å‚è€ƒæ–‡çŒ®

1. **Jacot, A. et al. (2018)**. "Neural Tangent Kernel: Convergence and Generalization in Neural Networks". *NeurIPS*.

2. **Lee, J. et al. (2019)**. "Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent". *NeurIPS*.

3. **Arora, S. et al. (2019)**. "On Exact Computation with an Infinitely Wide Neural Net". *NeurIPS*.

4. **Chizat, L. & Bach, F. (2020)**. "Implicit Bias of Gradient Descent for Wide Two-layer Neural Networks Trained with the Logistic Loss". *COLT*.

---

## ğŸ”— ç›¸å…³æ–‡æ¡£

- [é€šç”¨é€¼è¿‘å®šç†](01-Universal-Approximation-Theorem.md)
- [åå‘ä¼ æ’­ç®—æ³•](03-Backpropagation.md)
- [VCç»´ä¸Rademacherå¤æ‚åº¦](../01-Statistical-Learning/02-VC-Dimension-Rademacher-Complexity.md)
- [å‡¸ä¼˜åŒ–ç†è®º](../03-Optimization/01-Convex-Optimization.md)

---

## âœï¸ ç»ƒä¹ 

**ç»ƒä¹  1 (åŸºç¡€)**ï¼šæ¨å¯¼ä¸¤å±‚ReLUç½‘ç»œçš„NTKæ˜¾å¼å…¬å¼ã€‚

**ç»ƒä¹  2 (ä¸­ç­‰)**ï¼šå®ç°NTKçŸ©é˜µè®¡ç®—ï¼Œå¹¶åœ¨toyæ•°æ®é›†ä¸ŠéªŒè¯çº¿æ€§åŒ–è¿‘ä¼¼çš„å‡†ç¡®æ€§ã€‚

**ç»ƒä¹  3 (ä¸­ç­‰)**ï¼šè¯æ˜åœ¨NTK regimeä¸‹ï¼Œè®­ç»ƒåŠ¨åŠ›å­¦ç­‰ä»·äºæ ¸å›å½’ã€‚

**ç»ƒä¹  4 (å›°éš¾)**ï¼šåˆ†æNTKçš„ç‰¹å¾å€¼åˆ†å¸ƒï¼Œå¹¶è§£é‡Šå…¶å¯¹æ”¶æ•›é€Ÿç‡çš„å½±å“ã€‚

**ç»ƒä¹  5 (ç ”ç©¶)**ï¼šé˜…è¯»Chizat & Bachå…³äºimplicit biasçš„è®ºæ–‡ï¼Œç†è§£NTKä¸ç‰¹å¾å­¦ä¹ çš„åŒºåˆ«ã€‚

**ç»ƒä¹  6 (å®è·µ)**ï¼šåœ¨MNISTä¸Šæ¯”è¾ƒä¸åŒå®½åº¦ç½‘ç»œçš„NTKå˜åŒ–ï¼Œè§‚å¯Ÿlazy trainingç°è±¡ã€‚

---

*æœ€åæ›´æ–°ï¼š2025å¹´10æœˆ*-
