# æ®‹å·®ç½‘ç»œ (ResNet) æ•°å­¦åŸç†

> **Residual Networks: Mathematics of Deep Network Training**
>
> æ·±åº¦ç½‘ç»œè®­ç»ƒçš„çªç ´ï¼šæ®‹å·®è¿æ¥çš„æ•°å­¦ç†è®º

---

## ç›®å½•

- [æ®‹å·®ç½‘ç»œ (ResNet) æ•°å­¦åŸç†](#æ®‹å·®ç½‘ç»œ-resnet-æ•°å­¦åŸç†)
  - [ç›®å½•](#ç›®å½•)
  - [ğŸ“‹ æ ¸å¿ƒæ€æƒ³](#-æ ¸å¿ƒæ€æƒ³)
  - [ğŸ¯ æ·±åº¦ç½‘ç»œçš„é€€åŒ–é—®é¢˜](#-æ·±åº¦ç½‘ç»œçš„é€€åŒ–é—®é¢˜)
    - [1. é—®é¢˜è§‚å¯Ÿ](#1-é—®é¢˜è§‚å¯Ÿ)
    - [2. ç†è®ºåˆ†æ](#2-ç†è®ºåˆ†æ)
  - [ğŸ“Š æ®‹å·®å­¦ä¹ ](#-æ®‹å·®å­¦ä¹ )
    - [1. æ®‹å·®å—](#1-æ®‹å·®å—)
    - [2. æ’ç­‰æ˜ å°„](#2-æ’ç­‰æ˜ å°„)
    - [3. æ•°å­¦å½¢å¼åŒ–](#3-æ•°å­¦å½¢å¼åŒ–)
  - [ğŸ”¬ æ¢¯åº¦æµåˆ†æ](#-æ¢¯åº¦æµåˆ†æ)
    - [1. åå‘ä¼ æ’­](#1-åå‘ä¼ æ’­)
    - [2. æ¢¯åº¦æ¶ˆå¤±ç¼“è§£](#2-æ¢¯åº¦æ¶ˆå¤±ç¼“è§£)
    - [3. æ¢¯åº¦çˆ†ç‚¸æ§åˆ¶](#3-æ¢¯åº¦çˆ†ç‚¸æ§åˆ¶)
  - [ğŸ’» å‰å‘ä¼ æ’­ä¸åå‘ä¼ æ’­](#-å‰å‘ä¼ æ’­ä¸åå‘ä¼ æ’­)
    - [1. å‰å‘ä¼ æ’­](#1-å‰å‘ä¼ æ’­)
    - [2. åå‘ä¼ æ’­æ¨å¯¼](#2-åå‘ä¼ æ’­æ¨å¯¼)
  - [ğŸ¨ Pythonå®ç°](#-pythonå®ç°)
  - [ğŸ“š ç†è®ºæ·±åŒ–](#-ç†è®ºæ·±åŒ–)
    - [1. é›†æˆå­¦ä¹ è§†è§’](#1-é›†æˆå­¦ä¹ è§†è§’)
    - [2. ä¼˜åŒ–æ™¯è§‚](#2-ä¼˜åŒ–æ™¯è§‚)
    - [3. è¡¨ç¤ºèƒ½åŠ›](#3-è¡¨ç¤ºèƒ½åŠ›)
  - [ğŸ”§ ResNetå˜ä½“](#-resnetå˜ä½“)
    - [1. Pre-Activation ResNet](#1-pre-activation-resnet)
    - [2. Wide ResNet](#2-wide-resnet)
    - [3. ResNeXt](#3-resnext)
  - [ğŸ“ ç›¸å…³è¯¾ç¨‹](#-ç›¸å…³è¯¾ç¨‹)
  - [ğŸ“– å‚è€ƒæ–‡çŒ®](#-å‚è€ƒæ–‡çŒ®)

---

## ğŸ“‹ æ ¸å¿ƒæ€æƒ³

**æ®‹å·®è¿æ¥**é€šè¿‡**è·³è·ƒè¿æ¥**è§£å†³æ·±åº¦ç½‘ç»œè®­ç»ƒéš¾é¢˜ã€‚

**æ ¸å¿ƒåˆ›æ–°**ï¼š

```text
ä¼ ç»Ÿç½‘ç»œ: x â†’ F(x)
æ®‹å·®ç½‘ç»œ: x â†’ F(x) + x
```

**å…³é”®æ´å¯Ÿ**ï¼šå­¦ä¹ **æ®‹å·®** $F(x) = H(x) - x$ æ¯”ç›´æ¥å­¦ä¹  $H(x)$ æ›´å®¹æ˜“ã€‚

---

## ğŸ¯ æ·±åº¦ç½‘ç»œçš„é€€åŒ–é—®é¢˜

### 1. é—®é¢˜è§‚å¯Ÿ

**å®éªŒå‘ç°** (He et al., 2016)ï¼š

- 56å±‚ç½‘ç»œæ¯”20å±‚ç½‘ç»œ**è®­ç»ƒè¯¯å·®æ›´é«˜**
- ä¸æ˜¯è¿‡æ‹Ÿåˆï¼ˆæµ‹è¯•è¯¯å·®ä¹Ÿæ›´é«˜ï¼‰
- ä¸æ˜¯æ¢¯åº¦æ¶ˆå¤±ï¼ˆä½¿ç”¨BNåä»å­˜åœ¨ï¼‰

**é€€åŒ–é—®é¢˜**ï¼šæ›´æ·±çš„ç½‘ç»œè¡¨ç°æ›´å·®ã€‚

---

### 2. ç†è®ºåˆ†æ

**å‡è®¾**ï¼šæµ…å±‚ç½‘ç»œå·²è¾¾åˆ°è¾ƒå¥½è§£ã€‚

**é—®é¢˜**ï¼šæ·±å±‚ç½‘ç»œåº”è¯¥è‡³å°‘èƒ½å­¦åˆ°æ’ç­‰æ˜ å°„ï¼ˆå¤åˆ¶æµ…å±‚è§£ï¼‰ã€‚

**å›°éš¾**ï¼šç›´æ¥å­¦ä¹ æ’ç­‰æ˜ å°„ $H(x) = x$ å¾ˆéš¾ï¼

**åŸå› **ï¼š

- å¤šå±‚éçº¿æ€§å˜æ¢
- åˆå§‹åŒ–è¿œç¦»æ’ç­‰æ˜ å°„
- ä¼˜åŒ–å›°éš¾

---

## ğŸ“Š æ®‹å·®å­¦ä¹ 

### 1. æ®‹å·®å—

**å®šä¹‰ 1.1 (æ®‹å·®å—)**:

$$
y = F(x, \{W_i\}) + x
$$

å…¶ä¸­ï¼š

- $x$ï¼šè¾“å…¥
- $F(x, \{W_i\})$ï¼šæ®‹å·®å‡½æ•°ï¼ˆå‡ å±‚å·ç§¯+æ¿€æ´»ï¼‰
- $y$ï¼šè¾“å‡º

**ç›´è§‰**ï¼šå­¦ä¹ **æ®‹å·®** $F(x) = H(x) - x$ è€Œé $H(x)$ã€‚

---

### 2. æ’ç­‰æ˜ å°„

**å…³é”®æ€§è´¨**ï¼šå¦‚æœæ’ç­‰æ˜ å°„æ˜¯æœ€ä¼˜çš„ï¼Œåªéœ€å­¦ä¹  $F(x) = 0$ã€‚

**ä¼˜åŠ¿**ï¼š

- å°† $F(x)$ æ¨å‘0æ¯”å­¦ä¹ æ’ç­‰æ˜ å°„å®¹æ˜“
- åˆå§‹åŒ–æ—¶ $F(x) \approx 0$ï¼Œç½‘ç»œä»æ’ç­‰æ˜ å°„å¼€å§‹
- æ¢¯åº¦å¯ä»¥ç›´æ¥é€šè¿‡è·³è·ƒè¿æ¥ä¼ æ’­

---

### 3. æ•°å­¦å½¢å¼åŒ–

**æ ‡å‡†æ®‹å·®å—**ï¼š

$$
\begin{align}
\mathbf{z}^{(l)} &= W^{(l)} \mathbf{h}^{(l-1)} + \mathbf{b}^{(l)} \\
\mathbf{h}^{(l)} &= \sigma(\mathbf{z}^{(l)}) + \mathbf{h}^{(l-1)}
\end{align}
$$

**ä¸€èˆ¬å½¢å¼**ï¼š

$$
\mathbf{h}^{(l)} = \mathbf{h}^{(l-1)} + F(\mathbf{h}^{(l-1)}, W^{(l)})
$$

**é€’å½’å±•å¼€**ï¼š

$$
\mathbf{h}^{(L)} = \mathbf{h}^{(0)} + \sum_{l=1}^{L} F(\mathbf{h}^{(l-1)}, W^{(l)})
$$

**è§£é‡Š**ï¼šè¾“å‡ºæ˜¯è¾“å…¥åŠ ä¸Šæ‰€æœ‰æ®‹å·®å—çš„ç´¯ç§¯ã€‚

---

## ğŸ”¬ æ¢¯åº¦æµåˆ†æ

### 1. åå‘ä¼ æ’­

**æŸå¤±å‡½æ•°**ï¼š$\mathcal{L}(\mathbf{h}^{(L)})$

**æ¢¯åº¦ä¼ æ’­**ï¼š

$$
\frac{\partial \mathcal{L}}{\partial \mathbf{h}^{(l-1)}} = \frac{\partial \mathcal{L}}{\partial \mathbf{h}^{(l)}} \cdot \frac{\partial \mathbf{h}^{(l)}}{\partial \mathbf{h}^{(l-1)}}
$$

**æ®‹å·®å—çš„æ¢¯åº¦**ï¼š

$$
\frac{\partial \mathbf{h}^{(l)}}{\partial \mathbf{h}^{(l-1)}} = I + \frac{\partial F(\mathbf{h}^{(l-1)})}{\partial \mathbf{h}^{(l-1)}}
$$

**å…³é”®**ï¼šæ’ç­‰é¡¹ $I$ ä¿è¯æ¢¯åº¦è‡³å°‘æœ‰ä¸€æ¡ç›´æ¥é€šè·¯ï¼

---

### 2. æ¢¯åº¦æ¶ˆå¤±ç¼“è§£

**å®šç† 2.1 (æ¢¯åº¦ä¼ æ’­)**:

å¯¹äºLå±‚ResNetï¼š

$$
\frac{\partial \mathcal{L}}{\partial \mathbf{h}^{(0)}} = \frac{\partial \mathcal{L}}{\partial \mathbf{h}^{(L)}} \prod_{l=1}^{L} \left(I + \frac{\partial F^{(l)}}{\partial \mathbf{h}^{(l-1)}}\right)
$$

**å±•å¼€**ï¼š

$$
\frac{\partial \mathcal{L}}{\partial \mathbf{h}^{(0)}} = \frac{\partial \mathcal{L}}{\partial \mathbf{h}^{(L)}} \left(I + \sum_{l=1}^{L} \frac{\partial F^{(l)}}{\partial \mathbf{h}^{(l-1)}} + \text{é«˜é˜¶é¡¹}\right)
$$

**å…³é”®æ´å¯Ÿ**ï¼š

- è‡³å°‘æœ‰æ’ç­‰è·¯å¾„ï¼š$\frac{\partial \mathcal{L}}{\partial \mathbf{h}^{(L)}}$ ç›´æ¥ä¼ åˆ° $\mathbf{h}^{(0)}$
- å³ä½¿ $F$ çš„æ¢¯åº¦å¾ˆå°ï¼Œæ€»æ¢¯åº¦ä¹Ÿä¸ä¼šæ¶ˆå¤±

---

### 3. æ¢¯åº¦çˆ†ç‚¸æ§åˆ¶

**é—®é¢˜**ï¼š$\frac{\partial F}{\partial \mathbf{h}}$ å¯èƒ½å¾ˆå¤§ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼š

1. **Batch Normalization**ï¼šå½’ä¸€åŒ–æ¿€æ´»
2. **æƒé‡åˆå§‹åŒ–**ï¼šHeåˆå§‹åŒ–
3. **æ¢¯åº¦è£å‰ª**ï¼šé™åˆ¶æ¢¯åº¦èŒƒæ•°

**å®è·µ**ï¼šResNeté€šå¸¸ä¸ä¼šæ¢¯åº¦çˆ†ç‚¸ï¼ˆBN + è‰¯å¥½åˆå§‹åŒ–ï¼‰ã€‚

---

## ğŸ’» å‰å‘ä¼ æ’­ä¸åå‘ä¼ æ’­

### 1. å‰å‘ä¼ æ’­

**æ ‡å‡†æ®‹å·®å—**ï¼š

```text
è¾“å…¥: x
  â†“
Conv1 + BN + ReLU
  â†“
Conv2 + BN
  â†“
åŠ ä¸Šx (è·³è·ƒè¿æ¥)
  â†“
ReLU
  â†“
è¾“å‡º: y
```

**æ•°å­¦**ï¼š

$$
\begin{align}
\mathbf{a}_1 &= \text{ReLU}(\text{BN}(W_1 \mathbf{x})) \\
\mathbf{a}_2 &= \text{BN}(W_2 \mathbf{a}_1) \\
\mathbf{y} &= \text{ReLU}(\mathbf{a}_2 + \mathbf{x})
\end{align}
$$

---

### 2. åå‘ä¼ æ’­æ¨å¯¼

**æŸå¤±**ï¼š$\mathcal{L}$

**è¾“å‡ºæ¢¯åº¦**ï¼š$\frac{\partial \mathcal{L}}{\partial \mathbf{y}}$

**æ­¥éª¤1**ï¼šReLUæ¢¯åº¦

$$
\frac{\partial \mathcal{L}}{\partial (\mathbf{a}_2 + \mathbf{x})} = \frac{\partial \mathcal{L}}{\partial \mathbf{y}} \odot \mathbb{1}[\mathbf{a}_2 + \mathbf{x} > 0]
$$

**æ­¥éª¤2**ï¼šåŠ æ³•æ¢¯åº¦ï¼ˆå…³é”®ï¼ï¼‰

$$
\begin{align}
\frac{\partial \mathcal{L}}{\partial \mathbf{a}_2} &= \frac{\partial \mathcal{L}}{\partial (\mathbf{a}_2 + \mathbf{x})} \\
\frac{\partial \mathcal{L}}{\partial \mathbf{x}} &= \frac{\partial \mathcal{L}}{\partial (\mathbf{a}_2 + \mathbf{x})} \quad \text{(ç›´æ¥é€šè·¯ï¼)}
\end{align}
$$

**æ­¥éª¤3**ï¼šç»§ç»­åå‘ä¼ æ’­é€šè¿‡ $W_2, W_1$

$$
\frac{\partial \mathcal{L}}{\partial W_2} = \frac{\partial \mathcal{L}}{\partial \mathbf{a}_2} \cdot \mathbf{a}_1^T
$$

---

## ğŸ¨ Pythonå®ç°

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class BasicBlock(nn.Module):
    """åŸºæœ¬æ®‹å·®å— (ç”¨äºResNet-18/34)"""
    expansion = 1
    
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        
        # ä¸»è·¯å¾„
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, 
                               stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,
                               stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        
        # è·³è·ƒè¿æ¥ (å¦‚æœç»´åº¦ä¸åŒ¹é…ï¼Œéœ€è¦æŠ•å½±)
        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1,
                          stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )
    
    def forward(self, x):
        # ä¸»è·¯å¾„
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        
        # æ®‹å·®è¿æ¥
        out += self.shortcut(x)
        out = F.relu(out)
        
        return out


class Bottleneck(nn.Module):
    """ç“¶é¢ˆæ®‹å·®å— (ç”¨äºResNet-50/101/152)"""
    expansion = 4
    
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        
        # 1x1å·ç§¯é™ç»´
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        
        # 3x3å·ç§¯
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,
                               stride=stride, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        
        # 1x1å·ç§¯å‡ç»´
        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion,
                               kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)
        
        # è·³è·ƒè¿æ¥
        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels * self.expansion:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels * self.expansion,
                          kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels * self.expansion)
            )
    
    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = F.relu(self.bn2(self.conv2(out)))
        out = self.bn3(self.conv3(out))
        
        out += self.shortcut(x)
        out = F.relu(out)
        
        return out


class ResNet(nn.Module):
    """ResNetæ¶æ„"""
    def __init__(self, block, num_blocks, num_classes=10):
        super().__init__()
        self.in_channels = 64
        
        # åˆå§‹å·ç§¯
        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        
        # æ®‹å·®å±‚
        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)
        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)
        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)
        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)
        
        # åˆ†ç±»å¤´
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512 * block.expansion, num_classes)
    
    def _make_layer(self, block, out_channels, num_blocks, stride):
        strides = [stride] + [1] * (num_blocks - 1)
        layers = []
        for stride in strides:
            layers.append(block(self.in_channels, out_channels, stride))
            self.in_channels = out_channels * block.expansion
        return nn.Sequential(*layers)
    
    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.maxpool(out)
        
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        out = self.layer4(out)
        
        out = self.avgpool(out)
        out = torch.flatten(out, 1)
        out = self.fc(out)
        
        return out


def ResNet18(num_classes=10):
    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes)

def ResNet34(num_classes=10):
    return ResNet(BasicBlock, [3, 4, 6, 3], num_classes)

def ResNet50(num_classes=10):
    return ResNet(Bottleneck, [3, 4, 6, 3], num_classes)

def ResNet101(num_classes=10):
    return ResNet(Bottleneck, [3, 4, 23, 3], num_classes)

def ResNet152(num_classes=10):
    return ResNet(Bottleneck, [3, 8, 36, 3], num_classes)


# ç¤ºä¾‹ä½¿ç”¨
if __name__ == "__main__":
    model = ResNet18(num_classes=10)
    x = torch.randn(2, 3, 224, 224)
    y = model(x)
    print(f"Input shape: {x.shape}")
    print(f"Output shape: {y.shape}")
    print(f"Total parameters: {sum(p.numel() for p in model.parameters()):,}")
```

---

## ğŸ“š ç†è®ºæ·±åŒ–

### 1. é›†æˆå­¦ä¹ è§†è§’

**å®šç† 1.1 (ResNet as Ensemble, Veit et al. 2016)**:

ResNetå¯ä»¥çœ‹ä½œæŒ‡æ•°çº§æ•°é‡çš„æµ…å±‚ç½‘ç»œçš„é›†æˆã€‚

**è¯æ˜æ€è·¯**ï¼š

- æ¯ä¸ªæ®‹å·®å—æœ‰ä¸¤æ¡è·¯å¾„ï¼šæ’ç­‰ + æ®‹å·®
- $L$ å±‚ResNetæœ‰ $2^L$ æ¡è·¯å¾„
- ä¸åŒè·¯å¾„é•¿åº¦ä¸åŒï¼ˆç±»ä¼¼ä¸åŒæ·±åº¦çš„ç½‘ç»œï¼‰

**ç›´è§‰**ï¼š

```text
x â†’ [+F1] â†’ [+F2] â†’ [+F3] â†’ y

å±•å¼€ä¸º:
x â†’ y  (é•¿åº¦0)
x â†’ F1 â†’ y  (é•¿åº¦1)
x â†’ F2 â†’ y  (é•¿åº¦1)
x â†’ F1 â†’ F2 â†’ y  (é•¿åº¦2)
...
```

---

### 2. ä¼˜åŒ–æ™¯è§‚

**å®šç† 2.1 (Loss Surface, Li et al. 2018)**:

ResNetçš„æŸå¤±æ›²é¢æ¯”æ™®é€šç½‘ç»œæ›´å¹³æ»‘ã€‚

**è¯æ˜è¦ç‚¹**ï¼š

- è·³è·ƒè¿æ¥å‡å°‘äº†æŸå¤±æ›²é¢çš„éå‡¸æ€§
- æ’ç­‰æ˜ å°„æä¾›äº†"å®‰å…¨è·¯å¾„"
- æ¢¯åº¦Lipschitzå¸¸æ•°æ›´å°

**å®è·µæ„ä¹‰**ï¼š

- æ›´å®¹æ˜“ä¼˜åŒ–
- å¯¹å­¦ä¹ ç‡ä¸æ•æ„Ÿ
- æ›´å¥½çš„æ³›åŒ–

---

### 3. è¡¨ç¤ºèƒ½åŠ›

**å®šç† 3.1 (Expressiveness)**:

ResNetçš„è¡¨ç¤ºèƒ½åŠ›è‡³å°‘ä¸æ™®é€šç½‘ç»œç›¸åŒã€‚

**è¯æ˜**ï¼š

- å¦‚æœ $F(x) = H(x) - x$ï¼Œåˆ™ $H(x) = F(x) + x$
- ResNetå¯ä»¥è¡¨ç¤ºä»»ä½•æ™®é€šç½‘ç»œï¼ˆä»¤ $F$ å­¦ä¹  $H - x$ï¼‰
- åä¹‹ä¸ä¸€å®šæˆç«‹

---

## ğŸ”§ ResNetå˜ä½“

### 1. Pre-Activation ResNet

**æ ¸å¿ƒæ”¹è¿›**ï¼šæ¿€æ´»å‡½æ•°æ”¾åœ¨æ®‹å·®å‡½æ•°ä¹‹å‰ã€‚

**ç»“æ„**ï¼š

```text
x â†’ BN â†’ ReLU â†’ Conv â†’ BN â†’ ReLU â†’ Conv â†’ (+x) â†’ y
```

**ä¼˜åŠ¿**ï¼š

- æ›´çº¯ç²¹çš„æ’ç­‰æ˜ å°„
- æ¢¯åº¦æµæ›´ç•…é€š
- è®­ç»ƒæ›´ç¨³å®š

---

### 2. Wide ResNet

**æ ¸å¿ƒæ”¹è¿›**ï¼šå¢åŠ å®½åº¦è€Œéæ·±åº¦ã€‚

**å‚æ•°**ï¼š

- å®½åº¦å› å­ $k$
- æ¯å±‚é€šé“æ•° $\times k$

**ä¼˜åŠ¿**ï¼š

- æ›´å¥½çš„å¹¶è¡Œæ€§
- æ›´å°‘çš„å±‚æ•°
- ç›¸ä¼¼æˆ–æ›´å¥½çš„æ€§èƒ½

---

### 3. ResNeXt

**æ ¸å¿ƒæ”¹è¿›**ï¼šå¼•å…¥"åŸºæ•°"ï¼ˆcardinalityï¼‰ã€‚

**ç»“æ„**ï¼š

- å¤šä¸ªå¹¶è¡Œçš„æ®‹å·®è·¯å¾„
- ç±»ä¼¼Inceptionçš„åˆ†ç»„å·ç§¯

**å…¬å¼**ï¼š

$$
y = x + \sum_{i=1}^{C} \mathcal{T}_i(x)
$$

å…¶ä¸­ $C$ æ˜¯åŸºæ•°ï¼Œ$\mathcal{T}_i$ æ˜¯ç¬¬ $i$ ä¸ªå˜æ¢ã€‚

---

## ğŸ“ ç›¸å…³è¯¾ç¨‹

| å¤§å­¦ | è¯¾ç¨‹ |
|------|------|
| **Stanford** | CS231n Convolutional Neural Networks |
| **MIT** | 6.S191 Introduction to Deep Learning |
| **UC Berkeley** | CS182 Deep Learning |
| **CMU** | 11-785 Introduction to Deep Learning |

---

## ğŸ“– å‚è€ƒæ–‡çŒ®

1. **He et al. (2016)**. "Deep Residual Learning for Image Recognition". *CVPR*.

2. **He et al. (2016)**. "Identity Mappings in Deep Residual Networks". *ECCV*.

3. **Veit et al. (2016)**. "Residual Networks Behave Like Ensembles of Relatively Shallow Networks". *NeurIPS*.

4. **Li et al. (2018)**. "Visualizing the Loss Landscape of Neural Nets". *NeurIPS*.

5. **Zagoruyko & Komodakis (2016)**. "Wide Residual Networks". *BMVC*.

6. **Xie et al. (2017)**. "Aggregated Residual Transformations for Deep Neural Networks". *CVPR*.

---

*æœ€åæ›´æ–°ï¼š2025å¹´10æœˆ*-
