# æ³¨æ„åŠ›æœºåˆ¶ (Attention Mechanism) æ•°å­¦åŸç†

> **Attention Mechanism: Mathematics and Theory**
>
> Transformerä¸ç°ä»£LLMçš„æ ¸å¿ƒæŠ€æœ¯

---

## ç›®å½•

- [æ³¨æ„åŠ›æœºåˆ¶ (Attention Mechanism) æ•°å­¦åŸç†](#æ³¨æ„åŠ›æœºåˆ¶-attention-mechanism-æ•°å­¦åŸç†)
  - [ç›®å½•](#ç›®å½•)
  - [ğŸ“‹ æ ¸å¿ƒæ€æƒ³](#-æ ¸å¿ƒæ€æƒ³)
  - [ğŸ¯ é—®é¢˜åŠ¨æœº](#-é—®é¢˜åŠ¨æœº)
    - [1. åºåˆ—å»ºæ¨¡çš„æŒ‘æˆ˜](#1-åºåˆ—å»ºæ¨¡çš„æŒ‘æˆ˜)
    - [2. æ³¨æ„åŠ›çš„ç›´è§‰](#2-æ³¨æ„åŠ›çš„ç›´è§‰)
  - [ğŸ“Š Scaled Dot-Product Attention](#-scaled-dot-product-attention)
    - [1. æ•°å­¦å®šä¹‰](#1-æ•°å­¦å®šä¹‰)
    - [2. ç¼©æ”¾å› å­çš„ä½œç”¨](#2-ç¼©æ”¾å› å­çš„ä½œç”¨)
    - [3. Softmaxçš„ä½œç”¨](#3-softmaxçš„ä½œç”¨)
  - [ğŸ”¬ Multi-Head Attention](#-multi-head-attention)
    - [1. æ ¸å¿ƒæ€æƒ³](#1-æ ¸å¿ƒæ€æƒ³)
    - [2. æ•°å­¦å½¢å¼åŒ–](#2-æ•°å­¦å½¢å¼åŒ–)
    - [3. ä¸ºä»€ä¹ˆå¤šå¤´æœ‰æ•ˆ](#3-ä¸ºä»€ä¹ˆå¤šå¤´æœ‰æ•ˆ)
  - [ğŸ’» Self-Attention vs Cross-Attention](#-self-attention-vs-cross-attention)
    - [1. Self-Attention](#1-self-attention)
    - [2. Cross-Attention](#2-cross-attention)
    - [3. åº”ç”¨åœºæ™¯](#3-åº”ç”¨åœºæ™¯)
  - [ğŸ¨ Pythonå®ç°](#-pythonå®ç°)
  - [ğŸ“š ç†è®ºåˆ†æ](#-ç†è®ºåˆ†æ)
    - [1. æ³¨æ„åŠ›çš„è¡¨ç¤ºèƒ½åŠ›](#1-æ³¨æ„åŠ›çš„è¡¨ç¤ºèƒ½åŠ›)
    - [2. è®¡ç®—å¤æ‚åº¦](#2-è®¡ç®—å¤æ‚åº¦)
    - [3. é•¿åºåˆ—é—®é¢˜](#3-é•¿åºåˆ—é—®é¢˜)
  - [ğŸ”§ æ³¨æ„åŠ›å˜ä½“](#-æ³¨æ„åŠ›å˜ä½“)
    - [1. Sparse Attention](#1-sparse-attention)
    - [2. Linear Attention](#2-linear-attention)
    - [3. Flash Attention](#3-flash-attention)
  - [ğŸ“ ç›¸å…³è¯¾ç¨‹](#-ç›¸å…³è¯¾ç¨‹)
  - [ğŸ“– å‚è€ƒæ–‡çŒ®](#-å‚è€ƒæ–‡çŒ®)

---

## ğŸ“‹ æ ¸å¿ƒæ€æƒ³

**æ³¨æ„åŠ›æœºåˆ¶**å…è®¸æ¨¡å‹**åŠ¨æ€å…³æ³¨**è¾“å…¥çš„ä¸åŒéƒ¨åˆ†ã€‚

**æ ¸å¿ƒæµç¨‹**ï¼š

```text
Query (æŸ¥è¯¢) + Key (é”®) + Value (å€¼)
        â†“
  è®¡ç®—ç›¸ä¼¼åº¦ (QÂ·K^T)
        â†“
   å½’ä¸€åŒ– (Softmax)
        â†“
  åŠ æƒæ±‚å’Œ (AttentionÂ·V)
        â†“
      è¾“å‡º
```

**å…³é”®ä¼˜åŠ¿**ï¼š

- æ•è·é•¿è·ç¦»ä¾èµ–
- å¹¶è¡Œè®¡ç®—ï¼ˆç›¸æ¯”RNNï¼‰
- å¯è§£é‡Šæ€§ï¼ˆæ³¨æ„åŠ›æƒé‡ï¼‰

---

## ğŸ¯ é—®é¢˜åŠ¨æœº

### 1. åºåˆ—å»ºæ¨¡çš„æŒ‘æˆ˜

**RNNçš„é—®é¢˜**ï¼š

- **é¡ºåºä¾èµ–**ï¼šå¿…é¡»é€æ­¥å¤„ç†ï¼Œæ— æ³•å¹¶è¡Œ
- **é•¿è·ç¦»ä¾èµ–**ï¼šæ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸
- **å›ºå®šå®¹é‡**ï¼šéšçŠ¶æ€ç»´åº¦é™åˆ¶ä¿¡æ¯é‡

**ç¤ºä¾‹**ï¼š

```text
"The cat, which was very hungry, ate the fish."
```

è¦ç†è§£"ate"ï¼Œéœ€è¦å…³æ³¨"cat"ï¼ˆä¸»è¯­ï¼‰ï¼Œä½†ä¸­é—´éš”äº†å¾ˆå¤šè¯ã€‚

---

### 2. æ³¨æ„åŠ›çš„ç›´è§‰

**äººç±»é˜…è¯»**ï¼š

- ä¸æ˜¯å‡åŒ€å…³æ³¨æ‰€æœ‰è¯
- æ ¹æ®ä»»åŠ¡åŠ¨æ€è°ƒæ•´æ³¨æ„åŠ›
- å¯ä»¥"è·³è·ƒ"å…³æ³¨ç›¸å…³ä¿¡æ¯

**æ³¨æ„åŠ›æœºåˆ¶**ï¼š

- ä¸ºæ¯ä¸ªè¾“å‡ºä½ç½®è®¡ç®—å¯¹æ‰€æœ‰è¾“å…¥çš„æ³¨æ„åŠ›æƒé‡
- æƒé‡åæ˜ ç›¸å…³æ€§
- è¾“å‡ºæ˜¯è¾“å…¥çš„åŠ æƒå’Œ

---

## ğŸ“Š Scaled Dot-Product Attention

### 1. æ•°å­¦å®šä¹‰

**å®šä¹‰ 1.1 (Scaled Dot-Product Attention)**:

ç»™å®šæŸ¥è¯¢ $Q \in \mathbb{R}^{n \times d_k}$ï¼Œé”® $K \in \mathbb{R}^{m \times d_k}$ï¼Œå€¼ $V \in \mathbb{R}^{m \times d_v}$ï¼š

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V
$$

**ç¬¦å·è¯´æ˜**ï¼š

- $n$ï¼šæŸ¥è¯¢åºåˆ—é•¿åº¦
- $m$ï¼šé”®/å€¼åºåˆ—é•¿åº¦
- $d_k$ï¼šé”®/æŸ¥è¯¢ç»´åº¦
- $d_v$ï¼šå€¼ç»´åº¦

**æ­¥éª¤åˆ†è§£**ï¼š

1. **è®¡ç®—ç›¸ä¼¼åº¦**ï¼š$S = QK^T \in \mathbb{R}^{n \times m}$
2. **ç¼©æ”¾**ï¼š$S' = S / \sqrt{d_k}$
3. **å½’ä¸€åŒ–**ï¼š$A = \text{softmax}(S') \in \mathbb{R}^{n \times m}$
4. **åŠ æƒæ±‚å’Œ**ï¼š$\text{Output} = AV \in \mathbb{R}^{n \times d_v}$

---

### 2. ç¼©æ”¾å› å­çš„ä½œç”¨

**ä¸ºä»€ä¹ˆé™¤ä»¥ $\sqrt{d_k}$ï¼Ÿ**

**å®šç† 2.1 (ç¼©æ”¾å¿…è¦æ€§, Vaswani et al. 2017)**:

å‡è®¾ $Q, K$ çš„å…ƒç´ ç‹¬ç«‹åŒåˆ†å¸ƒï¼Œå‡å€¼0ï¼Œæ–¹å·®1ï¼Œåˆ™ï¼š

$$
\mathbb{E}[QK^T] = 0, \quad \text{Var}[QK^T] = d_k
$$

**é—®é¢˜**ï¼šå½“ $d_k$ å¾ˆå¤§æ—¶ï¼Œç‚¹ç§¯æ–¹å·®å¾ˆå¤§ã€‚

**åæœ**ï¼š

- Softmaxè¾“å…¥è¿›å…¥é¥±å’ŒåŒº
- æ¢¯åº¦æ¥è¿‘0
- è®­ç»ƒå›°éš¾

**è§£å†³æ–¹æ¡ˆ**ï¼šé™¤ä»¥ $\sqrt{d_k}$ï¼Œä½¿æ–¹å·®å½’ä¸€åŒ–ä¸º1ã€‚

$$
\text{Var}\left[\frac{QK^T}{\sqrt{d_k}}\right] = 1
$$

---

### 3. Softmaxçš„ä½œç”¨

**Softmaxå®šä¹‰**ï¼š

$$
\text{softmax}(z_i) = \frac{\exp(z_i)}{\sum_{j=1}^{m} \exp(z_j)}
$$

**æ€§è´¨**ï¼š

1. **å½’ä¸€åŒ–**ï¼š$\sum_{j=1}^{m} A_{ij} = 1$ï¼ˆæ¯è¡Œå’Œä¸º1ï¼‰
2. **éè´Ÿ**ï¼š$A_{ij} \geq 0$
3. **å¯å¾®**ï¼šæ¢¯åº¦å­˜åœ¨

**æ„ä¹‰**ï¼š

- å°†ç›¸ä¼¼åº¦è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ
- é«˜ç›¸ä¼¼åº¦ä½ç½®è·å¾—æ›´å¤§æƒé‡
- ä½ç›¸ä¼¼åº¦ä½ç½®æƒé‡æ¥è¿‘0

---

## ğŸ”¬ Multi-Head Attention

### 1. æ ¸å¿ƒæ€æƒ³

**å•å¤´æ³¨æ„åŠ›çš„å±€é™**ï¼š

- åªèƒ½å­¦ä¹ ä¸€ç§ç›¸ä¼¼åº¦åº¦é‡
- éš¾ä»¥åŒæ—¶æ•è·å¤šç§å…³ç³»

**å¤šå¤´æ³¨æ„åŠ›**ï¼š

- å¹¶è¡Œè¿è¡Œå¤šä¸ªæ³¨æ„åŠ›"å¤´"
- æ¯ä¸ªå¤´å­¦ä¹ ä¸åŒçš„è¡¨ç¤ºå­ç©ºé—´
- æ‹¼æ¥æ‰€æœ‰å¤´çš„è¾“å‡º

---

### 2. æ•°å­¦å½¢å¼åŒ–

**å®šä¹‰ 2.1 (Multi-Head Attention)**:

$$
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h) W^O
$$

å…¶ä¸­æ¯ä¸ªå¤´ï¼š

$$
\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
$$

**å‚æ•°**ï¼š

- $W_i^Q \in \mathbb{R}^{d_{\text{model}} \times d_k}$ï¼šæŸ¥è¯¢æŠ•å½±
- $W_i^K \in \mathbb{R}^{d_{\text{model}} \times d_k}$ï¼šé”®æŠ•å½±
- $W_i^V \in \mathbb{R}^{d_{\text{model}} \times d_v}$ï¼šå€¼æŠ•å½±
- $W^O \in \mathbb{R}^{hd_v \times d_{\text{model}}}$ï¼šè¾“å‡ºæŠ•å½±

**å…¸å‹è®¾ç½®**ï¼š

- $h = 8$ï¼ˆå¤´æ•°ï¼‰
- $d_k = d_v = d_{\text{model}} / h = 64$ï¼ˆTransformerä¸­ $d_{\text{model}} = 512$ï¼‰

---

### 3. ä¸ºä»€ä¹ˆå¤šå¤´æœ‰æ•ˆ

**å®šç† 3.1 (è¡¨ç¤ºèƒ½åŠ›)**:

å¤šå¤´æ³¨æ„åŠ›å¯ä»¥åŒæ—¶å…³æ³¨ä¸åŒçš„è¡¨ç¤ºå­ç©ºé—´ã€‚

**ç›´è§‰**ï¼š

- **Head 1**ï¼šå¯èƒ½å…³æ³¨å¥æ³•å…³ç³»ï¼ˆä¸»è°“å®¾ï¼‰
- **Head 2**ï¼šå¯èƒ½å…³æ³¨è¯­ä¹‰å…³ç³»ï¼ˆåŒä¹‰è¯ï¼‰
- **Head 3**ï¼šå¯èƒ½å…³æ³¨ä½ç½®å…³ç³»ï¼ˆç›¸é‚»è¯ï¼‰

**å®éªŒè¯æ®**ï¼š

- ä¸åŒå¤´å­¦ä¹ åˆ°ä¸åŒçš„æ³¨æ„åŠ›æ¨¡å¼
- æŸäº›å¤´ä¸“æ³¨äºå±€éƒ¨ï¼ŒæŸäº›å¤´ä¸“æ³¨äºå…¨å±€
- å¤šå¤´ä¼˜äºå•å¤´ï¼ˆå®éªŒéªŒè¯ï¼‰

---

## ğŸ’» Self-Attention vs Cross-Attention

### 1. Self-Attention

**å®šä¹‰**ï¼š$Q, K, V$ æ¥è‡ªåŒä¸€åºåˆ—ã€‚

$$
\text{SelfAttention}(X) = \text{Attention}(XW^Q, XW^K, XW^V)
$$

**åº”ç”¨**ï¼š

- Transformerç¼–ç å™¨
- BERT
- GPT

**ä½œç”¨**ï¼š

- æ¯ä¸ªä½ç½®å…³æ³¨åºåˆ—ä¸­çš„æ‰€æœ‰ä½ç½®
- æ•è·ä¸Šä¸‹æ–‡ä¿¡æ¯
- å­¦ä¹ è¯ä¹‹é—´çš„å…³ç³»

---

### 2. Cross-Attention

**å®šä¹‰**ï¼š$Q$ æ¥è‡ªä¸€ä¸ªåºåˆ—ï¼Œ$K, V$ æ¥è‡ªå¦ä¸€ä¸ªåºåˆ—ã€‚

$$
\text{CrossAttention}(X, Y) = \text{Attention}(XW^Q, YW^K, YW^V)
$$

**åº”ç”¨**ï¼š

- Transformerè§£ç å™¨
- æœºå™¨ç¿»è¯‘
- å›¾åƒæè¿°ç”Ÿæˆ

**ä½œç”¨**ï¼š

- æŸ¥è¯¢åºåˆ—å…³æ³¨æºåºåˆ—
- å¯¹é½ä¸åŒæ¨¡æ€
- ä¿¡æ¯èåˆ

---

### 3. åº”ç”¨åœºæ™¯

| ç±»å‹ | Qæ¥æº | K/Væ¥æº | åº”ç”¨ |
|------|-------|---------|------|
| **Self-Attention** | åŒä¸€åºåˆ— | åŒä¸€åºåˆ— | BERT, GPT |
| **Cross-Attention** | ç›®æ ‡åºåˆ— | æºåºåˆ— | ç¿»è¯‘, VQA |
| **Masked Self-Attention** | åŒä¸€åºåˆ— | åŒä¸€åºåˆ—ï¼ˆæ©ç ï¼‰ | GPTè§£ç  |

---

## ğŸ¨ Pythonå®ç°

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class ScaledDotProductAttention(nn.Module):
    """Scaled Dot-Product Attention"""
    def __init__(self, dropout=0.1):
        super().__init__()
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, Q, K, V, mask=None):
        """
        Args:
            Q: (batch, n_heads, seq_len_q, d_k)
            K: (batch, n_heads, seq_len_k, d_k)
            V: (batch, n_heads, seq_len_v, d_v)
            mask: (batch, 1, seq_len_q, seq_len_k) or None
        
        Returns:
            output: (batch, n_heads, seq_len_q, d_v)
            attention_weights: (batch, n_heads, seq_len_q, seq_len_k)
        """
        d_k = Q.size(-1)
        
        # 1. è®¡ç®—ç›¸ä¼¼åº¦: (batch, n_heads, seq_len_q, seq_len_k)
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)
        
        # 2. åº”ç”¨æ©ç  (å¯é€‰)
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        # 3. Softmaxå½’ä¸€åŒ–
        attention_weights = F.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)
        
        # 4. åŠ æƒæ±‚å’Œ
        output = torch.matmul(attention_weights, V)
        
        return output, attention_weights


class MultiHeadAttention(nn.Module):
    """Multi-Head Attention"""
    def __init__(self, d_model=512, n_heads=8, dropout=0.1):
        super().__init__()
        assert d_model % n_heads == 0, "d_model must be divisible by n_heads"
        
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        self.d_v = d_model // n_heads
        
        # çº¿æ€§æŠ•å½±å±‚
        self.W_Q = nn.Linear(d_model, d_model)
        self.W_K = nn.Linear(d_model, d_model)
        self.W_V = nn.Linear(d_model, d_model)
        self.W_O = nn.Linear(d_model, d_model)
        
        self.attention = ScaledDotProductAttention(dropout)
        self.dropout = nn.Dropout(dropout)
        self.layer_norm = nn.LayerNorm(d_model)
    
    def forward(self, Q, K, V, mask=None):
        """
        Args:
            Q: (batch, seq_len_q, d_model)
            K: (batch, seq_len_k, d_model)
            V: (batch, seq_len_v, d_model)
            mask: (batch, seq_len_q, seq_len_k) or None
        
        Returns:
            output: (batch, seq_len_q, d_model)
            attention_weights: (batch, n_heads, seq_len_q, seq_len_k)
        """
        batch_size = Q.size(0)
        residual = Q
        
        # 1. çº¿æ€§æŠ•å½±å¹¶åˆ†å‰²æˆå¤šå¤´
        # (batch, seq_len, d_model) -> (batch, seq_len, n_heads, d_k) -> (batch, n_heads, seq_len, d_k)
        Q = self.W_Q(Q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        K = self.W_K(K).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        V = self.W_V(V).view(batch_size, -1, self.n_heads, self.d_v).transpose(1, 2)
        
        # 2. è°ƒæ•´maskç»´åº¦
        if mask is not None:
            mask = mask.unsqueeze(1)  # (batch, 1, seq_len_q, seq_len_k)
        
        # 3. åº”ç”¨æ³¨æ„åŠ›
        output, attention_weights = self.attention(Q, K, V, mask)
        
        # 4. æ‹¼æ¥å¤šå¤´
        # (batch, n_heads, seq_len_q, d_v) -> (batch, seq_len_q, n_heads, d_v) -> (batch, seq_len_q, d_model)
        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)
        
        # 5. è¾“å‡ºæŠ•å½±
        output = self.W_O(output)
        output = self.dropout(output)
        
        # 6. æ®‹å·®è¿æ¥ + Layer Norm
        output = self.layer_norm(output + residual)
        
        return output, attention_weights


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # å‚æ•°
    batch_size = 2
    seq_len = 10
    d_model = 512
    n_heads = 8
    
    # åˆ›å»ºæ¨¡å‹
    mha = MultiHeadAttention(d_model=d_model, n_heads=n_heads)
    
    # è¾“å…¥
    x = torch.randn(batch_size, seq_len, d_model)
    
    # Self-Attention
    output, attn_weights = mha(x, x, x)
    
    print(f"Input shape: {x.shape}")
    print(f"Output shape: {output.shape}")
    print(f"Attention weights shape: {attn_weights.shape}")
    
    # å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡
    import matplotlib.pyplot as plt
    
    # å–ç¬¬ä¸€ä¸ªæ ·æœ¬çš„ç¬¬ä¸€ä¸ªå¤´
    attn = attn_weights[0, 0].detach().numpy()
    
    plt.figure(figsize=(8, 6))
    plt.imshow(attn, cmap='viridis')
    plt.colorbar()
    plt.xlabel('Key Position')
    plt.ylabel('Query Position')
    plt.title('Attention Weights (Head 1)')
    plt.tight_layout()
    # plt.show()


# Cross-Attentionç¤ºä¾‹
class CrossAttentionLayer(nn.Module):
    """Cross-Attentionå±‚ (ç”¨äºTransformerè§£ç å™¨)"""
    def __init__(self, d_model=512, n_heads=8, dropout=0.1):
        super().__init__()
        self.cross_attn = MultiHeadAttention(d_model, n_heads, dropout)
    
    def forward(self, decoder_input, encoder_output, mask=None):
        """
        Args:
            decoder_input: (batch, seq_len_dec, d_model) - Query
            encoder_output: (batch, seq_len_enc, d_model) - Key & Value
            mask: (batch, seq_len_dec, seq_len_enc) or None
        
        Returns:
            output: (batch, seq_len_dec, d_model)
        """
        output, attn_weights = self.cross_attn(
            Q=decoder_input,
            K=encoder_output,
            V=encoder_output,
            mask=mask
        )
        return output, attn_weights


# Masked Self-Attention (ç”¨äºGPT)
def create_causal_mask(seq_len):
    """åˆ›å»ºå› æœæ©ç  (ä¸‹ä¸‰è§’çŸ©é˜µ)"""
    mask = torch.tril(torch.ones(seq_len, seq_len))
    return mask  # (seq_len, seq_len)


# ç¤ºä¾‹ï¼šGPTé£æ ¼çš„Masked Self-Attention
if __name__ == "__main__":
    seq_len = 5
    d_model = 512
    
    mha = MultiHeadAttention(d_model=d_model, n_heads=8)
    x = torch.randn(1, seq_len, d_model)
    
    # åˆ›å»ºå› æœæ©ç 
    causal_mask = create_causal_mask(seq_len).unsqueeze(0)  # (1, seq_len, seq_len)
    
    # Masked Self-Attention
    output, attn_weights = mha(x, x, x, mask=causal_mask)
    
    print(f"\nCausal Mask:\n{causal_mask[0]}")
    print(f"\nAttention weights (with mask):\n{attn_weights[0, 0].detach()}")
```

---

## ğŸ“š ç†è®ºåˆ†æ

### 1. æ³¨æ„åŠ›çš„è¡¨ç¤ºèƒ½åŠ›

**å®šç† 1.1 (Universal Approximation)**:

Transformerå¯ä»¥è¿‘ä¼¼ä»»æ„åºåˆ—åˆ°åºåˆ—çš„å‡½æ•°ï¼ˆåœ¨é€‚å½“æ¡ä»¶ä¸‹ï¼‰ã€‚

**è¯æ˜è¦ç‚¹**ï¼š

- æ³¨æ„åŠ›å¯ä»¥å®ç°ä»»æ„çš„åŠ æƒå¹³å‡
- å¤šå±‚Transformerå¯ä»¥ç»„åˆå¤æ‚æ“ä½œ
- FFNæä¾›éçº¿æ€§å˜æ¢

**å®è·µæ„ä¹‰**ï¼š

- ç†è®ºä¸Šå¯ä»¥å­¦ä¹ ä»»æ„åºåˆ—æ¨¡å¼
- å®é™…å—é™äºæ•°æ®å’Œä¼˜åŒ–

---

### 2. è®¡ç®—å¤æ‚åº¦

**Self-Attentionå¤æ‚åº¦**ï¼š

| æ“ä½œ | å¤æ‚åº¦ |
|------|--------|
| **è®¡ç®— $QK^T$** | $O(n^2 d)$ |
| **Softmax** | $O(n^2)$ |
| **åŠ æƒæ±‚å’Œ** | $O(n^2 d)$ |
| **æ€»è®¡** | $O(n^2 d)$ |

å…¶ä¸­ $n$ æ˜¯åºåˆ—é•¿åº¦ï¼Œ$d$ æ˜¯ç»´åº¦ã€‚

**å¯¹æ¯”RNN**ï¼š

- RNNï¼š$O(nd^2)$ï¼ˆé¡ºåºè®¡ç®—ï¼‰
- Attentionï¼š$O(n^2d)$ï¼ˆå¹¶è¡Œè®¡ç®—ï¼‰

**é—®é¢˜**ï¼šåºåˆ—é•¿åº¦ $n$ å¾ˆå¤§æ—¶ï¼Œ$n^2$ é¡¹æˆä¸ºç“¶é¢ˆã€‚

---

### 3. é•¿åºåˆ—é—®é¢˜

**æŒ‘æˆ˜**ï¼š

- å†…å­˜ï¼šå­˜å‚¨ $n \times n$ æ³¨æ„åŠ›çŸ©é˜µ
- è®¡ç®—ï¼š$O(n^2)$ å¤æ‚åº¦

**ç¤ºä¾‹**ï¼š

- $n = 1024$ï¼š1Mæ³¨æ„åŠ›æƒé‡
- $n = 4096$ï¼š16Mæ³¨æ„åŠ›æƒé‡
- $n = 100k$ï¼š10Bæ³¨æ„åŠ›æƒé‡ï¼ˆä¸å¯è¡Œï¼ï¼‰

**è§£å†³æ–¹æ¡ˆ**ï¼šè§ä¸‹èŠ‚"æ³¨æ„åŠ›å˜ä½“"

---

## ğŸ”§ æ³¨æ„åŠ›å˜ä½“

### 1. Sparse Attention

**æ ¸å¿ƒæ€æƒ³**ï¼šåªè®¡ç®—éƒ¨åˆ†æ³¨æ„åŠ›æƒé‡ã€‚

**Longformer (Beltagy et al., 2020)**ï¼š

- **å±€éƒ¨æ³¨æ„åŠ›**ï¼šæ¯ä¸ªtokenåªå…³æ³¨çª—å£å†…çš„token
- **å…¨å±€æ³¨æ„åŠ›**ï¼šç‰¹æ®Štokenå…³æ³¨æ‰€æœ‰token
- **å¤æ‚åº¦**ï¼š$O(n \cdot w)$ï¼Œå…¶ä¸­ $w$ æ˜¯çª—å£å¤§å°

**BigBird (Zaheer et al., 2020)**ï¼š

- **éšæœºæ³¨æ„åŠ›** + **çª—å£æ³¨æ„åŠ›** + **å…¨å±€æ³¨æ„åŠ›**
- **ç†è®ºä¿è¯**ï¼šä»æ˜¯é€šç”¨è¿‘ä¼¼å™¨

---

### 2. Linear Attention

**æ ¸å¿ƒæ€æƒ³**ï¼šé€šè¿‡æ ¸æŠ€å·§é™ä½å¤æ‚åº¦ã€‚

**Linformer (Wang et al., 2020)**ï¼š

- å°† $K, V$ æŠ•å½±åˆ°ä½ç»´
- å¤æ‚åº¦ï¼š$O(nk)$ï¼Œå…¶ä¸­ $k \ll n$

**Performer (Choromanski et al., 2021)**ï¼š

- ç”¨éšæœºç‰¹å¾è¿‘ä¼¼Softmax
- å¤æ‚åº¦ï¼š$O(nd)$ï¼ˆçº¿æ€§ï¼ï¼‰

**å…¬å¼**ï¼š

$$
\text{Attention}(Q, K, V) \approx \phi(Q) (\phi(K)^T V)
$$

å…¶ä¸­ $\phi$ æ˜¯ç‰¹å¾æ˜ å°„ã€‚

---

### 3. Flash Attention

**æ ¸å¿ƒæ€æƒ³**ï¼šä¼˜åŒ–å†…å­˜è®¿é—®æ¨¡å¼ã€‚

**Flash Attention (Dao et al., 2022)**ï¼š

- ä¸æ˜¾å¼å­˜å‚¨ $n \times n$ æ³¨æ„åŠ›çŸ©é˜µ
- åˆ†å—è®¡ç®—ï¼Œå‡å°‘HBMè®¿é—®
- **åŠ é€Ÿ**ï¼š2-4å€
- **å†…å­˜**ï¼šçº¿æ€§è€ŒéäºŒæ¬¡

**æ„ä¹‰**ï¼š

- å…è®¸æ›´é•¿åºåˆ—
- æ›´é«˜æ•ˆè®­ç»ƒ
- æˆä¸ºæ–°æ ‡å‡†

---

## ğŸ“ ç›¸å…³è¯¾ç¨‹

| å¤§å­¦ | è¯¾ç¨‹ |
|------|------|
| **Stanford** | CS224N Natural Language Processing |
| **Stanford** | CS25 Transformers United |
| **MIT** | 6.S191 Introduction to Deep Learning |
| **CMU** | 11-747 Neural Networks for NLP |

---

## ğŸ“– å‚è€ƒæ–‡çŒ®

1. **Vaswani et al. (2017)**. "Attention Is All You Need". *NeurIPS*.

2. **Bahdanau et al. (2015)**. "Neural Machine Translation by Jointly Learning to Align and Translate". *ICLR*.

3. **Beltagy et al. (2020)**. "Longformer: The Long-Document Transformer". *arXiv*.

4. **Zaheer et al. (2020)**. "Big Bird: Transformers for Longer Sequences". *NeurIPS*.

5. **Choromanski et al. (2021)**. "Rethinking Attention with Performers". *ICLR*.

6. **Dao et al. (2022)**. "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness". *NeurIPS*.

---

*æœ€åæ›´æ–°ï¼š2025å¹´10æœˆ*-
