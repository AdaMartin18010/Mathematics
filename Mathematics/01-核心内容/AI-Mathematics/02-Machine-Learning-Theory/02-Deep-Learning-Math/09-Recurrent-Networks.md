# å¾ªç¯ç¥ç»ç½‘ç»œ (RNN/LSTM) æ•°å­¦åŸç†

> **Recurrent Neural Networks: Mathematics and Theory**
>
> åºåˆ—å»ºæ¨¡çš„æ•°å­¦åŸºç¡€

---

## ç›®å½•

- [å¾ªç¯ç¥ç»ç½‘ç»œ (RNN/LSTM) æ•°å­¦åŸç†](#å¾ªç¯ç¥ç»ç½‘ç»œ-rnnlstm-æ•°å­¦åŸç†)
  - [ç›®å½•](#ç›®å½•)
  - [ğŸ“‹ æ ¸å¿ƒæ€æƒ³](#-æ ¸å¿ƒæ€æƒ³)
  - [ğŸ¯ åºåˆ—å»ºæ¨¡é—®é¢˜](#-åºåˆ—å»ºæ¨¡é—®é¢˜)
    - [1. ä¸ºä»€ä¹ˆéœ€è¦RNN](#1-ä¸ºä»€ä¹ˆéœ€è¦rnn)
    - [2. RNNçš„ä¼˜åŠ¿](#2-rnnçš„ä¼˜åŠ¿)
  - [ğŸ“Š åŸºç¡€RNN](#-åŸºç¡€rnn)
    - [1. æ•°å­¦å®šä¹‰](#1-æ•°å­¦å®šä¹‰)
    - [2. å±•å¼€å›¾](#2-å±•å¼€å›¾)
    - [3. å‰å‘ä¼ æ’­](#3-å‰å‘ä¼ æ’­)
  - [ğŸ”¬ BPTT (åå‘ä¼ æ’­)](#-bptt-åå‘ä¼ æ’­)
    - [1. æ—¶é—´åå‘ä¼ æ’­](#1-æ—¶é—´åå‘ä¼ æ’­)
    - [2. æ¢¯åº¦è®¡ç®—](#2-æ¢¯åº¦è®¡ç®—)
    - [3. æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸](#3-æ¢¯åº¦æ¶ˆå¤±çˆ†ç‚¸)
  - [ğŸ’» LSTM (é•¿çŸ­æœŸè®°å¿†ç½‘ç»œ)](#-lstm-é•¿çŸ­æœŸè®°å¿†ç½‘ç»œ)
    - [1. åŠ¨æœº](#1-åŠ¨æœº)
    - [2. LSTMæ¶æ„](#2-lstmæ¶æ„)
    - [3. é—¨æ§æœºåˆ¶](#3-é—¨æ§æœºåˆ¶)
  - [ğŸ¨ GRU (é—¨æ§å¾ªç¯å•å…ƒ)](#-gru-é—¨æ§å¾ªç¯å•å…ƒ)
    - [1. GRUæ¶æ„](#1-gruæ¶æ„)
    - [2. GRU vs LSTM](#2-gru-vs-lstm)
  - [ğŸ“ åŒå‘RNN](#-åŒå‘rnn)
    - [1. åŠ¨æœº1](#1-åŠ¨æœº1)
    - [2. æ•°å­¦å®šä¹‰](#2-æ•°å­¦å®šä¹‰)
  - [ğŸ”§ æ¢¯åº¦è£å‰ª](#-æ¢¯åº¦è£å‰ª)
    - [1. æ¢¯åº¦çˆ†ç‚¸é—®é¢˜](#1-æ¢¯åº¦çˆ†ç‚¸é—®é¢˜)
    - [2. æ¢¯åº¦è£å‰ªæ–¹æ³•](#2-æ¢¯åº¦è£å‰ªæ–¹æ³•)
  - [ğŸ’¡ Pythonå®ç°](#-pythonå®ç°)
  - [ğŸ“š åº”ç”¨åœºæ™¯](#-åº”ç”¨åœºæ™¯)
    - [1. è¯­è¨€æ¨¡å‹](#1-è¯­è¨€æ¨¡å‹)
    - [2. æœºå™¨ç¿»è¯‘](#2-æœºå™¨ç¿»è¯‘)
    - [3. æ—¶é—´åºåˆ—é¢„æµ‹](#3-æ—¶é—´åºåˆ—é¢„æµ‹)
  - [ğŸ“ ç›¸å…³è¯¾ç¨‹](#-ç›¸å…³è¯¾ç¨‹)
  - [ğŸ“– å‚è€ƒæ–‡çŒ®](#-å‚è€ƒæ–‡çŒ®)

---

## ğŸ“‹ æ ¸å¿ƒæ€æƒ³

**å¾ªç¯ç¥ç»ç½‘ç»œ (RNN)** é€šè¿‡**å¾ªç¯è¿æ¥**å¤„ç†åºåˆ—æ•°æ®ã€‚

**æ ¸å¿ƒåŸç†**ï¼š

```text
è¾“å…¥åºåˆ—: xâ‚, xâ‚‚, xâ‚ƒ, ..., xâ‚œ
    â†“
éšçŠ¶æ€æ›´æ–°: hâ‚œ = f(hâ‚œâ‚‹â‚, xâ‚œ)
    â†“
è¾“å‡º: yâ‚œ = g(hâ‚œ)
```

**å…³é”®æ¦‚å¿µ**ï¼š

- **éšçŠ¶æ€**ï¼šè®°å¿†è¿‡å»ä¿¡æ¯
- **å‚æ•°å…±äº«**ï¼šæ‰€æœ‰æ—¶é—´æ­¥å…±äº«æƒé‡
- **æ—¶é—´å±•å¼€**ï¼šå¾ªç¯å˜ä¸ºå‰é¦ˆ

---

## ğŸ¯ åºåˆ—å»ºæ¨¡é—®é¢˜

### 1. ä¸ºä»€ä¹ˆéœ€è¦RNN

**åºåˆ—æ•°æ®çš„ç‰¹ç‚¹**ï¼š

- **æ—¶é—´ä¾èµ–**ï¼šå½“å‰è¾“å‡ºä¾èµ–å†å²è¾“å…¥
- **å˜é•¿è¾“å…¥**ï¼šåºåˆ—é•¿åº¦ä¸å›ºå®š
- **é¡ºåºé‡è¦**ï¼šæ”¹å˜é¡ºåºæ”¹å˜å«ä¹‰

**ç¤ºä¾‹**ï¼š

```text
"The cat sat on the mat"
vs
"The mat sat on the cat"
```

**ä¼ ç»Ÿç¥ç»ç½‘ç»œçš„é—®é¢˜**ï¼š

- å›ºå®šè¾“å…¥é•¿åº¦
- æ— æ³•æ•è·æ—¶é—´ä¾èµ–
- å‚æ•°æ•°é‡éšåºåˆ—é•¿åº¦å¢é•¿

---

### 2. RNNçš„ä¼˜åŠ¿

**å‚æ•°å…±äº«**ï¼š

- æ‰€æœ‰æ—¶é—´æ­¥å…±äº«æƒé‡
- å‚æ•°æ•°é‡ä¸åºåˆ—é•¿åº¦æ— å…³

**è®°å¿†èƒ½åŠ›**ï¼š

- éšçŠ¶æ€å­˜å‚¨å†å²ä¿¡æ¯
- ç†è®ºä¸Šå¯ä»¥æ•è·ä»»æ„é•¿åº¦ä¾èµ–

**çµæ´»æ€§**ï¼š

- å¤šå¯¹ä¸€ï¼šæƒ…æ„Ÿåˆ†æ
- ä¸€å¯¹å¤šï¼šå›¾åƒæè¿°
- å¤šå¯¹å¤šï¼šæœºå™¨ç¿»è¯‘

---

## ğŸ“Š åŸºç¡€RNN

### 1. æ•°å­¦å®šä¹‰

**å®šä¹‰ 1.1 (Vanilla RNN)**:

ç»™å®šè¾“å…¥åºåˆ— $\mathbf{x} = (x_1, x_2, \ldots, x_T)$ï¼š

$$
\mathbf{h}_t = \tanh(W_{hh} \mathbf{h}_{t-1} + W_{xh} \mathbf{x}_t + \mathbf{b}_h)
$$

$$
\mathbf{y}_t = W_{hy} \mathbf{h}_t + \mathbf{b}_y
$$

**å‚æ•°**ï¼š

- $W_{hh} \in \mathbb{R}^{d_h \times d_h}$ï¼šéšçŠ¶æ€åˆ°éšçŠ¶æ€
- $W_{xh} \in \mathbb{R}^{d_h \times d_x}$ï¼šè¾“å…¥åˆ°éšçŠ¶æ€
- $W_{hy} \in \mathbb{R}^{d_y \times d_h}$ï¼šéšçŠ¶æ€åˆ°è¾“å‡º
- $\mathbf{h}_0$ï¼šåˆå§‹éšçŠ¶æ€ï¼ˆé€šå¸¸ä¸º0ï¼‰

---

### 2. å±•å¼€å›¾

**å¾ªç¯è§†å›¾**ï¼š

```text
    â”Œâ”€â”€â”€â”
    â”‚ h â”‚â†â”€â”
    â””â”€â”€â”€â”˜  â”‚
      â†‘    â”‚
      x    â””â”€ (å¾ªç¯è¿æ¥)
```

**å±•å¼€è§†å›¾**ï¼š

```text
hâ‚€ â†’ hâ‚ â†’ hâ‚‚ â†’ hâ‚ƒ â†’ ... â†’ hâ‚œ
     â†‘    â†‘    â†‘         â†‘
     xâ‚   xâ‚‚   xâ‚ƒ        xâ‚œ
     â†“    â†“    â†“         â†“
     yâ‚   yâ‚‚   yâ‚ƒ        yâ‚œ
```

**å…³é”®**ï¼šå±•å¼€åå˜ä¸ºæ·±åº¦å‰é¦ˆç½‘ç»œï¼

---

### 3. å‰å‘ä¼ æ’­

**ç®—æ³• 1.1 (RNN Forward Pass)**:

**è¾“å…¥**ï¼šåºåˆ— $\mathbf{x} = (x_1, \ldots, x_T)$

**æ­¥éª¤**ï¼š

1. åˆå§‹åŒ– $\mathbf{h}_0 = \mathbf{0}$
2. **for** $t = 1$ **to** $T$:
   - $\mathbf{h}_t = \tanh(W_{hh} \mathbf{h}_{t-1} + W_{xh} \mathbf{x}_t + \mathbf{b}_h)$
   - $\mathbf{y}_t = W_{hy} \mathbf{h}_t + \mathbf{b}_y$
3. **return** $(\mathbf{h}_1, \ldots, \mathbf{h}_T)$, $(\mathbf{y}_1, \ldots, \mathbf{y}_T)$

**å¤æ‚åº¦**ï¼š$O(T \cdot d_h^2)$

---

## ğŸ”¬ BPTT (åå‘ä¼ æ’­)

### 1. æ—¶é—´åå‘ä¼ æ’­

**BPTT (Backpropagation Through Time)**ï¼š

- å°†å±•å¼€çš„RNNè§†ä¸ºæ·±åº¦ç½‘ç»œ
- ä» $t = T$ åå‘ä¼ æ’­åˆ° $t = 1$
- ç´¯ç§¯æ‰€æœ‰æ—¶é—´æ­¥çš„æ¢¯åº¦

**å…³é”®**ï¼šæ¢¯åº¦éœ€è¦é€šè¿‡æ—¶é—´å›ä¼ ï¼

---

### 2. æ¢¯åº¦è®¡ç®—

**æŸå¤±å‡½æ•°**ï¼š

$$
\mathcal{L} = \sum_{t=1}^{T} \mathcal{L}_t(\mathbf{y}_t, \hat{\mathbf{y}}_t)
$$

**éšçŠ¶æ€æ¢¯åº¦**ï¼š

$$
\frac{\partial \mathcal{L}}{\partial \mathbf{h}_t} = \frac{\partial \mathcal{L}_t}{\partial \mathbf{h}_t} + \frac{\partial \mathcal{L}}{\partial \mathbf{h}_{t+1}} \frac{\partial \mathbf{h}_{t+1}}{\partial \mathbf{h}_t}
$$

**æƒé‡æ¢¯åº¦**ï¼š

$$
\frac{\partial \mathcal{L}}{\partial W_{hh}} = \sum_{t=1}^{T} \frac{\partial \mathcal{L}}{\partial \mathbf{h}_t} \frac{\partial \mathbf{h}_t}{\partial W_{hh}}
$$

---

### 3. æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸

**å®šç† 3.1 (æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸)**:

è€ƒè™‘ $\frac{\partial \mathbf{h}_t}{\partial \mathbf{h}_k}$ (å…¶ä¸­ $t > k$)ï¼š

$$
\frac{\partial \mathbf{h}_t}{\partial \mathbf{h}_k} = \prod_{i=k+1}^{t} \frac{\partial \mathbf{h}_i}{\partial \mathbf{h}_{i-1}} = \prod_{i=k+1}^{t} W_{hh}^T \text{diag}(\tanh'(\cdot))
$$

**åˆ†æ**ï¼š

- å¦‚æœ $\|W_{hh}\| < 1$ï¼šæ¢¯åº¦æŒ‡æ•°è¡°å‡ â†’ **æ¢¯åº¦æ¶ˆå¤±**
- å¦‚æœ $\|W_{hh}\| > 1$ï¼šæ¢¯åº¦æŒ‡æ•°å¢é•¿ â†’ **æ¢¯åº¦çˆ†ç‚¸**

**åæœ**ï¼š

- **æ¢¯åº¦æ¶ˆå¤±**ï¼šæ— æ³•å­¦ä¹ é•¿æœŸä¾èµ–
- **æ¢¯åº¦çˆ†ç‚¸**ï¼šè®­ç»ƒä¸ç¨³å®š

---

## ğŸ’» LSTM (é•¿çŸ­æœŸè®°å¿†ç½‘ç»œ)

### 1. åŠ¨æœº

**é—®é¢˜**ï¼šVanilla RNNçš„æ¢¯åº¦æ¶ˆå¤±

**è§£å†³æ–¹æ¡ˆ**ï¼šå¼•å…¥**é—¨æ§æœºåˆ¶**å’Œ**ç»†èƒçŠ¶æ€**

**å…³é”®æ€æƒ³**ï¼š

- **ç»†èƒçŠ¶æ€** $\mathbf{c}_t$ï¼šé•¿æœŸè®°å¿†
- **é—¨æ§å•å…ƒ**ï¼šæ§åˆ¶ä¿¡æ¯æµ

---

### 2. LSTMæ¶æ„

**å®šä¹‰ 2.1 (LSTM)**:

$$
\mathbf{f}_t = \sigma(W_f [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_f) \quad \text{(é—å¿˜é—¨)}
$$

$$
\mathbf{i}_t = \sigma(W_i [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_i) \quad \text{(è¾“å…¥é—¨)}
$$

$$
\tilde{\mathbf{c}}_t = \tanh(W_c [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_c) \quad \text{(å€™é€‰å€¼)}
$$

$$
\mathbf{c}_t = \mathbf{f}_t \odot \mathbf{c}_{t-1} + \mathbf{i}_t \odot \tilde{\mathbf{c}}_t \quad \text{(ç»†èƒçŠ¶æ€)}
$$

$$
\mathbf{o}_t = \sigma(W_o [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_o) \quad \text{(è¾“å‡ºé—¨)}
$$

$$
\mathbf{h}_t = \mathbf{o}_t \odot \tanh(\mathbf{c}_t) \quad \text{(éšçŠ¶æ€)}
$$

å…¶ä¸­ $\odot$ è¡¨ç¤ºé€å…ƒç´ ä¹˜æ³•ï¼Œ$[\cdot, \cdot]$ è¡¨ç¤ºæ‹¼æ¥ã€‚

---

### 3. é—¨æ§æœºåˆ¶

**é—å¿˜é—¨ $\mathbf{f}_t$**ï¼š

- å†³å®šä¸¢å¼ƒå¤šå°‘æ—§ä¿¡æ¯
- $f_t \approx 0$ï¼šå®Œå…¨é—å¿˜
- $f_t \approx 1$ï¼šå®Œå…¨ä¿ç•™

**è¾“å…¥é—¨ $\mathbf{i}_t$**ï¼š

- å†³å®šæ·»åŠ å¤šå°‘æ–°ä¿¡æ¯
- $i_t \approx 0$ï¼šå¿½ç•¥æ–°è¾“å…¥
- $i_t \approx 1$ï¼šå®Œå…¨æ¥å—

**è¾“å‡ºé—¨ $\mathbf{o}_t$**ï¼š

- å†³å®šè¾“å‡ºå¤šå°‘ä¿¡æ¯
- $o_t \approx 0$ï¼šä¸è¾“å‡º
- $o_t \approx 1$ï¼šå®Œå…¨è¾“å‡º

**å…³é”®**ï¼šé—¨æ§å€¼åœ¨ $(0, 1)$ ä¹‹é—´ï¼Œæ¢¯åº¦ä¸ä¼šæ¶ˆå¤±ï¼

---

## ğŸ¨ GRU (é—¨æ§å¾ªç¯å•å…ƒ)

### 1. GRUæ¶æ„

**å®šä¹‰ 1.1 (GRU)**:

$$
\mathbf{z}_t = \sigma(W_z [\mathbf{h}_{t-1}, \mathbf{x}_t]) \quad \text{(æ›´æ–°é—¨)}
$$

$$
\mathbf{r}_t = \sigma(W_r [\mathbf{h}_{t-1}, \mathbf{x}_t]) \quad \text{(é‡ç½®é—¨)}
$$

$$
\tilde{\mathbf{h}}_t = \tanh(W [\mathbf{r}_t \odot \mathbf{h}_{t-1}, \mathbf{x}_t]) \quad \text{(å€™é€‰éšçŠ¶æ€)}
$$

$$
\mathbf{h}_t = (1 - \mathbf{z}_t) \odot \mathbf{h}_{t-1} + \mathbf{z}_t \odot \tilde{\mathbf{h}}_t
$$

**æ›´æ–°é—¨ $\mathbf{z}_t$**ï¼š

- æ§åˆ¶æ–°æ—§ä¿¡æ¯çš„æ··åˆ
- $z_t \approx 0$ï¼šä¿ç•™æ—§ä¿¡æ¯
- $z_t \approx 1$ï¼šä½¿ç”¨æ–°ä¿¡æ¯

**é‡ç½®é—¨ $\mathbf{r}_t$**ï¼š

- æ§åˆ¶ä½¿ç”¨å¤šå°‘å†å²ä¿¡æ¯
- $r_t \approx 0$ï¼šå¿½ç•¥å†å²
- $r_t \approx 1$ï¼šä½¿ç”¨å†å²

---

### 2. GRU vs LSTM

| ç‰¹æ€§ | LSTM | GRU |
|------|------|-----|
| **é—¨æ•°é‡** | 3ä¸ªï¼ˆé—å¿˜ã€è¾“å…¥ã€è¾“å‡ºï¼‰ | 2ä¸ªï¼ˆæ›´æ–°ã€é‡ç½®ï¼‰ |
| **çŠ¶æ€** | ç»†èƒçŠ¶æ€ + éšçŠ¶æ€ | ä»…éšçŠ¶æ€ |
| **å‚æ•°æ•°é‡** | æ›´å¤š | æ›´å°‘ |
| **è®¡ç®—å¤æ‚åº¦** | æ›´é«˜ | æ›´ä½ |
| **è¡¨è¾¾èƒ½åŠ›** | æ›´å¼º | ç•¥å¼± |
| **è®­ç»ƒé€Ÿåº¦** | è¾ƒæ…¢ | è¾ƒå¿« |

**å®è·µå»ºè®®**ï¼š

- æ•°æ®å……è¶³ â†’ LSTM
- æ•°æ®æœ‰é™ â†’ GRU
- å…ˆå°è¯•GRUï¼Œä¸è¡Œå†ç”¨LSTM

---

## ğŸ“ åŒå‘RNN

### 1. åŠ¨æœº1

**é—®é¢˜**ï¼šå•å‘RNNåªèƒ½çœ‹åˆ°è¿‡å»

**ç¤ºä¾‹**ï¼š

```text
"The cat sat on the ___"
```

éœ€è¦çœ‹åˆ°åé¢çš„è¯æ‰èƒ½é¢„æµ‹ï¼

---

### 2. æ•°å­¦å®šä¹‰

**å®šä¹‰ 2.1 (Bidirectional RNN)**:

**å‰å‘RNN**ï¼š

$$
\overrightarrow{\mathbf{h}}_t = \text{RNN}_{\text{forward}}(\overrightarrow{\mathbf{h}}_{t-1}, \mathbf{x}_t)
$$

**åå‘RNN**ï¼š

$$
\overleftarrow{\mathbf{h}}_t = \text{RNN}_{\text{backward}}(\overleftarrow{\mathbf{h}}_{t+1}, \mathbf{x}_t)
$$

**è¾“å‡º**ï¼š

$$
\mathbf{h}_t = [\overrightarrow{\mathbf{h}}_t; \overleftarrow{\mathbf{h}}_t]
$$

**åº”ç”¨**ï¼š

- å‘½åå®ä½“è¯†åˆ«
- è¯æ€§æ ‡æ³¨
- æœºå™¨ç¿»è¯‘ï¼ˆç¼–ç å™¨ï¼‰

---

## ğŸ”§ æ¢¯åº¦è£å‰ª

### 1. æ¢¯åº¦çˆ†ç‚¸é—®é¢˜

**ç°è±¡**ï¼š

- æ¢¯åº¦èŒƒæ•°çªç„¶å˜å¾—å¾ˆå¤§
- å‚æ•°æ›´æ–°è¿‡å¤§
- è®­ç»ƒå‘æ•£

**æ£€æµ‹**ï¼š

$$
\|\nabla \mathcal{L}\| > \text{threshold}
$$

---

### 2. æ¢¯åº¦è£å‰ªæ–¹æ³•

**æ–¹æ³•1ï¼šæŒ‰å€¼è£å‰ª**:

$$
g_i = \begin{cases}
\text{threshold} & \text{if } g_i > \text{threshold} \\
-\text{threshold} & \text{if } g_i < -\text{threshold} \\
g_i & \text{otherwise}
\end{cases}
$$

**æ–¹æ³•2ï¼šæŒ‰èŒƒæ•°è£å‰ª**:

$$
\mathbf{g} = \begin{cases}
\frac{\text{threshold}}{\|\mathbf{g}\|} \mathbf{g} & \text{if } \|\mathbf{g}\| > \text{threshold} \\
\mathbf{g} & \text{otherwise}
\end{cases}
$$

**æ¨è**ï¼šæŒ‰èŒƒæ•°è£å‰ªï¼ˆä¿æŒæ¢¯åº¦æ–¹å‘ï¼‰

---

## ğŸ’¡ Pythonå®ç°

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

# 1. ä»é›¶å®ç°Vanilla RNN
class VanillaRNN(nn.Module):
    """ä»é›¶å®ç°çš„RNN"""
    def __init__(self, input_size, hidden_size, output_size):
        super().__init__()
        self.hidden_size = hidden_size
        
        # æƒé‡çŸ©é˜µ
        self.W_xh = nn.Linear(input_size, hidden_size)
        self.W_hh = nn.Linear(hidden_size, hidden_size)
        self.W_hy = nn.Linear(hidden_size, output_size)
    
    def forward(self, x, h_prev=None):
        """
        Args:
            x: (batch, seq_len, input_size)
            h_prev: (batch, hidden_size) or None
        
        Returns:
            outputs: (batch, seq_len, output_size)
            h: (batch, hidden_size)
        """
        batch_size, seq_len, _ = x.size()
        
        # åˆå§‹åŒ–éšçŠ¶æ€
        if h_prev is None:
            h = torch.zeros(batch_size, self.hidden_size, device=x.device)
        else:
            h = h_prev
        
        outputs = []
        
        # æ—¶é—´æ­¥å¾ªç¯
        for t in range(seq_len):
            x_t = x[:, t, :]  # (batch, input_size)
            
            # RNNæ›´æ–°
            h = torch.tanh(self.W_xh(x_t) + self.W_hh(h))
            
            # è¾“å‡º
            y_t = self.W_hy(h)
            outputs.append(y_t.unsqueeze(1))
        
        outputs = torch.cat(outputs, dim=1)  # (batch, seq_len, output_size)
        
        return outputs, h


# 2. ä½¿ç”¨PyTorchçš„LSTM
class LSTMModel(nn.Module):
    """LSTMæ¨¡å‹"""
    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.5):
        super().__init__()
        
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # LSTMå±‚
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )
        
        # è¾“å‡ºå±‚
        self.fc = nn.Linear(hidden_size, output_size)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x, hidden=None):
        """
        Args:
            x: (batch, seq_len, input_size)
            hidden: (h_0, c_0) or None
        
        Returns:
            output: (batch, seq_len, output_size)
            hidden: (h_n, c_n)
        """
        # LSTM
        lstm_out, hidden = self.lstm(x, hidden)
        
        # Dropout + FC
        lstm_out = self.dropout(lstm_out)
        output = self.fc(lstm_out)
        
        return output, hidden
    
    def init_hidden(self, batch_size, device):
        """åˆå§‹åŒ–éšçŠ¶æ€"""
        h_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=device)
        c_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=device)
        return (h_0, c_0)


# 3. GRUå®ç°
class GRUModel(nn.Module):
    """GRUæ¨¡å‹"""
    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.5):
        super().__init__()
        
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # GRUå±‚
        self.gru = nn.GRU(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )
        
        # è¾“å‡ºå±‚
        self.fc = nn.Linear(hidden_size, output_size)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x, hidden=None):
        gru_out, hidden = self.gru(x, hidden)
        gru_out = self.dropout(gru_out)
        output = self.fc(gru_out)
        return output, hidden


# 4. åŒå‘LSTM
class BiLSTM(nn.Module):
    """åŒå‘LSTM"""
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super().__init__()
        
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            bidirectional=True  # åŒå‘
        )
        
        # æ³¨æ„ï¼šåŒå‘LSTMçš„è¾“å‡ºç»´åº¦æ˜¯ 2*hidden_size
        self.fc = nn.Linear(hidden_size * 2, output_size)
    
    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        output = self.fc(lstm_out)
        return output


# 5. æ¢¯åº¦è£å‰ª
def clip_gradient(model, clip_value):
    """æ¢¯åº¦è£å‰ª"""
    torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)


# ç¤ºä¾‹ä½¿ç”¨
if __name__ == "__main__":
    # å‚æ•°
    batch_size = 32
    seq_len = 10
    input_size = 50
    hidden_size = 128
    num_layers = 2
    output_size = 10
    
    # æµ‹è¯•Vanilla RNN
    print("=== Vanilla RNN ===")
    rnn = VanillaRNN(input_size, hidden_size, output_size)
    x = torch.randn(batch_size, seq_len, input_size)
    outputs, h = rnn(x)
    print(f"Input: {x.shape}")
    print(f"Output: {outputs.shape}")
    print(f"Hidden: {h.shape}")
    
    # æµ‹è¯•LSTM
    print("\n=== LSTM ===")
    lstm_model = LSTMModel(input_size, hidden_size, num_layers, output_size)
    hidden = lstm_model.init_hidden(batch_size, x.device)
    outputs, hidden = lstm_model(x, hidden)
    print(f"Output: {outputs.shape}")
    print(f"Hidden h: {hidden[0].shape}")
    print(f"Hidden c: {hidden[1].shape}")
    
    # æµ‹è¯•GRU
    print("\n=== GRU ===")
    gru_model = GRUModel(input_size, hidden_size, num_layers, output_size)
    outputs, hidden = gru_model(x)
    print(f"Output: {outputs.shape}")
    print(f"Hidden: {hidden.shape}")
    
    # æµ‹è¯•åŒå‘LSTM
    print("\n=== Bidirectional LSTM ===")
    bilstm = BiLSTM(input_size, hidden_size, num_layers, output_size)
    outputs = bilstm(x)
    print(f"Output: {outputs.shape}")
    
    # æ¢¯åº¦è£å‰ªç¤ºä¾‹
    print("\n=== Gradient Clipping ===")
    optimizer = torch.optim.Adam(lstm_model.parameters())
    loss = outputs.sum()
    loss.backward()
    
    # è£å‰ªå‰
    total_norm_before = 0
    for p in lstm_model.parameters():
        if p.grad is not None:
            total_norm_before += p.grad.data.norm(2).item() ** 2
    total_norm_before = total_norm_before ** 0.5
    print(f"Gradient norm before clipping: {total_norm_before:.4f}")
    
    # è£å‰ª
    clip_gradient(lstm_model, clip_value=1.0)
    
    # è£å‰ªå
    total_norm_after = 0
    for p in lstm_model.parameters():
        if p.grad is not None:
            total_norm_after += p.grad.data.norm(2).item() ** 2
    total_norm_after = total_norm_after ** 0.5
    print(f"Gradient norm after clipping: {total_norm_after:.4f}")
```

---

## ğŸ“š åº”ç”¨åœºæ™¯

### 1. è¯­è¨€æ¨¡å‹

**ä»»åŠ¡**ï¼šé¢„æµ‹ä¸‹ä¸€ä¸ªè¯

**æ¶æ„**ï¼š

```text
è¾“å…¥: "The cat sat on"
    â†“
LSTM
    â†“
è¾“å‡º: "the" (æ¦‚ç‡æœ€é«˜)
```

**æŸå¤±å‡½æ•°**ï¼šäº¤å‰ç†µ

---

### 2. æœºå™¨ç¿»è¯‘

**Seq2Seqæ¶æ„**ï¼š

```text
ç¼–ç å™¨ (Encoder):
    è‹±æ–‡ â†’ LSTM â†’ ä¸Šä¸‹æ–‡å‘é‡

è§£ç å™¨ (Decoder):
    ä¸Šä¸‹æ–‡å‘é‡ â†’ LSTM â†’ ä¸­æ–‡
```

**å…³é”®**ï¼šç¼–ç å™¨çš„æœ€åéšçŠ¶æ€ä½œä¸ºè§£ç å™¨çš„åˆå§‹éšçŠ¶æ€

---

### 3. æ—¶é—´åºåˆ—é¢„æµ‹

**ä»»åŠ¡**ï¼šé¢„æµ‹è‚¡ä»·ã€å¤©æ°”ç­‰

**æ¶æ„**ï¼š

```text
å†å²æ•°æ® â†’ LSTM â†’ æœªæ¥å€¼
```

**ç‰¹ç‚¹**ï¼š

- å¤šå¯¹ä¸€ï¼šé¢„æµ‹å•ä¸ªå€¼
- å¤šå¯¹å¤šï¼šé¢„æµ‹åºåˆ—

---

## ğŸ“ ç›¸å…³è¯¾ç¨‹

| å¤§å­¦ | è¯¾ç¨‹ |
|------|------|
| **Stanford** | CS224N Natural Language Processing |
| **MIT** | 6.S191 Introduction to Deep Learning |
| **CMU** | 11-747 Neural Networks for NLP |
| **UC Berkeley** | CS182 Deep Learning |

---

## ğŸ“– å‚è€ƒæ–‡çŒ®

1. **Hochreiter & Schmidhuber (1997)**. "Long Short-Term Memory". *Neural Computation*.

2. **Cho et al. (2014)**. "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation". *EMNLP*. (GRU)

3. **Graves (2013)**. "Generating Sequences With Recurrent Neural Networks". *arXiv*.

4. **Sutskever et al. (2014)**. "Sequence to Sequence Learning with Neural Networks". *NeurIPS*.

5. **Pascanu et al. (2013)**. "On the difficulty of training Recurrent Neural Networks". *ICML*. (æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸)

---

*æœ€åæ›´æ–°ï¼š2025å¹´10æœˆ*-
