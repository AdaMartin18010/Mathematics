# ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ (GAN) ç†è®º

> **Generative Adversarial Networks: Theory and Mathematics**
>
> å¯¹æŠ—è®­ç»ƒçš„æ•°å­¦åŸºç¡€ä¸ç†è®ºåˆ†æ

---

## ç›®å½•

- [ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ (GAN) ç†è®º](#ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ-gan-ç†è®º)
  - [ç›®å½•](#ç›®å½•)
  - [ğŸ“‹ æ ¸å¿ƒæ€æƒ³](#-æ ¸å¿ƒæ€æƒ³)
  - [ğŸ¯ é—®é¢˜å½¢å¼åŒ–](#-é—®é¢˜å½¢å¼åŒ–)
    - [1. å¯¹æŠ—åšå¼ˆ](#1-å¯¹æŠ—åšå¼ˆ)
    - [2. ç›®æ ‡å‡½æ•°](#2-ç›®æ ‡å‡½æ•°)
  - [ğŸ“Š ç†è®ºåˆ†æ](#-ç†è®ºåˆ†æ)
    - [1. å…¨å±€æœ€ä¼˜è§£](#1-å…¨å±€æœ€ä¼˜è§£)
    - [2. Nashå‡è¡¡](#2-nashå‡è¡¡)
    - [3. æ”¶æ•›æ€§åˆ†æ](#3-æ”¶æ•›æ€§åˆ†æ)
  - [ğŸ”¬ è®­ç»ƒåŠ¨åŠ›å­¦](#-è®­ç»ƒåŠ¨åŠ›å­¦)
    - [1. åˆ¤åˆ«å™¨æ›´æ–°](#1-åˆ¤åˆ«å™¨æ›´æ–°)
    - [2. ç”Ÿæˆå™¨æ›´æ–°](#2-ç”Ÿæˆå™¨æ›´æ–°)
    - [3. æ¨¡å¼åç¼©](#3-æ¨¡å¼åç¼©)
  - [ğŸ’» Pythonå®ç°](#-pythonå®ç°)
  - [ğŸ¨ GANå˜ä½“](#-ganå˜ä½“)
    - [1. DCGAN](#1-dcgan)
    - [2. Wasserstein GAN (WGAN)](#2-wasserstein-gan-wgan)
    - [3. Conditional GAN (cGAN)](#3-conditional-gan-cgan)
  - [ğŸ“š ç†è®ºæ·±åŒ–](#-ç†è®ºæ·±åŒ–)
    - [1. f-æ•£åº¦è§†è§’](#1-f-æ•£åº¦è§†è§’)
    - [2. ç§¯åˆ†æ¦‚ç‡åº¦é‡ (IPM)](#2-ç§¯åˆ†æ¦‚ç‡åº¦é‡-ipm)
  - [ğŸ“ ç›¸å…³è¯¾ç¨‹](#-ç›¸å…³è¯¾ç¨‹)
  - [ğŸ“– å‚è€ƒæ–‡çŒ®](#-å‚è€ƒæ–‡çŒ®)

---

## ğŸ“‹ æ ¸å¿ƒæ€æƒ³

**GAN**é€šè¿‡**å¯¹æŠ—è®­ç»ƒ**å­¦ä¹ ç”Ÿæˆæ•°æ®åˆ†å¸ƒã€‚

**æ ¸å¿ƒæµç¨‹**ï¼š

```text
å™ªå£° z â†’ ç”Ÿæˆå™¨ G â†’ å‡æ ·æœ¬ G(z)
                         â†“
çœŸæ ·æœ¬ x â†’ åˆ¤åˆ«å™¨ D â†’ çœŸ/å‡åˆ¤æ–­
                         â†“
                    åé¦ˆç»™Gå’ŒD
```

**å¯¹æŠ—è¿‡ç¨‹**ï¼š

- **ç”Ÿæˆå™¨ G**ï¼šæ¬ºéª—åˆ¤åˆ«å™¨ï¼ˆç”Ÿæˆé€¼çœŸæ ·æœ¬ï¼‰
- **åˆ¤åˆ«å™¨ D**ï¼šåŒºåˆ†çœŸå‡æ ·æœ¬

---

## ğŸ¯ é—®é¢˜å½¢å¼åŒ–

### 1. å¯¹æŠ—åšå¼ˆ

**ç”Ÿæˆå™¨**ï¼š$G: \mathcal{Z} \to \mathcal{X}$

- è¾“å…¥ï¼šå™ªå£° $z \sim p_z(z)$ï¼ˆå¦‚ $\mathcal{N}(0, I)$ï¼‰
- è¾“å‡ºï¼šç”Ÿæˆæ ·æœ¬ $G(z)$

**åˆ¤åˆ«å™¨**ï¼š$D: \mathcal{X} \to [0, 1]$

- è¾“å…¥ï¼šæ ·æœ¬ $x$
- è¾“å‡ºï¼š$D(x)$ = æ ·æœ¬ä¸ºçœŸçš„æ¦‚ç‡

---

### 2. ç›®æ ‡å‡½æ•°

**å®šç† 2.1 (GANç›®æ ‡å‡½æ•°, Goodfellow et al. 2014)**:

$$
\min_G \max_D V(D, G) = \mathbb{E}_{x \sim p_{\text{data}}}[\log D(x)] + \mathbb{E}_{z \sim p_z}[\log(1 - D(G(z)))]
$$

**è§£é‡Š**ï¼š

- **åˆ¤åˆ«å™¨D**ï¼šæœ€å¤§åŒ– $V(D, G)$
  - å¯¹çœŸæ ·æœ¬è¾“å‡ºæ¥è¿‘1ï¼š$\log D(x) \to 0$
  - å¯¹å‡æ ·æœ¬è¾“å‡ºæ¥è¿‘0ï¼š$\log(1 - D(G(z))) \to 0$

- **ç”Ÿæˆå™¨G**ï¼šæœ€å°åŒ– $V(D, G)$
  - è®©å‡æ ·æœ¬æ¬ºéª—åˆ¤åˆ«å™¨ï¼š$D(G(z)) \to 1$

---

## ğŸ“Š ç†è®ºåˆ†æ

### 1. å…¨å±€æœ€ä¼˜è§£

**å®šç† 1.1 (æœ€ä¼˜åˆ¤åˆ«å™¨)**:

å¯¹äºå›ºå®šçš„ç”Ÿæˆå™¨ $G$ï¼Œæœ€ä¼˜åˆ¤åˆ«å™¨ä¸ºï¼š

$$
D_G^*(x) = \frac{p_{\text{data}}(x)}{p_{\text{data}}(x) + p_g(x)}
$$

å…¶ä¸­ $p_g$ æ˜¯ç”Ÿæˆå™¨è¯±å¯¼çš„åˆ†å¸ƒã€‚

**è¯æ˜**ï¼š

ç›®æ ‡æ˜¯æœ€å¤§åŒ–ï¼š

$$
V(D, G) = \int_x p_{\text{data}}(x) \log D(x) dx + \int_x p_g(x) \log(1 - D(x)) dx
$$

å¯¹ $D(x)$ æ±‚å¯¼å¹¶ä»¤å…¶ä¸º0ï¼š

$$
\frac{p_{\text{data}}(x)}{D(x)} - \frac{p_g(x)}{1 - D(x)} = 0
$$

è§£å¾—ï¼š

$$
D^*(x) = \frac{p_{\text{data}}(x)}{p_{\text{data}}(x) + p_g(x)}
$$

---

**å®šç† 1.2 (å…¨å±€æœ€ä¼˜ç”Ÿæˆå™¨)**:

å½“ $D = D_G^*$ æ—¶ï¼Œ$G$ çš„ç›®æ ‡å‡½æ•°ç­‰ä»·äºæœ€å°åŒ–ï¼š

$$
C(G) = -\log 4 + 2 \cdot \text{JSD}(p_{\text{data}} \| p_g)
$$

å…¶ä¸­ $\text{JSD}$ æ˜¯Jensen-Shannonæ•£åº¦ã€‚

**æ¨è®º**ï¼šå…¨å±€æœ€ä¼˜è§£ä¸º $p_g = p_{\text{data}}$ï¼Œæ­¤æ—¶ $D_G^*(x) = \frac{1}{2}$ã€‚

---

### 2. Nashå‡è¡¡

**å®šä¹‰ 2.1 (Nashå‡è¡¡)**:

$(G^*, D^*)$ æ˜¯Nashå‡è¡¡ï¼Œè‹¥ï¼š

$$
\begin{align}
V(D^*, G^*) &\geq V(D, G^*) \quad \forall D \\
V(D^*, G^*) &\leq V(D^*, G) \quad \forall G
\end{align}
$$

**GANçš„Nashå‡è¡¡**ï¼š$p_g = p_{\text{data}}$ï¼Œ$D(x) = \frac{1}{2}$ã€‚

**é—®é¢˜**ï¼šå®è·µä¸­éš¾ä»¥è¾¾åˆ°Nashå‡è¡¡ï¼

---

### 3. æ”¶æ•›æ€§åˆ†æ

**å®šç† 3.1 (æ”¶æ•›æ€§, Goodfellow et al. 2014)**:

å¦‚æœ $G$ å’Œ $D$ æœ‰è¶³å¤Ÿå®¹é‡ï¼Œä¸”æ¯æ­¥æ›´æ–°éƒ½èƒ½è¾¾åˆ°æœ€ä¼˜ï¼Œåˆ™ç®—æ³•æ”¶æ•›åˆ° $p_g = p_{\text{data}}$ã€‚

**å®è·µé—®é¢˜**ï¼š

- æœ‰é™å®¹é‡
- æœ‰é™æ›´æ–°æ­¥æ•°
- éå‡¸ä¼˜åŒ–
- æ¢¯åº¦æ¶ˆå¤±

---

## ğŸ”¬ è®­ç»ƒåŠ¨åŠ›å­¦

### 1. åˆ¤åˆ«å™¨æ›´æ–°

**ç›®æ ‡**ï¼šæœ€å¤§åŒ– $V(D, G)$

**æ¢¯åº¦**ï¼š

$$
\nabla_\theta V(D, G) = \mathbb{E}_{x \sim p_{\text{data}}}\left[\nabla_\theta \log D_\theta(x)\right] + \mathbb{E}_{z \sim p_z}\left[\nabla_\theta \log(1 - D_\theta(G(z)))\right]
$$

**æ›´æ–°**ï¼š

$$
\theta_D \leftarrow \theta_D + \alpha \nabla_\theta V(D, G)
$$

---

### 2. ç”Ÿæˆå™¨æ›´æ–°

**åŸå§‹ç›®æ ‡**ï¼šæœ€å°åŒ– $\mathbb{E}_{z \sim p_z}[\log(1 - D(G(z)))]$

**é—®é¢˜**ï¼šæ—©æœŸè®­ç»ƒæ—¶ $D(G(z)) \approx 0$ï¼Œæ¢¯åº¦æ¶ˆå¤±ã€‚

**æ”¹è¿›ç›®æ ‡**ï¼šæœ€å¤§åŒ– $\mathbb{E}_{z \sim p_z}[\log D(G(z))]$

**æ¢¯åº¦**ï¼š

$$
\nabla_\phi \mathbb{E}_{z \sim p_z}[\log D(G_\phi(z))] = \mathbb{E}_{z \sim p_z}\left[\nabla_\phi \log D(G_\phi(z))\right]
$$

---

### 3. æ¨¡å¼åç¼©

**é—®é¢˜**ï¼šç”Ÿæˆå™¨åªç”Ÿæˆå°‘æ•°å‡ ç§æ ·æœ¬ã€‚

**åŸå› **ï¼š

- ç”Ÿæˆå™¨æ‰¾åˆ°"æ·å¾„"æ¬ºéª—åˆ¤åˆ«å™¨
- ç¼ºä¹å¤šæ ·æ€§æƒ©ç½š

**è§£å†³æ–¹æ¡ˆ**ï¼š

1. **Unrolled GAN**ï¼šåˆ¤åˆ«å™¨å‰ç»å¤šæ­¥
2. **Minibatch Discrimination**ï¼šè€ƒè™‘æ‰¹å†…å¤šæ ·æ€§
3. **Mode Regularization**ï¼šæ˜¾å¼å¤šæ ·æ€§æ­£åˆ™åŒ–

---

## ğŸ’» Pythonå®ç°

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt

# ç”Ÿæˆå™¨
class Generator(nn.Module):
    """ç®€å•çš„å…¨è¿æ¥ç”Ÿæˆå™¨"""
    def __init__(self, latent_dim=100, hidden_dim=256, output_dim=784):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim),
            nn.Tanh()  # è¾“å‡ºèŒƒå›´ [-1, 1]
        )
    
    def forward(self, z):
        return self.net(z)


# åˆ¤åˆ«å™¨
class Discriminator(nn.Module):
    """ç®€å•çš„å…¨è¿æ¥åˆ¤åˆ«å™¨"""
    def __init__(self, input_dim=784, hidden_dim=256):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.LeakyReLU(0.2),
            nn.Linear(hidden_dim, hidden_dim),
            nn.LeakyReLU(0.2),
            nn.Linear(hidden_dim, 1),
            nn.Sigmoid()  # è¾“å‡ºæ¦‚ç‡
        )
    
    def forward(self, x):
        return self.net(x)


def train_gan(generator, discriminator, train_loader, epochs=50, lr=2e-4, latent_dim=100):
    """è®­ç»ƒGAN"""
    # ä¼˜åŒ–å™¨
    g_optimizer = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))
    d_optimizer = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))
    
    # æŸå¤±å‡½æ•°
    criterion = nn.BCELoss()
    
    for epoch in range(epochs):
        for batch_idx, (real_images, _) in enumerate(train_loader):
            batch_size = real_images.size(0)
            real_images = real_images.view(batch_size, -1)
            
            # çœŸå‡æ ‡ç­¾
            real_labels = torch.ones(batch_size, 1)
            fake_labels = torch.zeros(batch_size, 1)
            
            # ========== è®­ç»ƒåˆ¤åˆ«å™¨ ==========
            d_optimizer.zero_grad()
            
            # çœŸæ ·æœ¬
            real_outputs = discriminator(real_images)
            d_loss_real = criterion(real_outputs, real_labels)
            
            # å‡æ ·æœ¬
            z = torch.randn(batch_size, latent_dim)
            fake_images = generator(z)
            fake_outputs = discriminator(fake_images.detach())
            d_loss_fake = criterion(fake_outputs, fake_labels)
            
            # æ€»åˆ¤åˆ«å™¨æŸå¤±
            d_loss = d_loss_real + d_loss_fake
            d_loss.backward()
            d_optimizer.step()
            
            # ========== è®­ç»ƒç”Ÿæˆå™¨ ==========
            g_optimizer.zero_grad()
            
            # ç”Ÿæˆå‡æ ·æœ¬å¹¶æ¬ºéª—åˆ¤åˆ«å™¨
            z = torch.randn(batch_size, latent_dim)
            fake_images = generator(z)
            fake_outputs = discriminator(fake_images)
            
            # ç”Ÿæˆå™¨æŸå¤±ï¼ˆå¸Œæœ›åˆ¤åˆ«å™¨è¾“å‡º1ï¼‰
            g_loss = criterion(fake_outputs, real_labels)
            g_loss.backward()
            g_optimizer.step()
        
        # æ‰“å°è¿›åº¦
        if (epoch + 1) % 10 == 0:
            print(f"Epoch [{epoch+1}/{epochs}], D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}")


def generate_samples(generator, n_samples=16, latent_dim=100):
    """ç”Ÿæˆæ ·æœ¬"""
    generator.eval()
    with torch.no_grad():
        z = torch.randn(n_samples, latent_dim)
        samples = generator(z)
        samples = samples.view(-1, 28, 28)
        samples = (samples + 1) / 2  # ä» [-1, 1] è½¬æ¢åˆ° [0, 1]
    return samples


# ç¤ºä¾‹ï¼šåœ¨MNISTä¸Šè®­ç»ƒGAN
if __name__ == "__main__":
    # æ•°æ®åŠ è½½
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.5], [0.5])  # å½’ä¸€åŒ–åˆ° [-1, 1]
    ])
    
    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)
    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
    
    # åˆ›å»ºæ¨¡å‹
    latent_dim = 100
    generator = Generator(latent_dim=latent_dim)
    discriminator = Discriminator()
    
    # è®­ç»ƒ
    print("Training GAN...")
    train_gan(generator, discriminator, train_loader, epochs=50, latent_dim=latent_dim)
    
    # ç”Ÿæˆæ ·æœ¬
    print("\nGenerating samples...")
    samples = generate_samples(generator, n_samples=16, latent_dim=latent_dim)
    
    # å¯è§†åŒ–
    fig, axes = plt.subplots(4, 4, figsize=(8, 8))
    for i, ax in enumerate(axes.flat):
        ax.imshow(samples[i].numpy(), cmap='gray')
        ax.axis('off')
    plt.suptitle('Generated Samples from GAN')
    plt.tight_layout()
    plt.show()
```

---

## ğŸ¨ GANå˜ä½“

### 1. DCGAN

**Deep Convolutional GAN** (Radford et al. 2016)

**æ ¸å¿ƒæ”¹è¿›**ï¼š

- ä½¿ç”¨å·ç§¯å’Œè½¬ç½®å·ç§¯
- Batch Normalization
- LeakyReLUæ¿€æ´»
- ç§»é™¤å…¨è¿æ¥å±‚

**æ¶æ„æŒ‡å¯¼**ï¼š

```text
ç”Ÿæˆå™¨:
  - è½¬ç½®å·ç§¯ä¸Šé‡‡æ ·
  - Batch Norm
  - ReLU (æœ€åä¸€å±‚Tanh)

åˆ¤åˆ«å™¨:
  - å·ç§¯ä¸‹é‡‡æ ·
  - Batch Norm
  - LeakyReLU
  - æœ€åä¸€å±‚Sigmoid
```

---

### 2. Wasserstein GAN (WGAN)

**æ ¸å¿ƒæ€æƒ³**ï¼šç”¨Wassersteinè·ç¦»æ›¿ä»£JSæ•£åº¦ã€‚

**Wassersteinè·ç¦»**ï¼š

$$
W(p_{\text{data}}, p_g) = \inf_{\gamma \in \Pi(p_{\text{data}}, p_g)} \mathbb{E}_{(x, y) \sim \gamma}[\|x - y\|]
$$

**Kantorovich-Rubinsteinå¯¹å¶**ï¼š

$$
W(p_{\text{data}}, p_g) = \sup_{\|f\|_L \leq 1} \mathbb{E}_{x \sim p_{\text{data}}}[f(x)] - \mathbb{E}_{x \sim p_g}[f(x)]
$$

**WGANç›®æ ‡**ï¼š

$$
\min_G \max_{D \in \mathcal{D}} \mathbb{E}_{x \sim p_{\text{data}}}[D(x)] - \mathbb{E}_{z \sim p_z}[D(G(z))]
$$

å…¶ä¸­ $\mathcal{D}$ æ˜¯1-Lipschitzå‡½æ•°ç±»ã€‚

**ä¼˜åŠ¿**ï¼š

- æ›´ç¨³å®šçš„è®­ç»ƒ
- æœ‰æ„ä¹‰çš„æŸå¤±æ›²çº¿
- ç¼“è§£æ¨¡å¼åç¼©

**å®ç°**ï¼šæƒé‡è£å‰ªæˆ–æ¢¯åº¦æƒ©ç½šï¼ˆWGAN-GPï¼‰ã€‚

---

### 3. Conditional GAN (cGAN)

**æ ¸å¿ƒæ€æƒ³**ï¼šæ¡ä»¶ç”Ÿæˆã€‚

**æ¨¡å‹**ï¼š

$$
\begin{align}
G(z, y) &\quad \text{(æ¡ä»¶ç”Ÿæˆå™¨)} \\
D(x, y) &\quad \text{(æ¡ä»¶åˆ¤åˆ«å™¨)}
\end{align}
$$

**ç›®æ ‡å‡½æ•°**ï¼š

$$
\min_G \max_D \mathbb{E}_{x \sim p_{\text{data}}}[\log D(x, y)] + \mathbb{E}_{z \sim p_z}[\log(1 - D(G(z, y), y))]
$$

**åº”ç”¨**ï¼š

- ç±»åˆ«æ¡ä»¶å›¾åƒç”Ÿæˆ
- å›¾åƒåˆ°å›¾åƒç¿»è¯‘ï¼ˆPix2Pixï¼‰
- æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ

---

## ğŸ“š ç†è®ºæ·±åŒ–

### 1. f-æ•£åº¦è§†è§’

**å®šç† 1.1 (f-GAN, Nowozin et al. 2016)**:

GANå¯ä»¥æœ€å°åŒ–ä»»æ„f-æ•£åº¦ï¼š

$$
D_f(p_{\text{data}} \| p_g) = \int p_g(x) f\left(\frac{p_{\text{data}}(x)}{p_g(x)}\right) dx
$$

**å¸¸è§f-æ•£åº¦**ï¼š

| f-æ•£åº¦ | $f(t)$ |
|--------|--------|
| **KL** | $t \log t$ |
| **JS** | $-\log(2) - \frac{1}{2}(t+1)\log\frac{t+1}{2}$ |
| **Total Variation** | $\frac{1}{2}\|t-1\|$ |

---

### 2. ç§¯åˆ†æ¦‚ç‡åº¦é‡ (IPM)

**å®šä¹‰ 2.1 (IPM)**:

$$
d_{\mathcal{F}}(p, q) = \sup_{f \in \mathcal{F}} \left|\mathbb{E}_{x \sim p}[f(x)] - \mathbb{E}_{x \sim q}[f(x)]\right|
$$

**ä¾‹å­**ï¼š

- **Wassersteinè·ç¦»**ï¼š$\mathcal{F}$ = 1-Lipschitzå‡½æ•°
- **Maximum Mean Discrepancy (MMD)**ï¼š$\mathcal{F}$ = RKHSå•ä½çƒ

---

## ğŸ“ ç›¸å…³è¯¾ç¨‹

| å¤§å­¦ | è¯¾ç¨‹ |
|------|------|
| **Stanford** | CS236 Deep Generative Models |
| **MIT** | 6.S191 Introduction to Deep Learning |
| **UC Berkeley** | CS294 Deep Unsupervised Learning |
| **NYU** | DS-GA 1008 Deep Learning |

---

## ğŸ“– å‚è€ƒæ–‡çŒ®

1. **Goodfellow et al. (2014)**. "Generative Adversarial Networks". *NeurIPS*.

2. **Radford et al. (2016)**. "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks". *ICLR*.

3. **Arjovsky et al. (2017)**. "Wasserstein GAN". *ICML*.

4. **Gulrajani et al. (2017)**. "Improved Training of Wasserstein GANs". *NeurIPS*.

5. **Nowozin et al. (2016)**. "f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization". *NeurIPS*.

---

*æœ€åæ›´æ–°ï¼š2025å¹´10æœˆ*-
