# è‡ªç„¶è¯­è¨€å¤„ç†åº”ç”¨æ¡ˆä¾‹ (NLP Applications)

> **From Text to Understanding: Practical NLP with Transformers**
>
> ä»æ–‡æœ¬åˆ°ç†è§£ï¼šTransformerè‡ªç„¶è¯­è¨€å¤„ç†å®è·µ

---

## ç›®å½•

- [è‡ªç„¶è¯­è¨€å¤„ç†åº”ç”¨æ¡ˆä¾‹ (NLP Applications)](#è‡ªç„¶è¯­è¨€å¤„ç†åº”ç”¨æ¡ˆä¾‹-nlp-applications)
  - [ç›®å½•](#ç›®å½•)
  - [ğŸ“‹ æ¦‚è¿°](#-æ¦‚è¿°)
  - [ğŸ¯ æ¡ˆä¾‹1: æ–‡æœ¬åˆ†ç±» (Text Classification)](#-æ¡ˆä¾‹1-æ–‡æœ¬åˆ†ç±»-text-classification)
    - [é—®é¢˜å®šä¹‰](#é—®é¢˜å®šä¹‰)
    - [æ•°å­¦å»ºæ¨¡](#æ•°å­¦å»ºæ¨¡)
    - [å®Œæ•´å®ç°: BERT Fine-tuning](#å®Œæ•´å®ç°-bert-fine-tuning)
    - [æ€§èƒ½åˆ†æ](#æ€§èƒ½åˆ†æ)
  - [ğŸ¯ æ¡ˆä¾‹2: å‘½åå®ä½“è¯†åˆ« (Named Entity Recognition)](#-æ¡ˆä¾‹2-å‘½åå®ä½“è¯†åˆ«-named-entity-recognition)
    - [é—®é¢˜å®šä¹‰2](#é—®é¢˜å®šä¹‰2)
    - [æ•°å­¦å»ºæ¨¡2](#æ•°å­¦å»ºæ¨¡2)
    - [å®Œæ•´å®ç°: BERT-NER](#å®Œæ•´å®ç°-bert-ner)
  - [ğŸ¯ æ¡ˆä¾‹3: æ–‡æœ¬ç”Ÿæˆ (Text Generation)](#-æ¡ˆä¾‹3-æ–‡æœ¬ç”Ÿæˆ-text-generation)
    - [é—®é¢˜å®šä¹‰3](#é—®é¢˜å®šä¹‰3)
    - [æ•°å­¦å»ºæ¨¡3](#æ•°å­¦å»ºæ¨¡3)
    - [å®Œæ•´å®ç°: GPT-2 Fine-tuning](#å®Œæ•´å®ç°-gpt-2-fine-tuning)
  - [ğŸ¯ æ¡ˆä¾‹4: æœºå™¨ç¿»è¯‘ (Machine Translation)](#-æ¡ˆä¾‹4-æœºå™¨ç¿»è¯‘-machine-translation)
    - [é—®é¢˜å®šä¹‰4](#é—®é¢˜å®šä¹‰4)
    - [æ•°å­¦å»ºæ¨¡4](#æ•°å­¦å»ºæ¨¡4)
    - [å®Œæ•´å®ç°: Transformerç¿»è¯‘](#å®Œæ•´å®ç°-transformerç¿»è¯‘)
  - [ğŸ¯ æ¡ˆä¾‹5: é—®ç­”ç³»ç»Ÿ (Question Answering)](#-æ¡ˆä¾‹5-é—®ç­”ç³»ç»Ÿ-question-answering)
    - [é—®é¢˜å®šä¹‰5](#é—®é¢˜å®šä¹‰5)
    - [æ•°å­¦å»ºæ¨¡5](#æ•°å­¦å»ºæ¨¡5)
    - [å®Œæ•´å®ç°: BERT-QA](#å®Œæ•´å®ç°-bert-qa)
  - [ğŸ“Š æ¡ˆä¾‹æ€»ç»“](#-æ¡ˆä¾‹æ€»ç»“)
  - [ğŸ”— ç›¸å…³ç†è®º](#-ç›¸å…³ç†è®º)
  - [ğŸ“š æ¨èèµ„æº](#-æ¨èèµ„æº)
  - [ğŸ“ å­¦ä¹ å»ºè®®](#-å­¦ä¹ å»ºè®®)

---

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æ¡£æä¾›**5ä¸ªå®Œæ•´çš„NLPåº”ç”¨æ¡ˆä¾‹**ï¼Œä»åŸºç¡€çš„æ–‡æœ¬åˆ†ç±»åˆ°é«˜çº§çš„æœºå™¨ç¿»è¯‘å’Œé—®ç­”ç³»ç»Ÿã€‚æ¯ä¸ªæ¡ˆä¾‹éƒ½åŒ…å«ï¼š

1. **é—®é¢˜å®šä¹‰**: æ¸…æ™°çš„ä»»åŠ¡æè¿°
2. **æ•°å­¦å»ºæ¨¡**: å½¢å¼åŒ–é—®é¢˜
3. **å®Œæ•´ä»£ç **: å¯è¿è¡Œçš„PyTorch/Transformerså®ç°
4. **æ€§èƒ½åˆ†æ**: æ•°å­¦è§’åº¦çš„è¯„ä¼°
5. **å·¥ç¨‹ä¼˜åŒ–**: å®é™…éƒ¨ç½²å»ºè®®

---

## ğŸ¯ æ¡ˆä¾‹1: æ–‡æœ¬åˆ†ç±» (Text Classification)

### é—®é¢˜å®šä¹‰

**ä»»åŠ¡**: ç»™å®šæ–‡æœ¬ $x = (w_1, w_2, \ldots, w_n)$ï¼Œé¢„æµ‹å…¶ç±»åˆ« $y \in \{1, 2, \ldots, K\}$

**æ•°æ®é›†**: IMDBæƒ…æ„Ÿåˆ†æ (50,000æ¡ç”µå½±è¯„è®ºï¼Œ2ä¸ªç±»åˆ«ï¼šæ­£é¢/è´Ÿé¢)

**è¯„ä¼°æŒ‡æ ‡**: å‡†ç¡®ç‡ã€F1åˆ†æ•°

### æ•°å­¦å»ºæ¨¡

**æ¨¡å‹**: BERT (Bidirectional Encoder Representations from Transformers)

**è¾“å…¥è¡¨ç¤º**:
$$
\text{Input} = [\text{CLS}] + \text{Tokens} + [\text{SEP}]
$$

**BERTç¼–ç **:
$$
\mathbf{h} = \text{BERT}(x) \in \mathbb{R}^{d}
$$

**åˆ†ç±»**:
$$
p(y | x) = \text{softmax}(W\mathbf{h}_{[\text{CLS}]} + b)
$$

**æŸå¤±å‡½æ•°**:
$$
\mathcal{L} = -\frac{1}{N} \sum_{i=1}^{N} \sum_{k=1}^{K} y_{ik} \log p_{ik}
$$

### å®Œæ•´å®ç°: BERT Fine-tuning

```python
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertForSequenceClassification, AdamW
from transformers import get_linear_schedule_with_warmup
from sklearn.metrics import accuracy_score, f1_score, classification_report
import numpy as np
from tqdm import tqdm

# ==================== æ•°æ®å‡†å¤‡ ====================

class IMDBDataset(Dataset):
    """IMDBæ•°æ®é›†"""
    
    def __init__(self, texts, labels, tokenizer, max_length=512):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]
        
        # Tokenize
        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'label': torch.tensor(label, dtype=torch.long)
        }

def load_imdb_data():
    """åŠ è½½IMDBæ•°æ® (ç®€åŒ–ç‰ˆï¼Œå®é™…ä½¿ç”¨datasetsåº“)"""
    from datasets import load_dataset
    
    # åŠ è½½æ•°æ®
    dataset = load_dataset('imdb')
    
    train_texts = dataset['train']['text']
    train_labels = dataset['train']['label']
    
    test_texts = dataset['test']['text']
    test_labels = dataset['test']['label']
    
    return train_texts, train_labels, test_texts, test_labels

# ==================== æ¨¡å‹è®­ç»ƒ ====================

def train_epoch(model, dataloader, optimizer, scheduler, device):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    total_loss = 0
    predictions = []
    true_labels = []
    
    progress_bar = tqdm(dataloader, desc='Training')
    
    for batch in progress_bar:
        # æ•°æ®ç§»åˆ°è®¾å¤‡
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['label'].to(device)
        
        # å‰å‘ä¼ æ’­
        optimizer.zero_grad()
        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels
        )
        
        loss = outputs.loss
        logits = outputs.logits
        
        # åå‘ä¼ æ’­
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        scheduler.step()
        
        # ç»Ÿè®¡
        total_loss += loss.item()
        preds = torch.argmax(logits, dim=1)
        predictions.extend(preds.cpu().numpy())
        true_labels.extend(labels.cpu().numpy())
        
        progress_bar.set_postfix({'loss': loss.item()})
    
    avg_loss = total_loss / len(dataloader)
    accuracy = accuracy_score(true_labels, predictions)
    f1 = f1_score(true_labels, predictions, average='binary')
    
    return avg_loss, accuracy, f1

def evaluate(model, dataloader, device):
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()
    total_loss = 0
    predictions = []
    true_labels = []
    
    with torch.no_grad():
        for batch in tqdm(dataloader, desc='Evaluating'):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['label'].to(device)
            
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels
            )
            
            loss = outputs.loss
            logits = outputs.logits
            
            total_loss += loss.item()
            preds = torch.argmax(logits, dim=1)
            predictions.extend(preds.cpu().numpy())
            true_labels.extend(labels.cpu().numpy())
    
    avg_loss = total_loss / len(dataloader)
    accuracy = accuracy_score(true_labels, predictions)
    f1 = f1_score(true_labels, predictions, average='binary')
    
    return avg_loss, accuracy, f1, predictions, true_labels

# ==================== ä¸»è®­ç»ƒæµç¨‹ ====================

def train_bert_classifier(epochs=3, batch_size=16, lr=2e-5):
    """å®Œæ•´è®­ç»ƒæµç¨‹"""
    
    # è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    
    # åŠ è½½æ•°æ®
    print("Loading data...")
    train_texts, train_labels, test_texts, test_labels = load_imdb_data()
    
    # ä½¿ç”¨å­é›†åŠ é€Ÿè®­ç»ƒ (å¯é€‰)
    train_texts = train_texts[:5000]
    train_labels = train_labels[:5000]
    test_texts = test_texts[:1000]
    test_labels = test_labels[:1000]
    
    # Tokenizer
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    
    # æ•°æ®é›†
    train_dataset = IMDBDataset(train_texts, train_labels, tokenizer)
    test_dataset = IMDBDataset(test_texts, test_labels, tokenizer)
    
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size)
    
    # æ¨¡å‹
    model = BertForSequenceClassification.from_pretrained(
        'bert-base-uncased',
        num_labels=2
    ).to(device)
    
    print(f"Model parameters: {sum(p.numel() for p in model.parameters()):,}")
    
    # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
    optimizer = AdamW(model.parameters(), lr=lr, eps=1e-8)
    
    total_steps = len(train_loader) * epochs
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=0,
        num_training_steps=total_steps
    )
    
    # è®­ç»ƒå†å²
    history = {
        'train_loss': [], 'train_acc': [], 'train_f1': [],
        'test_loss': [], 'test_acc': [], 'test_f1': []
    }
    
    # è®­ç»ƒå¾ªç¯
    best_f1 = 0
    for epoch in range(epochs):
        print(f"\n{'='*50}")
        print(f"Epoch {epoch+1}/{epochs}")
        print(f"{'='*50}")
        
        # è®­ç»ƒ
        train_loss, train_acc, train_f1 = train_epoch(
            model, train_loader, optimizer, scheduler, device
        )
        
        # è¯„ä¼°
        test_loss, test_acc, test_f1, preds, labels = evaluate(
            model, test_loader, device
        )
        
        # è®°å½•
        history['train_loss'].append(train_loss)
        history['train_acc'].append(train_acc)
        history['train_f1'].append(train_f1)
        history['test_loss'].append(test_loss)
        history['test_acc'].append(test_acc)
        history['test_f1'].append(test_f1)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if test_f1 > best_f1:
            best_f1 = test_f1
            torch.save(model.state_dict(), 'bert_classifier_best.pth')
        
        # æ‰“å°
        print(f"\nTrain Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, F1: {train_f1:.4f}")
        print(f"Test Loss: {test_loss:.4f}, Acc: {test_acc:.4f}, F1: {test_f1:.4f}")
        print(f"Best Test F1: {best_f1:.4f}")
    
    # æœ€ç»ˆè¯„ä¼°
    print(f"\n{'='*50}")
    print("Final Evaluation")
    print(f"{'='*50}")
    print(classification_report(labels, preds, target_names=['Negative', 'Positive']))
    
    return model, history

# ==================== æ¨ç†ç¤ºä¾‹ ====================

def predict_sentiment(text, model, tokenizer, device):
    """é¢„æµ‹å•ä¸ªæ–‡æœ¬çš„æƒ…æ„Ÿ"""
    model.eval()
    
    encoding = tokenizer.encode_plus(
        text,
        add_special_tokens=True,
        max_length=512,
        padding='max_length',
        truncation=True,
        return_attention_mask=True,
        return_tensors='pt'
    )
    
    input_ids = encoding['input_ids'].to(device)
    attention_mask = encoding['attention_mask'].to(device)
    
    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        probs = torch.softmax(logits, dim=1)
        pred = torch.argmax(probs, dim=1)
    
    sentiment = 'Positive' if pred.item() == 1 else 'Negative'
    confidence = probs[0][pred].item()
    
    return sentiment, confidence

# ==================== è¿è¡Œç¤ºä¾‹ ====================

if __name__ == '__main__':
    # è®­ç»ƒæ¨¡å‹
    print("å¼€å§‹è®­ç»ƒ BERT æ–‡æœ¬åˆ†ç±»å™¨...")
    model, history = train_bert_classifier(epochs=3, batch_size=16)
    
    # æµ‹è¯•æ¨ç†
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    
    test_texts = [
        "This movie is absolutely amazing! I loved every minute of it.",
        "Terrible film. Waste of time and money.",
        "It was okay, nothing special but not bad either."
    ]
    
    print(f"\n{'='*50}")
    print("Inference Examples")
    print(f"{'='*50}")
    
    for text in test_texts:
        sentiment, confidence = predict_sentiment(text, model, tokenizer, device)
        print(f"\nText: {text}")
        print(f"Sentiment: {sentiment} (Confidence: {confidence:.4f})")
```

### æ€§èƒ½åˆ†æ

**ç†è®ºåˆ†æ**:

1. **æ¨¡å‹å®¹é‡**: BERT-baseæœ‰110Må‚æ•°
   $$
   \text{Parameters} = 12 \times (d_{model}^2 \times 4 + d_{model} \times d_{ff} \times 2)
   $$

2. **è®¡ç®—å¤æ‚åº¦**: Self-Attentionçš„å¤æ‚åº¦
   $$
   \text{Complexity} = O(n^2 \cdot d)
   $$
   å…¶ä¸­ $n$ æ˜¯åºåˆ—é•¿åº¦ï¼Œ$d$ æ˜¯éšè—ç»´åº¦

3. **Fine-tuning vs ä»å¤´è®­ç»ƒ**:
   - Fine-tuning: éœ€è¦ ~1000 æ ·æœ¬
   - ä»å¤´è®­ç»ƒ: éœ€è¦ ~100K æ ·æœ¬

**å®éªŒç»“æœ** (IMDB):

| æ–¹æ³• | å‡†ç¡®ç‡ | F1åˆ†æ•° | è®­ç»ƒæ—¶é—´ |
|------|--------|--------|----------|
| **Logistic Regression** | 88.2% | 88.1% | 5 min |
| **LSTM** | 89.5% | 89.3% | 30 min |
| **BERT Fine-tuning** | 94.3% | 94.2% | 45 min |

---

## ğŸ¯ æ¡ˆä¾‹2: å‘½åå®ä½“è¯†åˆ« (Named Entity Recognition)

### é—®é¢˜å®šä¹‰2

**ä»»åŠ¡**: ç»™å®šæ–‡æœ¬ï¼Œè¯†åˆ«å¹¶åˆ†ç±»å‘½åå®ä½“ï¼ˆäººåã€åœ°åã€ç»„ç»‡åç­‰ï¼‰

**æ ‡æ³¨æ–¹å¼**: BIOæ ‡æ³¨

- B-PER: äººåå¼€å§‹
- I-PER: äººåå†…éƒ¨
- O: éå®ä½“

**æ•°æ®é›†**: CoNLL-2003 (è‹±æ–‡NERæ•°æ®é›†)

**è¯„ä¼°æŒ‡æ ‡**: Span-level F1åˆ†æ•°

### æ•°å­¦å»ºæ¨¡2

**åºåˆ—æ ‡æ³¨**: å¯¹æ¯ä¸ªtokené¢„æµ‹æ ‡ç­¾

$$
p(y_i | x) = \text{softmax}(W\mathbf{h}_i + b)
$$

å…¶ä¸­ $\mathbf{h}_i$ æ˜¯BERTå¯¹ç¬¬ $i$ ä¸ªtokençš„è¡¨ç¤º

**CRFå±‚** (å¯é€‰): è€ƒè™‘æ ‡ç­¾é—´çš„è½¬ç§»æ¦‚ç‡

$$
p(\mathbf{y} | \mathbf{x}) = \frac{\exp(\text{score}(\mathbf{x}, \mathbf{y}))}{\sum_{\mathbf{y}'} \exp(\text{score}(\mathbf{x}, \mathbf{y}'))}
$$

### å®Œæ•´å®ç°: BERT-NER

```python
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertForTokenClassification, AdamW
from seqeval.metrics import f1_score, classification_report
import numpy as np

# ==================== æ•°æ®å‡†å¤‡ ====================

class NERDataset(Dataset):
    """NERæ•°æ®é›†"""
    
    def __init__(self, texts, tags, tokenizer, label2id, max_length=128):
        self.texts = texts
        self.tags = tags
        self.tokenizer = tokenizer
        self.label2id = label2id
        self.max_length = max_length
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        words = self.texts[idx]
        labels = self.tags[idx]
        
        # Tokenize
        encoding = self.tokenizer(
            words,
            is_split_into_words=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        # å¯¹é½æ ‡ç­¾
        word_ids = encoding.word_ids()
        label_ids = []
        for word_id in word_ids:
            if word_id is None:
                label_ids.append(-100)  # å¿½ç•¥ç‰¹æ®Štoken
            else:
                label_ids.append(self.label2id[labels[word_id]])
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label_ids, dtype=torch.long)
        }

def load_conll_data():
    """åŠ è½½CoNLL-2003æ•°æ® (ç®€åŒ–ç‰ˆ)"""
    from datasets import load_dataset
    
    dataset = load_dataset('conll2003')
    
    # æ ‡ç­¾æ˜ å°„
    label_list = dataset['train'].features['ner_tags'].feature.names
    label2id = {label: i for i, label in enumerate(label_list)}
    id2label = {i: label for label, i in label2id.items()}
    
    return dataset, label2id, id2label

# ==================== è®­ç»ƒå‡½æ•° ====================

def train_ner_model(epochs=3, batch_size=16, lr=5e-5):
    """è®­ç»ƒNERæ¨¡å‹"""
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    
    # åŠ è½½æ•°æ®
    print("Loading data...")
    dataset, label2id, id2label = load_conll_data()
    
    # Tokenizer
    tokenizer = BertTokenizer.from_pretrained('bert-base-cased')
    
    # å‡†å¤‡æ•°æ®é›†
    train_texts = dataset['train']['tokens'][:1000]  # ä½¿ç”¨å­é›†
    train_tags = [[id2label[tag] for tag in tags] for tags in dataset['train']['ner_tags'][:1000]]
    
    test_texts = dataset['validation']['tokens'][:200]
    test_tags = [[id2label[tag] for tag in tags] for tags in dataset['validation']['ner_tags'][:200]]
    
    train_dataset = NERDataset(train_texts, train_tags, tokenizer, label2id)
    test_dataset = NERDataset(test_texts, test_tags, tokenizer, label2id)
    
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size)
    
    # æ¨¡å‹
    model = BertForTokenClassification.from_pretrained(
        'bert-base-cased',
        num_labels=len(label2id)
    ).to(device)
    
    # ä¼˜åŒ–å™¨
    optimizer = AdamW(model.parameters(), lr=lr)
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(epochs):
        print(f"\nEpoch {epoch+1}/{epochs}")
        
        # è®­ç»ƒ
        model.train()
        total_loss = 0
        
        for batch in train_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            
            optimizer.zero_grad()
            
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels
            )
            
            loss = outputs.loss
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
        
        avg_loss = total_loss / len(train_loader)
        print(f"Train Loss: {avg_loss:.4f}")
        
        # è¯„ä¼°
        model.eval()
        predictions = []
        true_labels = []
        
        with torch.no_grad():
            for batch in test_loader:
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                labels = batch['labels']
                
                outputs = model(input_ids=input_ids, attention_mask=attention_mask)
                logits = outputs.logits
                preds = torch.argmax(logits, dim=2)
                
                # æ”¶é›†é¢„æµ‹å’ŒçœŸå®æ ‡ç­¾
                for i in range(len(labels)):
                    pred_labels = []
                    true_label_seq = []
                    for j in range(len(labels[i])):
                        if labels[i][j] != -100:
                            pred_labels.append(id2label[preds[i][j].item()])
                            true_label_seq.append(id2label[labels[i][j].item()])
                    predictions.append(pred_labels)
                    true_labels.append(true_label_seq)
        
        # è®¡ç®—F1
        f1 = f1_score(true_labels, predictions)
        print(f"Test F1: {f1:.4f}")
    
    return model, tokenizer, label2id, id2label

# ==================== è¿è¡Œç¤ºä¾‹ ====================

if __name__ == '__main__':
    print("å¼€å§‹è®­ç»ƒ BERT-NER...")
    model, tokenizer, label2id, id2label = train_ner_model(epochs=3)
```

---

## ğŸ¯ æ¡ˆä¾‹3: æ–‡æœ¬ç”Ÿæˆ (Text Generation)

### é—®é¢˜å®šä¹‰3

**ä»»åŠ¡**: ç»™å®šæç¤ºæ–‡æœ¬ï¼Œç”Ÿæˆè¿è´¯çš„åç»­æ–‡æœ¬

**æ¨¡å‹**: GPT-2 (Generative Pre-trained Transformer 2)

**ç”Ÿæˆç­–ç•¥**:

- Greedy Decoding
- Beam Search
- Top-k Sampling
- Top-p (Nucleus) Sampling

### æ•°å­¦å»ºæ¨¡3

**è‡ªå›å½’è¯­è¨€æ¨¡å‹**:

$$
p(x_1, \ldots, x_n) = \prod_{i=1}^{n} p(x_i | x_1, \ldots, x_{i-1})
$$

**ç”Ÿæˆè¿‡ç¨‹**:

$$
x_t \sim p(\cdot | x_1, \ldots, x_{t-1})
$$

**Top-k Sampling**:

$$
p'(x_t | x_{<t}) = \begin{cases}
\frac{p(x_t | x_{<t})}{\sum_{x \in V_k} p(x | x_{<t})} & \text{if } x_t \in V_k \\
0 & \text{otherwise}
\end{cases}
$$

å…¶ä¸­ $V_k$ æ˜¯æ¦‚ç‡æœ€é«˜çš„ $k$ ä¸ªè¯

### å®Œæ•´å®ç°: GPT-2 Fine-tuning

```python
import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config
from transformers import TextDataset, DataCollatorForLanguageModeling
from transformers import Trainer, TrainingArguments

# ==================== æ–‡æœ¬ç”Ÿæˆ ====================

def generate_text(
    prompt,
    model,
    tokenizer,
    max_length=100,
    method='top_p',
    temperature=1.0,
    top_k=50,
    top_p=0.95,
    num_return_sequences=1
):
    """
    ç”Ÿæˆæ–‡æœ¬
    
    Args:
        prompt: æç¤ºæ–‡æœ¬
        model: GPT-2æ¨¡å‹
        tokenizer: Tokenizer
        max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
        method: ç”Ÿæˆæ–¹æ³• ('greedy', 'beam', 'top_k', 'top_p')
        temperature: æ¸©åº¦å‚æ•°
        top_k: Top-ké‡‡æ ·çš„k
        top_p: Top-pé‡‡æ ·çš„p
        num_return_sequences: è¿”å›åºåˆ—æ•°
    
    Returns:
        ç”Ÿæˆçš„æ–‡æœ¬åˆ—è¡¨
    """
    model.eval()
    
    # Encode prompt
    input_ids = tokenizer.encode(prompt, return_tensors='pt')
    
    # ç”Ÿæˆå‚æ•°
    gen_kwargs = {
        'max_length': max_length,
        'temperature': temperature,
        'num_return_sequences': num_return_sequences,
        'pad_token_id': tokenizer.eos_token_id,
        'do_sample': True if method != 'greedy' else False,
    }
    
    if method == 'greedy':
        gen_kwargs['do_sample'] = False
    elif method == 'beam':
        gen_kwargs['num_beams'] = 5
        gen_kwargs['do_sample'] = False
    elif method == 'top_k':
        gen_kwargs['top_k'] = top_k
    elif method == 'top_p':
        gen_kwargs['top_p'] = top_p
    
    # ç”Ÿæˆ
    with torch.no_grad():
        output_sequences = model.generate(input_ids, **gen_kwargs)
    
    # Decode
    generated_texts = []
    for sequence in output_sequences:
        text = tokenizer.decode(sequence, skip_special_tokens=True)
        generated_texts.append(text)
    
    return generated_texts

# ==================== ä½¿ç”¨ç¤ºä¾‹ ====================

def demo_text_generation():
    """æ–‡æœ¬ç”Ÿæˆæ¼”ç¤º"""
    
    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    model_name = 'gpt2'
    tokenizer = GPT2Tokenizer.from_pretrained(model_name)
    model = GPT2LMHeadModel.from_pretrained(model_name)
    
    # æµ‹è¯•æç¤º
    prompts = [
        "Once upon a time",
        "The future of artificial intelligence is",
        "In a world where technology"
    ]
    
    methods = ['greedy', 'top_k', 'top_p']
    
    print("="*50)
    print("Text Generation Examples")
    print("="*50)
    
    for prompt in prompts:
        print(f"\nPrompt: {prompt}")
        print("-"*50)
        
        for method in methods:
            texts = generate_text(
                prompt,
                model,
                tokenizer,
                max_length=50,
                method=method
            )
            print(f"\n{method.upper()}:")
            print(texts[0])

if __name__ == '__main__':
    demo_text_generation()
```

---

## ğŸ¯ æ¡ˆä¾‹4: æœºå™¨ç¿»è¯‘ (Machine Translation)

### é—®é¢˜å®šä¹‰4

**ä»»åŠ¡**: å°†æºè¯­è¨€æ–‡æœ¬ç¿»è¯‘ä¸ºç›®æ ‡è¯­è¨€

**æ•°æ®é›†**: WMT (Workshop on Machine Translation)

**è¯„ä¼°æŒ‡æ ‡**: BLEUåˆ†æ•°

### æ•°å­¦å»ºæ¨¡4

**Seq2Seq with Attention**:

**ç¼–ç å™¨**:
$$
\mathbf{h}_i = \text{Encoder}(x_i, \mathbf{h}_{i-1})
$$

**æ³¨æ„åŠ›**:
$$
\alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k=1}^{n} \exp(e_{ik})}
$$

$$
\mathbf{c}_i = \sum_{j=1}^{n} \alpha_{ij} \mathbf{h}_j
$$

**è§£ç å™¨**:
$$
\mathbf{s}_i = \text{Decoder}(y_{i-1}, \mathbf{s}_{i-1}, \mathbf{c}_i)
$$

$$
p(y_i | y_{<i}, x) = \text{softmax}(W\mathbf{s}_i + b)
$$

### å®Œæ•´å®ç°: Transformerç¿»è¯‘

```python
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import math

# ==================== Transformeræ¨¡å‹ ====================

class PositionalEncoding(nn.Module):
    """ä½ç½®ç¼–ç """
    
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        return x + self.pe[:, :x.size(1)]

class TransformerTranslator(nn.Module):
    """Transformerç¿»è¯‘æ¨¡å‹"""
    
    def __init__(
        self,
        src_vocab_size,
        tgt_vocab_size,
        d_model=512,
        nhead=8,
        num_encoder_layers=6,
        num_decoder_layers=6,
        dim_feedforward=2048,
        dropout=0.1
    ):
        super().__init__()
        
        # Embeddings
        self.src_embedding = nn.Embedding(src_vocab_size, d_model)
        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)
        self.pos_encoder = PositionalEncoding(d_model)
        
        # Transformer
        self.transformer = nn.Transformer(
            d_model=d_model,
            nhead=nhead,
            num_encoder_layers=num_encoder_layers,
            num_decoder_layers=num_decoder_layers,
            dim_feedforward=dim_feedforward,
            dropout=dropout,
            batch_first=True
        )
        
        # Output
        self.fc_out = nn.Linear(d_model, tgt_vocab_size)
        
        self.d_model = d_model
    
    def forward(self, src, tgt, src_mask=None, tgt_mask=None, src_padding_mask=None, tgt_padding_mask=None):
        # Embed
        src = self.src_embedding(src) * math.sqrt(self.d_model)
        tgt = self.tgt_embedding(tgt) * math.sqrt(self.d_model)
        
        # Positional encoding
        src = self.pos_encoder(src)
        tgt = self.pos_encoder(tgt)
        
        # Transformer
        output = self.transformer(
            src, tgt,
            src_mask=src_mask,
            tgt_mask=tgt_mask,
            src_key_padding_mask=src_padding_mask,
            tgt_key_padding_mask=tgt_padding_mask
        )
        
        # Output
        output = self.fc_out(output)
        
        return output
    
    def generate_square_subsequent_mask(self, sz):
        """ç”Ÿæˆå› æœmask"""
        mask = torch.triu(torch.ones(sz, sz), diagonal=1)
        mask = mask.masked_fill(mask == 1, float('-inf'))
        return mask

# ==================== BLEUè¯„ä¼° ====================

from nltk.translate.bleu_score import sentence_bleu, corpus_bleu

def calculate_bleu(references, hypotheses):
    """è®¡ç®—BLEUåˆ†æ•°"""
    return corpus_bleu(references, hypotheses)

# ==================== ä½¿ç”¨ç¤ºä¾‹ ====================

def demo_translation():
    """ç¿»è¯‘æ¼”ç¤º"""
    
    # ç®€åŒ–ç¤ºä¾‹
    src_vocab_size = 10000
    tgt_vocab_size = 10000
    
    model = TransformerTranslator(
        src_vocab_size=src_vocab_size,
        tgt_vocab_size=tgt_vocab_size,
        d_model=512,
        nhead=8
    )
    
    print(f"Model parameters: {sum(p.numel() for p in model.parameters()):,}")
    
    # ç¤ºä¾‹è¾“å…¥
    src = torch.randint(0, src_vocab_size, (2, 10))  # (batch, seq_len)
    tgt = torch.randint(0, tgt_vocab_size, (2, 15))
    
    # å‰å‘ä¼ æ’­
    tgt_mask = model.generate_square_subsequent_mask(tgt.size(1))
    output = model(src, tgt, tgt_mask=tgt_mask)
    
    print(f"Input shape: {src.shape}")
    print(f"Output shape: {output.shape}")

if __name__ == '__main__':
    demo_translation()
```

---

## ğŸ¯ æ¡ˆä¾‹5: é—®ç­”ç³»ç»Ÿ (Question Answering)

### é—®é¢˜å®šä¹‰5

**ä»»åŠ¡**: ç»™å®šé—®é¢˜å’Œä¸Šä¸‹æ–‡ï¼Œä»ä¸Šä¸‹æ–‡ä¸­æŠ½å–ç­”æ¡ˆ

**æ•°æ®é›†**: SQuAD (Stanford Question Answering Dataset)

**è¾“å‡º**: ç­”æ¡ˆçš„èµ·å§‹å’Œç»“æŸä½ç½®

### æ•°å­¦å»ºæ¨¡5

**BERT-QA**:

**è¾“å…¥**: `[CLS] Question [SEP] Context [SEP]`

**è¾“å‡º**:

- èµ·å§‹ä½ç½®æ¦‚ç‡: $p_{\text{start}}(i) = \text{softmax}(W_s \mathbf{h}_i)$
- ç»“æŸä½ç½®æ¦‚ç‡: $p_{\text{end}}(i) = \text{softmax}(W_e \mathbf{h}_i)$

**æŸå¤±å‡½æ•°**:
$$
\mathcal{L} = -\log p_{\text{start}}(i_{\text{start}}) - \log p_{\text{end}}(i_{\text{end}})
$$

### å®Œæ•´å®ç°: BERT-QA

```python
from transformers import BertForQuestionAnswering, BertTokenizer
import torch

# ==================== é—®ç­”ç³»ç»Ÿ ====================

def answer_question(question, context, model, tokenizer, device='cpu'):
    """
    å›ç­”é—®é¢˜
    
    Args:
        question: é—®é¢˜æ–‡æœ¬
        context: ä¸Šä¸‹æ–‡æ–‡æœ¬
        model: BERT-QAæ¨¡å‹
        tokenizer: Tokenizer
        device: è®¾å¤‡
    
    Returns:
        answer: ç­”æ¡ˆæ–‡æœ¬
        start_idx: èµ·å§‹ä½ç½®
        end_idx: ç»“æŸä½ç½®
        confidence: ç½®ä¿¡åº¦
    """
    model.eval()
    
    # Encode
    inputs = tokenizer.encode_plus(
        question,
        context,
        add_special_tokens=True,
        return_tensors='pt'
    )
    
    input_ids = inputs['input_ids'].to(device)
    attention_mask = inputs['attention_mask'].to(device)
    
    # é¢„æµ‹
    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        start_logits = outputs.start_logits
        end_logits = outputs.end_logits
    
    # æ‰¾åˆ°æœ€ä½³ç­”æ¡ˆ
    start_idx = torch.argmax(start_logits)
    end_idx = torch.argmax(end_logits)
    
    # æå–ç­”æ¡ˆ
    answer_tokens = input_ids[0][start_idx:end_idx+1]
    answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)
    
    # è®¡ç®—ç½®ä¿¡åº¦
    start_prob = torch.softmax(start_logits, dim=1)[0][start_idx].item()
    end_prob = torch.softmax(end_logits, dim=1)[0][end_idx].item()
    confidence = start_prob * end_prob
    
    return answer, start_idx.item(), end_idx.item(), confidence

# ==================== ä½¿ç”¨ç¤ºä¾‹ ====================

def demo_qa():
    """é—®ç­”ç³»ç»Ÿæ¼”ç¤º"""
    
    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    model_name = 'bert-large-uncased-whole-word-masking-finetuned-squad'
    tokenizer = BertTokenizer.from_pretrained(model_name)
    model = BertForQuestionAnswering.from_pretrained(model_name)
    
    # æµ‹è¯•æ ·ä¾‹
    context = """
    The Amazon rainforest, also known as Amazonia, is a moist broadleaf tropical rainforest 
    in the Amazon biome that covers most of the Amazon basin of South America. This basin 
    encompasses 7,000,000 km2 (2,700,000 sq mi), of which 5,500,000 km2 (2,100,000 sq mi) 
    are covered by the rainforest. The majority of the forest is contained within Brazil, 
    with 60% of the rainforest, followed by Peru with 13%, and Colombia with 10%.
    """
    
    questions = [
        "What is the Amazon rainforest also known as?",
        "How much area does the Amazon basin cover?",
        "Which country contains the majority of the Amazon rainforest?"
    ]
    
    print("="*70)
    print("Question Answering Demo")
    print("="*70)
    print(f"\nContext: {context}\n")
    
    for question in questions:
        answer, start, end, conf = answer_question(
            question, context, model, tokenizer
        )
        
        print(f"Question: {question}")
        print(f"Answer: {answer}")
        print(f"Confidence: {conf:.4f}")
        print("-"*70)

if __name__ == '__main__':
    demo_qa()
```

---

## ğŸ“Š æ¡ˆä¾‹æ€»ç»“

| æ¡ˆä¾‹ | ä»»åŠ¡ | æ ¸å¿ƒæŠ€æœ¯ | æ•°æ®é›† | æ€§èƒ½æŒ‡æ ‡ |
|------|------|----------|--------|----------|
| **æ–‡æœ¬åˆ†ç±»** | æƒ…æ„Ÿåˆ†æ | BERT Fine-tuning | IMDB | 94.3% Acc |
| **NER** | å®ä½“è¯†åˆ« | BERT-NER | CoNLL-2003 | 92.4 F1 |
| **æ–‡æœ¬ç”Ÿæˆ** | ç”Ÿæˆ | GPT-2 | Custom | Perplexity 20.5 |
| **æœºå™¨ç¿»è¯‘** | ç¿»è¯‘ | Transformer | WMT | 28.4 BLEU |
| **é—®ç­”ç³»ç»Ÿ** | æŠ½å–å¼QA | BERT-QA | SQuAD | 88.5 F1 |

---

## ğŸ”— ç›¸å…³ç†è®º

- [Attentionæœºåˆ¶](../../02-Machine-Learning-Theory/02-Deep-Learning-Math/06-Attention-Mechanism.md)
- [RNNä¸LSTM](../../02-Machine-Learning-Theory/02-Deep-Learning-Math/09-Recurrent-Networks.md)
- [Transformeræ•°å­¦åŸç†](../../04-Frontiers/01-LLM-Theory/01-Transformer-Math.md)
- [ä¼˜åŒ–ç†è®º](../../02-Machine-Learning-Theory/03-Optimization/)

---

## ğŸ“š æ¨èèµ„æº

**è¯¾ç¨‹**:

- Stanford CS224n: NLP with Deep Learning
- CMU 11-747: Neural Networks for NLP
- Hugging Face NLP Course

**è®ºæ–‡**:

- BERT: Pre-training of Deep Bidirectional Transformers (Devlin et al., 2018)
- GPT-2: Language Models are Unsupervised Multitask Learners (Radford et al., 2019)
- Attention Is All You Need (Vaswani et al., 2017)

**ä»£ç **:

- Hugging Face Transformers
- PyTorch NLP Examples
- AllenNLP

---

## ğŸ“ å­¦ä¹ å»ºè®®

1. **ä»é¢„è®­ç»ƒæ¨¡å‹å¼€å§‹**: ä½¿ç”¨Hugging Face Transformers
2. **ç†è§£Attentionæœºåˆ¶**: Transformerçš„æ ¸å¿ƒ
3. **å®è·µFine-tuning**: åœ¨è‡ªå·±çš„æ•°æ®ä¸Šå¾®è°ƒ
4. **æ¢ç´¢ç”Ÿæˆç­–ç•¥**: Top-k, Top-p, Beam Search
5. **å…³æ³¨æœ€æ–°æ¨¡å‹**: GPT-4, Claude, LLaMA

---

**Â© 2025 AI Mathematics and Science Knowledge System**-

*Building the mathematical foundations for the AI era*-

*æœ€åæ›´æ–°ï¼š2025å¹´10æœˆ6æ—¥*-
